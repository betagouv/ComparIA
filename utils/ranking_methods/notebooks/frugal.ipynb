{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement du jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "cache_dir = input(\"Indicate path to all Hugging Face caches:\")\n",
    "os.environ[\"HF_DATASETS_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_HUB_CACHE\"] = cache_dir\n",
    "os.environ[\"HF_TOKEN\"] = getpass(\"Enter your HuggingFace token:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the dataset since ministere-culture/comparia-reactions couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-reactions/default/0.0.0/80befa851337d9f295096cef3d100b40d220dc07 (last modified on Mon Jul 28 10:06:54 2025).\n",
      "Using the latest cached version of the dataset since ministere-culture/comparia-conversations couldn't be found on the Hugging Face Hub (offline mode is enabled).\n",
      "Found the latest cached dataset configuration 'default' at /home/jupyterhub-users/shared/projet_comparia/huggingface_hub/ministere-culture___comparia-conversations/default/0.0.0/dc40af6af1c14e68bf39d55f6e1573d2d6582f19 (last modified on Wed Jun  4 17:40:30 2025).\n"
     ]
    }
   ],
   "source": [
    "from rank_comparia.utils import load_comparia\n",
    "\n",
    "reactions = load_comparia(\"ministere-culture/comparia-reactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul des scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On calcule des scores comme dans le notebook `rankers.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.data_transformation import get_matches_with_score\n",
    "\n",
    "matches = get_matches_with_score(reactions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_a_name</th><th>model_b_name</th><th>conversation_pair_id</th><th>score_a</th><th>score_b</th></tr><tr><td>str</td><td>str</td><td>str</td><td>i64</td><td>i64</td></tr></thead><tbody><tr><td>&quot;mixtral-8x7b-instruct-v0.1&quot;</td><td>&quot;gemma-2-9b-it&quot;</td><td>&quot;622ef8352512492fa2239daa1f785f…</td><td>2</td><td>0</td></tr><tr><td>&quot;llama-3.1-70b&quot;</td><td>&quot;ministral-8b-instruct-2410&quot;</td><td>&quot;3c340eca372942b78191a9988326b3…</td><td>0</td><td>1</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>&quot;gpt-4o-2024-08-06&quot;</td><td>&quot;428999cee1f44c0ba0bba50a7b86d0…</td><td>-1</td><td>3</td></tr><tr><td>&quot;lfm-40b&quot;</td><td>&quot;ministral-8b-instruct-2410&quot;</td><td>&quot;497ad5a7b27844b3bb71c40941e63d…</td><td>-2</td><td>-2</td></tr><tr><td>&quot;mixtral-8x22b-instruct-v0.1&quot;</td><td>&quot;gpt-4o-2024-08-06&quot;</td><td>&quot;4708a0e330c341d09d7d875b3194d5…</td><td>0</td><td>1</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌──────────────────────────┬─────────────────────────┬─────────────────────────┬─────────┬─────────┐\n",
       "│ model_a_name             ┆ model_b_name            ┆ conversation_pair_id    ┆ score_a ┆ score_b │\n",
       "│ ---                      ┆ ---                     ┆ ---                     ┆ ---     ┆ ---     │\n",
       "│ str                      ┆ str                     ┆ str                     ┆ i64     ┆ i64     │\n",
       "╞══════════════════════════╪═════════════════════════╪═════════════════════════╪═════════╪═════════╡\n",
       "│ mixtral-8x7b-instruct-v0 ┆ gemma-2-9b-it           ┆ 622ef8352512492fa2239da ┆ 2       ┆ 0       │\n",
       "│ .1                       ┆                         ┆ a1f785f…                ┆         ┆         │\n",
       "│ llama-3.1-70b            ┆ ministral-8b-instruct-2 ┆ 3c340eca372942b78191a99 ┆ 0       ┆ 1       │\n",
       "│                          ┆ 410                     ┆ 88326b3…                ┆         ┆         │\n",
       "│ mixtral-8x22b-instruct-v ┆ gpt-4o-2024-08-06       ┆ 428999cee1f44c0ba0bba50 ┆ -1      ┆ 3       │\n",
       "│ 0.1                      ┆                         ┆ a7b86d0…                ┆         ┆         │\n",
       "│ lfm-40b                  ┆ ministral-8b-instruct-2 ┆ 497ad5a7b27844b3bb71c40 ┆ -2      ┆ -2      │\n",
       "│                          ┆ 410                     ┆ 941e63d…                ┆         ┆         │\n",
       "│ mixtral-8x22b-instruct-v ┆ gpt-4o-2024-08-06       ┆ 4708a0e330c341d09d7d875 ┆ 0       ┆ 1       │\n",
       "│ 0.1                      ┆                         ┆ b3194d5…                ┆         ┆         │\n",
       "└──────────────────────────┴─────────────────────────┴─────────────────────────┴─────────┴─────────┘"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.elo import ELORanker\n",
    "from rank_comparia.ranker import Match, MatchScore\n",
    "import random\n",
    "\n",
    "\n",
    "def compute_match_score(score_a: int, score_b: int) -> MatchScore:\n",
    "    final_score = score_b - score_a\n",
    "    if final_score > 0:\n",
    "        return MatchScore.B\n",
    "    elif final_score < 0:\n",
    "        return MatchScore.A\n",
    "    else:\n",
    "        return MatchScore.Draw\n",
    "\n",
    "\n",
    "def get_shuffled_results(matches: list[Match], model_names: list[str], seed: int = 0):\n",
    "    random.seed(seed)\n",
    "    ranker_shuffle = ELORanker(K=40)\n",
    "    matches_shuffle = random.sample(matches, k=len(matches))\n",
    "    ranker_shuffle.add_players(model_names)\n",
    "    ranker_shuffle.compute_scores(matches=matches_shuffle)\n",
    "    return ranker_shuffle.players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = set(matches[\"model_a_name\"].unique()) | set(matches[\"model_b_name\"].unique())\n",
    "matches = [\n",
    "    Match(\n",
    "        match_dict[\"model_a_name\"],\n",
    "        match_dict[\"model_b_name\"],\n",
    "        compute_match_score(match_dict[\"score_a\"], match_dict[\"score_b\"]),\n",
    "    )\n",
    "    for match_dict in matches.to_dicts()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gemini-2.0-flash-001': 1258.9655833200165,\n",
       " 'deepseek-v3-0324': 1177.1091683714053,\n",
       " 'deepseek-v3-chat': 1159.9438645364053,\n",
       " 'gemma-3-27b': 1158.247500469015,\n",
       " 'command-a': 1157.1991483691977,\n",
       " 'gemma-3-12b': 1141.1285927642145,\n",
       " 'claude-3-7-sonnet': 1140.5209524446682,\n",
       " 'gpt-4.1-mini': 1130.9506425045497,\n",
       " 'llama-3.1-nemotron-70b-instruct': 1091.9081381237697,\n",
       " 'gemini-2.0-flash-exp': 1076.808884411677,\n",
       " 'gemini-1.5-pro-002': 1073.939447114147,\n",
       " 'qwq-32b': 1062.3915259252617,\n",
       " 'grok-3-mini-beta': 1062.0263937587404,\n",
       " 'gemma-3-4b': 1058.852285307133,\n",
       " 'mistral-small-3.1-24b': 1041.5666417376424,\n",
       " 'deepseek-r1': 1041.0442108000136,\n",
       " 'llama-4-scout': 1031.917419216266,\n",
       " 'llama-3.1-8b': 1028.4494332999684,\n",
       " 'llama-3.1-405b': 1020.942776220401,\n",
       " 'hermes-3-llama-3.1-405b': 1011.016273848348,\n",
       " 'llama-3.1-70b': 1006.2484033416458,\n",
       " 'mistral-large-2411': 1001.1414437303283,\n",
       " 'gpt-4o-mini-2024-07-18': 995.8648045943313,\n",
       " 'mistral-saba': 993.2139637662532,\n",
       " 'gpt-4.1-nano': 986.1600880739813,\n",
       " 'phi-4': 980.0251459122336,\n",
       " 'mistral-small-24b-instruct-2501': 970.5604685616461,\n",
       " 'ministral-8b-instruct-2410': 968.3311311711643,\n",
       " 'gemini-1.5-pro-001': 967.4122859864686,\n",
       " 'gpt-4o-2024-08-06': 959.2031120549349,\n",
       " 'jamba-1.5-large': 958.2962757102937,\n",
       " 'phi-3.5-mini-instruct': 951.6661061123283,\n",
       " 'lfm-40b': 950.3422469117698,\n",
       " 'qwen2.5-coder-32b-instruct': 946.8511635988831,\n",
       " 'o4-mini': 943.3583169883915,\n",
       " 'mixtral-8x7b-instruct-v0.1': 942.0717704662195,\n",
       " 'aya-expanse-8b': 941.580344468212,\n",
       " 'llama-3.3-70b': 940.6189821234998,\n",
       " 'gemma-2-27b-it-q8': 931.5658511361904,\n",
       " 'qwen2.5-7b-instruct': 930.4756342866153,\n",
       " 'o3-mini': 923.8430135654562,\n",
       " 'deepseek-r1-distill-llama-70b': 918.1596890894061,\n",
       " 'mixtral-8x22b-instruct-v0.1': 876.8702263241242,\n",
       " 'claude-3-5-sonnet-v2': 860.0923308235131,\n",
       " 'c4ai-command-r-08-2024': 846.0793432495353,\n",
       " 'gemma-2-9b-it': 845.842519145004,\n",
       " 'mistral-nemo-2407': 771.3220455750526,\n",
       " 'chocolatine-2-14b-instruct-v2.0.3-q8': 767.8744106896739}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker = ELORanker(K=40)\n",
    "\n",
    "random.seed(1337)\n",
    "matches = random.sample(matches, k=len(matches))\n",
    "ranker.add_players(model_names)  # type: ignore\n",
    "ranker.compute_scores(matches=matches)\n",
    "ranker.get_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul d'un score de frugalité\n",
    "\n",
    "Le score de frugalité est calculé à partir de données de consommation présentes dans le jeu de données `comparia-conversations`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du nombre de match et du nombre total de tokens générés par modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>n_match</th><th>total_output_tokens</th></tr><tr><td>str</td><td>u32</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>965</td><td>961445.0</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>2571</td><td>5.944341e6</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>1098</td><td>511086.0</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>3537</td><td>2.818225e6</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>557</td><td>2.162331e6</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>786</td><td>851005.0</td></tr><tr><td>&quot;phi-4&quot;</td><td>2927</td><td>3.086417e6</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>756</td><td>870902.0</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>3222</td><td>4.787749e6</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>577</td><td>946986.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 3)\n",
       "┌─────────────────────────────────┬─────────┬─────────────────────┐\n",
       "│ model_name                      ┆ n_match ┆ total_output_tokens │\n",
       "│ ---                             ┆ ---     ┆ ---                 │\n",
       "│ str                             ┆ u32     ┆ f64                 │\n",
       "╞═════════════════════════════════╪═════════╪═════════════════════╡\n",
       "│ aya-expanse-8b                  ┆ 965     ┆ 961445.0            │\n",
       "│ c4ai-command-r-08-2024          ┆ 2571    ┆ 5.944341e6          │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 1098    ┆ 511086.0            │\n",
       "│ claude-3-5-sonnet-v2            ┆ 3537    ┆ 2.818225e6          │\n",
       "│ claude-3-7-sonnet               ┆ 557     ┆ 2.162331e6          │\n",
       "│ …                               ┆ …       ┆ …                   │\n",
       "│ phi-3.5-mini-instruct           ┆ 786     ┆ 851005.0            │\n",
       "│ phi-4                           ┆ 2927    ┆ 3.086417e6          │\n",
       "│ qwen2.5-7b-instruct             ┆ 756     ┆ 870902.0            │\n",
       "│ qwen2.5-coder-32b-instruct      ┆ 3222    ┆ 4.787749e6          │\n",
       "│ qwq-32b                         ┆ 577     ┆ 946986.0            │\n",
       "└─────────────────────────────────┴─────────┴─────────────────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "from rank_comparia.frugality import get_n_match, get_models_output_tokens\n",
    "\n",
    "reactions = reactions.rename({\"model_a_name\": \"model_a\", \"model_b_name\": \"model_b\"})\n",
    "number_by_model = get_n_match(reactions)\n",
    "total_tokens = get_models_output_tokens(reactions)\n",
    "\n",
    "number_by_model = number_by_model.join(total_tokens, on=\"model_name\")\n",
    "\n",
    "number_by_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du score de frugalité\n",
    "\n",
    "Calcul du score énergétique. Il est possible de moyenner les scores avec le paramètre `mean` (si True, le score est moyenné, sinon non).  \n",
    "Si on décide de moyenner, le moyennage par tokens et par nombre de match est effectué."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 7)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;aya-expanse-8b&quot;</td><td>961445.0</td><td>3.62259</td><td>965</td><td>961445.0</td><td>0.003754</td><td>0.000004</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>5.944341e6</td><td>44.921088</td><td>2571</td><td>5.944341e6</td><td>0.017472</td><td>0.000008</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>511086.0</td><td>1.853976</td><td>1098</td><td>511086.0</td><td>0.001689</td><td>0.000004</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>2.818225e6</td><td>378.314297</td><td>3537</td><td>2.818225e6</td><td>0.106959</td><td>0.000134</td></tr><tr><td>&quot;claude-3-7-sonnet&quot;</td><td>2.162331e6</td><td>290.26807</td><td>557</td><td>2.162331e6</td><td>0.521128</td><td>0.000134</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;phi-3.5-mini-instruct&quot;</td><td>851005.0</td><td>2.609332</td><td>786</td><td>851005.0</td><td>0.00332</td><td>0.000003</td></tr><tr><td>&quot;phi-4&quot;</td><td>3.086417e6</td><td>14.228012</td><td>2927</td><td>3.086417e6</td><td>0.004861</td><td>0.000005</td></tr><tr><td>&quot;qwen2.5-7b-instruct&quot;</td><td>870902.0</td><td>3.159217</td><td>756</td><td>870902.0</td><td>0.004179</td><td>0.000004</td></tr><tr><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>4.787749e6</td><td>34.16509</td><td>3222</td><td>4.787749e6</td><td>0.010604</td><td>0.000007</td></tr><tr><td>&quot;qwq-32b&quot;</td><td>946986.0</td><td>6.757635</td><td>577</td><td>946986.0</td><td>0.011712</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 7)\n",
       "┌──────────────┬──────────────┬──────────────┬─────────┬──────────────┬──────────────┬─────────────┐\n",
       "│ model_name   ┆ total_output ┆ conso_all_co ┆ n_match ┆ total_output ┆ mean_conso_p ┆ mean_conso_ │\n",
       "│ ---          ┆ _tokens      ┆ nv           ┆ ---     ┆ _tokens_righ ┆ er_match     ┆ per_token   │\n",
       "│ str          ┆ ---          ┆ ---          ┆ u32     ┆ t            ┆ ---          ┆ ---         │\n",
       "│              ┆ f64          ┆ f64          ┆         ┆ ---          ┆ f64          ┆ f64         │\n",
       "│              ┆              ┆              ┆         ┆ f64          ┆              ┆             │\n",
       "╞══════════════╪══════════════╪══════════════╪═════════╪══════════════╪══════════════╪═════════════╡\n",
       "│ aya-expanse- ┆ 961445.0     ┆ 3.62259      ┆ 965     ┆ 961445.0     ┆ 0.003754     ┆ 0.000004    │\n",
       "│ 8b           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ c4ai-command ┆ 5.944341e6   ┆ 44.921088    ┆ 2571    ┆ 5.944341e6   ┆ 0.017472     ┆ 0.000008    │\n",
       "│ -r-08-2024   ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ chocolatine- ┆ 511086.0     ┆ 1.853976     ┆ 1098    ┆ 511086.0     ┆ 0.001689     ┆ 0.000004    │\n",
       "│ 2-14b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct-v2.…      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-5-s ┆ 2.818225e6   ┆ 378.314297   ┆ 3537    ┆ 2.818225e6   ┆ 0.106959     ┆ 0.000134    │\n",
       "│ onnet-v2     ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ claude-3-7-s ┆ 2.162331e6   ┆ 290.26807    ┆ 557     ┆ 2.162331e6   ┆ 0.521128     ┆ 0.000134    │\n",
       "│ onnet        ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ …            ┆ …            ┆ …            ┆ …       ┆ …            ┆ …            ┆ …           │\n",
       "│ phi-3.5-mini ┆ 851005.0     ┆ 2.609332     ┆ 786     ┆ 851005.0     ┆ 0.00332      ┆ 0.000003    │\n",
       "│ -instruct    ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ phi-4        ┆ 3.086417e6   ┆ 14.228012    ┆ 2927    ┆ 3.086417e6   ┆ 0.004861     ┆ 0.000005    │\n",
       "│ qwen2.5-7b-i ┆ 870902.0     ┆ 3.159217     ┆ 756     ┆ 870902.0     ┆ 0.004179     ┆ 0.000004    │\n",
       "│ nstruct      ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwen2.5-code ┆ 4.787749e6   ┆ 34.16509     ┆ 3222    ┆ 4.787749e6   ┆ 0.010604     ┆ 0.000007    │\n",
       "│ r-32b-instru ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ ct           ┆              ┆              ┆         ┆              ┆              ┆             │\n",
       "│ qwq-32b      ┆ 946986.0     ┆ 6.757635     ┆ 577     ┆ 946986.0     ┆ 0.011712     ┆ 0.000007    │\n",
       "└──────────────┴──────────────┴──────────────┴─────────┴──────────────┴──────────────┴─────────────┘"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.frugality import calculate_frugality_score\n",
    "\n",
    "frugal_scores = calculate_frugality_score(reactions, number_by_model)\n",
    "\n",
    "frugal_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (48, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>model_name</th><th>elo_score</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;gemini-2.0-flash-001&quot;</td><td>1258.965583</td></tr><tr><td>&quot;deepseek-v3-0324&quot;</td><td>1177.109168</td></tr><tr><td>&quot;deepseek-v3-chat&quot;</td><td>1159.943865</td></tr><tr><td>&quot;gemma-3-27b&quot;</td><td>1158.2475</td></tr><tr><td>&quot;command-a&quot;</td><td>1157.199148</td></tr><tr><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>860.092331</td></tr><tr><td>&quot;c4ai-command-r-08-2024&quot;</td><td>846.079343</td></tr><tr><td>&quot;gemma-2-9b-it&quot;</td><td>845.842519</td></tr><tr><td>&quot;mistral-nemo-2407&quot;</td><td>771.322046</td></tr><tr><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>767.874411</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (48, 2)\n",
       "┌─────────────────────────────────┬─────────────┐\n",
       "│ model_name                      ┆ elo_score   │\n",
       "│ ---                             ┆ ---         │\n",
       "│ str                             ┆ f64         │\n",
       "╞═════════════════════════════════╪═════════════╡\n",
       "│ gemini-2.0-flash-001            ┆ 1258.965583 │\n",
       "│ deepseek-v3-0324                ┆ 1177.109168 │\n",
       "│ deepseek-v3-chat                ┆ 1159.943865 │\n",
       "│ gemma-3-27b                     ┆ 1158.2475   │\n",
       "│ command-a                       ┆ 1157.199148 │\n",
       "│ …                               ┆ …           │\n",
       "│ claude-3-5-sonnet-v2            ┆ 860.092331  │\n",
       "│ c4ai-command-r-08-2024          ┆ 846.079343  │\n",
       "│ gemma-2-9b-it                   ┆ 845.842519  │\n",
       "│ mistral-nemo-2407               ┆ 771.322046  │\n",
       "│ chocolatine-2-14b-instruct-v2.… ┆ 767.874411  │\n",
       "└─────────────────────────────────┴─────────────┘"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo_scores = pl.DataFrame(\n",
    "    {\n",
    "        \"model_name\": ranker.players.keys(),\n",
    "        \"elo_score\": ranker.players.values(),\n",
    "    },\n",
    "    strict=False,\n",
    ").sort(by=\"elo_score\", descending=True)\n",
    "\n",
    "elo_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du graphique de frugalité"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des informations concernant les modèles du comparateur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "info_model = pl.read_json(source=Path(\".\").resolve().parent / \"data\" / \"models_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (39, 11)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>name</th><th>model_name</th><th>organization</th><th>license</th><th>elo_score</th><th>total_output_tokens</th><th>conso_all_conv</th><th>n_match</th><th>total_output_tokens_right</th><th>mean_conso_per_match</th><th>mean_conso_per_token</th></tr><tr><td>str</td><td>str</td><td>str</td><td>str</td><td>f64</td><td>f64</td><td>f64</td><td>u32</td><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>&quot;Aya-Expanse-8B&quot;</td><td>&quot;aya-expanse-8b&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>941.580344</td><td>961445.0</td><td>3.62259</td><td>965</td><td>961445.0</td><td>0.003754</td><td>0.000004</td></tr><tr><td>&quot;Command R (08-2024)&quot;</td><td>&quot;c4ai-command-r-08-2024&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>846.079343</td><td>5.944341e6</td><td>44.921088</td><td>2571</td><td>5.944341e6</td><td>0.017472</td><td>0.000008</td></tr><tr><td>&quot;Chocolatine-2-14b Instruct&quot;</td><td>&quot;chocolatine-2-14b-instruct-v2.…</td><td>&quot;jpacifico (individual)&quot;</td><td>&quot;Apache 2.0&quot;</td><td>767.874411</td><td>511086.0</td><td>1.853976</td><td>1098</td><td>511086.0</td><td>0.001689</td><td>0.000004</td></tr><tr><td>&quot;Claude 3.5 Sonnet V2&quot;</td><td>&quot;claude-3-5-sonnet-v2&quot;</td><td>&quot;Anthropic&quot;</td><td>&quot;Proprietary&quot;</td><td>860.092331</td><td>2.818225e6</td><td>378.314297</td><td>3537</td><td>2.818225e6</td><td>0.106959</td><td>0.000134</td></tr><tr><td>&quot;Command A&quot;</td><td>&quot;command-a&quot;</td><td>&quot;Cohere&quot;</td><td>&quot;CC-BY-NC-4.0&quot;</td><td>1157.199148</td><td>1.03253e6</td><td>18.815316</td><td>760</td><td>1.03253e6</td><td>0.024757</td><td>0.000018</td></tr><tr><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td><td>&hellip;</td></tr><tr><td>&quot;Phi-3.5 Mini Instruct&quot;</td><td>&quot;phi-3.5-mini-instruct&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>951.666106</td><td>851005.0</td><td>2.609332</td><td>786</td><td>851005.0</td><td>0.00332</td><td>0.000003</td></tr><tr><td>&quot;Phi 4&quot;</td><td>&quot;phi-4&quot;</td><td>&quot;Microsoft&quot;</td><td>&quot;MIT&quot;</td><td>980.025146</td><td>3.086417e6</td><td>14.228012</td><td>2927</td><td>3.086417e6</td><td>0.004861</td><td>0.000005</td></tr><tr><td>&quot;Qwen2.5-7B&quot;</td><td>&quot;qwen2.5-7b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>930.475634</td><td>870902.0</td><td>3.159217</td><td>756</td><td>870902.0</td><td>0.004179</td><td>0.000004</td></tr><tr><td>&quot;Qwen2.5-Coder-32B-Instruct&quot;</td><td>&quot;qwen2.5-coder-32b-instruct&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>946.851164</td><td>4.787749e6</td><td>34.16509</td><td>3222</td><td>4.787749e6</td><td>0.010604</td><td>0.000007</td></tr><tr><td>&quot;QwQ 32B&quot;</td><td>&quot;qwq-32b&quot;</td><td>&quot;Alibaba&quot;</td><td>&quot;Apache 2.0&quot;</td><td>1062.391526</td><td>946986.0</td><td>6.757635</td><td>577</td><td>946986.0</td><td>0.011712</td><td>0.000007</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (39, 11)\n",
       "┌────────────┬───────────┬───────────┬───────────┬───┬─────────┬───────────┬───────────┬───────────┐\n",
       "│ name       ┆ model_nam ┆ organizat ┆ license   ┆ … ┆ n_match ┆ total_out ┆ mean_cons ┆ mean_cons │\n",
       "│ ---        ┆ e         ┆ ion       ┆ ---       ┆   ┆ ---     ┆ put_token ┆ o_per_mat ┆ o_per_tok │\n",
       "│ str        ┆ ---       ┆ ---       ┆ str       ┆   ┆ u32     ┆ s_right   ┆ ch        ┆ en        │\n",
       "│            ┆ str       ┆ str       ┆           ┆   ┆         ┆ ---       ┆ ---       ┆ ---       │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ f64       ┆ f64       ┆ f64       │\n",
       "╞════════════╪═══════════╪═══════════╪═══════════╪═══╪═════════╪═══════════╪═══════════╪═══════════╡\n",
       "│ Aya-Expans ┆ aya-expan ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 965     ┆ 961445.0  ┆ 0.003754  ┆ 0.000004  │\n",
       "│ e-8B       ┆ se-8b     ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command R  ┆ c4ai-comm ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 2571    ┆ 5.944341e ┆ 0.017472  ┆ 0.000008  │\n",
       "│ (08-2024)  ┆ and-r-08- ┆           ┆ 4.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ 2024      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Chocolatin ┆ chocolati ┆ jpacifico ┆ Apache    ┆ … ┆ 1098    ┆ 511086.0  ┆ 0.001689  ┆ 0.000004  │\n",
       "│ e-2-14b    ┆ ne-2-14b- ┆ (individu ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ instruct- ┆ al)       ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ v2.…      ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Claude 3.5 ┆ claude-3- ┆ Anthropic ┆ Proprieta ┆ … ┆ 3537    ┆ 2.818225e ┆ 0.106959  ┆ 0.000134  │\n",
       "│ Sonnet V2  ┆ 5-sonnet- ┆           ┆ ry        ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│            ┆ v2        ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Command A  ┆ command-a ┆ Cohere    ┆ CC-BY-NC- ┆ … ┆ 760     ┆ 1.03253e6 ┆ 0.024757  ┆ 0.000018  │\n",
       "│            ┆           ┆           ┆ 4.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│ …          ┆ …         ┆ …         ┆ …         ┆ … ┆ …       ┆ …         ┆ …         ┆ …         │\n",
       "│ Phi-3.5    ┆ phi-3.5-m ┆ Microsoft ┆ MIT       ┆ … ┆ 786     ┆ 851005.0  ┆ 0.00332   ┆ 0.000003  │\n",
       "│ Mini       ┆ ini-instr ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Instruct   ┆ uct       ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Phi 4      ┆ phi-4     ┆ Microsoft ┆ MIT       ┆ … ┆ 2927    ┆ 3.086417e ┆ 0.004861  ┆ 0.000005  │\n",
       "│            ┆           ┆           ┆           ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ Qwen2.5-7B ┆ qwen2.5-7 ┆ Alibaba   ┆ Apache    ┆ … ┆ 756     ┆ 870902.0  ┆ 0.004179  ┆ 0.000004  │\n",
       "│            ┆ b-instruc ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "│            ┆ t         ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ Qwen2.5-Co ┆ qwen2.5-c ┆ Alibaba   ┆ Apache    ┆ … ┆ 3222    ┆ 4.787749e ┆ 0.010604  ┆ 0.000007  │\n",
       "│ der-32B-In ┆ oder-32b- ┆           ┆ 2.0       ┆   ┆         ┆ 6         ┆           ┆           │\n",
       "│ struct     ┆ instruct  ┆           ┆           ┆   ┆         ┆           ┆           ┆           │\n",
       "│ QwQ 32B    ┆ qwq-32b   ┆ Alibaba   ┆ Apache    ┆ … ┆ 577     ┆ 946986.0  ┆ 0.011712  ┆ 0.000007  │\n",
       "│            ┆           ┆           ┆ 2.0       ┆   ┆         ┆           ┆           ┆           │\n",
       "└────────────┴───────────┴───────────┴───────────┴───┴─────────┴───────────┴───────────┴───────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = info_model.join(elo_scores, on=\"model_name\").join(frugal_scores, on=\"model_name\")\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_comparia.utils import save_data\n",
    "\n",
    "save_path = Path(\".\").resolve().parent / \"data\"\n",
    "save_data(final_df, \"all_info_for_chart_drawing\", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Génération du graphique de frugalité\n",
    "\n",
    "Les paramètres possibles :  \n",
    "- `log` : Ajuster l'échelle du graphique en linéaire (`log = False`) ou en log (`log = True`) ; \n",
    "- `mean` : Utiliser les consommation moyenné (`mean = True`) ou non (`mean = False`) ;  \n",
    "- `scale` :  choix du moyennage si `mean = True`. `token` si on utilise le moyennage par token, `match` si on utilise le moyennage par nombre de match ;  \n",
    "- `save` : Enregistrement du graphique au format html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "  #altair-viz-6ca1066e86ac43ac89adc81a6e409f9c.vega-embed {\n",
       "    width: 100%;\n",
       "    display: flex;\n",
       "  }\n",
       "\n",
       "  #altair-viz-6ca1066e86ac43ac89adc81a6e409f9c.vega-embed details,\n",
       "  #altair-viz-6ca1066e86ac43ac89adc81a6e409f9c.vega-embed details summary {\n",
       "    position: relative;\n",
       "  }\n",
       "</style>\n",
       "<div id=\"altair-viz-6ca1066e86ac43ac89adc81a6e409f9c\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-6ca1066e86ac43ac89adc81a6e409f9c\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-6ca1066e86ac43ac89adc81a6e409f9c\");\n",
       "    }\n",
       "\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm/vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm/vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm/vega-lite@5.20.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm/vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      let deps = [\"vega-embed\"];\n",
       "      require(deps, displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"5.20.1\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"name\": \"data-2c0603f1b5b2566a40b3b64b419df3a0\"}, \"mark\": {\"type\": \"point\", \"filled\": true}, \"encoding\": {\"color\": {\"field\": \"organization\", \"type\": \"nominal\"}, \"opacity\": {\"condition\": [{\"param\": \"param_2\", \"value\": 1}], \"value\": 0.3}, \"tooltip\": [{\"field\": \"model_name\", \"type\": \"nominal\"}, {\"field\": \"organization\", \"type\": \"nominal\"}, {\"field\": \"license\", \"type\": \"nominal\"}, {\"field\": \"median\", \"type\": \"quantitative\"}, {\"field\": \"mean_conso_per_token\", \"type\": \"quantitative\"}], \"x\": {\"field\": \"mean_conso_per_token\", \"scale\": {\"type\": \"log\"}, \"type\": \"quantitative\"}, \"y\": {\"field\": \"median\", \"scale\": {\"zero\": false}, \"title\": \"elo score\", \"type\": \"quantitative\"}}, \"height\": 300, \"params\": [{\"name\": \"param_2\", \"select\": {\"type\": \"point\", \"fields\": [\"organization\"]}, \"bind\": \"legend\"}, {\"name\": \"param_1\", \"select\": {\"type\": \"point\", \"fields\": [\"license\"]}, \"bind\": {\"input\": \"select\", \"options\": [\"Unknown\", \"Jamba Open Model License\", \"Gemma license\", \"CC-BY-NC-4.0\", \"Mistral Research\", \"DeepSeek\", \"Proprietary\", \"MIT\", \"Gemma\", \"Llama 3.1 Community\", \"Llama 3.3 Community\", \"MRL\", \"llama3\", \"Apache 2.0\"], \"labels\": [\"MRL\", \"llama3\", \"Apache 2.0\", \"Proprietary\", \"Mistral Research\", \"MIT\", \"Gemma\", \"Gemma license\", \"DeepSeek\", \"Llama 3.3 Community\", \"Jamba Open Model License\", \"Llama 3.1 Community\", \"Unknown\", \"CC-BY-NC-4.0\"], \"name\": \"License : \"}}], \"title\": \"consommation selon classement\", \"transform\": [{\"filter\": {\"param\": \"param_1\"}}], \"width\": 500, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.20.1.json\", \"datasets\": {\"data-2c0603f1b5b2566a40b3b64b419df3a0\": [{\"name\": \"Claude 3.5 Sonnet V2\", \"model_name\": \"claude-3-5-sonnet-v2\", \"organization\": \"Anthropic\", \"license\": \"Proprietary\", \"median\": 860.0923308235131, \"total_output_tokens\": 2818225.0, \"conso_all_conv\": 378.31429666249966, \"n_match\": 3537, \"total_output_tokens_right\": 2818225.0, \"mean_conso_per_match\": 0.10695908868037876, \"mean_conso_per_token\": 0.00013423849999999988, \"name_right\": \"Claude 3.5 Sonnet V2\", \"organization_right\": \"Anthropic\", \"license_right\": \"Proprietary\"}, {\"name\": \"GPT-4o mini\", \"model_name\": \"gpt-4o-mini-2024-07-18\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"median\": 995.8648045943313, \"total_output_tokens\": 3805215.0, \"conso_all_conv\": 28.75581949424996, \"n_match\": 3857, \"total_output_tokens_right\": 3805215.0, \"mean_conso_per_match\": 0.007455488590679273, \"mean_conso_per_token\": 7.5569499999999895e-06, \"name_right\": \"GPT-4o mini\", \"organization_right\": \"OpenAI\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-exp\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1076.808884411677, \"total_output_tokens\": 3076613.0, \"conso_all_conv\": 25.408618675566665, \"n_match\": 1740, \"total_output_tokens_right\": 3076613.0, \"mean_conso_per_match\": 0.01460265441124521, \"mean_conso_per_token\": 8.258633333333333e-06, \"name_right\": \"Gemini 2.0 Flash\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Llama 3.1 405B\", \"model_name\": \"llama-3.1-405b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 1020.942776220401, \"total_output_tokens\": 4276508.0, \"conso_all_conv\": 1017.4927275085383, \"n_match\": 3793, \"total_output_tokens_right\": 4276508.0, \"mean_conso_per_match\": 0.2682553987631264, \"mean_conso_per_token\": 0.00023792606666666783, \"name_right\": \"Llama 3.1 405B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Mistral-Large-2411\", \"model_name\": \"mistral-large-2411\", \"organization\": \"Mistral\", \"license\": \"Mistral Research\", \"median\": 1001.1414437303283, \"total_output_tokens\": 9064145.0, \"conso_all_conv\": 180.43609736028264, \"n_match\": 3300, \"total_output_tokens_right\": 9064145.0, \"mean_conso_per_match\": 0.05467760526069171, \"mean_conso_per_token\": 1.990657666666659e-05, \"name_right\": \"Mistral-Large-2411\", \"organization_right\": \"Mistral\", \"license_right\": \"Mistral Research\"}, {\"name\": \"Llama-3.1-70B\", \"model_name\": \"llama-3.1-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 1006.2484033416458, \"total_output_tokens\": 2450777.0, \"conso_all_conv\": 30.55808487246673, \"n_match\": 2794, \"total_output_tokens_right\": 2450777.0, \"mean_conso_per_match\": 0.010937038250703912, \"mean_conso_per_token\": 1.2468733333333359e-05, \"name_right\": \"Llama-3.1-70B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Llama 3.3 70B\", \"model_name\": \"llama-3.3-70b\", \"organization\": \"Meta\", \"license\": \"Llama 3.3 Community\", \"median\": 940.6189821234998, \"total_output_tokens\": 5793973.0, \"conso_all_conv\": 72.24350427753315, \"n_match\": 2161, \"total_output_tokens_right\": 5793973.0, \"mean_conso_per_match\": 0.03343058967030687, \"mean_conso_per_token\": 1.2468733333333301e-05, \"name_right\": \"Llama 3.3 70B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.3 Community\"}, {\"name\": \"Qwen2.5-Coder-32B-Instruct\", \"model_name\": \"qwen2.5-coder-32b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 946.8511635988831, \"total_output_tokens\": 4787749.0, \"conso_all_conv\": 34.16508959906006, \"n_match\": 3222, \"total_output_tokens_right\": 4787749.0, \"mean_conso_per_match\": 0.010603690130062092, \"mean_conso_per_token\": 7.135940000000013e-06, \"name_right\": \"Qwen2.5-Coder-32B-Instruct\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Gemini 1.5 pro 001\", \"model_name\": \"gemini-1.5-pro-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 967.4122859864686, \"total_output_tokens\": 612722.0, \"conso_all_conv\": 82.15188674586669, \"n_match\": 692, \"total_output_tokens_right\": 612722.0, \"mean_conso_per_match\": 0.11871659934373799, \"mean_conso_per_token\": 0.00013407693333333337, \"name_right\": \"Gemini 1.5 pro 001\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemini 1.5 pro 002\", \"model_name\": \"gemini-1.5-pro-002\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1073.939447114147, \"total_output_tokens\": 3284077.0, \"conso_all_conv\": 440.3189729905331, \"n_match\": 3021, \"total_output_tokens_right\": 3284077.0, \"mean_conso_per_match\": 0.14575272194324168, \"mean_conso_per_token\": 0.00013407693333333326, \"name_right\": \"Gemini 1.5 pro 002\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Aya-Expanse-8B\", \"model_name\": \"aya-expanse-8b\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 941.580344468212, \"total_output_tokens\": 961445.0, \"conso_all_conv\": 3.6225901576999946, \"n_match\": 965, \"total_output_tokens_right\": 961445.0, \"mean_conso_per_match\": 0.0037539794380310825, \"mean_conso_per_token\": 3.767859999999994e-06, \"name_right\": \"Aya-Expanse-8B\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Command R (08-2024)\", \"model_name\": \"c4ai-command-r-08-2024\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 846.0793432495353, \"total_output_tokens\": 5944341.0, \"conso_all_conv\": 44.92108771994969, \"n_match\": 2571, \"total_output_tokens_right\": 5944341.0, \"mean_conso_per_match\": 0.01747222392841295, \"mean_conso_per_token\": 7.556949999999948e-06, \"name_right\": \"Command R (08-2024)\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Ministral 8B-Instruct\", \"model_name\": \"ministral-8b-instruct-2410\", \"organization\": \"Mistral\", \"license\": \"MRL\", \"median\": 968.3311311711643, \"total_output_tokens\": 3227726.0, \"conso_all_conv\": 12.161619686360076, \"n_match\": 3388, \"total_output_tokens_right\": 3227726.0, \"mean_conso_per_match\": 0.0035896162002243435, \"mean_conso_per_token\": 3.767860000000024e-06, \"name_right\": \"Ministral 8B-Instruct\", \"organization_right\": \"Mistral\", \"license_right\": \"MRL\"}, {\"name\": \"Llama 3.1 8B\", \"model_name\": \"llama-3.1-8b\", \"organization\": \"Meta\", \"license\": \"Llama 3.1 Community\", \"median\": 1028.4494332999684, \"total_output_tokens\": 2993533.0, \"conso_all_conv\": 11.279213249379985, \"n_match\": 3163, \"total_output_tokens_right\": 2993533.0, \"mean_conso_per_match\": 0.003565985851843182, \"mean_conso_per_token\": 3.767859999999995e-06, \"name_right\": \"Llama 3.1 8B\", \"organization_right\": \"Meta\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"DeepSeek-V3 Chat\", \"model_name\": \"deepseek-v3-chat\", \"organization\": \"DeepSeek\", \"license\": \"DeepSeek\", \"median\": 1159.9438645364053, \"total_output_tokens\": 9475940.0, \"conso_all_conv\": 445.6130906956012, \"n_match\": 3884, \"total_output_tokens_right\": 9475940.0, \"mean_conso_per_match\": 0.11473045589485098, \"mean_conso_per_token\": 4.702574000000013e-05, \"name_right\": \"DeepSeek-V3 Chat\", \"organization_right\": \"DeepSeek\", \"license_right\": \"DeepSeek\"}, {\"name\": \"DeepSeek-R1\", \"model_name\": \"deepseek-r1\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"median\": 1041.0442108000136, \"total_output_tokens\": 1160060.0, \"conso_all_conv\": 54.55267994439995, \"n_match\": 808, \"total_output_tokens_right\": 1160060.0, \"mean_conso_per_match\": 0.06751569300049498, \"mean_conso_per_token\": 4.7025739999999955e-05, \"name_right\": \"DeepSeek-R1\", \"organization_right\": \"DeepSeek\", \"license_right\": \"MIT\"}, {\"name\": \"DeepSeek-R1 distiil\", \"model_name\": \"deepseek-r1-distill-llama-70b\", \"organization\": \"DeepSeek\", \"license\": \"MIT\", \"median\": 918.1596890894061, \"total_output_tokens\": 741901.0, \"conso_all_conv\": 9.250565728733326, \"n_match\": 766, \"total_output_tokens_right\": 741901.0, \"mean_conso_per_match\": 0.012076456564926013, \"mean_conso_per_token\": 1.2468733333333323e-05, \"name_right\": \"DeepSeek-R1 distiil\", \"organization_right\": \"DeepSeek\", \"license_right\": \"MIT\"}, {\"name\": \"Gemini 2.0 Flash\", \"model_name\": \"gemini-2.0-flash-001\", \"organization\": \"Google\", \"license\": \"Proprietary\", \"median\": 1258.9655833200165, \"total_output_tokens\": 2300660.0, \"conso_all_conv\": 19.000307364666675, \"n_match\": 1398, \"total_output_tokens_right\": 2300660.0, \"mean_conso_per_match\": 0.01359106392322366, \"mean_conso_per_token\": 8.258633333333338e-06, \"name_right\": \"Gemini 2.0 Flash\", \"organization_right\": \"Google\", \"license_right\": \"Proprietary\"}, {\"name\": \"Gemma 2 9B\", \"model_name\": \"gemma-2-9b-it\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 845.842519145004, \"total_output_tokens\": 2016207.0, \"conso_all_conv\": 7.879733476710017, \"n_match\": 2657, \"total_output_tokens_right\": 2016207.0, \"mean_conso_per_match\": 0.002965650536962746, \"mean_conso_per_token\": 3.908196666666675e-06, \"name_right\": \"Gemma 2 9B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 3 12B\", \"model_name\": \"gemma-3-12b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 1141.1285927642145, \"total_output_tokens\": 1088635.0, \"conso_all_conv\": 4.712925899566667, \"n_match\": 773, \"total_output_tokens_right\": 1088635.0, \"mean_conso_per_match\": 0.006096928718714963, \"mean_conso_per_token\": 4.329206666666667e-06, \"name_right\": \"Gemma 3 12B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 3 4B\", \"model_name\": \"gemma-3-4b\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 1058.852285307133, \"total_output_tokens\": 1147829.0, \"conso_all_conv\": 3.6805289928866673, \"n_match\": 783, \"total_output_tokens_right\": 1147829.0, \"mean_conso_per_match\": 0.00470054788363559, \"mean_conso_per_token\": 3.206513333333334e-06, \"name_right\": \"Gemma 3 4B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"Gemma 2 27B q8\", \"model_name\": \"gemma-2-27b-it-q8\", \"organization\": \"Google\", \"license\": \"Gemma license\", \"median\": 931.5658511361904, \"total_output_tokens\": 242100.0, \"conso_all_conv\": 1.5577335389999991, \"n_match\": 296, \"total_output_tokens_right\": 242100.0, \"mean_conso_per_match\": 0.0052626133074324295, \"mean_conso_per_token\": 6.434256666666663e-06, \"name_right\": \"Gemma 2 27B q8\", \"organization_right\": \"Google\", \"license_right\": \"Gemma license\"}, {\"name\": \"o3-mini\", \"model_name\": \"o3-mini\", \"organization\": \"OpenAI\", \"license\": \"Proprietary\", \"median\": 923.8430135654562, \"total_output_tokens\": 928508.0, \"conso_all_conv\": 57.0336039000001, \"n_match\": 743, \"total_output_tokens_right\": 928508.0, \"mean_conso_per_match\": 0.07676124347240929, \"mean_conso_per_token\": 6.142500000000011e-05, \"name_right\": \"o3-mini\", \"organization_right\": \"OpenAI\", \"license_right\": \"Proprietary\"}, {\"name\": \"Mistral-Small-24B-Instruct-2501\", \"model_name\": \"mistral-small-24b-instruct-2501\", \"organization\": \"Mistral\", \"license\": \"Apache 2.0\", \"median\": 970.5604685616461, \"total_output_tokens\": 2440992.0, \"conso_all_conv\": 14.678287007359955, \"n_match\": 2025, \"total_output_tokens_right\": 2440992.0, \"mean_conso_per_match\": 0.007248536793758003, \"mean_conso_per_token\": 6.013246666666649e-06, \"name_right\": \"Mistral-Small-24B-Instruct-2501\", \"organization_right\": \"Mistral\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Gemma-3 27B\", \"model_name\": \"gemma-3-27b\", \"organization\": \"Google\", \"license\": \"Gemma\", \"median\": 1158.247500469015, \"total_output_tokens\": 1405974.0, \"conso_all_conv\": 9.046397582660008, \"n_match\": 966, \"total_output_tokens_right\": 1405974.0, \"mean_conso_per_match\": 0.009364800810207047, \"mean_conso_per_token\": 6.434256666666672e-06, \"name_right\": \"Gemma-3 27B\", \"organization_right\": \"Google\", \"license_right\": \"Gemma\"}, {\"name\": \"Qwen2.5-7B\", \"model_name\": \"qwen2.5-7b-instruct\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 930.4756342866153, \"total_output_tokens\": 870902.0, \"conso_all_conv\": 3.159217326046669, \"n_match\": 756, \"total_output_tokens_right\": 870902.0, \"mean_conso_per_match\": 0.0041788588968871285, \"mean_conso_per_token\": 3.627523333333336e-06, \"name_right\": \"Qwen2.5-7B\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Chocolatine-2-14b Instruct\", \"model_name\": \"chocolatine-2-14b-instruct-v2.0.3-q8\", \"organization\": \"jpacifico (individual)\", \"license\": \"Apache 2.0\", \"median\": 767.8744106896739, \"total_output_tokens\": 511086.0, \"conso_all_conv\": 1.853976390340002, \"n_match\": 1098, \"total_output_tokens_right\": 511086.0, \"mean_conso_per_match\": 0.0016885030877413496, \"mean_conso_per_token\": 3.627523333333337e-06, \"name_right\": \"Chocolatine-2-14b Instruct\", \"organization_right\": \"jpacifico (individual)\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Command A\", \"model_name\": \"command-a\", \"organization\": \"Cohere\", \"license\": \"CC-BY-NC-4.0\", \"median\": 1157.1991483691977, \"total_output_tokens\": 1032530.0, \"conso_all_conv\": 18.815315784433313, \"n_match\": 760, \"total_output_tokens_right\": 1032530.0, \"mean_conso_per_match\": 0.024756994453201727, \"mean_conso_per_token\": 1.8222536666666648e-05, \"name_right\": \"Command A\", \"organization_right\": \"Cohere\", \"license_right\": \"CC-BY-NC-4.0\"}, {\"name\": \"Hermes 3\", \"model_name\": \"hermes-3-llama-3.1-405b\", \"organization\": \"Nous Research\", \"license\": \"llama3\", \"median\": 1011.016273848348, \"total_output_tokens\": 4182418.0, \"conso_all_conv\": 995.1062638958654, \"n_match\": 2756, \"total_output_tokens_right\": 4182418.0, \"mean_conso_per_match\": 0.36106903624668557, \"mean_conso_per_token\": 0.00023792606666666637, \"name_right\": \"Hermes 3\", \"organization_right\": \"Nous Research\", \"license_right\": \"llama3\"}, {\"name\": \"Jamba Large\", \"model_name\": \"jamba-1.5-large\", \"organization\": \"AI21 Labs\", \"license\": \"Jamba Open Model License\", \"median\": 958.2962757102937, \"total_output_tokens\": 581721.0, \"conso_all_conv\": 27.637820667239993, \"n_match\": 237, \"total_output_tokens_right\": 581721.0, \"mean_conso_per_match\": 0.116615277076962, \"mean_conso_per_token\": 4.751043999999999e-05, \"name_right\": \"Jamba Large\", \"organization_right\": \"AI21 Labs\", \"license_right\": \"Jamba Open Model License\"}, {\"name\": \"Liquid Foundation Model\", \"model_name\": \"lfm-40b\", \"organization\": \"Liquid\", \"license\": \"Unknown\", \"median\": 950.3422469117698, \"total_output_tokens\": 1933376.0, \"conso_all_conv\": 15.96704347946668, \"n_match\": 2097, \"total_output_tokens_right\": 1933376.0, \"mean_conso_per_match\": 0.007614231511429032, \"mean_conso_per_token\": 8.258633333333341e-06, \"name_right\": \"Liquid Foundation Model\", \"organization_right\": \"Liquid\", \"license_right\": \"Unknown\"}, {\"name\": \"Nemotron 70B Instruct\", \"model_name\": \"llama-3.1-nemotron-70b-instruct\", \"organization\": \"Nvidia\", \"license\": \"Llama 3.1 Community\", \"median\": 1091.9081381237697, \"total_output_tokens\": 3957200.0, \"conso_all_conv\": 49.341271546666775, \"n_match\": 2346, \"total_output_tokens_right\": 3957200.0, \"mean_conso_per_match\": 0.021032085058255233, \"mean_conso_per_token\": 1.246873333333336e-05, \"name_right\": \"Nemotron 70B Instruct\", \"organization_right\": \"Nvidia\", \"license_right\": \"Llama 3.1 Community\"}, {\"name\": \"Mistral Nemo Instruct\", \"model_name\": \"mistral-nemo-2407\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 771.3220455750526, \"total_output_tokens\": 2101470.0, \"conso_all_conv\": 9.09769793379999, \"n_match\": 3225, \"total_output_tokens_right\": 2101470.0, \"mean_conso_per_match\": 0.002820991607379842, \"mean_conso_per_token\": 4.329206666666662e-06, \"name_right\": \"Mistral Nemo Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mistral Small-3.1 24B\", \"model_name\": \"mistral-small-3.1-24b\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 1041.5666417376424, \"total_output_tokens\": 841162.0, \"conso_all_conv\": 5.058114592626664, \"n_match\": 824, \"total_output_tokens_right\": 841162.0, \"mean_conso_per_match\": 0.006138488583284786, \"mean_conso_per_token\": 6.013246666666663e-06, \"name_right\": \"Mistral Small-3.1 24B\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mixtral 8x22B Instruct\", \"model_name\": \"mixtral-8x22b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 876.8702263241242, \"total_output_tokens\": 2889185.0, \"conso_all_conv\": 50.96510783260014, \"n_match\": 3160, \"total_output_tokens_right\": 2889185.0, \"mean_conso_per_match\": 0.016128198681202577, \"mean_conso_per_token\": 1.763996000000005e-05, \"name_right\": \"Mixtral 8x22B Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Mixtral 8x7B Instruct\", \"model_name\": \"mixtral-8x7b-instruct-v0.1\", \"organization\": \"Mistral AI\", \"license\": \"Apache 2.0\", \"median\": 942.0717704662195, \"total_output_tokens\": 900068.0, \"conso_all_conv\": 4.14920547184, \"n_match\": 1151, \"total_output_tokens_right\": 900068.0, \"mean_conso_per_match\": 0.003604870088479583, \"mean_conso_per_token\": 4.60988e-06, \"name_right\": \"Mixtral 8x7B Instruct\", \"organization_right\": \"Mistral AI\", \"license_right\": \"Apache 2.0\"}, {\"name\": \"Phi-3.5 Mini Instruct\", \"model_name\": \"phi-3.5-mini-instruct\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"median\": 951.6661061123283, \"total_output_tokens\": 851005.0, \"conso_all_conv\": 2.609331674216668, \"n_match\": 786, \"total_output_tokens_right\": 851005.0, \"mean_conso_per_match\": 0.003319760399766753, \"mean_conso_per_token\": 3.066176666666668e-06, \"name_right\": \"Phi-3.5 Mini Instruct\", \"organization_right\": \"Microsoft\", \"license_right\": \"MIT\"}, {\"name\": \"Phi 4\", \"model_name\": \"phi-4\", \"organization\": \"Microsoft\", \"license\": \"MIT\", \"median\": 980.0251459122336, \"total_output_tokens\": 3086417.0, \"conso_all_conv\": 14.228011999959978, \"n_match\": 2927, \"total_output_tokens_right\": 3086417.0, \"mean_conso_per_match\": 0.004860953877676794, \"mean_conso_per_token\": 4.609879999999993e-06, \"name_right\": \"Phi 4\", \"organization_right\": \"Microsoft\", \"license_right\": \"MIT\"}, {\"name\": \"QwQ 32B\", \"model_name\": \"qwq-32b\", \"organization\": \"Alibaba\", \"license\": \"Apache 2.0\", \"median\": 1062.3915259252617, \"total_output_tokens\": 946986.0, \"conso_all_conv\": 6.757635276840006, \"n_match\": 577, \"total_output_tokens_right\": 946986.0, \"mean_conso_per_match\": 0.011711672923466214, \"mean_conso_per_token\": 7.135940000000006e-06, \"name_right\": \"QwQ 32B\", \"organization_right\": \"Alibaba\", \"license_right\": \"Apache 2.0\"}]}}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from rank_comparia.plot import draw_frugality_chart\n",
    "\n",
    "final_df = final_df.rename({\"elo_score\": \"median\"})\n",
    "draw_frugality_chart(final_df, title=\"consommation selon classement\", log=True, scale=\"token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
