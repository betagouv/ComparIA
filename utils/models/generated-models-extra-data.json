{
  "timestamp": 1769077945.572353,
  "models": [
    {
      "rank": 1,
      "model_name": "gemini-3-flash-preview",
      "median": 1172.6019700754073,
      "p2.5": 1134.073521351721,
      "p97.5": 1211.1304187990936,
      "rank_p2.5": 1,
      "rank_p97.5": 6,
      "n_match": 467,
      "useful": 66,
      "creative": 18,
      "complete": 71,
      "clear_formatting": 54,
      "incorrect": 18,
      "superficial": 16,
      "instructions_not_followed": 6,
      "total_prefs": 249,
      "positive_prefs_ratio": 0.8393574297188755,
      "win_rate": 0.6685714285714286,
      "mean_win_prob": 0.5410271662742042,
      "style_controlled": {
        "median": 1107.9980604443867,
        "p2.5": 1072.7535381302216,
        "p97.5": 1147.404412772399,
        "rank": 6,
        "rank_p2.5": 2,
        "rank_p97.5": 17
      }
    },
    {
      "rank": 2,
      "model_name": "mistral-large-2512",
      "median": 1146.0242179161387,
      "p2.5": 1129.0876989018345,
      "p97.5": 1162.9607369304426,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 2226,
      "useful": 271,
      "creative": 75,
      "complete": 303,
      "clear_formatting": 263,
      "incorrect": 51,
      "superficial": 63,
      "instructions_not_followed": 31,
      "total_prefs": 1057,
      "positive_prefs_ratio": 0.8628192999053926,
      "win_rate": 0.6138841078600115,
      "mean_win_prob": 0.5352734429065781,
      "style_controlled": {
        "median": 1047.0366082720402,
        "p2.5": 1028.9192825904192,
        "p97.5": 1062.5363638746069,
        "rank": 24,
        "rank_p2.5": 13,
        "rank_p97.5": 41
      }
    },
    {
      "rank": 3,
      "model_name": "mistral-medium-2508",
      "median": 1140.058558950714,
      "p2.5": 1129.957492000475,
      "p97.5": 1150.1596259009532,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 7017,
      "useful": 272,
      "creative": 77,
      "complete": 235,
      "clear_formatting": 240,
      "incorrect": 47,
      "superficial": 69,
      "instructions_not_followed": 35,
      "total_prefs": 975,
      "positive_prefs_ratio": 0.8451282051282051,
      "win_rate": 0.6390296886314265,
      "mean_win_prob": 0.5339622936727353,
      "style_controlled": {
        "median": 1062.4588470086476,
        "p2.5": 1051.7441083043475,
        "p97.5": 1072.8505220873094,
        "rank": 17,
        "rank_p2.5": 8,
        "rank_p97.5": 30
      }
    },
    {
      "rank": 4,
      "model_name": "gemini-2.5-flash",
      "median": 1136.4101741912273,
      "p2.5": 1124.7494277264445,
      "p97.5": 1148.0709206560098,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 5081,
      "useful": 263,
      "creative": 86,
      "complete": 253,
      "clear_formatting": 230,
      "incorrect": 42,
      "superficial": 63,
      "instructions_not_followed": 22,
      "total_prefs": 959,
      "positive_prefs_ratio": 0.867570385818561,
      "win_rate": 0.6410321274981027,
      "mean_win_prob": 0.5331568282451215,
      "style_controlled": {
        "median": 1077.6471911798865,
        "p2.5": 1066.7351883369713,
        "p97.5": 1089.8083550969473,
        "rank": 13,
        "rank_p2.5": 5,
        "rank_p97.5": 18
      }
    },
    {
      "rank": 5,
      "model_name": "gemini-3-pro-preview",
      "median": 1135.5129262545267,
      "p2.5": 1119.8228124048162,
      "p97.5": 1151.2030401042373,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 2741,
      "useful": 267,
      "creative": 77,
      "complete": 243,
      "clear_formatting": 194,
      "incorrect": 41,
      "superficial": 62,
      "instructions_not_followed": 26,
      "total_prefs": 910,
      "positive_prefs_ratio": 0.8582417582417582,
      "win_rate": 0.6055739253660841,
      "mean_win_prob": 0.5329583177708129,
      "style_controlled": {
        "median": 1225.3463362770926,
        "p2.5": 1210.753264202568,
        "p97.5": 1243.6482039509683,
        "rank": 1,
        "rank_p2.5": 1,
        "rank_p97.5": 1
      }
    },
    {
      "rank": 6,
      "model_name": "qwen3-max-2025-09-23",
      "median": 1127.0480011210516,
      "p2.5": 1112.719686998854,
      "p97.5": 1141.3763152432489,
      "rank_p2.5": 1,
      "rank_p97.5": 10,
      "n_match": 3290,
      "useful": 151,
      "creative": 40,
      "complete": 107,
      "clear_formatting": 141,
      "incorrect": 32,
      "superficial": 34,
      "instructions_not_followed": 11,
      "total_prefs": 516,
      "positive_prefs_ratio": 0.8507751937984496,
      "win_rate": 0.635001970831691,
      "mean_win_prob": 0.5310772519021328,
      "style_controlled": {
        "median": 1074.543762673197,
        "p2.5": 1060.8008611700823,
        "p97.5": 1089.709247646542,
        "rank": 14,
        "rank_p2.5": 6,
        "rank_p97.5": 25
      }
    },
    {
      "rank": 7,
      "model_name": "magistral-medium",
      "median": 1117.1718431209824,
      "p2.5": 1101.380677309763,
      "p97.5": 1132.9630089322015,
      "rank_p2.5": 2,
      "rank_p97.5": 12,
      "n_match": 2668,
      "useful": 17,
      "creative": 4,
      "complete": 10,
      "clear_formatting": 12,
      "incorrect": 6,
      "superficial": 7,
      "instructions_not_followed": 0,
      "total_prefs": 56,
      "positive_prefs_ratio": 0.7678571428571429,
      "win_rate": 0.6241446725317693,
      "mean_win_prob": 0.5288635449196711,
      "style_controlled": {
        "median": 1070.7304237962035,
        "p2.5": 1053.3544846772397,
        "p97.5": 1088.0931357233715,
        "rank": 15,
        "rank_p2.5": 7,
        "rank_p97.5": 29
      }
    },
    {
      "rank": 8,
      "model_name": "gemini-2.0-flash",
      "median": 1109.5012538000715,
      "p2.5": 1101.0418004041153,
      "p97.5": 1117.9607071960274,
      "rank_p2.5": 6,
      "rank_p97.5": 13,
      "n_match": 10633,
      "useful": 1479,
      "creative": 473,
      "complete": 1551,
      "clear_formatting": 1332,
      "incorrect": 208,
      "superficial": 304,
      "instructions_not_followed": 109,
      "total_prefs": 5456,
      "positive_prefs_ratio": 0.8861803519061584,
      "win_rate": 0.6905578417924096,
      "mean_win_prob": 0.5271298872027821,
      "style_controlled": {
        "median": 1078.3068615041693,
        "p2.5": 1068.9737370125645,
        "p97.5": 1087.3628119375046,
        "rank": 12,
        "rank_p2.5": 7,
        "rank_p97.5": 17
      }
    },
    {
      "rank": 9,
      "model_name": "gpt-5.2",
      "median": 1103.5578227791043,
      "p2.5": 1083.2264968485354,
      "p97.5": 1123.8891487096732,
      "rank_p2.5": 5,
      "rank_p97.5": 24,
      "n_match": 1614,
      "useful": 223,
      "creative": 46,
      "complete": 165,
      "clear_formatting": 167,
      "incorrect": 27,
      "superficial": 77,
      "instructions_not_followed": 30,
      "total_prefs": 735,
      "positive_prefs_ratio": 0.817687074829932,
      "win_rate": 0.5357142857142857,
      "mean_win_prob": 0.525777884745397,
      "style_controlled": {
        "median": 1111.3960654316536,
        "p2.5": 1089.7409866266073,
        "p97.5": 1137.384559468222,
        "rank": 5,
        "rank_p2.5": 2,
        "rank_p97.5": 12
      }
    },
    {
      "rank": 10,
      "model_name": "deepseek-v3-0324",
      "median": 1103.256046559025,
      "p2.5": 1092.0468834406702,
      "p97.5": 1114.46520967738,
      "rank_p2.5": 6,
      "rank_p97.5": 19,
      "n_match": 5390,
      "useful": 852,
      "creative": 256,
      "complete": 696,
      "clear_formatting": 779,
      "incorrect": 158,
      "superficial": 226,
      "instructions_not_followed": 77,
      "total_prefs": 3044,
      "positive_prefs_ratio": 0.8485545335085414,
      "win_rate": 0.6450783259293897,
      "mean_win_prob": 0.5257090330316664,
      "style_controlled": {
        "median": 1063.2380735986962,
        "p2.5": 1051.7648482094146,
        "p97.5": 1073.9279129558574,
        "rank": 16,
        "rank_p2.5": 8,
        "rank_p97.5": 30
      }
    },
    {
      "rank": 11,
      "model_name": "claude-4-5-sonnet",
      "median": 1100.6149337220158,
      "p2.5": 1091.3661230717219,
      "p97.5": 1109.86374437231,
      "rank_p2.5": 7,
      "rank_p97.5": 19,
      "n_match": 7762,
      "useful": 294,
      "creative": 70,
      "complete": 167,
      "clear_formatting": 212,
      "incorrect": 52,
      "superficial": 112,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.7921108742004265,
      "win_rate": 0.5615625,
      "mean_win_prob": 0.5251056048644586,
      "style_controlled": {
        "median": 1050.6134162699116,
        "p2.5": 1040.9863338407504,
        "p97.5": 1059.1282125883208,
        "rank": 21,
        "rank_p2.5": 14,
        "rank_p97.5": 36
      }
    },
    {
      "rank": 12,
      "model_name": "gemma-3-27b",
      "median": 1090.6802257467798,
      "p2.5": 1082.3113053310349,
      "p97.5": 1099.049146162525,
      "rank_p2.5": 9,
      "rank_p97.5": 25,
      "n_match": 9542,
      "useful": 1013,
      "creative": 337,
      "complete": 1018,
      "clear_formatting": 849,
      "incorrect": 210,
      "superficial": 199,
      "instructions_not_followed": 109,
      "total_prefs": 3735,
      "positive_prefs_ratio": 0.8613119143239625,
      "win_rate": 0.6142342101749232,
      "mean_win_prob": 0.5228220994013246,
      "style_controlled": {
        "median": 1051.6497968157628,
        "p2.5": 1042.3638669371949,
        "p97.5": 1062.5951049971177,
        "rank": 20,
        "rank_p2.5": 12,
        "rank_p97.5": 34
      }
    },
    {
      "rank": 13,
      "model_name": "deepseek-chat-v3.1",
      "median": 1089.3080122057838,
      "p2.5": 1071.9985668418901,
      "p97.5": 1106.6174575696773,
      "rank_p2.5": 7,
      "rank_p97.5": 27,
      "n_match": 2188,
      "useful": 123,
      "creative": 30,
      "complete": 86,
      "clear_formatting": 92,
      "incorrect": 27,
      "superficial": 32,
      "instructions_not_followed": 17,
      "total_prefs": 407,
      "positive_prefs_ratio": 0.8132678132678133,
      "win_rate": 0.5671732522796352,
      "mean_win_prob": 0.52250498492276,
      "style_controlled": {
        "median": 1023.8739599805141,
        "p2.5": 1002.2212831685365,
        "p97.5": 1041.1664965550492,
        "rank": 36,
        "rank_p2.5": 19,
        "rank_p97.5": 49
      }
    },
    {
      "rank": 14,
      "model_name": "gpt-5.1",
      "median": 1085.586156512762,
      "p2.5": 1070.6653095159718,
      "p97.5": 1100.5070035095523,
      "rank_p2.5": 9,
      "rank_p97.5": 28,
      "n_match": 2870,
      "useful": 215,
      "creative": 53,
      "complete": 189,
      "clear_formatting": 158,
      "incorrect": 26,
      "superficial": 70,
      "instructions_not_followed": 34,
      "total_prefs": 745,
      "positive_prefs_ratio": 0.825503355704698,
      "win_rate": 0.5401987353206865,
      "mean_win_prob": 0.5216427711597434,
      "style_controlled": {
        "median": 1117.031272672194,
        "p2.5": 1104.8337196807104,
        "p97.5": 1136.0792010298624,
        "rank": 4,
        "rank_p2.5": 2,
        "rank_p97.5": 9
      }
    },
    {
      "rank": 15,
      "model_name": "DeepSeek-V3.2",
      "median": 1082.4311428473638,
      "p2.5": 1063.51553056905,
      "p97.5": 1101.3467551256776,
      "rank_p2.5": 8,
      "rank_p97.5": 32,
      "n_match": 1636,
      "useful": 171,
      "creative": 49,
      "complete": 153,
      "clear_formatting": 135,
      "incorrect": 50,
      "superficial": 53,
      "instructions_not_followed": 23,
      "total_prefs": 634,
      "positive_prefs_ratio": 0.8012618296529969,
      "win_rate": 0.5220643231114436,
      "mean_win_prob": 0.5209094560658317,
      "style_controlled": {
        "median": 1045.0789328479307,
        "p2.5": 1025.461697401081,
        "p97.5": 1066.1695370057446,
        "rank": 26,
        "rank_p2.5": 12,
        "rank_p97.5": 42
      }
    },
    {
      "rank": 16,
      "model_name": "glm-4.5",
      "median": 1079.336719875729,
      "p2.5": 1064.2821814589752,
      "p97.5": 1094.3912582924827,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 2729,
      "useful": 100,
      "creative": 35,
      "complete": 106,
      "clear_formatting": 90,
      "incorrect": 18,
      "superficial": 30,
      "instructions_not_followed": 15,
      "total_prefs": 394,
      "positive_prefs_ratio": 0.8401015228426396,
      "win_rate": 0.5657222480260102,
      "mean_win_prob": 0.5201880590698379,
      "style_controlled": {
        "median": 1096.4241235089632,
        "p2.5": 1080.3011547915733,
        "p97.5": 1117.389961928724,
        "rank": 8,
        "rank_p2.5": 3,
        "rank_p97.5": 15
      }
    },
    {
      "rank": 17,
      "model_name": "grok-4-fast",
      "median": 1078.1496090815326,
      "p2.5": 1064.1069046229147,
      "p97.5": 1092.1923135401505,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 3202,
      "useful": 57,
      "creative": 19,
      "complete": 42,
      "clear_formatting": 44,
      "incorrect": 8,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 187,
      "positive_prefs_ratio": 0.8663101604278075,
      "win_rate": 0.571828731492597,
      "mean_win_prob": 0.5199107391288531,
      "style_controlled": {
        "median": 1046.703270829142,
        "p2.5": 1032.021801264816,
        "p97.5": 1061.6677883090883,
        "rank": 25,
        "rank_p2.5": 13,
        "rank_p97.5": 40
      }
    },
    {
      "rank": 18,
      "model_name": "grok-4.1-fast",
      "median": 1076.501290429905,
      "p2.5": 1060.5656682903798,
      "p97.5": 1092.4369125694304,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 2361,
      "useful": 182,
      "creative": 71,
      "complete": 147,
      "clear_formatting": 134,
      "incorrect": 49,
      "superficial": 65,
      "instructions_not_followed": 22,
      "total_prefs": 670,
      "positive_prefs_ratio": 0.7970149253731343,
      "win_rate": 0.535751840168244,
      "mean_win_prob": 0.5195251502176125,
      "style_controlled": {
        "median": 1103.791239551435,
        "p2.5": 1088.993353293895,
        "p97.5": 1120.7480022268649,
        "rank": 7,
        "rank_p2.5": 2,
        "rank_p97.5": 13
      }
    },
    {
      "rank": 19,
      "model_name": "claude-4-sonnet",
      "median": 1074.3439370400918,
      "p2.5": 1060.088127875541,
      "p97.5": 1088.5997462046425,
      "rank_p2.5": 11,
      "rank_p97.5": 32,
      "n_match": 3077,
      "useful": 67,
      "creative": 24,
      "complete": 57,
      "clear_formatting": 69,
      "incorrect": 12,
      "superficial": 19,
      "instructions_not_followed": 7,
      "total_prefs": 255,
      "positive_prefs_ratio": 0.8509803921568627,
      "win_rate": 0.5642296571664601,
      "mean_win_prob": 0.5190195562174781,
      "style_controlled": {
        "median": 1053.1310605926162,
        "p2.5": 1040.9462368761135,
        "p97.5": 1064.9680480017278,
        "rank": 19,
        "rank_p2.5": 12,
        "rank_p97.5": 36
      }
    },
    {
      "rank": 20,
      "model_name": "mistral-small-2506",
      "median": 1071.6376695528022,
      "p2.5": 1060.6316214039955,
      "p97.5": 1082.643717701609,
      "rank_p2.5": 12,
      "rank_p97.5": 32,
      "n_match": 5575,
      "useful": 223,
      "creative": 53,
      "complete": 147,
      "clear_formatting": 191,
      "incorrect": 59,
      "superficial": 79,
      "instructions_not_followed": 30,
      "total_prefs": 782,
      "positive_prefs_ratio": 0.7851662404092071,
      "win_rate": 0.5387931034482759,
      "mean_win_prob": 0.5183838282675465,
      "style_controlled": {
        "median": 1019.0912751173295,
        "p2.5": 1008.450768546394,
        "p97.5": 1033.427991330621,
        "rank": 39,
        "rank_p2.5": 23,
        "rank_p97.5": 47
      }
    },
    {
      "rank": 21,
      "model_name": "gpt-oss-120b",
      "median": 1070.7719841493727,
      "p2.5": 1052.4650180846409,
      "p97.5": 1089.0789502141047,
      "rank_p2.5": 11,
      "rank_p97.5": 33,
      "n_match": 1893,
      "useful": 155,
      "creative": 62,
      "complete": 212,
      "clear_formatting": 146,
      "incorrect": 56,
      "superficial": 46,
      "instructions_not_followed": 35,
      "total_prefs": 712,
      "positive_prefs_ratio": 0.8075842696629213,
      "win_rate": 0.5598320503848845,
      "mean_win_prob": 0.5181801192206517,
      "style_controlled": {
        "median": 1082.4314205883275,
        "p2.5": 1062.5558092460674,
        "p97.5": 1101.2466129565894,
        "rank": 11,
        "rank_p2.5": 5,
        "rank_p97.5": 22
      }
    },
    {
      "rank": 22,
      "model_name": "gemma-3-12b",
      "median": 1067.9186731696507,
      "p2.5": 1059.460320492513,
      "p97.5": 1076.3770258467882,
      "rank_p2.5": 13,
      "rank_p97.5": 32,
      "n_match": 9107,
      "useful": 911,
      "creative": 259,
      "complete": 914,
      "clear_formatting": 758,
      "incorrect": 243,
      "superficial": 238,
      "instructions_not_followed": 165,
      "total_prefs": 3488,
      "positive_prefs_ratio": 0.8147935779816514,
      "win_rate": 0.5810734463276837,
      "mean_win_prob": 0.51750748202461,
      "style_controlled": {
        "median": 1026.6275703345855,
        "p2.5": 1017.3396352631349,
        "p97.5": 1035.3478543176495,
        "rank": 35,
        "rank_p2.5": 22,
        "rank_p97.5": 45
      }
    },
    {
      "rank": 23,
      "model_name": "kimi-k2-thinking",
      "median": 1067.9008985219282,
      "p2.5": 1040.4156269124678,
      "p97.5": 1095.3861701313886,
      "rank_p2.5": 9,
      "rank_p97.5": 36,
      "n_match": 813,
      "useful": 96,
      "creative": 24,
      "complete": 89,
      "clear_formatting": 68,
      "incorrect": 19,
      "superficial": 32,
      "instructions_not_followed": 19,
      "total_prefs": 347,
      "positive_prefs_ratio": 0.7982708933717579,
      "win_rate": 0.5434439178515008,
      "mean_win_prob": 0.5175032860170122,
      "style_controlled": {
        "median": 1146.8393127037023,
        "p2.5": 1119.5762615514898,
        "p97.5": 1174.8225540407004,
        "rank": 2,
        "rank_p2.5": 2,
        "rank_p97.5": 8
      }
    },
    {
      "rank": 24,
      "model_name": "deepseek-r1-0528",
      "median": 1066.559633438597,
      "p2.5": 1046.7098937796604,
      "p97.5": 1086.4093730975337,
      "rank_p2.5": 11,
      "rank_p97.5": 33,
      "n_match": 1660,
      "useful": 113,
      "creative": 32,
      "complete": 80,
      "clear_formatting": 72,
      "incorrect": 26,
      "superficial": 36,
      "instructions_not_followed": 17,
      "total_prefs": 376,
      "positive_prefs_ratio": 0.7898936170212766,
      "win_rate": 0.541908713692946,
      "mean_win_prob": 0.5171864490559616,
      "style_controlled": {
        "median": 1127.2401011859426,
        "p2.5": 1105.705358299507,
        "p97.5": 1147.1510323952393,
        "rank": 3,
        "rank_p2.5": 2,
        "rank_p97.5": 9
      }
    },
    {
      "rank": 25,
      "model_name": "kimi-k2",
      "median": 1064.6934329269739,
      "p2.5": 1044.9303404610482,
      "p97.5": 1084.4565253928993,
      "rank_p2.5": 11,
      "rank_p97.5": 34,
      "n_match": 1546,
      "useful": 119,
      "creative": 38,
      "complete": 75,
      "clear_formatting": 85,
      "incorrect": 23,
      "superficial": 39,
      "instructions_not_followed": 10,
      "total_prefs": 389,
      "positive_prefs_ratio": 0.8149100257069408,
      "win_rate": 0.530952380952381,
      "mean_win_prob": 0.5167449246454495,
      "style_controlled": {
        "median": 1040.9696739083831,
        "p2.5": 1020.9810307545027,
        "p97.5": 1059.6466258741184,
        "rank": 28,
        "rank_p2.5": 14,
        "rank_p97.5": 44
      }
    },
    {
      "rank": 26,
      "model_name": "deepseek-v3-chat",
      "median": 1061.2462398200141,
      "p2.5": 1051.270212295519,
      "p97.5": 1071.2222673445092,
      "rank_p2.5": 14,
      "rank_p97.5": 33,
      "n_match": 7560,
      "useful": 833,
      "creative": 209,
      "complete": 790,
      "clear_formatting": 769,
      "incorrect": 143,
      "superficial": 188,
      "instructions_not_followed": 81,
      "total_prefs": 3013,
      "positive_prefs_ratio": 0.8632592100896117,
      "win_rate": 0.6549130230371415,
      "mean_win_prob": 0.5159272475314463,
      "style_controlled": {
        "median": 1035.3943894007782,
        "p2.5": 1027.5327830813587,
        "p97.5": 1046.034792527844,
        "rank": 30,
        "rank_p2.5": 17,
        "rank_p97.5": 41
      }
    },
    {
      "rank": 27,
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1060.8700272662097,
      "p2.5": 1037.9881676325028,
      "p97.5": 1083.7518868999166,
      "rank_p2.5": 11,
      "rank_p97.5": 40,
      "n_match": 1146,
      "useful": 96,
      "creative": 25,
      "complete": 58,
      "clear_formatting": 81,
      "incorrect": 11,
      "superficial": 31,
      "instructions_not_followed": 10,
      "total_prefs": 312,
      "positive_prefs_ratio": 0.8333333333333334,
      "win_rate": 0.5376712328767124,
      "mean_win_prob": 0.5158378437783764,
      "style_controlled": {
        "median": 1004.9177784516335,
        "p2.5": 987.49583813326,
        "p97.5": 1026.80826741351,
        "rank": 44,
        "rank_p2.5": 26,
        "rank_p97.5": 56
      }
    },
    {
      "rank": 28,
      "model_name": "command-a",
      "median": 1059.7745574159771,
      "p2.5": 1051.2422545232735,
      "p97.5": 1068.3068603086808,
      "rank_p2.5": 15,
      "rank_p97.5": 33,
      "n_match": 9001,
      "useful": 917,
      "creative": 213,
      "complete": 779,
      "clear_formatting": 802,
      "incorrect": 198,
      "superficial": 278,
      "instructions_not_followed": 108,
      "total_prefs": 3295,
      "positive_prefs_ratio": 0.8227617602427921,
      "win_rate": 0.5693325661680092,
      "mean_win_prob": 0.5155773282757783,
      "style_controlled": {
        "median": 1026.6864555422537,
        "p2.5": 1019.4256534381352,
        "p97.5": 1035.9157113615809,
        "rank": 34,
        "rank_p2.5": 22,
        "rank_p97.5": 44
      }
    },
    {
      "rank": 29,
      "model_name": "claude-3-7-sonnet",
      "median": 1055.4509912270184,
      "p2.5": 1043.8317668028174,
      "p97.5": 1067.0702156512195,
      "rank_p2.5": 15,
      "rank_p97.5": 34,
      "n_match": 4743,
      "useful": 734,
      "creative": 161,
      "complete": 511,
      "clear_formatting": 621,
      "incorrect": 84,
      "superficial": 245,
      "instructions_not_followed": 44,
      "total_prefs": 2400,
      "positive_prefs_ratio": 0.8445833333333334,
      "win_rate": 0.5711649973642594,
      "mean_win_prob": 0.5145464187768457,
      "style_controlled": {
        "median": 1053.759077007856,
        "p2.5": 1045.0392576749264,
        "p97.5": 1066.6560037706815,
        "rank": 18,
        "rank_p2.5": 12,
        "rank_p97.5": 33
      }
    },
    {
      "rank": 30,
      "model_name": "grok-3-mini-beta",
      "median": 1054.1963785434164,
      "p2.5": 1035.997577582437,
      "p97.5": 1072.3951795043956,
      "rank_p2.5": 13,
      "rank_p97.5": 41,
      "n_match": 2066,
      "useful": 247,
      "creative": 65,
      "complete": 231,
      "clear_formatting": 234,
      "incorrect": 65,
      "superficial": 65,
      "instructions_not_followed": 42,
      "total_prefs": 949,
      "positive_prefs_ratio": 0.8187565858798735,
      "win_rate": 0.574085554866708,
      "mean_win_prob": 0.5142464557654027,
      "style_controlled": {
        "median": 1029.5619181798875,
        "p2.5": 1010.8733525641463,
        "p97.5": 1046.9803906401994,
        "rank": 32,
        "rank_p2.5": 17,
        "rank_p97.5": 46
      }
    },
    {
      "rank": 31,
      "model_name": "magistral-small-2506",
      "median": 1052.8871832370812,
      "p2.5": 1039.8261767691067,
      "p97.5": 1065.948189705056,
      "rank_p2.5": 15,
      "rank_p97.5": 38,
      "n_match": 3850,
      "useful": 83,
      "creative": 18,
      "complete": 59,
      "clear_formatting": 77,
      "incorrect": 20,
      "superficial": 28,
      "instructions_not_followed": 12,
      "total_prefs": 297,
      "positive_prefs_ratio": 0.797979797979798,
      "win_rate": 0.5278849526481936,
      "mean_win_prob": 0.51393305115596,
      "style_controlled": {
        "median": 1010.2208841757675,
        "p2.5": 997.993887931315,
        "p97.5": 1024.6418028782732,
        "rank": 42,
        "rank_p2.5": 29,
        "rank_p97.5": 53
      }
    },
    {
      "rank": 32,
      "model_name": "glm-4.6",
      "median": 1050.8821467216098,
      "p2.5": 1036.0844038396085,
      "p97.5": 1065.6798896036114,
      "rank_p2.5": 15,
      "rank_p97.5": 41,
      "n_match": 2778,
      "useful": 119,
      "creative": 38,
      "complete": 123,
      "clear_formatting": 108,
      "incorrect": 31,
      "superficial": 48,
      "instructions_not_followed": 24,
      "total_prefs": 491,
      "positive_prefs_ratio": 0.790224032586558,
      "win_rate": 0.5243195002231147,
      "mean_win_prob": 0.513452294168734,
      "style_controlled": {
        "median": 1083.359228049692,
        "p2.5": 1071.6538288873883,
        "p97.5": 1102.2611415556817,
        "rank": 10,
        "rank_p2.5": 5,
        "rank_p97.5": 17
      }
    },
    {
      "rank": 33,
      "model_name": "gemma-3n-e4b-it",
      "median": 1034.7150703848877,
      "p2.5": 1022.9141050786665,
      "p97.5": 1046.516035691109,
      "rank_p2.5": 26,
      "rank_p97.5": 41,
      "n_match": 4690,
      "useful": 216,
      "creative": 76,
      "complete": 160,
      "clear_formatting": 184,
      "incorrect": 83,
      "superficial": 92,
      "instructions_not_followed": 46,
      "total_prefs": 857,
      "positive_prefs_ratio": 0.7421236872812136,
      "win_rate": 0.5035360678925035,
      "mean_win_prob": 0.509541161303752,
      "style_controlled": {
        "median": 994.4573797378781,
        "p2.5": 985.1373860499452,
        "p97.5": 1008.8028728552374,
        "rank": 49,
        "rank_p2.5": 37,
        "rank_p97.5": 57
      }
    },
    {
      "rank": 34,
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1033.7843965963405,
      "p2.5": 1024.7864285158066,
      "p97.5": 1042.7823646768743,
      "rank_p2.5": 28,
      "rank_p97.5": 41,
      "n_match": 8481,
      "useful": 1052,
      "creative": 320,
      "complete": 1060,
      "clear_formatting": 978,
      "incorrect": 211,
      "superficial": 285,
      "instructions_not_followed": 133,
      "total_prefs": 4039,
      "positive_prefs_ratio": 0.844268383263184,
      "win_rate": 0.5898007731192387,
      "mean_win_prob": 0.5093141171439066,
      "style_controlled": {
        "median": 982.3039691053349,
        "p2.5": 974.6008167954941,
        "p97.5": 992.8041944380379,
        "rank": 55,
        "rank_p2.5": 43,
        "rank_p97.5": 60
      }
    },
    {
      "rank": 35,
      "model_name": "gpt-4.1-mini",
      "median": 1031.090864950941,
      "p2.5": 1022.30928422328,
      "p97.5": 1039.8724456786024,
      "rank_p2.5": 29,
      "rank_p97.5": 42,
      "n_match": 8592,
      "useful": 870,
      "creative": 190,
      "complete": 569,
      "clear_formatting": 722,
      "incorrect": 139,
      "superficial": 324,
      "instructions_not_followed": 83,
      "total_prefs": 2897,
      "positive_prefs_ratio": 0.8115291681049361,
      "win_rate": 0.519074613145396,
      "mean_win_prob": 0.5086558374230822,
      "style_controlled": {
        "median": 1032.4062500604862,
        "p2.5": 1024.6991550975908,
        "p97.5": 1042.8818379580568,
        "rank": 31,
        "rank_p2.5": 18,
        "rank_p97.5": 42
      }
    },
    {
      "rank": 36,
      "model_name": "gemma-3-4b",
      "median": 1030.439251368476,
      "p2.5": 1022.4986840253819,
      "p97.5": 1038.3798187115706,
      "rank_p2.5": 30,
      "rank_p97.5": 42,
      "n_match": 10380,
      "useful": 913,
      "creative": 254,
      "complete": 909,
      "clear_formatting": 793,
      "incorrect": 372,
      "superficial": 242,
      "instructions_not_followed": 184,
      "total_prefs": 3667,
      "positive_prefs_ratio": 0.7823834196891192,
      "win_rate": 0.529210042865891,
      "mean_win_prob": 0.5084963249568856,
      "style_controlled": {
        "median": 992.4852484383807,
        "p2.5": 984.8276655107936,
        "p97.5": 1000.2716178527739,
        "rank": 50,
        "rank_p2.5": 41,
        "rank_p97.5": 57
      }
    },
    {
      "rank": 37,
      "model_name": "deepseek-r1",
      "median": 1026.8607240734943,
      "p2.5": 1014.1559108983819,
      "p97.5": 1039.565537248607,
      "rank_p2.5": 30,
      "rank_p97.5": 46,
      "n_match": 4277,
      "useful": 576,
      "creative": 213,
      "complete": 554,
      "clear_formatting": 507,
      "incorrect": 119,
      "superficial": 148,
      "instructions_not_followed": 104,
      "total_prefs": 2221,
      "positive_prefs_ratio": 0.8329581269698334,
      "win_rate": 0.5648063433973772,
      "mean_win_prob": 0.5076184840705066,
      "style_controlled": {
        "median": 999.6321793049915,
        "p2.5": 987.4961683545383,
        "p97.5": 1011.6096008614231,
        "rank": 46,
        "rank_p2.5": 35,
        "rank_p97.5": 56
      }
    },
    {
      "rank": 38,
      "model_name": "qwen3-32b",
      "median": 1019.6360633341736,
      "p2.5": 1002.9978754505839,
      "p97.5": 1036.2742512177633,
      "rank_p2.5": 31,
      "rank_p97.5": 50,
      "n_match": 2274,
      "useful": 241,
      "creative": 80,
      "complete": 210,
      "clear_formatting": 176,
      "incorrect": 86,
      "superficial": 78,
      "instructions_not_followed": 55,
      "total_prefs": 926,
      "positive_prefs_ratio": 0.7634989200863931,
      "win_rate": 0.4921259842519685,
      "mean_win_prob": 0.5058367218676034,
      "style_controlled": {
        "median": 1023.7081445268882,
        "p2.5": 1009.9620634073465,
        "p97.5": 1046.3350701017082,
        "rank": 37,
        "rank_p2.5": 17,
        "rank_p97.5": 46
      }
    },
    {
      "rank": 39,
      "model_name": "mistral-saba",
      "median": 1010.7594346081431,
      "p2.5": 1000.3010475855216,
      "p97.5": 1021.2178216307647,
      "rank_p2.5": 37,
      "rank_p97.5": 50,
      "n_match": 6004,
      "useful": 506,
      "creative": 117,
      "complete": 359,
      "clear_formatting": 390,
      "incorrect": 134,
      "superficial": 239,
      "instructions_not_followed": 96,
      "total_prefs": 1841,
      "positive_prefs_ratio": 0.7452471482889734,
      "win_rate": 0.5004434589800444,
      "mean_win_prob": 0.503629987358709,
      "style_controlled": {
        "median": 984.6764763209248,
        "p2.5": 976.4434929469782,
        "p97.5": 996.0369114243734,
        "rank": 54,
        "rank_p2.5": 43,
        "rank_p97.5": 58
      }
    },
    {
      "rank": 40,
      "model_name": "glm-4.7",
      "median": 1010.1740609006456,
      "p2.5": 965.031459871873,
      "p97.5": 1055.3166619294182,
      "rank_p2.5": 22,
      "rank_p97.5": 65,
      "n_match": 273,
      "useful": 29,
      "creative": 13,
      "complete": 29,
      "clear_formatting": 30,
      "incorrect": 11,
      "superficial": 13,
      "instructions_not_followed": 12,
      "total_prefs": 137,
      "positive_prefs_ratio": 0.7372262773722628,
      "win_rate": 0.4549763033175355,
      "mean_win_prob": 0.5034837769673726,
      "style_controlled": {
        "median": 1094.3047752936336,
        "p2.5": 1026.6050573559642,
        "p97.5": 1133.9747063420855,
        "rank": 9,
        "rank_p2.5": 2,
        "rank_p97.5": 42
      }
    },
    {
      "rank": 41,
      "model_name": "llama-maverick",
      "median": 1009.9490194414567,
      "p2.5": 997.2041170286328,
      "p97.5": 1022.6939218542807,
      "rank_p2.5": 35,
      "rank_p97.5": 51,
      "n_match": 3893,
      "useful": 146,
      "creative": 30,
      "complete": 95,
      "clear_formatting": 105,
      "incorrect": 36,
      "superficial": 74,
      "instructions_not_followed": 20,
      "total_prefs": 506,
      "positive_prefs_ratio": 0.7430830039525692,
      "win_rate": 0.46589018302828616,
      "mean_win_prob": 0.503427545043502,
      "style_controlled": {
        "median": 996.1000332352758,
        "p2.5": 981.3499819662818,
        "p97.5": 1006.1066433787124,
        "rank": 47,
        "rank_p2.5": 39,
        "rank_p97.5": 58
      }
    },
    {
      "rank": 42,
      "model_name": "mistral-large-2411",
      "median": 1009.5382289073393,
      "p2.5": 1001.5268358075265,
      "p97.5": 1017.5496220071522,
      "rank_p2.5": 37,
      "rank_p97.5": 50,
      "n_match": 10960,
      "useful": 1392,
      "creative": 260,
      "complete": 1068,
      "clear_formatting": 1146,
      "incorrect": 213,
      "superficial": 465,
      "instructions_not_followed": 132,
      "total_prefs": 4676,
      "positive_prefs_ratio": 0.8267750213857998,
      "win_rate": 0.5500690607734806,
      "mean_win_prob": 0.5033248667912817,
      "style_controlled": {
        "median": 991.5071985453228,
        "p2.5": 984.9304338522389,
        "p97.5": 998.4509620616882,
        "rank": 51,
        "rank_p2.5": 42,
        "rank_p97.5": 57
      }
    },
    {
      "rank": 43,
      "model_name": "gemini-1.5-pro",
      "median": 1005.1691806749011,
      "p2.5": 995.499214073193,
      "p97.5": 1014.8391472766093,
      "rank_p2.5": 37,
      "rank_p97.5": 51,
      "n_match": 10027,
      "useful": 1656,
      "creative": 524,
      "complete": 1435,
      "clear_formatting": 1532,
      "incorrect": 287,
      "superficial": 498,
      "instructions_not_followed": 205,
      "total_prefs": 6137,
      "positive_prefs_ratio": 0.8386833957959915,
      "win_rate": 0.5857166203628044,
      "mean_win_prob": 0.5022302030858053,
      "style_controlled": {
        "median": 1008.0376777223603,
        "p2.5": 998.8526002680904,
        "p97.5": 1017.9174486270513,
        "rank": 43,
        "rank_p2.5": 33,
        "rank_p97.5": 52
      }
    },
    {
      "rank": 44,
      "model_name": "llama-4-scout",
      "median": 1002.577746105041,
      "p2.5": 993.2437103823486,
      "p97.5": 1011.9117818277335,
      "rank_p2.5": 38,
      "rank_p97.5": 52,
      "n_match": 7137,
      "useful": 627,
      "creative": 141,
      "complete": 469,
      "clear_formatting": 493,
      "incorrect": 171,
      "superficial": 296,
      "instructions_not_followed": 95,
      "total_prefs": 2292,
      "positive_prefs_ratio": 0.7547993019197208,
      "win_rate": 0.4803798602401003,
      "mean_win_prob": 0.5015786591948704,
      "style_controlled": {
        "median": 989.1503819937589,
        "p2.5": 980.3275336397327,
        "p97.5": 999.7823105433491,
        "rank": 52,
        "rank_p2.5": 41,
        "rank_p97.5": 58
      }
    },
    {
      "rank": 45,
      "model_name": "gpt-oss-20b",
      "median": 1001.052496519017,
      "p2.5": 983.7148200117836,
      "p97.5": 1018.3901730262505,
      "rank_p2.5": 37,
      "rank_p97.5": 56,
      "n_match": 2236,
      "useful": 157,
      "creative": 66,
      "complete": 144,
      "clear_formatting": 123,
      "incorrect": 60,
      "superficial": 81,
      "instructions_not_followed": 44,
      "total_prefs": 675,
      "positive_prefs_ratio": 0.725925925925926,
      "win_rate": 0.44563167818981775,
      "mean_win_prob": 0.5011943874344887,
      "style_controlled": {
        "median": 1022.1863298948504,
        "p2.5": 1006.6148086953082,
        "p97.5": 1038.6982106640323,
        "rank": 38,
        "rank_p2.5": 21,
        "rank_p97.5": 47
      }
    },
    {
      "rank": 46,
      "model_name": "EuroLLM-22B-Instruct-2512",
      "median": 999.9862187229191,
      "p2.5": 959.4654389243137,
      "p97.5": 1040.5069985215246,
      "rank_p2.5": 28,
      "rank_p97.5": 65,
      "n_match": 335,
      "useful": 52,
      "creative": 14,
      "complete": 31,
      "clear_formatting": 38,
      "incorrect": 9,
      "superficial": 20,
      "instructions_not_followed": 10,
      "total_prefs": 174,
      "positive_prefs_ratio": 0.7758620689655172,
      "win_rate": 0.4197080291970803,
      "mean_win_prob": 0.5009254004256204,
      "style_controlled": {
        "median": 927.511180654176,
        "p2.5": 872.7700859562988,
        "p97.5": 962.8570518328961,
        "rank": 72,
        "rank_p2.5": 57,
        "rank_p97.5": 86
      }
    },
    {
      "rank": 47,
      "model_name": "gpt-5-mini",
      "median": 997.1141981075106,
      "p2.5": 984.4565187569231,
      "p97.5": 1009.771877458098,
      "rank_p2.5": 38,
      "rank_p97.5": 56,
      "n_match": 4077,
      "useful": 176,
      "creative": 36,
      "complete": 171,
      "clear_formatting": 114,
      "incorrect": 33,
      "superficial": 73,
      "instructions_not_followed": 40,
      "total_prefs": 643,
      "positive_prefs_ratio": 0.7729393468118196,
      "win_rate": 0.4356687898089172,
      "mean_win_prob": 0.5001994522666154,
      "style_controlled": {
        "median": 1049.2351103456303,
        "p2.5": 1036.8623263405505,
        "p97.5": 1061.313285784303,
        "rank": 22,
        "rank_p2.5": 13,
        "rank_p97.5": 37
      }
    },
    {
      "rank": 48,
      "model_name": "mistral-small-3.1-24b",
      "median": 996.1160343503003,
      "p2.5": 985.5416160551945,
      "p97.5": 1006.690452645406,
      "rank_p2.5": 38,
      "rank_p97.5": 56,
      "n_match": 5935,
      "useful": 892,
      "creative": 192,
      "complete": 538,
      "clear_formatting": 722,
      "incorrect": 197,
      "superficial": 344,
      "instructions_not_followed": 124,
      "total_prefs": 3009,
      "positive_prefs_ratio": 0.7789963443004321,
      "win_rate": 0.5015695067264574,
      "mean_win_prob": 0.49994666069895655,
      "style_controlled": {
        "median": 978.1133486473589,
        "p2.5": 968.7450706541938,
        "p97.5": 990.0895569128733,
        "rank": 56,
        "rank_p2.5": 43,
        "rank_p97.5": 63
      }
    },
    {
      "rank": 49,
      "model_name": "o4-mini",
      "median": 990.9441291027233,
      "p2.5": 977.4162537389498,
      "p97.5": 1004.4720044664967,
      "rank_p2.5": 38,
      "rank_p97.5": 58,
      "n_match": 3568,
      "useful": 321,
      "creative": 71,
      "complete": 206,
      "clear_formatting": 182,
      "incorrect": 75,
      "superficial": 125,
      "instructions_not_followed": 34,
      "total_prefs": 1014,
      "positive_prefs_ratio": 0.7692307692307693,
      "win_rate": 0.4513338139870223,
      "mean_win_prob": 0.4986327749474583,
      "style_controlled": {
        "median": 1028.5910924091072,
        "p2.5": 1017.003647643289,
        "p97.5": 1041.7725143890482,
        "rank": 33,
        "rank_p2.5": 19,
        "rank_p97.5": 45
      }
    },
    {
      "rank": 50,
      "model_name": "gpt-5",
      "median": 983.015608939327,
      "p2.5": 970.5926551512998,
      "p97.5": 995.438562727354,
      "rank_p2.5": 43,
      "rank_p97.5": 61,
      "n_match": 4654,
      "useful": 1,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 1,
      "positive_prefs_ratio": 1.0,
      "win_rate": 0.4053893988747409,
      "mean_win_prob": 0.49660526006534467,
      "style_controlled": {
        "median": 1039.1943647734524,
        "p2.5": 1023.5615841717498,
        "p97.5": 1051.8463203568783,
        "rank": 29,
        "rank_p2.5": 15,
        "rank_p97.5": 43
      }
    },
    {
      "rank": 51,
      "model_name": "lfm2-8b-a1b",
      "median": 981.9398982843553,
      "p2.5": 964.5840231469341,
      "p97.5": 999.2957734217765,
      "rank_p2.5": 41,
      "rank_p97.5": 65,
      "n_match": 2074,
      "useful": 130,
      "creative": 21,
      "complete": 85,
      "clear_formatting": 109,
      "incorrect": 74,
      "superficial": 85,
      "instructions_not_followed": 39,
      "total_prefs": 543,
      "positive_prefs_ratio": 0.6353591160220995,
      "win_rate": 0.41094224924012157,
      "mean_win_prob": 0.49632892270467655,
      "style_controlled": {
        "median": 955.2248196170382,
        "p2.5": 938.7698203683452,
        "p97.5": 969.9662912269495,
        "rank": 63,
        "rank_p2.5": 55,
        "rank_p97.5": 73
      }
    },
    {
      "rank": 52,
      "model_name": "aya-expanse-32b",
      "median": 976.1192667807055,
      "p2.5": 965.8381572015597,
      "p97.5": 986.4003763598512,
      "rank_p2.5": 44,
      "rank_p97.5": 64,
      "n_match": 6060,
      "useful": 491,
      "creative": 88,
      "complete": 300,
      "clear_formatting": 348,
      "incorrect": 158,
      "superficial": 242,
      "instructions_not_followed": 87,
      "total_prefs": 1714,
      "positive_prefs_ratio": 0.7158693115519253,
      "win_rate": 0.44252631578947366,
      "mean_win_prob": 0.49482844329750303,
      "style_controlled": {
        "median": 972.5384532730363,
        "p2.5": 962.772313880414,
        "p97.5": 982.3072763978683,
        "rank": 58,
        "rank_p2.5": 50,
        "rank_p97.5": 67
      }
    },
    {
      "rank": 53,
      "model_name": "qwen-3-8b",
      "median": 975.8883799261627,
      "p2.5": 961.7218976352401,
      "p97.5": 990.0548622170852,
      "rank_p2.5": 44,
      "rank_p97.5": 65,
      "n_match": 3150,
      "useful": 127,
      "creative": 40,
      "complete": 111,
      "clear_formatting": 86,
      "incorrect": 79,
      "superficial": 52,
      "instructions_not_followed": 38,
      "total_prefs": 533,
      "positive_prefs_ratio": 0.6829268292682927,
      "win_rate": 0.40725806451612906,
      "mean_win_prob": 0.4947687411849724,
      "style_controlled": {
        "median": 1047.9544677087158,
        "p2.5": 1033.577267999082,
        "p97.5": 1060.1587026918507,
        "rank": 23,
        "rank_p2.5": 14,
        "rank_p97.5": 39
      }
    },
    {
      "rank": 54,
      "model_name": "qwen3-30b-a3b",
      "median": 975.0747742581261,
      "p2.5": 961.9998349662574,
      "p97.5": 988.1497135499948,
      "rank_p2.5": 44,
      "rank_p97.5": 65,
      "n_match": 3790,
      "useful": 143,
      "creative": 24,
      "complete": 128,
      "clear_formatting": 118,
      "incorrect": 77,
      "superficial": 77,
      "instructions_not_followed": 35,
      "total_prefs": 602,
      "positive_prefs_ratio": 0.686046511627907,
      "win_rate": 0.4062394031875212,
      "mean_win_prob": 0.4945582497160083,
      "style_controlled": {
        "median": 1016.2321153650973,
        "p2.5": 1001.671741730858,
        "p97.5": 1029.4228523189797,
        "rank": 40,
        "rank_p2.5": 24,
        "rank_p97.5": 49
      }
    },
    {
      "rank": 55,
      "model_name": "llama-3.3-70b",
      "median": 970.4250763015509,
      "p2.5": 962.8695268791853,
      "p97.5": 977.9806257239165,
      "rank_p2.5": 47,
      "rank_p97.5": 65,
      "n_match": 11394,
      "useful": 1094,
      "creative": 212,
      "complete": 783,
      "clear_formatting": 808,
      "incorrect": 257,
      "superficial": 505,
      "instructions_not_followed": 145,
      "total_prefs": 3804,
      "positive_prefs_ratio": 0.7615667718191378,
      "win_rate": 0.465857694429123,
      "mean_win_prob": 0.49335196443815477,
      "style_controlled": {
        "median": 968.4168230987576,
        "p2.5": 960.8278032379117,
        "p97.5": 975.8519581756146,
        "rank": 59,
        "rank_p2.5": 53,
        "rank_p97.5": 68
      }
    },
    {
      "rank": 56,
      "model_name": "o3-mini",
      "median": 969.5512841428867,
      "p2.5": 951.9594344529092,
      "p97.5": 987.1431338328644,
      "rank_p2.5": 44,
      "rank_p97.5": 67,
      "n_match": 1951,
      "useful": 252,
      "creative": 62,
      "complete": 165,
      "clear_formatting": 157,
      "incorrect": 35,
      "superficial": 97,
      "instructions_not_followed": 28,
      "total_prefs": 796,
      "positive_prefs_ratio": 0.7989949748743719,
      "win_rate": 0.5047679593134139,
      "mean_win_prob": 0.49312463693493525,
      "style_controlled": {
        "median": 1001.6876909651792,
        "p2.5": 982.8302502343312,
        "p97.5": 1021.7910526407308,
        "rank": 45,
        "rank_p2.5": 30,
        "rank_p97.5": 57
      }
    },
    {
      "rank": 57,
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 965.5638909046983,
      "p2.5": 953.4326151545242,
      "p97.5": 977.6951666548724,
      "rank_p2.5": 47,
      "rank_p97.5": 67,
      "n_match": 4345,
      "useful": 524,
      "creative": 106,
      "complete": 415,
      "clear_formatting": 431,
      "incorrect": 89,
      "superficial": 189,
      "instructions_not_followed": 71,
      "total_prefs": 1825,
      "positive_prefs_ratio": 0.8087671232876712,
      "win_rate": 0.51994342291372,
      "mean_win_prob": 0.49208469829475476,
      "style_controlled": {
        "median": 953.8737812557605,
        "p2.5": 938.6785626380376,
        "p97.5": 966.7619741448643,
        "rank": 64,
        "rank_p2.5": 57,
        "rank_p97.5": 73
      }
    },
    {
      "rank": 58,
      "model_name": "gpt-4.1-nano",
      "median": 962.1058805247204,
      "p2.5": 952.3577869627776,
      "p97.5": 971.8539740866632,
      "rank_p2.5": 48,
      "rank_p97.5": 67,
      "n_match": 6875,
      "useful": 640,
      "creative": 108,
      "complete": 344,
      "clear_formatting": 465,
      "incorrect": 145,
      "superficial": 301,
      "instructions_not_followed": 83,
      "total_prefs": 2086,
      "positive_prefs_ratio": 0.7464046021093,
      "win_rate": 0.42656162070906023,
      "mean_win_prob": 0.4911793973981082,
      "style_controlled": {
        "median": 975.6722511230691,
        "p2.5": 967.2022838784628,
        "p97.5": 985.4078223384367,
        "rank": 57,
        "rank_p2.5": 46,
        "rank_p97.5": 64
      }
    },
    {
      "rank": 59,
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 961.7403877310497,
      "p2.5": 946.1221956303921,
      "p97.5": 977.3585798317074,
      "rank_p2.5": 48,
      "rank_p97.5": 69,
      "n_match": 2660,
      "useful": 98,
      "creative": 22,
      "complete": 62,
      "clear_formatting": 60,
      "incorrect": 35,
      "superficial": 63,
      "instructions_not_followed": 26,
      "total_prefs": 366,
      "positive_prefs_ratio": 0.6612021857923497,
      "win_rate": 0.38380450407283184,
      "mean_win_prob": 0.49108352518793696,
      "style_controlled": {
        "median": 943.6355037183475,
        "p2.5": 927.5177742998959,
        "p97.5": 959.6116958266919,
        "rank": 67,
        "rank_p2.5": 59,
        "rank_p97.5": 76
      }
    },
    {
      "rank": 60,
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 961.0104776522555,
      "p2.5": 952.0390035853111,
      "p97.5": 969.9819517191999,
      "rank_p2.5": 49,
      "rank_p97.5": 67,
      "n_match": 8990,
      "useful": 1033,
      "creative": 200,
      "complete": 653,
      "clear_formatting": 848,
      "incorrect": 192,
      "superficial": 364,
      "instructions_not_followed": 110,
      "total_prefs": 3400,
      "positive_prefs_ratio": 0.8041176470588235,
      "win_rate": 0.5002845759817871,
      "mean_win_prob": 0.49089195572746536,
      "style_controlled": {
        "median": 965.665547749179,
        "p2.5": 957.1061546215386,
        "p97.5": 972.535191555483,
        "rank": 60,
        "rank_p2.5": 55,
        "rank_p97.5": 69
      }
    },
    {
      "rank": 61,
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 959.9877573994552,
      "p2.5": 879.746905626345,
      "p97.5": 1040.2286091725655,
      "rank_p2.5": 29,
      "rank_p97.5": 81,
      "n_match": 97,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 0,
      "positive_prefs_ratio": 0.0,
      "win_rate": 0.38461538461538464,
      "mean_win_prob": 0.4906232959201793,
      "style_controlled": {
        "median": 1013.6567152068533,
        "p2.5": 844.5179902409554,
        "p97.5": 1067.2594431573496,
        "rank": 41,
        "rank_p2.5": 11,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 62,
      "model_name": "claude-3-5-sonnet-v2",
      "median": 957.6388783057748,
      "p2.5": 948.0928376951061,
      "p97.5": 967.1849189164434,
      "rank_p2.5": 49,
      "rank_p97.5": 68,
      "n_match": 7388,
      "useful": 846,
      "creative": 179,
      "complete": 524,
      "clear_formatting": 565,
      "incorrect": 130,
      "superficial": 402,
      "instructions_not_followed": 94,
      "total_prefs": 2740,
      "positive_prefs_ratio": 0.7715328467153285,
      "win_rate": 0.5058297396582015,
      "mean_win_prob": 0.49000520083487825,
      "style_controlled": {
        "median": 995.0068405926377,
        "p2.5": 986.08321361086,
        "p97.5": 1005.5709529755219,
        "rank": 48,
        "rank_p2.5": 39,
        "rank_p97.5": 56
      }
    },
    {
      "rank": 63,
      "model_name": "minimax-m2",
      "median": 953.4624571743462,
      "p2.5": 934.6774510628898,
      "p97.5": 972.2474632858027,
      "rank_p2.5": 48,
      "rank_p97.5": 71,
      "n_match": 1869,
      "useful": 135,
      "creative": 32,
      "complete": 86,
      "clear_formatting": 72,
      "incorrect": 34,
      "superficial": 67,
      "instructions_not_followed": 24,
      "total_prefs": 450,
      "positive_prefs_ratio": 0.7222222222222222,
      "win_rate": 0.3882270497547302,
      "mean_win_prob": 0.48890251947844826,
      "style_controlled": {
        "median": 1041.2346719427435,
        "p2.5": 1020.3823163650985,
        "p97.5": 1059.0725867052986,
        "rank": 27,
        "rank_p2.5": 14,
        "rank_p97.5": 44
      }
    },
    {
      "rank": 64,
      "model_name": "gpt-4o-2024-08-06",
      "median": 944.5247826846628,
      "p2.5": 934.7262350021284,
      "p97.5": 954.3233303671973,
      "rank_p2.5": 56,
      "rank_p97.5": 71,
      "n_match": 7915,
      "useful": 768,
      "creative": 153,
      "complete": 491,
      "clear_formatting": 618,
      "incorrect": 131,
      "superficial": 367,
      "instructions_not_followed": 115,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7680665909950813,
      "win_rate": 0.48274765463507713,
      "mean_win_prob": 0.48652680007065074,
      "style_controlled": {
        "median": 960.6124141492999,
        "p2.5": 950.6105605619591,
        "p97.5": 970.512559149687,
        "rank": 61,
        "rank_p2.5": 55,
        "rank_p97.5": 70
      }
    },
    {
      "rank": 65,
      "model_name": "phi-4",
      "median": 944.1450881114818,
      "p2.5": 936.833304794024,
      "p97.5": 951.4568714289394,
      "rank_p2.5": 60,
      "rank_p97.5": 71,
      "n_match": 12674,
      "useful": 1117,
      "creative": 215,
      "complete": 803,
      "clear_formatting": 855,
      "incorrect": 318,
      "superficial": 497,
      "instructions_not_followed": 209,
      "total_prefs": 4014,
      "positive_prefs_ratio": 0.7448928749377179,
      "win_rate": 0.4413745296098237,
      "mean_win_prob": 0.48642538896143517,
      "style_controlled": {
        "median": 930.8504944484007,
        "p2.5": 924.1215266283465,
        "p97.5": 938.9206154733656,
        "rank": 71,
        "rank_p2.5": 62,
        "rank_p97.5": 79
      }
    },
    {
      "rank": 66,
      "model_name": "llama-3.1-70b",
      "median": 943.1192414775771,
      "p2.5": 932.1947510939127,
      "p97.5": 954.0437318612416,
      "rank_p2.5": 56,
      "rank_p97.5": 73,
      "n_match": 7727,
      "useful": 1241,
      "creative": 280,
      "complete": 934,
      "clear_formatting": 1057,
      "incorrect": 289,
      "superficial": 495,
      "instructions_not_followed": 178,
      "total_prefs": 4474,
      "positive_prefs_ratio": 0.7849798837729102,
      "win_rate": 0.5072528453470208,
      "mean_win_prob": 0.4861512010893519,
      "style_controlled": {
        "median": 956.2580103432948,
        "p2.5": 945.1730038094468,
        "p97.5": 967.7141873148299,
        "rank": 62,
        "rank_p2.5": 56,
        "rank_p97.5": 70
      }
    },
    {
      "rank": 67,
      "model_name": "gemma-2-27b-it-q8",
      "median": 934.5200065667232,
      "p2.5": 899.7106993609988,
      "p97.5": 969.3293137724476,
      "rank_p2.5": 49,
      "rank_p97.5": 81,
      "n_match": 1107,
      "useful": 279,
      "creative": 81,
      "complete": 178,
      "clear_formatting": 257,
      "incorrect": 46,
      "superficial": 81,
      "instructions_not_followed": 29,
      "total_prefs": 951,
      "positive_prefs_ratio": 0.8359621451104101,
      "win_rate": 0.5354330708661418,
      "mean_win_prob": 0.4838413423515401,
      "style_controlled": {
        "median": 942.5225300100179,
        "p2.5": 894.7126663547344,
        "p97.5": 975.3348741740887,
        "rank": 68,
        "rank_p2.5": 53,
        "rank_p97.5": 83
      }
    },
    {
      "rank": 68,
      "model_name": "ministral-8b-instruct-2410",
      "median": 930.4759809555812,
      "p2.5": 922.3776168754257,
      "p97.5": 938.5743450357367,
      "rank_p2.5": 62,
      "rank_p97.5": 78,
      "n_match": 11905,
      "useful": 1391,
      "creative": 306,
      "complete": 1065,
      "clear_formatting": 1267,
      "incorrect": 509,
      "superficial": 667,
      "instructions_not_followed": 261,
      "total_prefs": 5466,
      "positive_prefs_ratio": 0.7371020856201976,
      "win_rate": 0.4477699530516432,
      "mean_win_prob": 0.48274793916779224,
      "style_controlled": {
        "median": 915.3437305412376,
        "p2.5": 908.1149153399258,
        "p97.5": 924.1472381825672,
        "rank": 77,
        "rank_p2.5": 69,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 69,
      "model_name": "aya-expanse-8b",
      "median": 929.4887102728893,
      "p2.5": 911.6159038291476,
      "p97.5": 947.3615167166311,
      "rank_p2.5": 61,
      "rank_p97.5": 79,
      "n_match": 1801,
      "useful": 141,
      "creative": 33,
      "complete": 133,
      "clear_formatting": 127,
      "incorrect": 62,
      "superficial": 63,
      "instructions_not_followed": 31,
      "total_prefs": 590,
      "positive_prefs_ratio": 0.735593220338983,
      "win_rate": 0.480946123521682,
      "mean_win_prob": 0.4824803084318806,
      "style_controlled": {
        "median": 918.0924125334036,
        "p2.5": 902.1097791295465,
        "p97.5": 938.3948098239351,
        "rank": 76,
        "rank_p2.5": 64,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 70,
      "model_name": "jamba-1.5-large",
      "median": 926.5496500543894,
      "p2.5": 887.9702604779766,
      "p97.5": 965.1290396308021,
      "rank_p2.5": 50,
      "rank_p97.5": 81,
      "n_match": 345,
      "useful": 19,
      "creative": 4,
      "complete": 18,
      "clear_formatting": 15,
      "incorrect": 5,
      "superficial": 9,
      "instructions_not_followed": 4,
      "total_prefs": 74,
      "positive_prefs_ratio": 0.7567567567567568,
      "win_rate": 0.49201277955271566,
      "mean_win_prob": 0.48168195672423275,
      "style_controlled": {
        "median": 919.5374201204446,
        "p2.5": 888.2861035447944,
        "p97.5": 962.4690566177685,
        "rank": 74,
        "rank_p2.5": 58,
        "rank_p97.5": 84
      }
    },
    {
      "rank": 71,
      "model_name": "qwq-32b",
      "median": 924.2191819519556,
      "p2.5": 906.1105591903478,
      "p97.5": 942.3278047135636,
      "rank_p2.5": 62,
      "rank_p97.5": 80,
      "n_match": 1952,
      "useful": 222,
      "creative": 106,
      "complete": 249,
      "clear_formatting": 170,
      "incorrect": 92,
      "superficial": 84,
      "instructions_not_followed": 88,
      "total_prefs": 1011,
      "positive_prefs_ratio": 0.7388724035608308,
      "win_rate": 0.43544303797468353,
      "mean_win_prob": 0.48104718275737696,
      "style_controlled": {
        "median": 881.4709350585778,
        "p2.5": 866.5007475972551,
        "p97.5": 896.6174433870442,
        "rank": 83,
        "rank_p2.5": 75,
        "rank_p97.5": 87
      }
    },
    {
      "rank": 72,
      "model_name": "llama-3.1-405b",
      "median": 922.4369058878755,
      "p2.5": 914.5832742611068,
      "p97.5": 930.2905375146443,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 13281,
      "useful": 1736,
      "creative": 426,
      "complete": 1350,
      "clear_formatting": 1415,
      "incorrect": 643,
      "superficial": 692,
      "instructions_not_followed": 430,
      "total_prefs": 6692,
      "positive_prefs_ratio": 0.736252241482367,
      "win_rate": 0.4449099762201336,
      "mean_win_prob": 0.4805606848602648,
      "style_controlled": {
        "median": 936.2159637600953,
        "p2.5": 927.2530081633898,
        "p97.5": 943.5274542053389,
        "rank": 69,
        "rank_p2.5": 62,
        "rank_p97.5": 76
      }
    },
    {
      "rank": 73,
      "model_name": "gpt-5-nano",
      "median": 920.5922286922253,
      "p2.5": 906.9127885707056,
      "p97.5": 934.2716688137448,
      "rank_p2.5": 65,
      "rank_p97.5": 79,
      "n_match": 3646,
      "useful": 118,
      "creative": 28,
      "complete": 105,
      "clear_formatting": 66,
      "incorrect": 59,
      "superficial": 79,
      "instructions_not_followed": 69,
      "total_prefs": 524,
      "positive_prefs_ratio": 0.6049618320610687,
      "win_rate": 0.3336762688614541,
      "mean_win_prob": 0.4800562007846389,
      "style_controlled": {
        "median": 986.2907512062337,
        "p2.5": 973.4603565767145,
        "p97.5": 999.3434929227022,
        "rank": 53,
        "rank_p2.5": 41,
        "rank_p97.5": 60
      }
    },
    {
      "rank": 74,
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 920.5574567668805,
      "p2.5": 911.6712737560725,
      "p97.5": 929.4436397776884,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 8877,
      "useful": 1088,
      "creative": 215,
      "complete": 616,
      "clear_formatting": 778,
      "incorrect": 246,
      "superficial": 545,
      "instructions_not_followed": 177,
      "total_prefs": 3665,
      "positive_prefs_ratio": 0.7358799454297408,
      "win_rate": 0.4260756569448455,
      "mean_win_prob": 0.48004668199991235,
      "style_controlled": {
        "median": 946.9836182446796,
        "p2.5": 938.2096010473468,
        "p97.5": 956.6681668872297,
        "rank": 66,
        "rank_p2.5": 60,
        "rank_p97.5": 74
      }
    },
    {
      "rank": 75,
      "model_name": "gemma-2-9b-it",
      "median": 920.1693951112068,
      "p2.5": 909.1536101818863,
      "p97.5": 931.1851800405274,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 7040,
      "useful": 983,
      "creative": 308,
      "complete": 665,
      "clear_formatting": 940,
      "incorrect": 278,
      "superficial": 467,
      "instructions_not_followed": 178,
      "total_prefs": 3819,
      "positive_prefs_ratio": 0.7583136946844724,
      "win_rate": 0.47050722974523757,
      "mean_win_prob": 0.47994042701254114,
      "style_controlled": {
        "median": 922.3951663343711,
        "p2.5": 912.0030142439766,
        "p97.5": 935.2932361635069,
        "rank": 73,
        "rank_p2.5": 66,
        "rank_p97.5": 80
      }
    },
    {
      "rank": 76,
      "model_name": "hermes-4-70b",
      "median": 918.3289078316985,
      "p2.5": 903.9281086768225,
      "p97.5": 932.7297069865746,
      "rank_p2.5": 65,
      "rank_p97.5": 80,
      "n_match": 3212,
      "useful": 119,
      "creative": 22,
      "complete": 44,
      "clear_formatting": 77,
      "incorrect": 43,
      "superficial": 96,
      "instructions_not_followed": 36,
      "total_prefs": 437,
      "positive_prefs_ratio": 0.5995423340961098,
      "win_rate": 0.33282325937260904,
      "mean_win_prob": 0.47943589734594283,
      "style_controlled": {
        "median": 949.0751308467428,
        "p2.5": 935.8024349234136,
        "p97.5": 964.8086465185401,
        "rank": 65,
        "rank_p2.5": 57,
        "rank_p97.5": 74
      }
    },
    {
      "rank": 77,
      "model_name": "llama-3.1-8b",
      "median": 916.2752664049635,
      "p2.5": 908.6070227335363,
      "p97.5": 923.9435100763908,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 13347,
      "useful": 1140,
      "creative": 269,
      "complete": 798,
      "clear_formatting": 820,
      "incorrect": 558,
      "superficial": 674,
      "instructions_not_followed": 367,
      "total_prefs": 4626,
      "positive_prefs_ratio": 0.6543450064850843,
      "win_rate": 0.4096063329054343,
      "mean_win_prob": 0.4788717896591611,
      "style_controlled": {
        "median": 918.9256819105619,
        "p2.5": 911.9342110038002,
        "p97.5": 926.7813530086304,
        "rank": 75,
        "rank_p2.5": 68,
        "rank_p97.5": 80
      }
    },
    {
      "rank": 78,
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 915.4158256910289,
      "p2.5": 902.061807289561,
      "p97.5": 928.7698440924968,
      "rank_p2.5": 66,
      "rank_p97.5": 80,
      "n_match": 3752,
      "useful": 340,
      "creative": 101,
      "complete": 238,
      "clear_formatting": 263,
      "incorrect": 102,
      "superficial": 204,
      "instructions_not_followed": 97,
      "total_prefs": 1345,
      "positive_prefs_ratio": 0.7003717472118959,
      "win_rate": 0.3879051119278316,
      "mean_win_prob": 0.4786353530488531,
      "style_controlled": {
        "median": 915.1310931198394,
        "p2.5": 900.1387863253092,
        "p97.5": 928.8106567766141,
        "rank": 78,
        "rank_p2.5": 66,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 79,
      "model_name": "c4ai-command-r-08-2024",
      "median": 911.0042119003234,
      "p2.5": 901.7138405456795,
      "p97.5": 920.2945832549673,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 7987,
      "useful": 812,
      "creative": 187,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 250,
      "superficial": 456,
      "instructions_not_followed": 157,
      "total_prefs": 2902,
      "positive_prefs_ratio": 0.7026188835286009,
      "win_rate": 0.40279913872654566,
      "mean_win_prob": 0.47741834041342274,
      "style_controlled": {
        "median": 933.9836190121036,
        "p2.5": 924.4672500129228,
        "p97.5": 944.1808520691768,
        "rank": 70,
        "rank_p2.5": 62,
        "rank_p97.5": 78
      }
    },
    {
      "rank": 80,
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 898.3367780038609,
      "p2.5": 889.8035851914827,
      "p97.5": 906.8699708162391,
      "rank_p2.5": 73,
      "rank_p97.5": 81,
      "n_match": 9547,
      "useful": 988,
      "creative": 211,
      "complete": 671,
      "clear_formatting": 750,
      "incorrect": 436,
      "superficial": 530,
      "instructions_not_followed": 226,
      "total_prefs": 3812,
      "positive_prefs_ratio": 0.6873032528856243,
      "win_rate": 0.3990280086967643,
      "mean_win_prob": 0.47389232296409506,
      "style_controlled": {
        "median": 901.7264840707794,
        "p2.5": 894.4948435144354,
        "p97.5": 910.1391572533425,
        "rank": 79,
        "rank_p2.5": 72,
        "rank_p97.5": 83
      }
    },
    {
      "rank": 81,
      "model_name": "qwen2.5-7b-instruct",
      "median": 875.337371398102,
      "p2.5": 850.8824738941581,
      "p97.5": 899.792268902046,
      "rank_p2.5": 77,
      "rank_p97.5": 85,
      "n_match": 2199,
      "useful": 360,
      "creative": 92,
      "complete": 284,
      "clear_formatting": 325,
      "incorrect": 168,
      "superficial": 167,
      "instructions_not_followed": 88,
      "total_prefs": 1484,
      "positive_prefs_ratio": 0.7149595687331537,
      "win_rate": 0.45285359801488834,
      "mean_win_prob": 0.4673683397188576,
      "style_controlled": {
        "median": 867.095515873111,
        "p2.5": 843.948625406898,
        "p97.5": 889.3390240512205,
        "rank": 85,
        "rank_p2.5": 77,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 82,
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 859.1639978079719,
      "p2.5": 841.1291756256331,
      "p97.5": 877.1988199903105,
      "rank_p2.5": 81,
      "rank_p97.5": 85,
      "n_match": 3938,
      "useful": 637,
      "creative": 143,
      "complete": 388,
      "clear_formatting": 452,
      "incorrect": 192,
      "superficial": 336,
      "instructions_not_followed": 162,
      "total_prefs": 2310,
      "positive_prefs_ratio": 0.7012987012987013,
      "win_rate": 0.40750158931977115,
      "mean_win_prob": 0.46268373268202284,
      "style_controlled": {
        "median": 898.1604709876566,
        "p2.5": 877.4416996880661,
        "p97.5": 917.3949714984199,
        "rank": 80,
        "rank_p2.5": 70,
        "rank_p97.5": 85
      }
    },
    {
      "rank": 83,
      "model_name": "lfm-40b",
      "median": 851.7854378540493,
      "p2.5": 839.0202220769968,
      "p97.5": 864.5506536311019,
      "rank_p2.5": 81,
      "rank_p97.5": 85,
      "n_match": 4974,
      "useful": 650,
      "creative": 130,
      "complete": 362,
      "clear_formatting": 480,
      "incorrect": 180,
      "superficial": 405,
      "instructions_not_followed": 121,
      "total_prefs": 2328,
      "positive_prefs_ratio": 0.6967353951890034,
      "win_rate": 0.3713458755426918,
      "mean_win_prob": 0.4605192909754538,
      "style_controlled": {
        "median": 886.2988097039055,
        "p2.5": 874.1178595865067,
        "p97.5": 897.6077286856905,
        "rank": 82,
        "rank_p2.5": 75,
        "rank_p97.5": 86
      }
    },
    {
      "rank": 84,
      "model_name": "phi-3.5-mini-instruct",
      "median": 833.3018792456671,
      "p2.5": 810.4181405148031,
      "p97.5": 856.1856179765311,
      "rank_p2.5": 81,
      "rank_p97.5": 87,
      "n_match": 3651,
      "useful": 582,
      "creative": 207,
      "complete": 480,
      "clear_formatting": 491,
      "incorrect": 362,
      "superficial": 414,
      "instructions_not_followed": 300,
      "total_prefs": 2836,
      "positive_prefs_ratio": 0.6205923836389281,
      "win_rate": 0.3793103448275862,
      "mean_win_prob": 0.45502061120354187,
      "style_controlled": {
        "median": 847.1965006145981,
        "p2.5": 828.1494092917555,
        "p97.5": 866.3589098450888,
        "rank": 87,
        "rank_p2.5": 83,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 85,
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 827.248513134412,
      "p2.5": 816.3264671074209,
      "p97.5": 838.170559161403,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "n_match": 7556,
      "useful": 884,
      "creative": 156,
      "complete": 441,
      "clear_formatting": 557,
      "incorrect": 304,
      "superficial": 676,
      "instructions_not_followed": 282,
      "total_prefs": 3300,
      "positive_prefs_ratio": 0.6175757575757576,
      "win_rate": 0.33665934943125125,
      "mean_win_prob": 0.45319555084341595,
      "style_controlled": {
        "median": 877.0563048272201,
        "p2.5": 867.3107440581123,
        "p97.5": 888.0838199404743,
        "rank": 84,
        "rank_p2.5": 78,
        "rank_p97.5": 87
      }
    },
    {
      "rank": 86,
      "model_name": "olmo-3-32b-think",
      "median": 819.2858481300324,
      "p2.5": 782.466055522279,
      "p97.5": 856.1056407377857,
      "rank_p2.5": 81,
      "rank_p97.5": 88,
      "n_match": 646,
      "useful": 46,
      "creative": 13,
      "complete": 44,
      "clear_formatting": 20,
      "incorrect": 37,
      "superficial": 33,
      "instructions_not_followed": 38,
      "total_prefs": 231,
      "positive_prefs_ratio": 0.5324675324675324,
      "win_rate": 0.2134387351778656,
      "mean_win_prob": 0.4507763035761386,
      "style_controlled": {
        "median": 897.0350239218486,
        "p2.5": 856.6047093261765,
        "p97.5": 926.988478614772,
        "rank": 81,
        "rank_p2.5": 68,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 87,
      "model_name": "mistral-nemo-2407",
      "median": 818.2253399075382,
      "p2.5": 807.5338127122096,
      "p97.5": 828.9168671028667,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "n_match": 8676,
      "useful": 1207,
      "creative": 193,
      "complete": 598,
      "clear_formatting": 743,
      "incorrect": 376,
      "superficial": 821,
      "instructions_not_followed": 299,
      "total_prefs": 4237,
      "positive_prefs_ratio": 0.646919990559358,
      "win_rate": 0.32752084912812734,
      "mean_win_prob": 0.4504524928083391,
      "style_controlled": {
        "median": 866.0581855368448,
        "p2.5": 855.6348394576845,
        "p97.5": 876.8463549453338,
        "rank": 86,
        "rank_p2.5": 79,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 88,
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 779.1761994623147,
      "p2.5": 761.7130511841576,
      "p97.5": 796.6393477404719,
      "rank_p2.5": 87,
      "rank_p97.5": 88,
      "n_match": 2338,
      "useful": 203,
      "creative": 24,
      "complete": 53,
      "clear_formatting": 134,
      "incorrect": 85,
      "superficial": 238,
      "instructions_not_followed": 50,
      "total_prefs": 787,
      "positive_prefs_ratio": 0.5260482846251588,
      "win_rate": 0.2738916256157635,
      "mean_win_prob": 0.4382598879825788,
      "style_controlled": {
        "median": 847.1845222979285,
        "p2.5": 833.2435697551733,
        "p97.5": 867.9753699537454,
        "rank": 88,
        "rank_p2.5": 81,
        "rank_p97.5": 88
      }
    }
  ]
}