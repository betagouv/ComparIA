{
  "timestamp": 1768365128.0,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1136.6618722817334,
      "p2.5": 1093.8975414598756,
      "p97.5": 1177.9233259171629,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 9,
      "total_output_tokens": 353926,
      "conso_all_conv": 0.0,
      "n_match": 191,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6798455740416425,
      "win_rate": 0.5832258064516129,
      "useful": 5,
      "creative": 3,
      "complete": 12,
      "clear_formatting": 8,
      "incorrect": 4,
      "superficial": 1,
      "instructions_not_followed": 0,
      "total_prefs": 33,
      "positive_prefs_ratio": 0.8484848484848485
    },
    {
      "model_name": "gemini-3-flash-preview",
      "median": 1132.0908012458908,
      "p2.5": 1086.5404688238186,
      "p97.5": 1174.370189447724,
      "rank": 2,
      "rank_p2.5": 1,
      "rank_p97.5": 11,
      "total_output_tokens": 165140,
      "conso_all_conv": 14.447299322399997,
      "n_match": 174,
      "mean_conso_per_match": 0.08303045587586205,
      "mean_conso_per_token": 8.748515999999999e-05,
      "mean_win_prob": 0.6742363693371289,
      "win_rate": 0.6205232558139535,
      "useful": 34,
      "creative": 18,
      "complete": 48,
      "clear_formatting": 31,
      "incorrect": 12,
      "superficial": 8,
      "instructions_not_followed": 3,
      "total_prefs": 154,
      "positive_prefs_ratio": 0.8506493506493507
    },
    {
      "model_name": "mistral-large-2512",
      "median": 1119.566630717371,
      "p2.5": 1105.0046805338463,
      "p97.5": 1133.067570483641,
      "rank": 3,
      "rank_p2.5": 1,
      "rank_p97.5": 6,
      "total_output_tokens": 3263055,
      "conso_all_conv": 164.43780632009998,
      "n_match": 1442,
      "mean_conso_per_match": 0.11403453975041608,
      "mean_conso_per_token": 5.039381999999999e-05,
      "mean_win_prob": 0.6586332315342088,
      "win_rate": 0.5722746185852983,
      "useful": 357,
      "creative": 96,
      "complete": 349,
      "clear_formatting": 276,
      "incorrect": 69,
      "superficial": 53,
      "instructions_not_followed": 50,
      "total_prefs": 1250,
      "positive_prefs_ratio": 0.8624
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1114.562093526603,
      "p2.5": 1106.6694464066384,
      "p97.5": 1122.9253798610353,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 5,
      "total_output_tokens": 8177056,
      "conso_all_conv": 162.7771921716267,
      "n_match": 5387,
      "mean_conso_per_match": 0.03021666830733742,
      "mean_conso_per_token": 1.990657666666667e-05,
      "mean_win_prob": 0.6523065412024422,
      "win_rate": 0.5854233197177868,
      "useful": 652,
      "creative": 166,
      "complete": 608,
      "clear_formatting": 499,
      "incorrect": 143,
      "superficial": 115,
      "instructions_not_followed": 67,
      "total_prefs": 2250,
      "positive_prefs_ratio": 0.8555555555555555
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1104.4883587547513,
      "p2.5": 1095.5556945047938,
      "p97.5": 1113.6512396628775,
      "rank": 5,
      "rank_p2.5": 4,
      "rank_p97.5": 9,
      "total_output_tokens": 5265898,
      "conso_all_conv": 460.68792907367987,
      "n_match": 3972,
      "mean_conso_per_match": 0.11598386935389725,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.6394222747823916,
      "win_rate": 0.5811256610425586,
      "useful": 442,
      "creative": 151,
      "complete": 512,
      "clear_formatting": 389,
      "incorrect": 105,
      "superficial": 93,
      "instructions_not_followed": 34,
      "total_prefs": 1726,
      "positive_prefs_ratio": 0.86558516801854
    },
    {
      "model_name": "gemini-3-pro-preview",
      "median": 1104.430537296194,
      "p2.5": 1090.9381560488023,
      "p97.5": 1116.532807310942,
      "rank": 6,
      "rank_p2.5": 3,
      "rank_p97.5": 10,
      "total_output_tokens": 3413894,
      "conso_all_conv": 183.53742783859997,
      "n_match": 1974,
      "mean_conso_per_match": 0.09297742038429584,
      "mean_conso_per_token": 5.376189999999999e-05,
      "mean_win_prob": 0.6393477680066618,
      "win_rate": 0.5589716312056738,
      "useful": 388,
      "creative": 131,
      "complete": 341,
      "clear_formatting": 275,
      "incorrect": 52,
      "superficial": 72,
      "instructions_not_followed": 31,
      "total_prefs": 1290,
      "positive_prefs_ratio": 0.8798449612403101
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1102.837322973744,
      "p2.5": 1091.6066623075762,
      "p97.5": 1114.594769217295,
      "rank": 7,
      "rank_p2.5": 4,
      "rank_p97.5": 10,
      "total_output_tokens": 2854780,
      "conso_all_conv": 162.97231034559994,
      "n_match": 2562,
      "mean_conso_per_match": 0.06361136235191255,
      "mean_conso_per_token": 5.708751999999998e-05,
      "mean_win_prob": 0.6372923992245966,
      "win_rate": 0.5786875,
      "useful": 284,
      "creative": 85,
      "complete": 270,
      "clear_formatting": 255,
      "incorrect": 98,
      "superficial": 64,
      "instructions_not_followed": 30,
      "total_prefs": 1086,
      "positive_prefs_ratio": 0.8232044198895028
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1099.5604532348875,
      "p2.5": 1092.4001604988464,
      "p97.5": 1106.297793809405,
      "rank": 8,
      "rank_p2.5": 5,
      "rank_p97.5": 10,
      "total_output_tokens": 12296785,
      "conso_all_conv": 1075.7862032105998,
      "n_match": 8684,
      "mean_conso_per_match": 0.12388141446460153,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.6330505880067738,
      "win_rate": 0.6317699216950714,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1091.3927785765068,
      "p2.5": 1081.7649800064166,
      "p97.5": 1101.1207774396553,
      "rank": 9,
      "rank_p2.5": 7,
      "rank_p97.5": 13,
      "total_output_tokens": 4435392,
      "conso_all_conv": 208.57759099007998,
      "n_match": 4385,
      "mean_conso_per_match": 0.04756615529990422,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.6223966528800393,
      "win_rate": 0.5851128848346637,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "magistral-medium",
      "median": 1088.4616101668814,
      "p2.5": 1076.646536092063,
      "p97.5": 1100.4633134866235,
      "rank": 10,
      "rank_p2.5": 7,
      "rank_p97.5": 16,
      "total_output_tokens": 2039837,
      "conso_all_conv": 40.60617162800334,
      "n_match": 2207,
      "mean_conso_per_match": 0.01839880907476363,
      "mean_conso_per_token": 1.9906576666666667e-05,
      "mean_win_prob": 0.618546255726468,
      "win_rate": 0.5809968282736747,
      "useful": 146,
      "creative": 41,
      "complete": 150,
      "clear_formatting": 104,
      "incorrect": 50,
      "superficial": 28,
      "instructions_not_followed": 3,
      "total_prefs": 522,
      "positive_prefs_ratio": 0.8448275862068966
    },
    {
      "model_name": "gpt-5.2",
      "median": 1084.9778792240427,
      "p2.5": 1066.9148771038751,
      "p97.5": 1100.9384052335965,
      "rank": 11,
      "rank_p2.5": 7,
      "rank_p97.5": 21,
      "total_output_tokens": 1214080,
      "conso_all_conv": 57.681474995199984,
      "n_match": 1080,
      "mean_conso_per_match": 0.05340877314370369,
      "mean_conso_per_token": 4.751043999999999e-05,
      "mean_win_prob": 0.6139523780918913,
      "win_rate": 0.5311574074074074,
      "useful": 221,
      "creative": 51,
      "complete": 181,
      "clear_formatting": 168,
      "incorrect": 31,
      "superficial": 71,
      "instructions_not_followed": 32,
      "total_prefs": 755,
      "positive_prefs_ratio": 0.8225165562913908
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1082.6501166415055,
      "p2.5": 1075.9019084771146,
      "p97.5": 1089.8535505993232,
      "rank": 12,
      "rank_p2.5": 10,
      "rank_p97.5": 17,
      "total_output_tokens": 10334728,
      "conso_all_conv": 66.49629253218666,
      "n_match": 7688,
      "mean_conso_per_match": 0.008649361671720429,
      "mean_conso_per_token": 6.4342566666666654e-06,
      "mean_win_prob": 0.6108724961596184,
      "win_rate": 0.5726369194744373,
      "useful": 1443,
      "creative": 488,
      "complete": 1619,
      "clear_formatting": 1184,
      "incorrect": 395,
      "superficial": 275,
      "instructions_not_followed": 137,
      "total_prefs": 5541,
      "positive_prefs_ratio": 0.8543584190579318
    },
    {
      "model_name": "grok-4.1-fast",
      "median": 1079.3552886547704,
      "p2.5": 1063.5117528462051,
      "p97.5": 1093.6034141360017,
      "rank": 13,
      "rank_p2.5": 9,
      "rank_p97.5": 23,
      "total_output_tokens": 1643832,
      "conso_all_conv": 419.40194306711993,
      "n_match": 1461,
      "mean_conso_per_match": 0.2870649849877618,
      "mean_conso_per_token": 0.0002551367433333333,
      "mean_win_prob": 0.6064994020399401,
      "win_rate": 0.5481930184804928,
      "useful": 262,
      "creative": 108,
      "complete": 226,
      "clear_formatting": 198,
      "incorrect": 66,
      "superficial": 69,
      "instructions_not_followed": 24,
      "total_prefs": 953,
      "positive_prefs_ratio": 0.8331584470094439
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1078.5535900960422,
      "p2.5": 1064.6618609272389,
      "p97.5": 1092.5107236104905,
      "rank": 14,
      "rank_p2.5": 10,
      "rank_p97.5": 22,
      "total_output_tokens": 2271778,
      "conso_all_conv": 106.83204156571999,
      "n_match": 1588,
      "mean_conso_per_match": 0.0672745853688413,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.6054329750415947,
      "win_rate": 0.5211405166981726,
      "useful": 188,
      "creative": 68,
      "complete": 172,
      "clear_formatting": 144,
      "incorrect": 57,
      "superficial": 44,
      "instructions_not_followed": 25,
      "total_prefs": 698,
      "positive_prefs_ratio": 0.8194842406876791
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1077.9171037246513,
      "p2.5": 1069.4301950532301,
      "p97.5": 1087.578061884986,
      "rank": 15,
      "rank_p2.5": 11,
      "rank_p97.5": 21,
      "total_output_tokens": 5638296,
      "conso_all_conv": 265.14504173904,
      "n_match": 5388,
      "mean_conso_per_match": 0.049210289855055676,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.6045856684128954,
      "win_rate": 0.6141481069042316,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "gpt-5.1",
      "median": 1073.8631863042565,
      "p2.5": 1061.2983581291728,
      "p97.5": 1086.5998084627468,
      "rank": 16,
      "rank_p2.5": 11,
      "rank_p97.5": 24,
      "total_output_tokens": 2679521,
      "conso_all_conv": 127.30522169923997,
      "n_match": 2202,
      "mean_conso_per_match": 0.057813452179491354,
      "mean_conso_per_token": 4.751043999999999e-05,
      "mean_win_prob": 0.5991758696132978,
      "win_rate": 0.5381562216167121,
      "useful": 292,
      "creative": 82,
      "complete": 300,
      "clear_formatting": 237,
      "incorrect": 51,
      "superficial": 77,
      "instructions_not_followed": 33,
      "total_prefs": 1072,
      "positive_prefs_ratio": 0.8498134328358209
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1072.9426973140394,
      "p2.5": 1064.8181964214536,
      "p97.5": 1081.1543997757256,
      "rank": 17,
      "rank_p2.5": 13,
      "rank_p97.5": 22,
      "total_output_tokens": 5465021,
      "conso_all_conv": 478.1082365883599,
      "n_match": 5031,
      "mean_conso_per_match": 0.09503244615153247,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.5979444277737465,
      "win_rate": 0.5360226595110316,
      "useful": 789,
      "creative": 289,
      "complete": 598,
      "clear_formatting": 427,
      "incorrect": 120,
      "superficial": 163,
      "instructions_not_followed": 72,
      "total_prefs": 2458,
      "positive_prefs_ratio": 0.855573637103336
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1072.0555838233838,
      "p2.5": 1058.930280136181,
      "p97.5": 1088.1012948258008,
      "rank": 18,
      "rank_p2.5": 11,
      "rank_p97.5": 26,
      "total_output_tokens": 3460376,
      "conso_all_conv": 11.5813594156,
      "n_match": 1423,
      "mean_conso_per_match": 0.008138692491637385,
      "mean_conso_per_token": 3.34685e-06,
      "mean_win_prob": 0.5967565838539347,
      "win_rate": 0.5377074542897328,
      "useful": 194,
      "creative": 79,
      "complete": 277,
      "clear_formatting": 183,
      "incorrect": 91,
      "superficial": 49,
      "instructions_not_followed": 37,
      "total_prefs": 910,
      "positive_prefs_ratio": 0.8054945054945055
    },
    {
      "model_name": "glm-4.5",
      "median": 1071.3044267278742,
      "p2.5": 1057.992607168282,
      "p97.5": 1083.8186071374728,
      "rank": 19,
      "rank_p2.5": 12,
      "rank_p97.5": 26,
      "total_output_tokens": 5128869,
      "conso_all_conv": 109.79790435558,
      "n_match": 2178,
      "mean_conso_per_match": 0.050412260952975206,
      "mean_conso_per_token": 2.140782e-05,
      "mean_win_prob": 0.5957499872655307,
      "win_rate": 0.5414882866329811,
      "useful": 227,
      "creative": 75,
      "complete": 282,
      "clear_formatting": 209,
      "incorrect": 48,
      "superficial": 57,
      "instructions_not_followed": 33,
      "total_prefs": 931,
      "positive_prefs_ratio": 0.8517722878625135
    },
    {
      "model_name": "DeepSeek-V3.2",
      "median": 1068.1651752017397,
      "p2.5": 1048.2423938804316,
      "p97.5": 1088.5461163720558,
      "rank": 20,
      "rank_p2.5": 11,
      "rank_p97.5": 31,
      "total_output_tokens": 980158,
      "conso_all_conv": 46.09265526692,
      "n_match": 838,
      "mean_conso_per_match": 0.055003168576276844,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.5915354517562028,
      "win_rate": 0.5388636363636363,
      "useful": 194,
      "creative": 67,
      "complete": 173,
      "clear_formatting": 149,
      "incorrect": 66,
      "superficial": 54,
      "instructions_not_followed": 28,
      "total_prefs": 731,
      "positive_prefs_ratio": 0.7975376196990424
    },
    {
      "model_name": "glm-4.7",
      "median": 1064.2652189263863,
      "p2.5": 1006.5686980220211,
      "p97.5": 1119.4573473493367,
      "rank": 21,
      "rank_p2.5": 3,
      "rank_p97.5": 47,
      "total_output_tokens": 246621,
      "conso_all_conv": 5.27961797622,
      "n_match": 103,
      "mean_conso_per_match": 0.051258426953592236,
      "mean_conso_per_token": 2.140782e-05,
      "mean_win_prob": 0.5862828832243193,
      "win_rate": 0.4968,
      "useful": 19,
      "creative": 7,
      "complete": 19,
      "clear_formatting": 18,
      "incorrect": 9,
      "superficial": 6,
      "instructions_not_followed": 3,
      "total_prefs": 81,
      "positive_prefs_ratio": 0.7777777777777778
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1063.5791818285531,
      "p2.5": 1056.8571621997498,
      "p97.5": 1070.6057711064716,
      "rank": 22,
      "rank_p2.5": 18,
      "rank_p97.5": 28,
      "total_output_tokens": 9532567,
      "conso_all_conv": 41.26845260684666,
      "n_match": 7332,
      "mean_conso_per_match": 0.0056285396354128015,
      "mean_conso_per_token": 4.329206666666666e-06,
      "mean_win_prob": 0.5853570622253123,
      "win_rate": 0.55807010365521,
      "useful": 1241,
      "creative": 372,
      "complete": 1399,
      "clear_formatting": 1042,
      "incorrect": 428,
      "superficial": 317,
      "instructions_not_followed": 171,
      "total_prefs": 4970,
      "positive_prefs_ratio": 0.8156941649899396
    },
    {
      "model_name": "grok-4-fast",
      "median": 1062.5509744042997,
      "p2.5": 1051.6316232487175,
      "p97.5": 1073.5902626937732,
      "rank": 23,
      "rank_p2.5": 17,
      "rank_p97.5": 30,
      "total_output_tokens": 3521762,
      "conso_all_conv": 898.5308874750866,
      "n_match": 2623,
      "mean_conso_per_match": 0.34255847787841653,
      "mean_conso_per_token": 0.0002551367433333333,
      "mean_win_prob": 0.5839684708105267,
      "win_rate": 0.5293971766501335,
      "useful": 228,
      "creative": 53,
      "complete": 176,
      "clear_formatting": 137,
      "incorrect": 61,
      "superficial": 52,
      "instructions_not_followed": 20,
      "total_prefs": 727,
      "positive_prefs_ratio": 0.8170563961485557
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1062.4770151094392,
      "p2.5": 1051.0294103876995,
      "p97.5": 1074.2914192506084,
      "rank": 24,
      "rank_p2.5": 17,
      "rank_p97.5": 30,
      "total_output_tokens": 1853912,
      "conso_all_conv": 162.18978794591996,
      "n_match": 2540,
      "mean_conso_per_match": 0.06385424722280314,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.5838685429662928,
      "win_rate": 0.5355651831429696,
      "useful": 234,
      "creative": 62,
      "complete": 198,
      "clear_formatting": 215,
      "incorrect": 41,
      "superficial": 64,
      "instructions_not_followed": 20,
      "total_prefs": 834,
      "positive_prefs_ratio": 0.8501199040767387
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1060.2107345097518,
      "p2.5": 1051.354218681237,
      "p97.5": 1068.7771906886258,
      "rank": 25,
      "rank_p2.5": 20,
      "rank_p97.5": 30,
      "total_output_tokens": 3390690,
      "conso_all_conv": 20.3890553402,
      "n_match": 4151,
      "mean_conso_per_match": 0.004911841806841725,
      "mean_conso_per_token": 6.0132466666666666e-06,
      "mean_win_prob": 0.5808035886354258,
      "win_rate": 0.5282076608046254,
      "useful": 417,
      "creative": 112,
      "complete": 354,
      "clear_formatting": 324,
      "incorrect": 162,
      "superficial": 121,
      "instructions_not_followed": 39,
      "total_prefs": 1529,
      "positive_prefs_ratio": 0.789404839764552
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1060.1350714310493,
      "p2.5": 1046.128536808966,
      "p97.5": 1075.060878154041,
      "rank": 26,
      "rank_p2.5": 17,
      "rank_p97.5": 32,
      "total_output_tokens": 3474752,
      "conso_all_conv": 39.83854131029333,
      "n_match": 1537,
      "mean_conso_per_match": 0.02591967554345695,
      "mean_conso_per_token": 1.1465146666666665e-05,
      "mean_win_prob": 0.5807011638207579,
      "win_rate": 0.5305338541666667,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "kimi-k2",
      "median": 1060.0152942271393,
      "p2.5": 1041.1881197549576,
      "p97.5": 1077.253436390084,
      "rank": 27,
      "rank_p2.5": 15,
      "rank_p97.5": 33,
      "total_output_tokens": 1348007,
      "conso_all_conv": 76.95437657263997,
      "n_match": 964,
      "mean_conso_per_match": 0.07982819146539416,
      "mean_conso_per_token": 5.708751999999998e-05,
      "mean_win_prob": 0.5805390092709625,
      "win_rate": 0.5202609603340292,
      "useful": 222,
      "creative": 54,
      "complete": 126,
      "clear_formatting": 118,
      "incorrect": 50,
      "superficial": 57,
      "instructions_not_followed": 11,
      "total_prefs": 638,
      "positive_prefs_ratio": 0.8150470219435737
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1056.3602517172255,
      "p2.5": 1040.7870579373748,
      "p97.5": 1071.9690016675672,
      "rank": 28,
      "rank_p2.5": 18,
      "rank_p97.5": 34,
      "total_output_tokens": 2302993,
      "conso_all_conv": 108.29995003981999,
      "n_match": 1229,
      "mean_conso_per_match": 0.08812038245713587,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.5755835374977671,
      "win_rate": 0.5324165988608626,
      "useful": 145,
      "creative": 43,
      "complete": 126,
      "clear_formatting": 90,
      "incorrect": 46,
      "superficial": 35,
      "instructions_not_followed": 19,
      "total_prefs": 504,
      "positive_prefs_ratio": 0.8015873015873016
    },
    {
      "model_name": "kimi-k2-thinking",
      "median": 1055.8105176193565,
      "p2.5": 1030.38902160648,
      "p97.5": 1078.3446416163795,
      "rank": 29,
      "rank_p2.5": 15,
      "rank_p97.5": 38,
      "total_output_tokens": 856510,
      "conso_all_conv": 48.896031755199985,
      "n_match": 488,
      "mean_conso_per_match": 0.10019678638360653,
      "mean_conso_per_token": 5.708751999999998e-05,
      "mean_win_prob": 0.5748370265224573,
      "win_rate": 0.5262704918032787,
      "useful": 95,
      "creative": 24,
      "complete": 94,
      "clear_formatting": 64,
      "incorrect": 32,
      "superficial": 35,
      "instructions_not_followed": 25,
      "total_prefs": 369,
      "positive_prefs_ratio": 0.7506775067750677
    },
    {
      "model_name": "command-a",
      "median": 1055.1740098519824,
      "p2.5": 1048.338707216333,
      "p97.5": 1062.1101361167957,
      "rank": 30,
      "rank_p2.5": 24,
      "rank_p97.5": 31,
      "total_output_tokens": 6905002,
      "conso_all_conv": 125.82665212840666,
      "n_match": 7088,
      "mean_conso_per_match": 0.01775206717387227,
      "mean_conso_per_token": 1.8222536666666665e-05,
      "mean_win_prob": 0.573972305137022,
      "win_rate": 0.5461272573363432,
      "useful": 1352,
      "creative": 318,
      "complete": 1233,
      "clear_formatting": 1139,
      "incorrect": 331,
      "superficial": 364,
      "instructions_not_followed": 124,
      "total_prefs": 4861,
      "positive_prefs_ratio": 0.8315161489405473
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1048.685268761748,
      "p2.5": 1039.2296849281636,
      "p97.5": 1059.1668433225634,
      "rank": 31,
      "rank_p2.5": 26,
      "rank_p97.5": 35,
      "total_output_tokens": 2445603,
      "conso_all_conv": 14.706014087739998,
      "n_match": 3147,
      "mean_conso_per_match": 0.0046730264022052746,
      "mean_conso_per_token": 6.013246666666666e-06,
      "mean_win_prob": 0.5651351768397839,
      "win_rate": 0.52184361093452,
      "useful": 248,
      "creative": 53,
      "complete": 198,
      "clear_formatting": 181,
      "incorrect": 88,
      "superficial": 84,
      "instructions_not_followed": 32,
      "total_prefs": 884,
      "positive_prefs_ratio": 0.7692307692307693
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1048.204067517769,
      "p2.5": 1038.7365915160078,
      "p97.5": 1058.2278022131136,
      "rank": 32,
      "rank_p2.5": 26,
      "rank_p97.5": 35,
      "total_output_tokens": 3546728,
      "conso_all_conv": 310.2860665564799,
      "n_match": 3907,
      "mean_conso_per_match": 0.07941798478538006,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.5644783119992962,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1045.240499589795,
      "p2.5": 1024.8273706201649,
      "p97.5": 1065.6148836216157,
      "rank": 33,
      "rank_p2.5": 22,
      "rank_p97.5": 40,
      "total_output_tokens": 803114,
      "conso_all_conv": 24.276369369199998,
      "n_match": 720,
      "mean_conso_per_match": 0.03371717967944444,
      "mean_conso_per_token": 3.0227799999999995e-05,
      "mean_win_prob": 0.5604286192293303,
      "win_rate": 0.5415181058495822,
      "useful": 105,
      "creative": 32,
      "complete": 76,
      "clear_formatting": 89,
      "incorrect": 30,
      "superficial": 38,
      "instructions_not_followed": 15,
      "total_prefs": 385,
      "positive_prefs_ratio": 0.7844155844155845
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1043.7833105088534,
      "p2.5": 1035.2038953651181,
      "p97.5": 1051.0496779234188,
      "rank": 34,
      "rank_p2.5": 29,
      "rank_p97.5": 36,
      "total_output_tokens": 7426282,
      "conso_all_conv": 92.59632991613334,
      "n_match": 6709,
      "mean_conso_per_match": 0.01380180800657823,
      "mean_conso_per_token": 1.2468733333333335e-05,
      "mean_win_prob": 0.5584347901525929,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "glm-4.6",
      "median": 1037.933948736411,
      "p2.5": 1024.7778268364837,
      "p97.5": 1050.0302282295268,
      "rank": 35,
      "rank_p2.5": 30,
      "rank_p97.5": 40,
      "total_output_tokens": 5837076,
      "conso_all_conv": 124.95907233432001,
      "n_match": 2089,
      "mean_conso_per_match": 0.05981765071054093,
      "mean_conso_per_token": 2.1407820000000003e-05,
      "mean_win_prob": 0.5504155065744422,
      "win_rate": 0.5014648157012924,
      "useful": 221,
      "creative": 83,
      "complete": 246,
      "clear_formatting": 173,
      "incorrect": 64,
      "superficial": 59,
      "instructions_not_followed": 42,
      "total_prefs": 888,
      "positive_prefs_ratio": 0.8141891891891891
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1037.1001721166706,
      "p2.5": 1030.396280391384,
      "p97.5": 1043.5136714055855,
      "rank": 36,
      "rank_p2.5": 33,
      "rank_p97.5": 38,
      "total_output_tokens": 10347021,
      "conso_all_conv": 33.17786079678,
      "n_match": 8383,
      "mean_conso_per_match": 0.0039577550753644285,
      "mean_conso_per_token": 3.2065133333333334e-06,
      "mean_win_prob": 0.5492705170066849,
      "win_rate": 0.5181116545389478,
      "useful": 1273,
      "creative": 383,
      "complete": 1396,
      "clear_formatting": 1114,
      "incorrect": 660,
      "superficial": 325,
      "instructions_not_followed": 234,
      "total_prefs": 5385,
      "positive_prefs_ratio": 0.7736304549675023
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1030.6811723338055,
      "p2.5": 1022.8352960026889,
      "p97.5": 1037.7122668887123,
      "rank": 37,
      "rank_p2.5": 35,
      "rank_p97.5": 41,
      "total_output_tokens": 5550374,
      "conso_all_conv": 73.10073823583333,
      "n_match": 7173,
      "mean_conso_per_match": 0.010191096923997398,
      "mean_conso_per_token": 1.3170416666666667e-05,
      "mean_win_prob": 0.5404416247784861,
      "win_rate": 0.5152977269557941,
      "useful": 1227,
      "creative": 286,
      "complete": 945,
      "clear_formatting": 1010,
      "incorrect": 244,
      "superficial": 455,
      "instructions_not_followed": 97,
      "total_prefs": 4264,
      "positive_prefs_ratio": 0.8133208255159474
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1029.971174524064,
      "p2.5": 1022.5571749255183,
      "p97.5": 1038.2270336838214,
      "rank": 38,
      "rank_p2.5": 35,
      "rank_p97.5": 41,
      "total_output_tokens": 6034223,
      "conso_all_conv": 1011.3126436118333,
      "n_match": 7387,
      "mean_conso_per_match": 0.13690437845022788,
      "mean_conso_per_token": 0.00016759616666666666,
      "mean_win_prob": 0.5394636868286673,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1027.6451327504928,
      "p2.5": 1017.955022576487,
      "p97.5": 1037.0928519857528,
      "rank": 39,
      "rank_p2.5": 35,
      "rank_p97.5": 43,
      "total_output_tokens": 4204566,
      "conso_all_conv": 13.48199693988,
      "n_match": 3736,
      "mean_conso_per_match": 0.003608671557783726,
      "mean_conso_per_token": 3.2065133333333334e-06,
      "mean_win_prob": 0.5362581275992493,
      "win_rate": 0.49503747323340475,
      "useful": 326,
      "creative": 114,
      "complete": 344,
      "clear_formatting": 292,
      "incorrect": 184,
      "superficial": 136,
      "instructions_not_followed": 72,
      "total_prefs": 1468,
      "positive_prefs_ratio": 0.7329700272479565
    },
    {
      "model_name": "deepseek-r1",
      "median": 1024.4658728771717,
      "p2.5": 1014.8862424527945,
      "p97.5": 1033.8648859948016,
      "rank": 40,
      "rank_p2.5": 36,
      "rank_p97.5": 44,
      "total_output_tokens": 3693582,
      "conso_all_conv": 173.69342680068,
      "n_match": 3510,
      "mean_conso_per_match": 0.04948530678082051,
      "mean_conso_per_token": 4.7025739999999995e-05,
      "mean_win_prob": 0.5318728503298658,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "qwen3-32b",
      "median": 1021.7586592163594,
      "p2.5": 1007.4491642415732,
      "p97.5": 1035.7588784327688,
      "rank": 41,
      "rank_p2.5": 36,
      "rank_p97.5": 47,
      "total_output_tokens": 3167877,
      "conso_all_conv": 22.605780199379993,
      "n_match": 1519,
      "mean_conso_per_match": 0.014882014614470042,
      "mean_conso_per_token": 7.1359399999999974e-06,
      "mean_win_prob": 0.5281355833405014,
      "win_rate": 0.5167458745874588,
      "useful": 374,
      "creative": 91,
      "complete": 289,
      "clear_formatting": 236,
      "incorrect": 145,
      "superficial": 117,
      "instructions_not_followed": 66,
      "total_prefs": 1318,
      "positive_prefs_ratio": 0.7511380880121397
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1020.6608325904929,
      "p2.5": 1006.7522367791574,
      "p97.5": 1033.6520263375735,
      "rank": 42,
      "rank_p2.5": 37,
      "rank_p97.5": 48,
      "total_output_tokens": 2802833,
      "conso_all_conv": 8.593981145163333,
      "n_match": 1740,
      "mean_conso_per_match": 0.004939069623657088,
      "mean_conso_per_token": 3.066176666666667e-06,
      "mean_win_prob": 0.526619334452011,
      "win_rate": 0.47207015526164464,
      "useful": 169,
      "creative": 62,
      "complete": 175,
      "clear_formatting": 159,
      "incorrect": 71,
      "superficial": 69,
      "instructions_not_followed": 45,
      "total_prefs": 750,
      "positive_prefs_ratio": 0.7533333333333333
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1017.7327280710138,
      "p2.5": 1010.9459607901986,
      "p97.5": 1024.6188000247632,
      "rank": 43,
      "rank_p2.5": 40,
      "rank_p97.5": 46,
      "total_output_tokens": 7677266,
      "conso_all_conv": 152.82808421939336,
      "n_match": 8829,
      "mean_conso_per_match": 0.017309784145361124,
      "mean_conso_per_token": 1.990657666666667e-05,
      "mean_win_prob": 0.5225734646836975,
      "win_rate": 0.5274413863404689,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "mistral-saba",
      "median": 1014.0797580318517,
      "p2.5": 1006.1137113385624,
      "p97.5": 1021.6658487484406,
      "rank": 44,
      "rank_p2.5": 41,
      "rank_p97.5": 49,
      "total_output_tokens": 4349650,
      "conso_all_conv": 26.155518363666665,
      "n_match": 4934,
      "mean_conso_per_match": 0.005301077901026888,
      "mean_conso_per_token": 6.0132466666666666e-06,
      "mean_win_prob": 0.5175230548088922,
      "win_rate": 0.48257855260490573,
      "useful": 745,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 586,
      "incorrect": 241,
      "superficial": 337,
      "instructions_not_followed": 113,
      "total_prefs": 2779,
      "positive_prefs_ratio": 0.751349406261245
    },
    {
      "model_name": "llama-maverick",
      "median": 1011.6776343678882,
      "p2.5": 1001.3281372795165,
      "p97.5": 1023.1078646384328,
      "rank": 45,
      "rank_p2.5": 41,
      "rank_p97.5": 50,
      "total_output_tokens": 2371444,
      "conso_all_conv": 35.79142171548,
      "n_match": 3018,
      "mean_conso_per_match": 0.011859317997176938,
      "mean_conso_per_token": 1.509267e-05,
      "mean_win_prob": 0.5142006423178721,
      "win_rate": 0.47889993373094764,
      "useful": 285,
      "creative": 56,
      "complete": 197,
      "clear_formatting": 189,
      "incorrect": 95,
      "superficial": 153,
      "instructions_not_followed": 48,
      "total_prefs": 1023,
      "positive_prefs_ratio": 0.7106549364613881
    },
    {
      "model_name": "llama-4-scout",
      "median": 1008.8121330961142,
      "p2.5": 1001.0352502190957,
      "p97.5": 1016.5247973169177,
      "rank": 46,
      "rank_p2.5": 43,
      "rank_p97.5": 51,
      "total_output_tokens": 4630736,
      "conso_all_conv": 23.296723435039997,
      "n_match": 5778,
      "mean_conso_per_match": 0.004031970134136379,
      "mean_conso_per_token": 5.03089e-06,
      "mean_win_prob": 0.5102363648517552,
      "win_rate": 0.4862495673243337,
      "useful": 862,
      "creative": 198,
      "complete": 736,
      "clear_formatting": 693,
      "incorrect": 304,
      "superficial": 419,
      "instructions_not_followed": 114,
      "total_prefs": 3326,
      "positive_prefs_ratio": 0.748346361996392
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1008.1893008772947,
      "p2.5": 997.8696252739605,
      "p97.5": 1018.1488969345353,
      "rank": 47,
      "rank_p2.5": 43,
      "rank_p97.5": 52,
      "total_output_tokens": 3776093,
      "conso_all_conv": 34.894837652069995,
      "n_match": 3221,
      "mean_conso_per_match": 0.010833541649199006,
      "mean_conso_per_token": 9.24099e-06,
      "mean_win_prob": 0.5093746121616394,
      "win_rate": 0.46506521739130435,
      "useful": 278,
      "creative": 79,
      "complete": 303,
      "clear_formatting": 176,
      "incorrect": 73,
      "superficial": 115,
      "instructions_not_followed": 79,
      "total_prefs": 1103,
      "positive_prefs_ratio": 0.7579329102447869
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1004.4052951493206,
      "p2.5": 996.2214419887391,
      "p97.5": 1013.328000199985,
      "rank": 48,
      "rank_p2.5": 45,
      "rank_p97.5": 53,
      "total_output_tokens": 4470466,
      "conso_all_conv": 26.882014772946665,
      "n_match": 5079,
      "mean_conso_per_match": 0.005292777076776268,
      "mean_conso_per_token": 6.0132466666666666e-06,
      "mean_win_prob": 0.5041387214764687,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "o4-mini",
      "median": 1002.3709378922836,
      "p2.5": 991.3388937389975,
      "p97.5": 1012.8200783851955,
      "rank": 49,
      "rank_p2.5": 45,
      "rank_p97.5": 54,
      "total_output_tokens": 2940457,
      "conso_all_conv": 15.205769650586666,
      "n_match": 2830,
      "mean_conso_per_match": 0.005373063480772673,
      "mean_conso_per_token": 5.171226666666666e-06,
      "mean_win_prob": 0.5013238235467228,
      "win_rate": 0.4738494167550371,
      "useful": 527,
      "creative": 111,
      "complete": 374,
      "clear_formatting": 286,
      "incorrect": 133,
      "superficial": 210,
      "instructions_not_followed": 45,
      "total_prefs": 1686,
      "positive_prefs_ratio": 0.7698695136417556
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1000.9984992483475,
      "p2.5": 977.3788989476241,
      "p97.5": 1024.5845375499118,
      "rank": 50,
      "rank_p2.5": 41,
      "rank_p97.5": 62,
      "total_output_tokens": 449540,
      "conso_all_conv": 2.8924557419333334,
      "n_match": 762,
      "mean_conso_per_match": 0.0037958736770778655,
      "mean_conso_per_token": 6.434256666666667e-06,
      "mean_win_prob": 0.49942492947185835,
      "win_rate": 0.5826115485564304,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "EuroLLM-22B-Instruct-2512",
      "median": 999.2889222146674,
      "p2.5": 961.1578885084332,
      "p97.5": 1034.4063721387884,
      "rank": 51,
      "rank_p2.5": 36,
      "rank_p97.5": 70,
      "total_output_tokens": 192139,
      "conso_all_conv": 1.1014509076933332,
      "n_match": 250,
      "mean_conso_per_match": 0.004405803630773333,
      "mean_conso_per_token": 5.7325733333333326e-06,
      "mean_win_prob": 0.49705979970382236,
      "win_rate": 0.4106854838709677,
      "useful": 74,
      "creative": 17,
      "complete": 46,
      "clear_formatting": 54,
      "incorrect": 15,
      "superficial": 25,
      "instructions_not_followed": 10,
      "total_prefs": 241,
      "positive_prefs_ratio": 0.7925311203319502
    },
    {
      "model_name": "gpt-5",
      "median": 998.6047273252868,
      "p2.5": 989.6564509578702,
      "p97.5": 1008.9922982230871,
      "rank": 52,
      "rank_p2.5": 47,
      "rank_p97.5": 55,
      "total_output_tokens": 4019540,
      "conso_all_conv": 190.97011399759995,
      "n_match": 3843,
      "mean_conso_per_match": 0.0496929778812386,
      "mean_conso_per_token": 4.751043999999999e-05,
      "mean_win_prob": 0.4961133349258067,
      "win_rate": 0.45334721499219155,
      "useful": 144,
      "creative": 54,
      "complete": 158,
      "clear_formatting": 68,
      "incorrect": 29,
      "superficial": 57,
      "instructions_not_followed": 32,
      "total_prefs": 542,
      "positive_prefs_ratio": 0.7822878228782287
    },
    {
      "model_name": "lfm2-8b-a1b",
      "median": 995.5104153167199,
      "p2.5": 980.6607388924691,
      "p97.5": 1009.8267565317326,
      "rank": 53,
      "rank_p2.5": 47,
      "rank_p97.5": 61,
      "total_output_tokens": 1157820,
      "conso_all_conv": 3.2251114694000003,
      "n_match": 1495,
      "mean_conso_per_match": 0.002157265196923077,
      "mean_conso_per_token": 2.7855033333333337e-06,
      "mean_win_prob": 0.49183375367471904,
      "win_rate": 0.4601872909698997,
      "useful": 163,
      "creative": 33,
      "complete": 117,
      "clear_formatting": 137,
      "incorrect": 125,
      "superficial": 109,
      "instructions_not_followed": 53,
      "total_prefs": 737,
      "positive_prefs_ratio": 0.6105834464043419
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 990.379432302487,
      "p2.5": 982.1313614749159,
      "p97.5": 997.9325915889274,
      "rank": 54,
      "rank_p2.5": 51,
      "rank_p97.5": 60,
      "total_output_tokens": 4082212,
      "conso_all_conv": 29.130419899279993,
      "n_match": 5113,
      "mean_conso_per_match": 0.005697324447345979,
      "mean_conso_per_token": 7.135939999999998e-06,
      "mean_win_prob": 0.4847417197272536,
      "win_rate": 0.46256796401329947,
      "useful": 722,
      "creative": 142,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 277,
      "superficial": 353,
      "instructions_not_followed": 109,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7203934922436626
    },
    {
      "model_name": "qwen-3-8b",
      "median": 987.2055526037802,
      "p2.5": 975.437310328198,
      "p97.5": 998.8199817362473,
      "rank": 55,
      "rank_p2.5": 51,
      "rank_p97.5": 63,
      "total_output_tokens": 4936726,
      "conso_all_conv": 18.600892426359998,
      "n_match": 2381,
      "mean_conso_per_match": 0.0078122185746997055,
      "mean_conso_per_token": 3.7678599999999997e-06,
      "mean_win_prob": 0.4803585090129633,
      "win_rate": 0.4322805543889122,
      "useful": 171,
      "creative": 59,
      "complete": 201,
      "clear_formatting": 135,
      "incorrect": 136,
      "superficial": 73,
      "instructions_not_followed": 59,
      "total_prefs": 834,
      "positive_prefs_ratio": 0.6786570743405276
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 986.1972726412005,
      "p2.5": 973.8756292353831,
      "p97.5": 995.7775890856657,
      "rank": 56,
      "rank_p2.5": 52,
      "rank_p97.5": 64,
      "total_output_tokens": 5038136,
      "conso_all_conv": 15.447815046693334,
      "n_match": 2911,
      "mean_conso_per_match": 0.005306703897867858,
      "mean_conso_per_token": 3.066176666666667e-06,
      "mean_win_prob": 0.4789667727791941,
      "win_rate": 0.4616975945017182,
      "useful": 222,
      "creative": 55,
      "complete": 221,
      "clear_formatting": 163,
      "incorrect": 157,
      "superficial": 123,
      "instructions_not_followed": 58,
      "total_prefs": 999,
      "positive_prefs_ratio": 0.6616616616616616
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 984.5871091916472,
      "p2.5": 974.4782496258653,
      "p97.5": 994.9142868434449,
      "rank": 57,
      "rank_p2.5": 52,
      "rank_p97.5": 64,
      "total_output_tokens": 2818040,
      "conso_all_conv": 16.94556963653333,
      "n_match": 3318,
      "mean_conso_per_match": 0.005107163844645368,
      "mean_conso_per_token": 6.013246666666666e-06,
      "mean_win_prob": 0.4767450573693937,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 984.5079864603183,
      "p2.5": 978.0354287193073,
      "p97.5": 991.2222986585655,
      "rank": 58,
      "rank_p2.5": 54,
      "rank_p97.5": 62,
      "total_output_tokens": 7485716,
      "conso_all_conv": 93.33739661306667,
      "n_match": 9113,
      "mean_conso_per_match": 0.010242225020637185,
      "mean_conso_per_token": 1.2468733333333335e-05,
      "mean_win_prob": 0.47663590994689525,
      "win_rate": 0.4752331833644245,
      "useful": 1515,
      "creative": 312,
      "complete": 1183,
      "clear_formatting": 1129,
      "incorrect": 501,
      "superficial": 700,
      "instructions_not_followed": 165,
      "total_prefs": 5505,
      "positive_prefs_ratio": 0.7518619436875568
    },
    {
      "model_name": "o3-mini",
      "median": 981.0266563817613,
      "p2.5": 966.234645691537,
      "p97.5": 996.0201897627318,
      "rank": 59,
      "rank_p2.5": 52,
      "rank_p97.5": 68,
      "total_output_tokens": 1655945,
      "conso_all_conv": 8.563266942533334,
      "n_match": 1619,
      "mean_conso_per_match": 0.005289232206629607,
      "mean_conso_per_token": 5.171226666666667e-06,
      "mean_win_prob": 0.4718362068541292,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 980.428210759537,
      "p2.5": 973.0067865407703,
      "p97.5": 987.9379038353428,
      "rank": 60,
      "rank_p2.5": 55,
      "rank_p97.5": 64,
      "total_output_tokens": 5207480,
      "conso_all_conv": 26.929059442133333,
      "n_match": 6990,
      "mean_conso_per_match": 0.0038525120804196473,
      "mean_conso_per_token": 5.171226666666666e-06,
      "mean_win_prob": 0.471011693486626,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 976.6463090746122,
      "p2.5": 969.1984203857269,
      "p97.5": 985.3736782324061,
      "rank": 61,
      "rank_p2.5": 57,
      "rank_p97.5": 67,
      "total_output_tokens": 3563423,
      "conso_all_conv": 20.927662570929996,
      "n_match": 5717,
      "mean_conso_per_match": 0.003660602163884904,
      "mean_conso_per_token": 5.872909999999999e-06,
      "mean_win_prob": 0.46580538867389903,
      "win_rate": 0.46743047052649994,
      "useful": 928,
      "creative": 159,
      "complete": 529,
      "clear_formatting": 649,
      "incorrect": 276,
      "superficial": 448,
      "instructions_not_followed": 103,
      "total_prefs": 3092,
      "positive_prefs_ratio": 0.732535575679172
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 976.4708640457088,
      "p2.5": 963.1115611960586,
      "p97.5": 989.5779966073106,
      "rank": 62,
      "rank_p2.5": 55,
      "rank_p97.5": 69,
      "total_output_tokens": 2056962,
      "conso_all_conv": 25.6477106548,
      "n_match": 2103,
      "mean_conso_per_match": 0.012195773017023301,
      "mean_conso_per_token": 1.2468733333333333e-05,
      "mean_win_prob": 0.4655640526410921,
      "win_rate": 0.4107322872087494,
      "useful": 206,
      "creative": 41,
      "complete": 136,
      "clear_formatting": 122,
      "incorrect": 108,
      "superficial": 134,
      "instructions_not_followed": 67,
      "total_prefs": 814,
      "positive_prefs_ratio": 0.6203931203931204
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 971.7560650746025,
      "p2.5": 955.9512351699207,
      "p97.5": 986.3751232455843,
      "rank": 63,
      "rank_p2.5": 57,
      "rank_p97.5": 72,
      "total_output_tokens": 1057810,
      "conso_all_conv": 3.9856799865999997,
      "n_match": 1302,
      "mean_conso_per_match": 0.003061198146390169,
      "mean_conso_per_token": 3.7678599999999997e-06,
      "mean_win_prob": 0.45908543235443267,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "minimax-m2",
      "median": 971.3341213048363,
      "p2.5": 956.0597305198243,
      "p97.5": 985.8103786883744,
      "rank": 64,
      "rank_p2.5": 57,
      "rank_p97.5": 72,
      "total_output_tokens": 2524396,
      "conso_all_conv": 20.440202705066667,
      "n_match": 1375,
      "mean_conso_per_match": 0.014865601967321214,
      "mean_conso_per_token": 8.097066666666667e-06,
      "mean_win_prob": 0.4585063230665616,
      "win_rate": 0.42609454545454545,
      "useful": 163,
      "creative": 38,
      "complete": 133,
      "clear_formatting": 96,
      "incorrect": 64,
      "superficial": 82,
      "instructions_not_followed": 39,
      "total_prefs": 615,
      "positive_prefs_ratio": 0.6991869918699187
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 971.1927108762984,
      "p2.5": 963.0260550232214,
      "p97.5": 979.9669622044487,
      "rank": 65,
      "rank_p2.5": 61,
      "rank_p97.5": 69,
      "total_output_tokens": 4057254,
      "conso_all_conv": 50.588818191600005,
      "n_match": 5583,
      "mean_conso_per_match": 0.009061224823858141,
      "mean_conso_per_token": 1.2468733333333335e-05,
      "mean_win_prob": 0.45831226664303887,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 970.6491820382507,
      "p2.5": 923.7335015182503,
      "p97.5": 1014.5248066636022,
      "rank": 66,
      "rank_p2.5": 44,
      "rank_p97.5": 82,
      "total_output_tokens": 143976,
      "conso_all_conv": 6.840363109439998,
      "n_match": 172,
      "mean_conso_per_match": 0.03976955296186046,
      "mean_conso_per_token": 4.751043999999999e-05,
      "mean_win_prob": 0.45756651157017103,
      "win_rate": 0.5449418604651163,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 970.057489816708,
      "p2.5": 961.3755978571082,
      "p97.5": 979.0523962291716,
      "rank": 67,
      "rank_p2.5": 61,
      "rank_p97.5": 70,
      "total_output_tokens": 3224219,
      "conso_all_conv": 282.07131509003995,
      "n_match": 5683,
      "mean_conso_per_match": 0.049634227536519436,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.45675490217709436,
      "win_rate": 0.4921080415273623,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "phi-4",
      "median": 965.4216179698175,
      "p2.5": 959.5523361968511,
      "p97.5": 971.4950915261167,
      "rank": 68,
      "rank_p2.5": 64,
      "rank_p97.5": 71,
      "total_output_tokens": 8226489,
      "conso_all_conv": 37.92312711132,
      "n_match": 9968,
      "mean_conso_per_match": 0.003804487069755217,
      "mean_conso_per_token": 4.60988e-06,
      "mean_win_prob": 0.45040469457078813,
      "win_rate": 0.46780074245008524,
      "useful": 1626,
      "creative": 336,
      "complete": 1193,
      "clear_formatting": 1311,
      "incorrect": 731,
      "superficial": 760,
      "instructions_not_followed": 253,
      "total_prefs": 6210,
      "positive_prefs_ratio": 0.7191626409017713
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 961.796341615712,
      "p2.5": 955.2987471749597,
      "p97.5": 967.9677026387005,
      "rank": 69,
      "rank_p2.5": 66,
      "rank_p97.5": 72,
      "total_output_tokens": 7488476,
      "conso_all_conv": 28.215529181359997,
      "n_match": 9278,
      "mean_conso_per_match": 0.003041121920819142,
      "mean_conso_per_token": 3.7678599999999997e-06,
      "mean_win_prob": 0.44545039363397837,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 960.7451475856564,
      "p2.5": 952.2963578076594,
      "p97.5": 968.660888643485,
      "rank": 70,
      "rank_p2.5": 66,
      "rank_p97.5": 73,
      "total_output_tokens": 3903917,
      "conso_all_conv": 341.53480337171993,
      "n_match": 5896,
      "mean_conso_per_match": 0.05792652703048167,
      "mean_conso_per_token": 8.748515999999998e-05,
      "mean_win_prob": 0.4440158800233731,
      "win_rate": 0.48863297150610585,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 960.3708777328543,
      "p2.5": 900.6299542552135,
      "p97.5": 1014.6700096178284,
      "rank": 71,
      "rank_p2.5": 44,
      "rank_p97.5": 83,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.19707414943999998,
      "n_match": 86,
      "mean_conso_per_match": 0.002291559877209302,
      "mean_conso_per_token": 3.7678599999999997e-06,
      "mean_win_prob": 0.4435053627451789,
      "win_rate": 0.46880952380952384,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 959.5182665067629,
      "p2.5": 899.8196266773415,
      "p97.5": 1015.3191677490905,
      "rank": 72,
      "rank_p2.5": 44,
      "rank_p97.5": 83,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.5409898832799999,
      "n_match": 142,
      "mean_conso_per_match": 0.003809787910422534,
      "mean_conso_per_token": 7.135939999999998e-06,
      "mean_win_prob": 0.44234283068848945,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 955.4047521662133,
      "p2.5": 948.5367414966248,
      "p97.5": 962.2326818743192,
      "rank": 73,
      "rank_p2.5": 69,
      "rank_p97.5": 75,
      "total_output_tokens": 10383810,
      "conso_all_conv": 2470.579070314,
      "n_match": 9973,
      "mean_conso_per_match": 0.24772676930853305,
      "mean_conso_per_token": 0.00023792606666666667,
      "mean_win_prob": 0.4367433870373405,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 951.2689408937782,
      "p2.5": 941.7120218129517,
      "p97.5": 960.4091759992139,
      "rank": 74,
      "rank_p2.5": 70,
      "rank_p97.5": 77,
      "total_output_tokens": 3108609,
      "conso_all_conv": 12.149055331769999,
      "n_match": 5115,
      "mean_conso_per_match": 0.002375181883043988,
      "mean_conso_per_token": 3.908196666666666e-06,
      "mean_win_prob": 0.4311301111619385,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "gpt-5-nano",
      "median": 947.4400551074772,
      "p2.5": 935.6943263039137,
      "p97.5": 958.0982030323331,
      "rank": 75,
      "rank_p2.5": 71,
      "rank_p97.5": 78,
      "total_output_tokens": 2792522,
      "conso_all_conv": 11.69751163014,
      "n_match": 2656,
      "mean_conso_per_match": 0.004404183595685241,
      "mean_conso_per_token": 4.188869999999999e-06,
      "mean_win_prob": 0.4259492969824915,
      "win_rate": 0.3978350903614458,
      "useful": 179,
      "creative": 46,
      "complete": 176,
      "clear_formatting": 121,
      "incorrect": 91,
      "superficial": 110,
      "instructions_not_followed": 112,
      "total_prefs": 835,
      "positive_prefs_ratio": 0.6251497005988024
    },
    {
      "model_name": "qwq-32b",
      "median": 947.3307055652926,
      "p2.5": 932.0057669364423,
      "p97.5": 962.4202922020223,
      "rank": 76,
      "rank_p2.5": 70,
      "rank_p97.5": 79,
      "total_output_tokens": 1895013,
      "conso_all_conv": 13.522699067219996,
      "n_match": 1566,
      "mean_conso_per_match": 0.008635184589540228,
      "mean_conso_per_token": 7.135939999999998e-06,
      "mean_win_prob": 0.42580157250301,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 944.3924178788652,
      "p2.5": 933.1976188899798,
      "p97.5": 955.4601374790227,
      "rank": 77,
      "rank_p2.5": 72,
      "rank_p97.5": 79,
      "total_output_tokens": 2863519,
      "conso_all_conv": 35.70445480593334,
      "n_match": 2981,
      "mean_conso_per_match": 0.011977341431041039,
      "mean_conso_per_token": 1.2468733333333335e-05,
      "mean_win_prob": 0.42183719685958715,
      "win_rate": 0.4397953706809795,
      "useful": 489,
      "creative": 130,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 190,
      "superficial": 342,
      "instructions_not_followed": 126,
      "total_prefs": 1995,
      "positive_prefs_ratio": 0.6701754385964912
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 939.897729252612,
      "p2.5": 932.6344845556285,
      "p97.5": 948.0090038144759,
      "rank": 78,
      "rank_p2.5": 74,
      "rank_p97.5": 79,
      "total_output_tokens": 4092822,
      "conso_all_conv": 973.7890400268,
      "n_match": 6735,
      "mean_conso_per_match": 0.144586345957951,
      "mean_conso_per_token": 0.00023792606666666667,
      "mean_win_prob": 0.4157925662421971,
      "win_rate": 0.46634743875278395,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "hermes-4-70b",
      "median": 933.1001493593242,
      "p2.5": 921.602778893615,
      "p97.5": 945.042365897112,
      "rank": 79,
      "rank_p2.5": 76,
      "rank_p97.5": 83,
      "total_output_tokens": 1181528,
      "conso_all_conv": 14.732157557866667,
      "n_match": 2508,
      "mean_conso_per_match": 0.00587406601190856,
      "mean_conso_per_token": 1.2468733333333335e-05,
      "mean_win_prob": 0.40669928833945435,
      "win_rate": 0.3761980830670926,
      "useful": 187,
      "creative": 31,
      "complete": 83,
      "clear_formatting": 116,
      "incorrect": 111,
      "superficial": 171,
      "instructions_not_followed": 70,
      "total_prefs": 769,
      "positive_prefs_ratio": 0.5422626788036411
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 931.7196877066816,
      "p2.5": 925.7443582791278,
      "p97.5": 938.0672521976993,
      "rank": 80,
      "rank_p2.5": 77,
      "rank_p97.5": 82,
      "total_output_tokens": 7372632,
      "conso_all_conv": 27.779045207519996,
      "n_match": 10444,
      "mean_conso_per_match": 0.002659809001103025,
      "mean_conso_per_token": 3.7678599999999993e-06,
      "mean_win_prob": 0.404860139286552,
      "win_rate": 0.4424234009957871,
      "useful": 1540,
      "creative": 373,
      "complete": 1014,
      "clear_formatting": 1159,
      "incorrect": 1043,
      "superficial": 978,
      "instructions_not_followed": 404,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.6275533712179389
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 927.8198225270328,
      "p2.5": 919.8606139128873,
      "p97.5": 936.0859692769271,
      "rank": 81,
      "rank_p2.5": 78,
      "rank_p97.5": 83,
      "total_output_tokens": 4319206,
      "conso_all_conv": 32.640023781699995,
      "n_match": 5959,
      "mean_conso_per_match": 0.005477433089729819,
      "mean_conso_per_token": 7.556949999999999e-06,
      "mean_win_prob": 0.39967889355281944,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 924.558683404665,
      "p2.5": 917.0435760427777,
      "p97.5": 932.02548348716,
      "rank": 82,
      "rank_p2.5": 79,
      "rank_p97.5": 83,
      "total_output_tokens": 6149486,
      "conso_all_conv": 43.882363126839984,
      "n_match": 7299,
      "mean_conso_per_match": 0.006012106196306341,
      "mean_conso_per_token": 7.1359399999999974e-06,
      "mean_win_prob": 0.3953631612959588,
      "win_rate": 0.43789423208658723,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 924.5033305400718,
      "p2.5": 907.3385952936716,
      "p97.5": 942.552342768268,
      "rank": 83,
      "rank_p2.5": 76,
      "rank_p97.5": 83,
      "total_output_tokens": 1186919,
      "conso_all_conv": 4.305576367276667,
      "n_match": 1417,
      "mean_conso_per_match": 0.0030385154320936255,
      "mean_conso_per_token": 3.6275233333333336e-06,
      "mean_win_prob": 0.39529004498197273,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 882.7043000876963,
      "p2.5": 869.579426297473,
      "p97.5": 896.0630403964497,
      "rank": 84,
      "rank_p2.5": 84,
      "rank_p97.5": 86,
      "total_output_tokens": 1575791,
      "conso_all_conv": 7.2642074150800005,
      "n_match": 2560,
      "mean_conso_per_match": 0.0028375810215156253,
      "mean_conso_per_token": 4.60988e-06,
      "mean_win_prob": 0.34158310979693135,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 879.9829744593812,
      "p2.5": 869.5143668099723,
      "p97.5": 891.1869671573525,
      "rank": 85,
      "rank_p2.5": 84,
      "rank_p97.5": 86,
      "total_output_tokens": 2000824,
      "conso_all_conv": 8.661980599626665,
      "n_match": 3578,
      "mean_conso_per_match": 0.0024209001116899565,
      "mean_conso_per_token": 4.329206666666666e-06,
      "mean_win_prob": 0.33820376303014876,
      "win_rate": 0.416296813862493,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "olmo-3-32b-think",
      "median": 870.5325371649399,
      "p2.5": 838.2146802747847,
      "p97.5": 904.9672698786495,
      "rank": 86,
      "rank_p2.5": 84,
      "rank_p97.5": 89,
      "total_output_tokens": 875594,
      "conso_all_conv": 6.248186248359998,
      "n_match": 377,
      "mean_conso_per_match": 0.016573438324562328,
      "mean_conso_per_token": 7.1359399999999974e-06,
      "mean_win_prob": 0.3265930578833985,
      "win_rate": 0.36514588859416447,
      "useful": 38,
      "creative": 10,
      "complete": 30,
      "clear_formatting": 14,
      "incorrect": 43,
      "superficial": 29,
      "instructions_not_followed": 31,
      "total_prefs": 195,
      "positive_prefs_ratio": 0.4717948717948718
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 858.9099907285341,
      "p2.5": 844.7417819272755,
      "p97.5": 873.194530580734,
      "rank": 87,
      "rank_p2.5": 86,
      "rank_p97.5": 88,
      "total_output_tokens": 2149046,
      "conso_all_conv": 6.589354700793333,
      "n_match": 2535,
      "mean_conso_per_match": 0.0025993509667823797,
      "mean_conso_per_token": 3.0661766666666664e-06,
      "mean_win_prob": 0.3125907014723345,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 849.8687760183573,
      "p2.5": 841.7223674796807,
      "p97.5": 858.7358532453263,
      "rank": 88,
      "rank_p2.5": 86,
      "rank_p97.5": 89,
      "total_output_tokens": 3305342,
      "conso_all_conv": 14.30950862201333,
      "n_match": 6251,
      "mean_conso_per_match": 0.0022891551147037805,
      "mean_conso_per_token": 4.329206666666666e-06,
      "mean_win_prob": 0.30191943091905105,
      "win_rate": 0.37482482802751554,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 839.5935864898674,
      "p2.5": 829.6237671176514,
      "p97.5": 849.5372617513586,
      "rank": 89,
      "rank_p2.5": 88,
      "rank_p97.5": 90,
      "total_output_tokens": 3041056,
      "conso_all_conv": 53.644106197759996,
      "n_match": 5455,
      "mean_conso_per_match": 0.009833933308480293,
      "mean_conso_per_token": 1.7639959999999998e-05,
      "mean_win_prob": 0.290036461218419,
      "win_rate": 0.36710357470210814,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 816.4987334694405,
      "p2.5": 800.9396786004604,
      "p97.5": 833.1498933972341,
      "rank": 90,
      "rank_p2.5": 90,
      "rank_p97.5": 91,
      "total_output_tokens": 533384,
      "conso_all_conv": 1.9348629056266669,
      "n_match": 1796,
      "mean_conso_per_match": 0.0010773178761841128,
      "mean_conso_per_token": 3.6275233333333336e-06,
      "mean_win_prob": 0.26432235036133783,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 761.9934025747327,
      "p2.5": 669.5545294185615,
      "p97.5": 853.198324151715,
      "rank": 91,
      "rank_p2.5": 88,
      "rank_p97.5": 93,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.1857604957633333,
      "n_match": 65,
      "mean_conso_per_match": 0.0028578537809743586,
      "mean_conso_per_token": 3.908196666666666e-06,
      "mean_win_prob": 0.20943977003833444,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 749.1329715363813,
      "p2.5": 701.0231614149202,
      "p97.5": 789.1746962177998,
      "rank": 92,
      "rank_p2.5": 91,
      "rank_p97.5": 93,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.6047563275600001,
      "n_match": 309,
      "mean_conso_per_match": 0.001957140218640777,
      "mean_conso_per_token": 4.609880000000001e-06,
      "mean_win_prob": 0.19772019030487983,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 719.5854639655307,
      "p2.5": 636.0627758148578,
      "p97.5": 805.6501597262626,
      "rank": 93,
      "rank_p2.5": 91,
      "rank_p97.5": 93,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.15797864116666668,
      "n_match": 80,
      "mean_conso_per_match": 0.0019747330145833335,
      "mean_conso_per_token": 3.6275233333333336e-06,
      "mean_win_prob": 0.1725824831512767,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}
