{
  "timestamp": 1769011714.676251,
  "models": [
    {
      "rank": 1,
      "model_name": "gemini-3-flash-preview",
      "median": 1172.6019700754073,
      "p2.5": 1134.073521351721,
      "p97.5": 1211.1304187990936,
      "rank_p2.5": 1,
      "rank_p97.5": 6,
      "n_match": 467,
      "useful": 66,
      "creative": 18,
      "complete": 71,
      "clear_formatting": 54,
      "incorrect": 18,
      "superficial": 16,
      "instructions_not_followed": 6,
      "total_prefs": 249,
      "positive_prefs_ratio": 0.8393574297188755,
      "win_rate": 0.6685714285714286,
      "mean_win_prob": 0.5410271662742042,
      "style_controlled": {
        "median": 705.3462066909437,
        "p2.5": 641.6292970550272,
        "p97.5": 771.1569252667159,
        "rank": 7,
        "rank_p2.5": 2,
        "rank_p97.5": 47
      }
    },
    {
      "rank": 2,
      "model_name": "mistral-large-2512",
      "median": 1146.0242179161387,
      "p2.5": 1129.0876989018345,
      "p97.5": 1162.9607369304426,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 2226,
      "useful": 271,
      "creative": 75,
      "complete": 303,
      "clear_formatting": 263,
      "incorrect": 51,
      "superficial": 63,
      "instructions_not_followed": 31,
      "total_prefs": 1057,
      "positive_prefs_ratio": 0.8628192999053926,
      "win_rate": 0.6138841078600115,
      "mean_win_prob": 0.5352734429065781,
      "style_controlled": {
        "median": 648.090045300592,
        "p2.5": 615.3147737074249,
        "p97.5": 699.4848237567828,
        "rank": 24,
        "rank_p2.5": 4,
        "rank_p97.5": 58
      }
    },
    {
      "rank": 3,
      "model_name": "mistral-medium-2508",
      "median": 1140.058558950714,
      "p2.5": 1129.957492000475,
      "p97.5": 1150.1596259009532,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 7017,
      "useful": 272,
      "creative": 77,
      "complete": 235,
      "clear_formatting": 240,
      "incorrect": 47,
      "superficial": 69,
      "instructions_not_followed": 35,
      "total_prefs": 975,
      "positive_prefs_ratio": 0.8451282051282051,
      "win_rate": 0.6390296886314265,
      "mean_win_prob": 0.5339622936727353,
      "style_controlled": {
        "median": 662.9981917424009,
        "p2.5": 628.4232439181766,
        "p97.5": 704.9463122943671,
        "rank": 16,
        "rank_p2.5": 4,
        "rank_p97.5": 52
      }
    },
    {
      "rank": 4,
      "model_name": "gemini-2.5-flash",
      "median": 1136.4101741912273,
      "p2.5": 1124.7494277264445,
      "p97.5": 1148.0709206560098,
      "rank_p2.5": 1,
      "rank_p97.5": 7,
      "n_match": 5081,
      "useful": 263,
      "creative": 86,
      "complete": 253,
      "clear_formatting": 230,
      "incorrect": 42,
      "superficial": 63,
      "instructions_not_followed": 22,
      "total_prefs": 959,
      "positive_prefs_ratio": 0.867570385818561,
      "win_rate": 0.6410321274981027,
      "mean_win_prob": 0.5331568282451215,
      "style_controlled": {
        "median": 676.5450698149243,
        "p2.5": 650.9618479113191,
        "p97.5": 725.7665001947952,
        "rank": 14,
        "rank_p2.5": 2,
        "rank_p97.5": 43
      }
    },
    {
      "rank": 5,
      "model_name": "gemini-3-pro-preview",
      "median": 1135.5129262545267,
      "p2.5": 1119.8228124048162,
      "p97.5": 1151.2030401042373,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 2741,
      "useful": 267,
      "creative": 77,
      "complete": 243,
      "clear_formatting": 194,
      "incorrect": 41,
      "superficial": 62,
      "instructions_not_followed": 26,
      "total_prefs": 910,
      "positive_prefs_ratio": 0.8582417582417582,
      "win_rate": 0.6055739253660841,
      "mean_win_prob": 0.5329583177708129,
      "style_controlled": {
        "median": 824.1365172015173,
        "p2.5": 793.2552858901598,
        "p97.5": 877.1214925369687,
        "rank": 1,
        "rank_p2.5": 1,
        "rank_p97.5": 3
      }
    },
    {
      "rank": 6,
      "model_name": "qwen3-max-2025-09-23",
      "median": 1127.0480011210516,
      "p2.5": 1112.719686998854,
      "p97.5": 1141.3763152432489,
      "rank_p2.5": 1,
      "rank_p97.5": 10,
      "n_match": 3290,
      "useful": 151,
      "creative": 40,
      "complete": 107,
      "clear_formatting": 141,
      "incorrect": 32,
      "superficial": 34,
      "instructions_not_followed": 11,
      "total_prefs": 516,
      "positive_prefs_ratio": 0.8507751937984496,
      "win_rate": 0.635001970831691,
      "mean_win_prob": 0.5310772519021328,
      "style_controlled": {
        "median": 675.2257765752317,
        "p2.5": 638.4730295964188,
        "p97.5": 724.6087274323174,
        "rank": 15,
        "rank_p2.5": 2,
        "rank_p97.5": 48
      }
    },
    {
      "rank": 7,
      "model_name": "magistral-medium",
      "median": 1117.1718431209824,
      "p2.5": 1101.380677309763,
      "p97.5": 1132.9630089322015,
      "rank_p2.5": 2,
      "rank_p97.5": 12,
      "n_match": 2668,
      "useful": 17,
      "creative": 4,
      "complete": 10,
      "clear_formatting": 12,
      "incorrect": 6,
      "superficial": 7,
      "instructions_not_followed": 0,
      "total_prefs": 56,
      "positive_prefs_ratio": 0.7678571428571429,
      "win_rate": 0.6241446725317693,
      "mean_win_prob": 0.5288635449196711,
      "style_controlled": {
        "median": 676.7440067333312,
        "p2.5": 649.7303952919344,
        "p97.5": 735.2763249254867,
        "rank": 13,
        "rank_p2.5": 2,
        "rank_p97.5": 44
      }
    },
    {
      "rank": 8,
      "model_name": "gemini-2.0-flash",
      "median": 1109.5012538000715,
      "p2.5": 1101.0418004041153,
      "p97.5": 1117.9607071960274,
      "rank_p2.5": 6,
      "rank_p97.5": 13,
      "n_match": 10633,
      "useful": 1479,
      "creative": 473,
      "complete": 1551,
      "clear_formatting": 1332,
      "incorrect": 208,
      "superficial": 304,
      "instructions_not_followed": 109,
      "total_prefs": 5456,
      "positive_prefs_ratio": 0.8861803519061584,
      "win_rate": 0.6905578417924096,
      "mean_win_prob": 0.5271298872027821,
      "style_controlled": {
        "median": 678.6033552806987,
        "p2.5": 646.0465452413323,
        "p97.5": 727.8667708032951,
        "rank": 12,
        "rank_p2.5": 2,
        "rank_p97.5": 44
      }
    },
    {
      "rank": 9,
      "model_name": "gpt-5.2",
      "median": 1103.5578227791043,
      "p2.5": 1083.2264968485354,
      "p97.5": 1123.8891487096732,
      "rank_p2.5": 5,
      "rank_p97.5": 24,
      "n_match": 1614,
      "useful": 223,
      "creative": 46,
      "complete": 165,
      "clear_formatting": 167,
      "incorrect": 27,
      "superficial": 77,
      "instructions_not_followed": 30,
      "total_prefs": 735,
      "positive_prefs_ratio": 0.817687074829932,
      "win_rate": 0.5357142857142857,
      "mean_win_prob": 0.525777884745397,
      "style_controlled": {
        "median": 694.587263919787,
        "p2.5": 655.2929308040535,
        "p97.5": 748.2527979882344,
        "rank": 8,
        "rank_p2.5": 2,
        "rank_p97.5": 41
      }
    },
    {
      "rank": 10,
      "model_name": "deepseek-v3-0324",
      "median": 1103.256046559025,
      "p2.5": 1092.0468834406702,
      "p97.5": 1114.46520967738,
      "rank_p2.5": 6,
      "rank_p97.5": 19,
      "n_match": 5390,
      "useful": 852,
      "creative": 256,
      "complete": 696,
      "clear_formatting": 779,
      "incorrect": 158,
      "superficial": 226,
      "instructions_not_followed": 77,
      "total_prefs": 3044,
      "positive_prefs_ratio": 0.8485545335085414,
      "win_rate": 0.6450783259293897,
      "mean_win_prob": 0.5257090330316664,
      "style_controlled": {
        "median": 659.3772470414651,
        "p2.5": 631.5391965877212,
        "p97.5": 712.8327395708347,
        "rank": 17,
        "rank_p2.5": 3,
        "rank_p97.5": 50
      }
    },
    {
      "rank": 11,
      "model_name": "claude-4-5-sonnet",
      "median": 1100.6149337220158,
      "p2.5": 1091.3661230717219,
      "p97.5": 1109.86374437231,
      "rank_p2.5": 7,
      "rank_p97.5": 19,
      "n_match": 7762,
      "useful": 294,
      "creative": 70,
      "complete": 167,
      "clear_formatting": 212,
      "incorrect": 52,
      "superficial": 112,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.7921108742004265,
      "win_rate": 0.5615625,
      "mean_win_prob": 0.5251056048644586,
      "style_controlled": {
        "median": 647.732192499368,
        "p2.5": 617.0047679532337,
        "p97.5": 700.4999315195473,
        "rank": 25,
        "rank_p2.5": 4,
        "rank_p97.5": 57
      }
    },
    {
      "rank": 12,
      "model_name": "gemma-3-27b",
      "median": 1090.6802257467798,
      "p2.5": 1082.3113053310349,
      "p97.5": 1099.049146162525,
      "rank_p2.5": 9,
      "rank_p97.5": 25,
      "n_match": 9542,
      "useful": 1013,
      "creative": 337,
      "complete": 1018,
      "clear_formatting": 849,
      "incorrect": 210,
      "superficial": 199,
      "instructions_not_followed": 109,
      "total_prefs": 3735,
      "positive_prefs_ratio": 0.8613119143239625,
      "win_rate": 0.6142342101749232,
      "mean_win_prob": 0.5228220994013246,
      "style_controlled": {
        "median": 652.5415000903679,
        "p2.5": 626.2936300923792,
        "p97.5": 703.681273453074,
        "rank": 22,
        "rank_p2.5": 4,
        "rank_p97.5": 53
      }
    },
    {
      "rank": 13,
      "model_name": "deepseek-chat-v3.1",
      "median": 1089.3080122057838,
      "p2.5": 1071.9985668418901,
      "p97.5": 1106.6174575696773,
      "rank_p2.5": 7,
      "rank_p97.5": 27,
      "n_match": 2188,
      "useful": 123,
      "creative": 30,
      "complete": 86,
      "clear_formatting": 92,
      "incorrect": 27,
      "superficial": 32,
      "instructions_not_followed": 17,
      "total_prefs": 407,
      "positive_prefs_ratio": 0.8132678132678133,
      "win_rate": 0.5671732522796352,
      "mean_win_prob": 0.52250498492276,
      "style_controlled": {
        "median": 604.7210608902162,
        "p2.5": 575.9260085069934,
        "p97.5": 654.9111404560487,
        "rank": 42,
        "rank_p2.5": 9,
        "rank_p97.5": 70
      }
    },
    {
      "rank": 14,
      "model_name": "gpt-5.1",
      "median": 1085.586156512762,
      "p2.5": 1070.6653095159718,
      "p97.5": 1100.5070035095523,
      "rank_p2.5": 9,
      "rank_p97.5": 28,
      "n_match": 2870,
      "useful": 215,
      "creative": 53,
      "complete": 189,
      "clear_formatting": 158,
      "incorrect": 26,
      "superficial": 70,
      "instructions_not_followed": 34,
      "total_prefs": 745,
      "positive_prefs_ratio": 0.825503355704698,
      "win_rate": 0.5401987353206865,
      "mean_win_prob": 0.5216427711597434,
      "style_controlled": {
        "median": 725.3744157697589,
        "p2.5": 691.714935747311,
        "p97.5": 775.9516017952388,
        "rank": 4,
        "rank_p2.5": 2,
        "rank_p97.5": 26
      }
    },
    {
      "rank": 15,
      "model_name": "DeepSeek-V3.2",
      "median": 1082.4311428473638,
      "p2.5": 1063.51553056905,
      "p97.5": 1101.3467551256776,
      "rank_p2.5": 8,
      "rank_p97.5": 32,
      "n_match": 1636,
      "useful": 171,
      "creative": 49,
      "complete": 153,
      "clear_formatting": 135,
      "incorrect": 50,
      "superficial": 53,
      "instructions_not_followed": 23,
      "total_prefs": 634,
      "positive_prefs_ratio": 0.8012618296529969,
      "win_rate": 0.5220643231114436,
      "mean_win_prob": 0.5209094560658317,
      "style_controlled": {
        "median": 635.3112921622975,
        "p2.5": 609.3603711420412,
        "p97.5": 684.550241743087,
        "rank": 28,
        "rank_p2.5": 5,
        "rank_p97.5": 59
      }
    },
    {
      "rank": 16,
      "model_name": "glm-4.5",
      "median": 1079.336719875729,
      "p2.5": 1064.2821814589752,
      "p97.5": 1094.3912582924827,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 2729,
      "useful": 100,
      "creative": 35,
      "complete": 106,
      "clear_formatting": 90,
      "incorrect": 18,
      "superficial": 30,
      "instructions_not_followed": 15,
      "total_prefs": 394,
      "positive_prefs_ratio": 0.8401015228426396,
      "win_rate": 0.5657222480260102,
      "mean_win_prob": 0.5201880590698379,
      "style_controlled": {
        "median": 686.9486232615914,
        "p2.5": 661.6877329909364,
        "p97.5": 737.6537374094898,
        "rank": 10,
        "rank_p2.5": 2,
        "rank_p97.5": 38
      }
    },
    {
      "rank": 17,
      "model_name": "grok-4-fast",
      "median": 1078.1496090815326,
      "p2.5": 1064.1069046229147,
      "p97.5": 1092.1923135401505,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 3202,
      "useful": 57,
      "creative": 19,
      "complete": 42,
      "clear_formatting": 44,
      "incorrect": 8,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 187,
      "positive_prefs_ratio": 0.8663101604278075,
      "win_rate": 0.571828731492597,
      "mean_win_prob": 0.5199107391288531,
      "style_controlled": {
        "median": 648.6683085441348,
        "p2.5": 617.9085822937105,
        "p97.5": 705.4216305797348,
        "rank": 23,
        "rank_p2.5": 4,
        "rank_p97.5": 56
      }
    },
    {
      "rank": 18,
      "model_name": "grok-4.1-fast",
      "median": 1076.501290429905,
      "p2.5": 1060.5656682903798,
      "p97.5": 1092.4369125694304,
      "rank_p2.5": 9,
      "rank_p97.5": 32,
      "n_match": 2361,
      "useful": 182,
      "creative": 71,
      "complete": 147,
      "clear_formatting": 134,
      "incorrect": 49,
      "superficial": 65,
      "instructions_not_followed": 22,
      "total_prefs": 670,
      "positive_prefs_ratio": 0.7970149253731343,
      "win_rate": 0.535751840168244,
      "mean_win_prob": 0.5195251502176125,
      "style_controlled": {
        "median": 709.0892205206205,
        "p2.5": 678.6277630257279,
        "p97.5": 757.5604321513499,
        "rank": 6,
        "rank_p2.5": 2,
        "rank_p97.5": 31
      }
    },
    {
      "rank": 19,
      "model_name": "claude-4-sonnet",
      "median": 1074.3439370400918,
      "p2.5": 1060.088127875541,
      "p97.5": 1088.5997462046425,
      "rank_p2.5": 11,
      "rank_p97.5": 32,
      "n_match": 3077,
      "useful": 67,
      "creative": 24,
      "complete": 57,
      "clear_formatting": 69,
      "incorrect": 12,
      "superficial": 19,
      "instructions_not_followed": 7,
      "total_prefs": 255,
      "positive_prefs_ratio": 0.8509803921568627,
      "win_rate": 0.5642296571664601,
      "mean_win_prob": 0.5190195562174781,
      "style_controlled": {
        "median": 646.0992513111105,
        "p2.5": 616.5331696155594,
        "p97.5": 700.4835787278932,
        "rank": 26,
        "rank_p2.5": 4,
        "rank_p97.5": 58
      }
    },
    {
      "rank": 20,
      "model_name": "mistral-small-2506",
      "median": 1071.6376695528022,
      "p2.5": 1060.6316214039955,
      "p97.5": 1082.643717701609,
      "rank_p2.5": 12,
      "rank_p97.5": 32,
      "n_match": 5575,
      "useful": 223,
      "creative": 53,
      "complete": 147,
      "clear_formatting": 191,
      "incorrect": 59,
      "superficial": 79,
      "instructions_not_followed": 30,
      "total_prefs": 782,
      "positive_prefs_ratio": 0.7851662404092071,
      "win_rate": 0.5387931034482759,
      "mean_win_prob": 0.5183838282675465,
      "style_controlled": {
        "median": 609.3287033920542,
        "p2.5": 579.6335466238339,
        "p97.5": 663.5295656343613,
        "rank": 38,
        "rank_p2.5": 7,
        "rank_p97.5": 69
      }
    },
    {
      "rank": 21,
      "model_name": "gpt-oss-120b",
      "median": 1070.7719841493727,
      "p2.5": 1052.4650180846409,
      "p97.5": 1089.0789502141047,
      "rank_p2.5": 11,
      "rank_p97.5": 33,
      "n_match": 1893,
      "useful": 155,
      "creative": 62,
      "complete": 212,
      "clear_formatting": 146,
      "incorrect": 56,
      "superficial": 46,
      "instructions_not_followed": 35,
      "total_prefs": 712,
      "positive_prefs_ratio": 0.8075842696629213,
      "win_rate": 0.5598320503848845,
      "mean_win_prob": 0.5181801192206517,
      "style_controlled": {
        "median": 688.2713998499949,
        "p2.5": 654.73447670194,
        "p97.5": 741.316405375333,
        "rank": 9,
        "rank_p2.5": 2,
        "rank_p97.5": 42
      }
    },
    {
      "rank": 22,
      "model_name": "gemma-3-12b",
      "median": 1067.9186731696507,
      "p2.5": 1059.460320492513,
      "p97.5": 1076.3770258467882,
      "rank_p2.5": 13,
      "rank_p97.5": 32,
      "n_match": 9107,
      "useful": 911,
      "creative": 259,
      "complete": 914,
      "clear_formatting": 758,
      "incorrect": 243,
      "superficial": 238,
      "instructions_not_followed": 165,
      "total_prefs": 3488,
      "positive_prefs_ratio": 0.8147935779816514,
      "win_rate": 0.5810734463276837,
      "mean_win_prob": 0.51750748202461,
      "style_controlled": {
        "median": 626.0263868131226,
        "p2.5": 592.7169386190586,
        "p97.5": 676.6571118049438,
        "rank": 31,
        "rank_p2.5": 6,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 23,
      "model_name": "kimi-k2-thinking",
      "median": 1067.9008985219282,
      "p2.5": 1040.4156269124678,
      "p97.5": 1095.3861701313886,
      "rank_p2.5": 9,
      "rank_p97.5": 36,
      "n_match": 813,
      "useful": 96,
      "creative": 24,
      "complete": 89,
      "clear_formatting": 68,
      "incorrect": 19,
      "superficial": 32,
      "instructions_not_followed": 19,
      "total_prefs": 347,
      "positive_prefs_ratio": 0.7982708933717579,
      "win_rate": 0.5434439178515008,
      "mean_win_prob": 0.5175032860170122,
      "style_controlled": {
        "median": 758.7926831534033,
        "p2.5": 711.8216471082632,
        "p97.5": 818.7819427590088,
        "rank": 2,
        "rank_p2.5": 1,
        "rank_p97.5": 17
      }
    },
    {
      "rank": 24,
      "model_name": "deepseek-r1-0528",
      "median": 1066.559633438597,
      "p2.5": 1046.7098937796604,
      "p97.5": 1086.4093730975337,
      "rank_p2.5": 11,
      "rank_p97.5": 33,
      "n_match": 1660,
      "useful": 113,
      "creative": 32,
      "complete": 80,
      "clear_formatting": 72,
      "incorrect": 26,
      "superficial": 36,
      "instructions_not_followed": 17,
      "total_prefs": 376,
      "positive_prefs_ratio": 0.7898936170212766,
      "win_rate": 0.541908713692946,
      "mean_win_prob": 0.5171864490559616,
      "style_controlled": {
        "median": 750.203101549607,
        "p2.5": 718.2767702509271,
        "p97.5": 798.0452551040768,
        "rank": 3,
        "rank_p2.5": 1,
        "rank_p97.5": 15
      }
    },
    {
      "rank": 25,
      "model_name": "kimi-k2",
      "median": 1064.6934329269739,
      "p2.5": 1044.9303404610482,
      "p97.5": 1084.4565253928993,
      "rank_p2.5": 11,
      "rank_p97.5": 34,
      "n_match": 1546,
      "useful": 119,
      "creative": 38,
      "complete": 75,
      "clear_formatting": 85,
      "incorrect": 23,
      "superficial": 39,
      "instructions_not_followed": 10,
      "total_prefs": 389,
      "positive_prefs_ratio": 0.8149100257069408,
      "win_rate": 0.530952380952381,
      "mean_win_prob": 0.5167449246454495,
      "style_controlled": {
        "median": 620.046134180986,
        "p2.5": 580.1489971072057,
        "p97.5": 671.820406073563,
        "rank": 35,
        "rank_p2.5": 6,
        "rank_p97.5": 69
      }
    },
    {
      "rank": 26,
      "model_name": "deepseek-v3-chat",
      "median": 1061.2462398200141,
      "p2.5": 1051.270212295519,
      "p97.5": 1071.2222673445092,
      "rank_p2.5": 14,
      "rank_p97.5": 33,
      "n_match": 7560,
      "useful": 833,
      "creative": 209,
      "complete": 790,
      "clear_formatting": 769,
      "incorrect": 143,
      "superficial": 188,
      "instructions_not_followed": 81,
      "total_prefs": 3013,
      "positive_prefs_ratio": 0.8632592100896117,
      "win_rate": 0.6549130230371415,
      "mean_win_prob": 0.5159272475314463,
      "style_controlled": {
        "median": 654.4464930605669,
        "p2.5": 621.5276467607246,
        "p97.5": 699.4341993336483,
        "rank": 20,
        "rank_p2.5": 4,
        "rank_p97.5": 54
      }
    },
    {
      "rank": 27,
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1060.8700272662097,
      "p2.5": 1037.9881676325028,
      "p97.5": 1083.7518868999166,
      "rank_p2.5": 11,
      "rank_p97.5": 40,
      "n_match": 1146,
      "useful": 96,
      "creative": 25,
      "complete": 58,
      "clear_formatting": 81,
      "incorrect": 11,
      "superficial": 31,
      "instructions_not_followed": 10,
      "total_prefs": 312,
      "positive_prefs_ratio": 0.8333333333333334,
      "win_rate": 0.5376712328767124,
      "mean_win_prob": 0.5158378437783764,
      "style_controlled": {
        "median": 605.8551977718175,
        "p2.5": 555.1791023326134,
        "p97.5": 660.2669357351062,
        "rank": 41,
        "rank_p2.5": 8,
        "rank_p97.5": 76
      }
    },
    {
      "rank": 28,
      "model_name": "command-a",
      "median": 1059.7745574159771,
      "p2.5": 1051.2422545232735,
      "p97.5": 1068.3068603086808,
      "rank_p2.5": 15,
      "rank_p97.5": 33,
      "n_match": 9001,
      "useful": 917,
      "creative": 213,
      "complete": 779,
      "clear_formatting": 802,
      "incorrect": 198,
      "superficial": 278,
      "instructions_not_followed": 108,
      "total_prefs": 3295,
      "positive_prefs_ratio": 0.8227617602427921,
      "win_rate": 0.5693325661680092,
      "mean_win_prob": 0.5155773282757783,
      "style_controlled": {
        "median": 615.2661170640233,
        "p2.5": 594.7536695587999,
        "p97.5": 664.461637466363,
        "rank": 37,
        "rank_p2.5": 7,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 29,
      "model_name": "claude-3-7-sonnet",
      "median": 1055.4509912270184,
      "p2.5": 1043.8317668028174,
      "p97.5": 1067.0702156512195,
      "rank_p2.5": 15,
      "rank_p97.5": 34,
      "n_match": 4743,
      "useful": 734,
      "creative": 161,
      "complete": 511,
      "clear_formatting": 621,
      "incorrect": 84,
      "superficial": 245,
      "instructions_not_followed": 44,
      "total_prefs": 2400,
      "positive_prefs_ratio": 0.8445833333333334,
      "win_rate": 0.5711649973642594,
      "mean_win_prob": 0.5145464187768457,
      "style_controlled": {
        "median": 658.9092293492627,
        "p2.5": 627.8725166894893,
        "p97.5": 705.0051954563979,
        "rank": 18,
        "rank_p2.5": 4,
        "rank_p97.5": 53
      }
    },
    {
      "rank": 30,
      "model_name": "grok-3-mini-beta",
      "median": 1054.1963785434164,
      "p2.5": 1035.997577582437,
      "p97.5": 1072.3951795043956,
      "rank_p2.5": 13,
      "rank_p97.5": 41,
      "n_match": 2066,
      "useful": 247,
      "creative": 65,
      "complete": 231,
      "clear_formatting": 234,
      "incorrect": 65,
      "superficial": 65,
      "instructions_not_followed": 42,
      "total_prefs": 949,
      "positive_prefs_ratio": 0.8187565858798735,
      "win_rate": 0.574085554866708,
      "mean_win_prob": 0.5142464557654027,
      "style_controlled": {
        "median": 634.7295050983371,
        "p2.5": 597.1236367751396,
        "p97.5": 686.1318359232549,
        "rank": 29,
        "rank_p2.5": 5,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 31,
      "model_name": "magistral-small-2506",
      "median": 1052.8871832370812,
      "p2.5": 1039.8261767691067,
      "p97.5": 1065.948189705056,
      "rank_p2.5": 15,
      "rank_p97.5": 38,
      "n_match": 3850,
      "useful": 83,
      "creative": 18,
      "complete": 59,
      "clear_formatting": 77,
      "incorrect": 20,
      "superficial": 28,
      "instructions_not_followed": 12,
      "total_prefs": 297,
      "positive_prefs_ratio": 0.797979797979798,
      "win_rate": 0.5278849526481936,
      "mean_win_prob": 0.51393305115596,
      "style_controlled": {
        "median": 594.5840515872517,
        "p2.5": 567.2093445662899,
        "p97.5": 649.782170877101,
        "rank": 46,
        "rank_p2.5": 12,
        "rank_p97.5": 72
      }
    },
    {
      "rank": 32,
      "model_name": "glm-4.6",
      "median": 1050.8821467216098,
      "p2.5": 1036.0844038396085,
      "p97.5": 1065.6798896036114,
      "rank_p2.5": 15,
      "rank_p97.5": 41,
      "n_match": 2778,
      "useful": 119,
      "creative": 38,
      "complete": 123,
      "clear_formatting": 108,
      "incorrect": 31,
      "superficial": 48,
      "instructions_not_followed": 24,
      "total_prefs": 491,
      "positive_prefs_ratio": 0.790224032586558,
      "win_rate": 0.5243195002231147,
      "mean_win_prob": 0.513452294168734,
      "style_controlled": {
        "median": 683.469726191542,
        "p2.5": 652.2479267314164,
        "p97.5": 737.7710869318147,
        "rank": 11,
        "rank_p2.5": 2,
        "rank_p97.5": 43
      }
    },
    {
      "rank": 33,
      "model_name": "gemma-3n-e4b-it",
      "median": 1034.7150703848877,
      "p2.5": 1022.9141050786665,
      "p97.5": 1046.516035691109,
      "rank_p2.5": 26,
      "rank_p97.5": 41,
      "n_match": 4690,
      "useful": 216,
      "creative": 76,
      "complete": 160,
      "clear_formatting": 184,
      "incorrect": 83,
      "superficial": 92,
      "instructions_not_followed": 46,
      "total_prefs": 857,
      "positive_prefs_ratio": 0.7421236872812136,
      "win_rate": 0.5035360678925035,
      "mean_win_prob": 0.509541161303752,
      "style_controlled": {
        "median": 595.1121007544818,
        "p2.5": 564.1988678429535,
        "p97.5": 642.4167860435541,
        "rank": 44,
        "rank_p2.5": 14,
        "rank_p97.5": 74
      }
    },
    {
      "rank": 34,
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1033.7843965963405,
      "p2.5": 1024.7864285158066,
      "p97.5": 1042.7823646768743,
      "rank_p2.5": 28,
      "rank_p97.5": 41,
      "n_match": 8481,
      "useful": 1052,
      "creative": 320,
      "complete": 1060,
      "clear_formatting": 978,
      "incorrect": 211,
      "superficial": 285,
      "instructions_not_followed": 133,
      "total_prefs": 4039,
      "positive_prefs_ratio": 0.844268383263184,
      "win_rate": 0.5898007731192387,
      "mean_win_prob": 0.5093141171439066,
      "style_controlled": {
        "median": 575.5615715892616,
        "p2.5": 550.5650972797262,
        "p97.5": 626.1368990907831,
        "rank": 53,
        "rank_p2.5": 20,
        "rank_p97.5": 77
      }
    },
    {
      "rank": 35,
      "model_name": "gpt-4.1-mini",
      "median": 1031.090864950941,
      "p2.5": 1022.30928422328,
      "p97.5": 1039.8724456786024,
      "rank_p2.5": 29,
      "rank_p97.5": 42,
      "n_match": 8592,
      "useful": 870,
      "creative": 190,
      "complete": 569,
      "clear_formatting": 722,
      "incorrect": 139,
      "superficial": 324,
      "instructions_not_followed": 83,
      "total_prefs": 2897,
      "positive_prefs_ratio": 0.8115291681049361,
      "win_rate": 0.519074613145396,
      "mean_win_prob": 0.5086558374230822,
      "style_controlled": {
        "median": 623.1946872570206,
        "p2.5": 592.1451001676674,
        "p97.5": 672.8479075285862,
        "rank": 34,
        "rank_p2.5": 6,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 36,
      "model_name": "gemma-3-4b",
      "median": 1030.439251368476,
      "p2.5": 1022.4986840253819,
      "p97.5": 1038.3798187115706,
      "rank_p2.5": 30,
      "rank_p97.5": 42,
      "n_match": 10380,
      "useful": 913,
      "creative": 254,
      "complete": 909,
      "clear_formatting": 793,
      "incorrect": 372,
      "superficial": 242,
      "instructions_not_followed": 184,
      "total_prefs": 3667,
      "positive_prefs_ratio": 0.7823834196891192,
      "win_rate": 0.529210042865891,
      "mean_win_prob": 0.5084963249568856,
      "style_controlled": {
        "median": 594.7980855632215,
        "p2.5": 569.7283179256553,
        "p97.5": 644.4096832838399,
        "rank": 45,
        "rank_p2.5": 14,
        "rank_p97.5": 72
      }
    },
    {
      "rank": 37,
      "model_name": "deepseek-r1",
      "median": 1026.8607240734943,
      "p2.5": 1014.1559108983819,
      "p97.5": 1039.565537248607,
      "rank_p2.5": 30,
      "rank_p97.5": 46,
      "n_match": 4277,
      "useful": 576,
      "creative": 213,
      "complete": 554,
      "clear_formatting": 507,
      "incorrect": 119,
      "superficial": 148,
      "instructions_not_followed": 104,
      "total_prefs": 2221,
      "positive_prefs_ratio": 0.8329581269698334,
      "win_rate": 0.5648063433973772,
      "mean_win_prob": 0.5076184840705066,
      "style_controlled": {
        "median": 578.0623379790916,
        "p2.5": 549.1920389720801,
        "p97.5": 628.3375575656634,
        "rank": 51,
        "rank_p2.5": 18,
        "rank_p97.5": 79
      }
    },
    {
      "rank": 38,
      "model_name": "qwen3-32b",
      "median": 1019.6360633341736,
      "p2.5": 1002.9978754505839,
      "p97.5": 1036.2742512177633,
      "rank_p2.5": 31,
      "rank_p97.5": 50,
      "n_match": 2274,
      "useful": 241,
      "creative": 80,
      "complete": 210,
      "clear_formatting": 176,
      "incorrect": 86,
      "superficial": 78,
      "instructions_not_followed": 55,
      "total_prefs": 926,
      "positive_prefs_ratio": 0.7634989200863931,
      "win_rate": 0.4921259842519685,
      "mean_win_prob": 0.5058367218676034,
      "style_controlled": {
        "median": 625.1484619003653,
        "p2.5": 582.9047772742772,
        "p97.5": 686.3450242005895,
        "rank": 32,
        "rank_p2.5": 5,
        "rank_p97.5": 68
      }
    },
    {
      "rank": 39,
      "model_name": "mistral-saba",
      "median": 1010.7594346081431,
      "p2.5": 1000.3010475855216,
      "p97.5": 1021.2178216307647,
      "rank_p2.5": 37,
      "rank_p97.5": 50,
      "n_match": 6004,
      "useful": 506,
      "creative": 117,
      "complete": 359,
      "clear_formatting": 390,
      "incorrect": 134,
      "superficial": 239,
      "instructions_not_followed": 96,
      "total_prefs": 1841,
      "positive_prefs_ratio": 0.7452471482889734,
      "win_rate": 0.5004434589800444,
      "mean_win_prob": 0.503629987358709,
      "style_controlled": {
        "median": 577.6895994267436,
        "p2.5": 543.0060198006836,
        "p97.5": 629.0208548983329,
        "rank": 52,
        "rank_p2.5": 17,
        "rank_p97.5": 79
      }
    },
    {
      "rank": 40,
      "model_name": "glm-4.7",
      "median": 1010.1740609006456,
      "p2.5": 965.031459871873,
      "p97.5": 1055.3166619294182,
      "rank_p2.5": 22,
      "rank_p97.5": 65,
      "n_match": 273,
      "useful": 29,
      "creative": 13,
      "complete": 29,
      "clear_formatting": 30,
      "incorrect": 11,
      "superficial": 13,
      "instructions_not_followed": 12,
      "total_prefs": 137,
      "positive_prefs_ratio": 0.7372262773722628,
      "win_rate": 0.4549763033175355,
      "mean_win_prob": 0.5034837769673726,
      "style_controlled": {
        "median": 721.4831327820671,
        "p2.5": 665.5082682884571,
        "p97.5": 784.451041434412,
        "rank": 5,
        "rank_p2.5": 2,
        "rank_p97.5": 36
      }
    },
    {
      "rank": 41,
      "model_name": "llama-maverick",
      "median": 1009.9490194414567,
      "p2.5": 997.2041170286328,
      "p97.5": 1022.6939218542807,
      "rank_p2.5": 35,
      "rank_p97.5": 51,
      "n_match": 3893,
      "useful": 146,
      "creative": 30,
      "complete": 95,
      "clear_formatting": 105,
      "incorrect": 36,
      "superficial": 74,
      "instructions_not_followed": 20,
      "total_prefs": 506,
      "positive_prefs_ratio": 0.7430830039525692,
      "win_rate": 0.46589018302828616,
      "mean_win_prob": 0.503427545043502,
      "style_controlled": {
        "median": 591.2517775451154,
        "p2.5": 555.8340493053277,
        "p97.5": 640.1289644330668,
        "rank": 47,
        "rank_p2.5": 15,
        "rank_p97.5": 75
      }
    },
    {
      "rank": 42,
      "model_name": "mistral-large-2411",
      "median": 1009.5382289073393,
      "p2.5": 1001.5268358075265,
      "p97.5": 1017.5496220071522,
      "rank_p2.5": 37,
      "rank_p97.5": 50,
      "n_match": 10960,
      "useful": 1392,
      "creative": 260,
      "complete": 1068,
      "clear_formatting": 1146,
      "incorrect": 213,
      "superficial": 465,
      "instructions_not_followed": 132,
      "total_prefs": 4676,
      "positive_prefs_ratio": 0.8267750213857998,
      "win_rate": 0.5500690607734806,
      "mean_win_prob": 0.5033248667912817,
      "style_controlled": {
        "median": 588.5037593836771,
        "p2.5": 562.2587766024358,
        "p97.5": 638.4366404089104,
        "rank": 48,
        "rank_p2.5": 16,
        "rank_p97.5": 74
      }
    },
    {
      "rank": 43,
      "model_name": "gemini-1.5-pro",
      "median": 1005.1691806749011,
      "p2.5": 995.499214073193,
      "p97.5": 1014.8391472766093,
      "rank_p2.5": 37,
      "rank_p97.5": 51,
      "n_match": 10027,
      "useful": 1656,
      "creative": 524,
      "complete": 1435,
      "clear_formatting": 1532,
      "incorrect": 287,
      "superficial": 498,
      "instructions_not_followed": 205,
      "total_prefs": 6137,
      "positive_prefs_ratio": 0.8386833957959915,
      "win_rate": 0.5857166203628044,
      "mean_win_prob": 0.5022302030858053,
      "style_controlled": {
        "median": 608.4884558833462,
        "p2.5": 575.0756062450602,
        "p97.5": 659.590733500107,
        "rank": 39,
        "rank_p2.5": 8,
        "rank_p97.5": 70
      }
    },
    {
      "rank": 44,
      "model_name": "llama-4-scout",
      "median": 1002.577746105041,
      "p2.5": 993.2437103823486,
      "p97.5": 1011.9117818277335,
      "rank_p2.5": 38,
      "rank_p97.5": 52,
      "n_match": 7137,
      "useful": 627,
      "creative": 141,
      "complete": 469,
      "clear_formatting": 493,
      "incorrect": 171,
      "superficial": 296,
      "instructions_not_followed": 95,
      "total_prefs": 2292,
      "positive_prefs_ratio": 0.7547993019197208,
      "win_rate": 0.4803798602401003,
      "mean_win_prob": 0.5015786591948704,
      "style_controlled": {
        "median": 586.6228266400547,
        "p2.5": 557.8698679004885,
        "p97.5": 636.6013189621983,
        "rank": 49,
        "rank_p2.5": 16,
        "rank_p97.5": 74
      }
    },
    {
      "rank": 45,
      "model_name": "gpt-oss-20b",
      "median": 1001.052496519017,
      "p2.5": 983.7148200117836,
      "p97.5": 1018.3901730262505,
      "rank_p2.5": 37,
      "rank_p97.5": 56,
      "n_match": 2236,
      "useful": 157,
      "creative": 66,
      "complete": 144,
      "clear_formatting": 123,
      "incorrect": 60,
      "superficial": 81,
      "instructions_not_followed": 44,
      "total_prefs": 675,
      "positive_prefs_ratio": 0.725925925925926,
      "win_rate": 0.44563167818981775,
      "mean_win_prob": 0.5011943874344887,
      "style_controlled": {
        "median": 607.8097750047639,
        "p2.5": 577.2549451232865,
        "p97.5": 657.9813867022289,
        "rank": 40,
        "rank_p2.5": 8,
        "rank_p97.5": 70
      }
    },
    {
      "rank": 46,
      "model_name": "EuroLLM-22B-Instruct-2512",
      "median": 999.9862187229191,
      "p2.5": 959.4654389243137,
      "p97.5": 1040.5069985215246,
      "rank_p2.5": 28,
      "rank_p97.5": 65,
      "n_match": 335,
      "useful": 52,
      "creative": 14,
      "complete": 31,
      "clear_formatting": 38,
      "incorrect": 9,
      "superficial": 20,
      "instructions_not_followed": 10,
      "total_prefs": 174,
      "positive_prefs_ratio": 0.7758620689655172,
      "win_rate": 0.4197080291970803,
      "mean_win_prob": 0.5009254004256204,
      "style_controlled": {
        "median": 476.45418494681553,
        "p2.5": 408.92933365761763,
        "p97.5": 541.7688549407883,
        "rank": 78,
        "rank_p2.5": 54,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 47,
      "model_name": "gpt-5-mini",
      "median": 997.1141981075106,
      "p2.5": 984.4565187569231,
      "p97.5": 1009.771877458098,
      "rank_p2.5": 38,
      "rank_p97.5": 56,
      "n_match": 4077,
      "useful": 176,
      "creative": 36,
      "complete": 171,
      "clear_formatting": 114,
      "incorrect": 33,
      "superficial": 73,
      "instructions_not_followed": 40,
      "total_prefs": 643,
      "positive_prefs_ratio": 0.7729393468118196,
      "win_rate": 0.4356687898089172,
      "mean_win_prob": 0.5001994522666154,
      "style_controlled": {
        "median": 652.6659800406571,
        "p2.5": 624.1153242799495,
        "p97.5": 700.2878899690813,
        "rank": 21,
        "rank_p2.5": 4,
        "rank_p97.5": 54
      }
    },
    {
      "rank": 48,
      "model_name": "mistral-small-3.1-24b",
      "median": 996.1160343503003,
      "p2.5": 985.5416160551945,
      "p97.5": 1006.690452645406,
      "rank_p2.5": 38,
      "rank_p97.5": 56,
      "n_match": 5935,
      "useful": 892,
      "creative": 192,
      "complete": 538,
      "clear_formatting": 722,
      "incorrect": 197,
      "superficial": 344,
      "instructions_not_followed": 124,
      "total_prefs": 3009,
      "positive_prefs_ratio": 0.7789963443004321,
      "win_rate": 0.5015695067264574,
      "mean_win_prob": 0.49994666069895655,
      "style_controlled": {
        "median": 560.4880305715169,
        "p2.5": 529.1533806427075,
        "p97.5": 616.7612213920762,
        "rank": 57,
        "rank_p2.5": 25,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 49,
      "model_name": "o4-mini",
      "median": 990.9441291027233,
      "p2.5": 977.4162537389498,
      "p97.5": 1004.4720044664967,
      "rank_p2.5": 38,
      "rank_p97.5": 58,
      "n_match": 3568,
      "useful": 321,
      "creative": 71,
      "complete": 206,
      "clear_formatting": 182,
      "incorrect": 75,
      "superficial": 125,
      "instructions_not_followed": 34,
      "total_prefs": 1014,
      "positive_prefs_ratio": 0.7692307692307693,
      "win_rate": 0.4513338139870223,
      "mean_win_prob": 0.4986327749474583,
      "style_controlled": {
        "median": 624.8045224404761,
        "p2.5": 593.9502004497203,
        "p97.5": 673.3466712903789,
        "rank": 33,
        "rank_p2.5": 6,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 50,
      "model_name": "gpt-5",
      "median": 983.015608939327,
      "p2.5": 970.5926551512998,
      "p97.5": 995.438562727354,
      "rank_p2.5": 43,
      "rank_p97.5": 61,
      "n_match": 4654,
      "useful": 1,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 1,
      "positive_prefs_ratio": 1.0,
      "win_rate": 0.4053893988747409,
      "mean_win_prob": 0.49660526006534467,
      "style_controlled": {
        "median": 634.345349093114,
        "p2.5": 592.169734628202,
        "p97.5": 687.1701896266446,
        "rank": 30,
        "rank_p2.5": 5,
        "rank_p97.5": 65
      }
    },
    {
      "rank": 51,
      "model_name": "lfm2-8b-a1b",
      "median": 981.9398982843553,
      "p2.5": 964.5840231469341,
      "p97.5": 999.2957734217765,
      "rank_p2.5": 41,
      "rank_p97.5": 65,
      "n_match": 2074,
      "useful": 130,
      "creative": 21,
      "complete": 85,
      "clear_formatting": 109,
      "incorrect": 74,
      "superficial": 85,
      "instructions_not_followed": 39,
      "total_prefs": 543,
      "positive_prefs_ratio": 0.6353591160220995,
      "win_rate": 0.41094224924012157,
      "mean_win_prob": 0.49632892270467655,
      "style_controlled": {
        "median": 557.455495394331,
        "p2.5": 525.5474242040299,
        "p97.5": 609.1410568805761,
        "rank": 59,
        "rank_p2.5": 29,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 52,
      "model_name": "aya-expanse-32b",
      "median": 976.1192667807055,
      "p2.5": 965.8381572015597,
      "p97.5": 986.4003763598512,
      "rank_p2.5": 44,
      "rank_p97.5": 64,
      "n_match": 6060,
      "useful": 491,
      "creative": 88,
      "complete": 300,
      "clear_formatting": 348,
      "incorrect": 158,
      "superficial": 242,
      "instructions_not_followed": 87,
      "total_prefs": 1714,
      "positive_prefs_ratio": 0.7158693115519253,
      "win_rate": 0.44252631578947366,
      "mean_win_prob": 0.49482844329750303,
      "style_controlled": {
        "median": 566.3322763097817,
        "p2.5": 541.1956870816479,
        "p97.5": 612.2029828466887,
        "rank": 56,
        "rank_p2.5": 27,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 53,
      "model_name": "qwen-3-8b",
      "median": 975.8883799261627,
      "p2.5": 961.7218976352401,
      "p97.5": 990.0548622170852,
      "rank_p2.5": 44,
      "rank_p97.5": 65,
      "n_match": 3150,
      "useful": 127,
      "creative": 40,
      "complete": 111,
      "clear_formatting": 86,
      "incorrect": 79,
      "superficial": 52,
      "instructions_not_followed": 38,
      "total_prefs": 533,
      "positive_prefs_ratio": 0.6829268292682927,
      "win_rate": 0.40725806451612906,
      "mean_win_prob": 0.4947687411849724,
      "style_controlled": {
        "median": 644.6960084714267,
        "p2.5": 611.9140843343237,
        "p97.5": 690.6845474968441,
        "rank": 27,
        "rank_p2.5": 5,
        "rank_p97.5": 59
      }
    },
    {
      "rank": 54,
      "model_name": "qwen3-30b-a3b",
      "median": 975.0747742581261,
      "p2.5": 961.9998349662574,
      "p97.5": 988.1497135499948,
      "rank_p2.5": 44,
      "rank_p97.5": 65,
      "n_match": 3790,
      "useful": 143,
      "creative": 24,
      "complete": 128,
      "clear_formatting": 118,
      "incorrect": 77,
      "superficial": 77,
      "instructions_not_followed": 35,
      "total_prefs": 602,
      "positive_prefs_ratio": 0.686046511627907,
      "win_rate": 0.4062394031875212,
      "mean_win_prob": 0.4945582497160083,
      "style_controlled": {
        "median": 619.6874207574126,
        "p2.5": 583.1479269338987,
        "p97.5": 665.7423029657779,
        "rank": 36,
        "rank_p2.5": 6,
        "rank_p97.5": 68
      }
    },
    {
      "rank": 55,
      "model_name": "llama-3.3-70b",
      "median": 970.4250763015509,
      "p2.5": 962.8695268791853,
      "p97.5": 977.9806257239165,
      "rank_p2.5": 47,
      "rank_p97.5": 65,
      "n_match": 11394,
      "useful": 1094,
      "creative": 212,
      "complete": 783,
      "clear_formatting": 808,
      "incorrect": 257,
      "superficial": 505,
      "instructions_not_followed": 145,
      "total_prefs": 3804,
      "positive_prefs_ratio": 0.7615667718191378,
      "win_rate": 0.465857694429123,
      "mean_win_prob": 0.49335196443815477,
      "style_controlled": {
        "median": 560.220732840615,
        "p2.5": 528.8474872736804,
        "p97.5": 608.4186881747239,
        "rank": 58,
        "rank_p2.5": 29,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 56,
      "model_name": "o3-mini",
      "median": 969.5512841428867,
      "p2.5": 951.9594344529092,
      "p97.5": 987.1431338328644,
      "rank_p2.5": 44,
      "rank_p97.5": 67,
      "n_match": 1951,
      "useful": 252,
      "creative": 62,
      "complete": 165,
      "clear_formatting": 157,
      "incorrect": 35,
      "superficial": 97,
      "instructions_not_followed": 28,
      "total_prefs": 796,
      "positive_prefs_ratio": 0.7989949748743719,
      "win_rate": 0.5047679593134139,
      "mean_win_prob": 0.49312463693493525,
      "style_controlled": {
        "median": 602.9147227784185,
        "p2.5": 570.1027412726336,
        "p97.5": 654.258761730622,
        "rank": 43,
        "rank_p2.5": 10,
        "rank_p97.5": 71
      }
    },
    {
      "rank": 57,
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 965.5638909046983,
      "p2.5": 953.4326151545242,
      "p97.5": 977.6951666548724,
      "rank_p2.5": 47,
      "rank_p97.5": 67,
      "n_match": 4345,
      "useful": 524,
      "creative": 106,
      "complete": 415,
      "clear_formatting": 431,
      "incorrect": 89,
      "superficial": 189,
      "instructions_not_followed": 71,
      "total_prefs": 1825,
      "positive_prefs_ratio": 0.8087671232876712,
      "win_rate": 0.51994342291372,
      "mean_win_prob": 0.49208469829475476,
      "style_controlled": {
        "median": 555.0862522020683,
        "p2.5": 523.5728486859626,
        "p97.5": 607.5035247191236,
        "rank": 61,
        "rank_p2.5": 29,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 58,
      "model_name": "gpt-4.1-nano",
      "median": 962.1058805247204,
      "p2.5": 952.3577869627776,
      "p97.5": 971.8539740866632,
      "rank_p2.5": 48,
      "rank_p97.5": 67,
      "n_match": 6875,
      "useful": 640,
      "creative": 108,
      "complete": 344,
      "clear_formatting": 465,
      "incorrect": 145,
      "superficial": 301,
      "instructions_not_followed": 83,
      "total_prefs": 2086,
      "positive_prefs_ratio": 0.7464046021093,
      "win_rate": 0.42656162070906023,
      "mean_win_prob": 0.4911793973981082,
      "style_controlled": {
        "median": 568.9784502326122,
        "p2.5": 539.8407985292704,
        "p97.5": 617.8997644124642,
        "rank": 54,
        "rank_p2.5": 24,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 59,
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 961.7403877310497,
      "p2.5": 946.1221956303921,
      "p97.5": 977.3585798317074,
      "rank_p2.5": 48,
      "rank_p97.5": 69,
      "n_match": 2660,
      "useful": 98,
      "creative": 22,
      "complete": 62,
      "clear_formatting": 60,
      "incorrect": 35,
      "superficial": 63,
      "instructions_not_followed": 26,
      "total_prefs": 366,
      "positive_prefs_ratio": 0.6612021857923497,
      "win_rate": 0.38380450407283184,
      "mean_win_prob": 0.49108352518793696,
      "style_controlled": {
        "median": 533.9458767768087,
        "p2.5": 506.0780024933882,
        "p97.5": 586.8579944421236,
        "rank": 67,
        "rank_p2.5": 35,
        "rank_p97.5": 84
      }
    },
    {
      "rank": 60,
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 961.0104776522555,
      "p2.5": 952.0390035853111,
      "p97.5": 969.9819517191999,
      "rank_p2.5": 49,
      "rank_p97.5": 67,
      "n_match": 8990,
      "useful": 1033,
      "creative": 200,
      "complete": 653,
      "clear_formatting": 848,
      "incorrect": 192,
      "superficial": 364,
      "instructions_not_followed": 110,
      "total_prefs": 3400,
      "positive_prefs_ratio": 0.8041176470588235,
      "win_rate": 0.5002845759817871,
      "mean_win_prob": 0.49089195572746536,
      "style_controlled": {
        "median": 555.7913155381473,
        "p2.5": 523.4628770519683,
        "p97.5": 606.0892010464557,
        "rank": 60,
        "rank_p2.5": 29,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 61,
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 959.9877573994552,
      "p2.5": 879.746905626345,
      "p97.5": 1040.2286091725655,
      "rank_p2.5": 29,
      "rank_p97.5": 81,
      "n_match": 97,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 0,
      "positive_prefs_ratio": 0.0,
      "win_rate": 0.38461538461538464,
      "mean_win_prob": 0.4906232959201793,
      "style_controlled": {
        "median": 386.4919099838261,
        "p2.5": -41.73988567943405,
        "p97.5": 645.4917328849207,
        "rank": 88,
        "rank_p2.5": 14,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 62,
      "model_name": "claude-3-5-sonnet-v2",
      "median": 957.6388783057748,
      "p2.5": 948.0928376951061,
      "p97.5": 967.1849189164434,
      "rank_p2.5": 49,
      "rank_p97.5": 68,
      "n_match": 7388,
      "useful": 846,
      "creative": 179,
      "complete": 524,
      "clear_formatting": 565,
      "incorrect": 130,
      "superficial": 402,
      "instructions_not_followed": 94,
      "total_prefs": 2740,
      "positive_prefs_ratio": 0.7715328467153285,
      "win_rate": 0.5058297396582015,
      "mean_win_prob": 0.49000520083487825,
      "style_controlled": {
        "median": 580.9668753244877,
        "p2.5": 548.7105163802663,
        "p97.5": 630.5149822516046,
        "rank": 50,
        "rank_p2.5": 17,
        "rank_p97.5": 79
      }
    },
    {
      "rank": 63,
      "model_name": "minimax-m2",
      "median": 953.4624571743462,
      "p2.5": 934.6774510628898,
      "p97.5": 972.2474632858027,
      "rank_p2.5": 48,
      "rank_p97.5": 71,
      "n_match": 1869,
      "useful": 135,
      "creative": 32,
      "complete": 86,
      "clear_formatting": 72,
      "incorrect": 34,
      "superficial": 67,
      "instructions_not_followed": 24,
      "total_prefs": 450,
      "positive_prefs_ratio": 0.7222222222222222,
      "win_rate": 0.3882270497547302,
      "mean_win_prob": 0.48890251947844826,
      "style_controlled": {
        "median": 656.7564539641963,
        "p2.5": 621.245109730297,
        "p97.5": 712.0872359188381,
        "rank": 19,
        "rank_p2.5": 3,
        "rank_p97.5": 54
      }
    },
    {
      "rank": 64,
      "model_name": "gpt-4o-2024-08-06",
      "median": 944.5247826846628,
      "p2.5": 934.7262350021284,
      "p97.5": 954.3233303671973,
      "rank_p2.5": 56,
      "rank_p97.5": 71,
      "n_match": 7915,
      "useful": 768,
      "creative": 153,
      "complete": 491,
      "clear_formatting": 618,
      "incorrect": 131,
      "superficial": 367,
      "instructions_not_followed": 115,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7680665909950813,
      "win_rate": 0.48274765463507713,
      "mean_win_prob": 0.48652680007065074,
      "style_controlled": {
        "median": 534.5868691919754,
        "p2.5": 504.4638939123228,
        "p97.5": 581.5209879208762,
        "rank": 65,
        "rank_p2.5": 37,
        "rank_p97.5": 85
      }
    },
    {
      "rank": 65,
      "model_name": "phi-4",
      "median": 944.1450881114818,
      "p2.5": 936.833304794024,
      "p97.5": 951.4568714289394,
      "rank_p2.5": 60,
      "rank_p97.5": 71,
      "n_match": 12674,
      "useful": 1117,
      "creative": 215,
      "complete": 803,
      "clear_formatting": 855,
      "incorrect": 318,
      "superficial": 497,
      "instructions_not_followed": 209,
      "total_prefs": 4014,
      "positive_prefs_ratio": 0.7448928749377179,
      "win_rate": 0.4413745296098237,
      "mean_win_prob": 0.48642538896143517,
      "style_controlled": {
        "median": 520.802342534909,
        "p2.5": 488.2361563940179,
        "p97.5": 570.7479009438525,
        "rank": 70,
        "rank_p2.5": 42,
        "rank_p97.5": 86
      }
    },
    {
      "rank": 66,
      "model_name": "llama-3.1-70b",
      "median": 943.1192414775771,
      "p2.5": 932.1947510939127,
      "p97.5": 954.0437318612416,
      "rank_p2.5": 56,
      "rank_p97.5": 73,
      "n_match": 7727,
      "useful": 1241,
      "creative": 280,
      "complete": 934,
      "clear_formatting": 1057,
      "incorrect": 289,
      "superficial": 495,
      "instructions_not_followed": 178,
      "total_prefs": 4474,
      "positive_prefs_ratio": 0.7849798837729102,
      "win_rate": 0.5072528453470208,
      "mean_win_prob": 0.4861512010893519,
      "style_controlled": {
        "median": 554.1279956971573,
        "p2.5": 516.7067283535358,
        "p97.5": 607.6764236878611,
        "rank": 62,
        "rank_p2.5": 29,
        "rank_p97.5": 83
      }
    },
    {
      "rank": 67,
      "model_name": "gemma-2-27b-it-q8",
      "median": 934.5200065667232,
      "p2.5": 899.7106993609988,
      "p97.5": 969.3293137724476,
      "rank_p2.5": 49,
      "rank_p97.5": 81,
      "n_match": 1107,
      "useful": 279,
      "creative": 81,
      "complete": 178,
      "clear_formatting": 257,
      "incorrect": 46,
      "superficial": 81,
      "instructions_not_followed": 29,
      "total_prefs": 951,
      "positive_prefs_ratio": 0.8359621451104101,
      "win_rate": 0.5354330708661418,
      "mean_win_prob": 0.4838413423515401,
      "style_controlled": {
        "median": 471.8309643689838,
        "p2.5": 398.29505260774135,
        "p97.5": 578.4881068261986,
        "rank": 80,
        "rank_p2.5": 39,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 68,
      "model_name": "ministral-8b-instruct-2410",
      "median": 930.4759809555812,
      "p2.5": 922.3776168754257,
      "p97.5": 938.5743450357367,
      "rank_p2.5": 62,
      "rank_p97.5": 78,
      "n_match": 11905,
      "useful": 1391,
      "creative": 306,
      "complete": 1065,
      "clear_formatting": 1267,
      "incorrect": 509,
      "superficial": 667,
      "instructions_not_followed": 261,
      "total_prefs": 5466,
      "positive_prefs_ratio": 0.7371020856201976,
      "win_rate": 0.4477699530516432,
      "mean_win_prob": 0.48274793916779224,
      "style_controlled": {
        "median": 509.021930041147,
        "p2.5": 482.57678279114265,
        "p97.5": 557.7690308443706,
        "rank": 73,
        "rank_p2.5": 48,
        "rank_p97.5": 86
      }
    },
    {
      "rank": 69,
      "model_name": "aya-expanse-8b",
      "median": 929.4887102728893,
      "p2.5": 911.6159038291476,
      "p97.5": 947.3615167166311,
      "rank_p2.5": 61,
      "rank_p97.5": 79,
      "n_match": 1801,
      "useful": 141,
      "creative": 33,
      "complete": 133,
      "clear_formatting": 127,
      "incorrect": 62,
      "superficial": 63,
      "instructions_not_followed": 31,
      "total_prefs": 590,
      "positive_prefs_ratio": 0.735593220338983,
      "win_rate": 0.480946123521682,
      "mean_win_prob": 0.4824803084318806,
      "style_controlled": {
        "median": 534.5691211448029,
        "p2.5": 503.36130166541477,
        "p97.5": 602.1936281960059,
        "rank": 66,
        "rank_p2.5": 29,
        "rank_p97.5": 85
      }
    },
    {
      "rank": 70,
      "model_name": "jamba-1.5-large",
      "median": 926.5496500543894,
      "p2.5": 887.9702604779766,
      "p97.5": 965.1290396308021,
      "rank_p2.5": 50,
      "rank_p97.5": 81,
      "n_match": 345,
      "useful": 19,
      "creative": 4,
      "complete": 18,
      "clear_formatting": 15,
      "incorrect": 5,
      "superficial": 9,
      "instructions_not_followed": 4,
      "total_prefs": 74,
      "positive_prefs_ratio": 0.7567567567567568,
      "win_rate": 0.49201277955271566,
      "mean_win_prob": 0.48168195672423275,
      "style_controlled": {
        "median": 521.7345491361457,
        "p2.5": 438.24220074271426,
        "p97.5": 620.1283600941052,
        "rank": 69,
        "rank_p2.5": 23,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 71,
      "model_name": "qwq-32b",
      "median": 924.2191819519556,
      "p2.5": 906.1105591903478,
      "p97.5": 942.3278047135636,
      "rank_p2.5": 62,
      "rank_p97.5": 80,
      "n_match": 1952,
      "useful": 222,
      "creative": 106,
      "complete": 249,
      "clear_formatting": 170,
      "incorrect": 92,
      "superficial": 84,
      "instructions_not_followed": 88,
      "total_prefs": 1011,
      "positive_prefs_ratio": 0.7388724035608308,
      "win_rate": 0.43544303797468353,
      "mean_win_prob": 0.48104718275737696,
      "style_controlled": {
        "median": 452.48092818182,
        "p2.5": 425.03580209319597,
        "p97.5": 504.9112935936863,
        "rank": 84,
        "rank_p2.5": 66,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 72,
      "model_name": "llama-3.1-405b",
      "median": 922.4369058878755,
      "p2.5": 914.5832742611068,
      "p97.5": 930.2905375146443,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 13281,
      "useful": 1736,
      "creative": 426,
      "complete": 1350,
      "clear_formatting": 1415,
      "incorrect": 643,
      "superficial": 692,
      "instructions_not_followed": 430,
      "total_prefs": 6692,
      "positive_prefs_ratio": 0.736252241482367,
      "win_rate": 0.4449099762201336,
      "mean_win_prob": 0.4805606848602648,
      "style_controlled": {
        "median": 518.7859953350851,
        "p2.5": 491.81492686974775,
        "p97.5": 570.06077573811,
        "rank": 71,
        "rank_p2.5": 43,
        "rank_p97.5": 85
      }
    },
    {
      "rank": 73,
      "model_name": "gpt-5-nano",
      "median": 920.5922286922253,
      "p2.5": 906.9127885707056,
      "p97.5": 934.2716688137448,
      "rank_p2.5": 65,
      "rank_p97.5": 79,
      "n_match": 3646,
      "useful": 118,
      "creative": 28,
      "complete": 105,
      "clear_formatting": 66,
      "incorrect": 59,
      "superficial": 79,
      "instructions_not_followed": 69,
      "total_prefs": 524,
      "positive_prefs_ratio": 0.6049618320610687,
      "win_rate": 0.3336762688614541,
      "mean_win_prob": 0.4800562007846389,
      "style_controlled": {
        "median": 568.386668604049,
        "p2.5": 529.0100675901438,
        "p97.5": 619.6224382086926,
        "rank": 55,
        "rank_p2.5": 23,
        "rank_p97.5": 81
      }
    },
    {
      "rank": 74,
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 920.5574567668805,
      "p2.5": 911.6712737560725,
      "p97.5": 929.4436397776884,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 8877,
      "useful": 1088,
      "creative": 215,
      "complete": 616,
      "clear_formatting": 778,
      "incorrect": 246,
      "superficial": 545,
      "instructions_not_followed": 177,
      "total_prefs": 3665,
      "positive_prefs_ratio": 0.7358799454297408,
      "win_rate": 0.4260756569448455,
      "mean_win_prob": 0.48004668199991235,
      "style_controlled": {
        "median": 537.4858827953522,
        "p2.5": 506.1871032798541,
        "p97.5": 591.5708148529823,
        "rank": 64,
        "rank_p2.5": 35,
        "rank_p97.5": 84
      }
    },
    {
      "rank": 75,
      "model_name": "gemma-2-9b-it",
      "median": 920.1693951112068,
      "p2.5": 909.1536101818863,
      "p97.5": 931.1851800405274,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 7040,
      "useful": 983,
      "creative": 308,
      "complete": 665,
      "clear_formatting": 940,
      "incorrect": 278,
      "superficial": 467,
      "instructions_not_followed": 178,
      "total_prefs": 3819,
      "positive_prefs_ratio": 0.7583136946844724,
      "win_rate": 0.47050722974523757,
      "mean_win_prob": 0.47994042701254114,
      "style_controlled": {
        "median": 507.0880875593888,
        "p2.5": 474.4018367824904,
        "p97.5": 555.5408831743609,
        "rank": 74,
        "rank_p2.5": 49,
        "rank_p97.5": 87
      }
    },
    {
      "rank": 76,
      "model_name": "hermes-4-70b",
      "median": 918.3289078316985,
      "p2.5": 903.9281086768225,
      "p97.5": 932.7297069865746,
      "rank_p2.5": 65,
      "rank_p97.5": 80,
      "n_match": 3212,
      "useful": 119,
      "creative": 22,
      "complete": 44,
      "clear_formatting": 77,
      "incorrect": 43,
      "superficial": 96,
      "instructions_not_followed": 36,
      "total_prefs": 437,
      "positive_prefs_ratio": 0.5995423340961098,
      "win_rate": 0.33282325937260904,
      "mean_win_prob": 0.47943589734594283,
      "style_controlled": {
        "median": 545.4177071328292,
        "p2.5": 510.8147922589292,
        "p97.5": 592.0159039212062,
        "rank": 63,
        "rank_p2.5": 35,
        "rank_p97.5": 84
      }
    },
    {
      "rank": 77,
      "model_name": "llama-3.1-8b",
      "median": 916.2752664049635,
      "p2.5": 908.6070227335363,
      "p97.5": 923.9435100763908,
      "rank_p2.5": 66,
      "rank_p97.5": 79,
      "n_match": 13347,
      "useful": 1140,
      "creative": 269,
      "complete": 798,
      "clear_formatting": 820,
      "incorrect": 558,
      "superficial": 674,
      "instructions_not_followed": 367,
      "total_prefs": 4626,
      "positive_prefs_ratio": 0.6543450064850843,
      "win_rate": 0.4096063329054343,
      "mean_win_prob": 0.4788717896591611,
      "style_controlled": {
        "median": 522.6096090675095,
        "p2.5": 495.5473245088223,
        "p97.5": 565.8795637988753,
        "rank": 68,
        "rank_p2.5": 45,
        "rank_p97.5": 85
      }
    },
    {
      "rank": 78,
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 915.4158256910289,
      "p2.5": 902.061807289561,
      "p97.5": 928.7698440924968,
      "rank_p2.5": 66,
      "rank_p97.5": 80,
      "n_match": 3752,
      "useful": 340,
      "creative": 101,
      "complete": 238,
      "clear_formatting": 263,
      "incorrect": 102,
      "superficial": 204,
      "instructions_not_followed": 97,
      "total_prefs": 1345,
      "positive_prefs_ratio": 0.7003717472118959,
      "win_rate": 0.3879051119278316,
      "mean_win_prob": 0.4786353530488531,
      "style_controlled": {
        "median": 497.20533268552913,
        "p2.5": 469.019941059617,
        "p97.5": 549.9982245016331,
        "rank": 76,
        "rank_p2.5": 51,
        "rank_p97.5": 87
      }
    },
    {
      "rank": 79,
      "model_name": "c4ai-command-r-08-2024",
      "median": 911.0042119003234,
      "p2.5": 901.7138405456795,
      "p97.5": 920.2945832549673,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 7987,
      "useful": 812,
      "creative": 187,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 250,
      "superficial": 456,
      "instructions_not_followed": 157,
      "total_prefs": 2902,
      "positive_prefs_ratio": 0.7026188835286009,
      "win_rate": 0.40279913872654566,
      "mean_win_prob": 0.47741834041342274,
      "style_controlled": {
        "median": 516.5743855459812,
        "p2.5": 485.8281287082802,
        "p97.5": 566.793550385405,
        "rank": 72,
        "rank_p2.5": 45,
        "rank_p97.5": 86
      }
    },
    {
      "rank": 80,
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 898.3367780038609,
      "p2.5": 889.8035851914827,
      "p97.5": 906.8699708162391,
      "rank_p2.5": 73,
      "rank_p97.5": 81,
      "n_match": 9547,
      "useful": 988,
      "creative": 211,
      "complete": 671,
      "clear_formatting": 750,
      "incorrect": 436,
      "superficial": 530,
      "instructions_not_followed": 226,
      "total_prefs": 3812,
      "positive_prefs_ratio": 0.6873032528856243,
      "win_rate": 0.3990280086967643,
      "mean_win_prob": 0.47389232296409506,
      "style_controlled": {
        "median": 488.5349203871506,
        "p2.5": 462.1818137010731,
        "p97.5": 541.6509182945961,
        "rank": 77,
        "rank_p2.5": 54,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 81,
      "model_name": "qwen2.5-7b-instruct",
      "median": 875.337371398102,
      "p2.5": 850.8824738941581,
      "p97.5": 899.792268902046,
      "rank_p2.5": 77,
      "rank_p97.5": 85,
      "n_match": 2199,
      "useful": 360,
      "creative": 92,
      "complete": 284,
      "clear_formatting": 325,
      "incorrect": 168,
      "superficial": 167,
      "instructions_not_followed": 88,
      "total_prefs": 1484,
      "positive_prefs_ratio": 0.7149595687331537,
      "win_rate": 0.45285359801488834,
      "mean_win_prob": 0.4673683397188576,
      "style_controlled": {
        "median": 455.7195186911172,
        "p2.5": 345.5736603589539,
        "p97.5": 549.2930270374669,
        "rank": 83,
        "rank_p2.5": 51,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 82,
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 859.1639978079719,
      "p2.5": 841.1291756256331,
      "p97.5": 877.1988199903105,
      "rank_p2.5": 81,
      "rank_p97.5": 85,
      "n_match": 3938,
      "useful": 637,
      "creative": 143,
      "complete": 388,
      "clear_formatting": 452,
      "incorrect": 192,
      "superficial": 336,
      "instructions_not_followed": 162,
      "total_prefs": 2310,
      "positive_prefs_ratio": 0.7012987012987013,
      "win_rate": 0.40750158931977115,
      "mean_win_prob": 0.46268373268202284,
      "style_controlled": {
        "median": 472.994146865738,
        "p2.5": 432.256532307648,
        "p97.5": 523.0240037349678,
        "rank": 79,
        "rank_p2.5": 62,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 83,
      "model_name": "lfm-40b",
      "median": 851.7854378540493,
      "p2.5": 839.0202220769968,
      "p97.5": 864.5506536311019,
      "rank_p2.5": 81,
      "rank_p97.5": 85,
      "n_match": 4974,
      "useful": 650,
      "creative": 130,
      "complete": 362,
      "clear_formatting": 480,
      "incorrect": 180,
      "superficial": 405,
      "instructions_not_followed": 121,
      "total_prefs": 2328,
      "positive_prefs_ratio": 0.6967353951890034,
      "win_rate": 0.3713458755426918,
      "mean_win_prob": 0.4605192909754538,
      "style_controlled": {
        "median": 460.75295892464067,
        "p2.5": 425.5868601856508,
        "p97.5": 512.3216408634302,
        "rank": 81,
        "rank_p2.5": 63,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 84,
      "model_name": "phi-3.5-mini-instruct",
      "median": 833.3018792456671,
      "p2.5": 810.4181405148031,
      "p97.5": 856.1856179765311,
      "rank_p2.5": 81,
      "rank_p97.5": 87,
      "n_match": 3651,
      "useful": 582,
      "creative": 207,
      "complete": 480,
      "clear_formatting": 491,
      "incorrect": 362,
      "superficial": 414,
      "instructions_not_followed": 300,
      "total_prefs": 2836,
      "positive_prefs_ratio": 0.6205923836389281,
      "win_rate": 0.3793103448275862,
      "mean_win_prob": 0.45502061120354187,
      "style_controlled": {
        "median": 406.5945722829199,
        "p2.5": 351.87703237518923,
        "p97.5": 480.878642767712,
        "rank": 87,
        "rank_p2.5": 73,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 85,
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 827.248513134412,
      "p2.5": 816.3264671074209,
      "p97.5": 838.170559161403,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "n_match": 7556,
      "useful": 884,
      "creative": 156,
      "complete": 441,
      "clear_formatting": 557,
      "incorrect": 304,
      "superficial": 676,
      "instructions_not_followed": 282,
      "total_prefs": 3300,
      "positive_prefs_ratio": 0.6175757575757576,
      "win_rate": 0.33665934943125125,
      "mean_win_prob": 0.45319555084341595,
      "style_controlled": {
        "median": 458.9061053432514,
        "p2.5": 425.26560036910723,
        "p97.5": 519.6560674407572,
        "rank": 82,
        "rank_p2.5": 62,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 86,
      "model_name": "olmo-3-32b-think",
      "median": 819.2858481300324,
      "p2.5": 782.466055522279,
      "p97.5": 856.1056407377857,
      "rank_p2.5": 81,
      "rank_p97.5": 88,
      "n_match": 646,
      "useful": 46,
      "creative": 13,
      "complete": 44,
      "clear_formatting": 20,
      "incorrect": 37,
      "superficial": 33,
      "instructions_not_followed": 38,
      "total_prefs": 231,
      "positive_prefs_ratio": 0.5324675324675324,
      "win_rate": 0.2134387351778656,
      "mean_win_prob": 0.4507763035761386,
      "style_controlled": {
        "median": 499.20787375227474,
        "p2.5": 444.36778745977347,
        "p97.5": 555.0237158744399,
        "rank": 75,
        "rank_p2.5": 50,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 87,
      "model_name": "mistral-nemo-2407",
      "median": 818.2253399075382,
      "p2.5": 807.5338127122096,
      "p97.5": 828.9168671028667,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "n_match": 8676,
      "useful": 1207,
      "creative": 193,
      "complete": 598,
      "clear_formatting": 743,
      "incorrect": 376,
      "superficial": 821,
      "instructions_not_followed": 299,
      "total_prefs": 4237,
      "positive_prefs_ratio": 0.646919990559358,
      "win_rate": 0.32752084912812734,
      "mean_win_prob": 0.4504524928083391,
      "style_controlled": {
        "median": 442.38086906515275,
        "p2.5": 410.747247472001,
        "p97.5": 491.62547687239726,
        "rank": 85,
        "rank_p2.5": 70,
        "rank_p97.5": 88
      }
    },
    {
      "rank": 88,
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 779.1761994623147,
      "p2.5": 761.7130511841576,
      "p97.5": 796.6393477404719,
      "rank_p2.5": 87,
      "rank_p97.5": 88,
      "n_match": 2338,
      "useful": 203,
      "creative": 24,
      "complete": 53,
      "clear_formatting": 134,
      "incorrect": 85,
      "superficial": 238,
      "instructions_not_followed": 50,
      "total_prefs": 787,
      "positive_prefs_ratio": 0.5260482846251588,
      "win_rate": 0.2738916256157635,
      "mean_win_prob": 0.4382598879825788,
      "style_controlled": {
        "median": 416.25221265612277,
        "p2.5": 382.6449926051774,
        "p97.5": 468.5558781929062,
        "rank": 86,
        "rank_p2.5": 75,
        "rank_p97.5": 88
      }
    }
  ]
}