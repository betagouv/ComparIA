{
  "timestamp": 1768814954.492154,
  "models": [
    {
      "rank": 1,
      "model_name": "mistral-medium-3.1",
      "median": 1184.3231134936198,
      "p2.5": 1128.2920289787794,
      "p97.5": 1240.35419800846,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 216,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 0,
      "positive_prefs_ratio": 0.0,
      "win_rate": 0.6975308641975309,
      "mean_win_prob": 0.5435455499149398
    },
    {
      "rank": 2,
      "model_name": "gemini-3-flash-preview",
      "median": 1168.8844785472443,
      "p2.5": 1127.6681921474362,
      "p97.5": 1210.1007649470523,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 400,
      "useful": 55,
      "creative": 17,
      "complete": 63,
      "clear_formatting": 49,
      "incorrect": 14,
      "superficial": 15,
      "instructions_not_followed": 5,
      "total_prefs": 218,
      "positive_prefs_ratio": 0.8440366972477065,
      "win_rate": 0.6666666666666666,
      "mean_win_prob": 0.5402570093717468
    },
    {
      "rank": 3,
      "model_name": "mistral-large-2512",
      "median": 1143.669913969347,
      "p2.5": 1126.337181262629,
      "p97.5": 1161.0026466760648,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 2114,
      "useful": 248,
      "creative": 71,
      "complete": 285,
      "clear_formatting": 239,
      "incorrect": 49,
      "superficial": 58,
      "instructions_not_followed": 29,
      "total_prefs": 979,
      "positive_prefs_ratio": 0.8610827374872319,
      "win_rate": 0.6140878988561108,
      "mean_win_prob": 0.534784135878063
    },
    {
      "rank": 4,
      "model_name": "mistral-medium-2508",
      "median": 1136.6547508357423,
      "p2.5": 1126.3847968354423,
      "p97.5": 1146.9247048360423,
      "rank_p2.5": 1,
      "rank_p97.5": 8,
      "n_match": 6723,
      "useful": 261,
      "creative": 77,
      "complete": 222,
      "clear_formatting": 235,
      "incorrect": 46,
      "superficial": 68,
      "instructions_not_followed": 35,
      "total_prefs": 944,
      "positive_prefs_ratio": 0.8421610169491526,
      "win_rate": 0.637304724261246,
      "mean_win_prob": 0.5332384808874576
    },
    {
      "rank": 5,
      "model_name": "gemini-2.5-flash",
      "median": 1133.1152444176448,
      "p2.5": 1121.4153383111404,
      "p97.5": 1144.8151505241494,
      "rank_p2.5": 1,
      "rank_p97.5": 9,
      "n_match": 5037,
      "useful": 256,
      "creative": 85,
      "complete": 250,
      "clear_formatting": 224,
      "incorrect": 38,
      "superficial": 58,
      "instructions_not_followed": 22,
      "total_prefs": 933,
      "positive_prefs_ratio": 0.8735262593783494,
      "win_rate": 0.6396534148827727,
      "mean_win_prob": 0.5324547552438748
    },
    {
      "rank": 6,
      "model_name": "gemini-3-pro-preview",
      "median": 1131.9914220776084,
      "p2.5": 1116.0899365289454,
      "p97.5": 1147.8929076262714,
      "rank_p2.5": 1,
      "rank_p97.5": 9,
      "n_match": 2663,
      "useful": 260,
      "creative": 75,
      "complete": 228,
      "clear_formatting": 186,
      "incorrect": 39,
      "superficial": 61,
      "instructions_not_followed": 25,
      "total_prefs": 874,
      "positive_prefs_ratio": 0.8569794050343249,
      "win_rate": 0.6036911121903836,
      "mean_win_prob": 0.5322053708677503
    },
    {
      "rank": 7,
      "model_name": "qwen3-max-2025-09-23",
      "median": 1125.0602338720505,
      "p2.5": 1110.5948275977048,
      "p97.5": 1139.5256401463962,
      "rank_p2.5": 1,
      "rank_p97.5": 11,
      "n_match": 3229,
      "useful": 141,
      "creative": 38,
      "complete": 102,
      "clear_formatting": 133,
      "incorrect": 29,
      "superficial": 31,
      "instructions_not_followed": 10,
      "total_prefs": 484,
      "positive_prefs_ratio": 0.8553719008264463,
      "win_rate": 0.6348697394789579,
      "mean_win_prob": 0.5306614513162338
    },
    {
      "rank": 8,
      "model_name": "magistral-medium",
      "median": 1115.0890799448507,
      "p2.5": 1099.2835908513368,
      "p97.5": 1130.8945690383648,
      "rank_p2.5": 1,
      "rank_p97.5": 14,
      "n_match": 2668,
      "useful": 17,
      "creative": 4,
      "complete": 10,
      "clear_formatting": 12,
      "incorrect": 6,
      "superficial": 7,
      "instructions_not_followed": 0,
      "total_prefs": 56,
      "positive_prefs_ratio": 0.7678571428571429,
      "win_rate": 0.6241446725317693,
      "mean_win_prob": 0.5284226057905203
    },
    {
      "rank": 9,
      "model_name": "gemini-2.0-flash",
      "median": 1107.5790023022655,
      "p2.5": 1099.0835013737817,
      "p97.5": 1116.074503230749,
      "rank_p2.5": 7,
      "rank_p97.5": 14,
      "n_match": 10633,
      "useful": 1479,
      "creative": 473,
      "complete": 1551,
      "clear_formatting": 1332,
      "incorrect": 208,
      "superficial": 304,
      "instructions_not_followed": 109,
      "total_prefs": 5456,
      "positive_prefs_ratio": 0.8861803519061584,
      "win_rate": 0.6905578417924096,
      "mean_win_prob": 0.5267223442321426
    },
    {
      "rank": 10,
      "model_name": "gpt-5.2",
      "median": 1103.593740069772,
      "p2.5": 1082.5910649917948,
      "p97.5": 1124.5964151477492,
      "rank_p2.5": 5,
      "rank_p97.5": 22,
      "n_match": 1520,
      "useful": 205,
      "creative": 45,
      "complete": 159,
      "clear_formatting": 159,
      "incorrect": 26,
      "superficial": 72,
      "instructions_not_followed": 30,
      "total_prefs": 696,
      "positive_prefs_ratio": 0.8160919540229885,
      "win_rate": 0.5385329619312906,
      "mean_win_prob": 0.5258151537156596
    },
    {
      "rank": 11,
      "model_name": "deepseek-v3-0324",
      "median": 1101.3507929971008,
      "p2.5": 1090.1152664976844,
      "p97.5": 1112.5863194965175,
      "rank_p2.5": 7,
      "rank_p97.5": 19,
      "n_match": 5390,
      "useful": 852,
      "creative": 256,
      "complete": 696,
      "clear_formatting": 779,
      "incorrect": 158,
      "superficial": 226,
      "instructions_not_followed": 77,
      "total_prefs": 3044,
      "positive_prefs_ratio": 0.8485545335085414,
      "win_rate": 0.6450783259293897,
      "mean_win_prob": 0.5253030618870457
    },
    {
      "rank": 12,
      "model_name": "claude-4-5-sonnet",
      "median": 1099.3641604176291,
      "p2.5": 1090.0031376033548,
      "p97.5": 1108.7251832319037,
      "rank_p2.5": 8,
      "rank_p97.5": 19,
      "n_match": 7580,
      "useful": 278,
      "creative": 65,
      "complete": 156,
      "clear_formatting": 202,
      "incorrect": 51,
      "superficial": 108,
      "instructions_not_followed": 27,
      "total_prefs": 887,
      "positive_prefs_ratio": 0.790304396843292,
      "win_rate": 0.5644644002565747,
      "mean_win_prob": 0.5248485745614869
    },
    {
      "rank": 13,
      "model_name": "gemma-3-27b",
      "median": 1089.3271735619746,
      "p2.5": 1080.8991096404302,
      "p97.5": 1097.7552374835193,
      "rank_p2.5": 10,
      "rank_p97.5": 25,
      "n_match": 9485,
      "useful": 1003,
      "creative": 333,
      "complete": 1012,
      "clear_formatting": 845,
      "incorrect": 206,
      "superficial": 194,
      "instructions_not_followed": 104,
      "total_prefs": 3697,
      "positive_prefs_ratio": 0.8636732485799297,
      "win_rate": 0.6153432755609297,
      "mean_win_prob": 0.5225391516783673
    },
    {
      "rank": 14,
      "model_name": "deepseek-chat-v3.1",
      "median": 1085.173469287682,
      "p2.5": 1067.6759237104216,
      "p97.5": 1102.6710148649424,
      "rank_p2.5": 8,
      "rank_p97.5": 30,
      "n_match": 2135,
      "useful": 116,
      "creative": 28,
      "complete": 83,
      "clear_formatting": 86,
      "incorrect": 25,
      "superficial": 30,
      "instructions_not_followed": 16,
      "total_prefs": 384,
      "positive_prefs_ratio": 0.8151041666666666,
      "win_rate": 0.5651902682470368,
      "mean_win_prob": 0.5215769064052771
    },
    {
      "rank": 15,
      "model_name": "gpt-5.1",
      "median": 1084.496552098034,
      "p2.5": 1069.4337121523954,
      "p97.5": 1099.5593920436727,
      "rank_p2.5": 8,
      "rank_p97.5": 28,
      "n_match": 2809,
      "useful": 205,
      "creative": 50,
      "complete": 186,
      "clear_formatting": 152,
      "incorrect": 25,
      "superficial": 66,
      "instructions_not_followed": 31,
      "total_prefs": 715,
      "positive_prefs_ratio": 0.8293706293706293,
      "win_rate": 0.5415129151291513,
      "mean_win_prob": 0.5214197281061659
    },
    {
      "rank": 16,
      "model_name": "glm-4.5",
      "median": 1077.217043651891,
      "p2.5": 1062.1526816849348,
      "p97.5": 1092.2814056188472,
      "rank_p2.5": 10,
      "rank_p97.5": 34,
      "n_match": 2729,
      "useful": 100,
      "creative": 35,
      "complete": 106,
      "clear_formatting": 90,
      "incorrect": 18,
      "superficial": 30,
      "instructions_not_followed": 15,
      "total_prefs": 394,
      "positive_prefs_ratio": 0.8401015228426396,
      "win_rate": 0.5657222480260102,
      "mean_win_prob": 0.5197229618987197
    },
    {
      "rank": 17,
      "model_name": "grok-4-fast",
      "median": 1076.1213997441223,
      "p2.5": 1062.0674898746506,
      "p97.5": 1090.175309613594,
      "rank_p2.5": 10,
      "rank_p97.5": 34,
      "n_match": 3202,
      "useful": 57,
      "creative": 19,
      "complete": 42,
      "clear_formatting": 44,
      "incorrect": 8,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 187,
      "positive_prefs_ratio": 0.8663101604278075,
      "win_rate": 0.571828731492597,
      "mean_win_prob": 0.5194665485883548
    },
    {
      "rank": 18,
      "model_name": "DeepSeek-V3.2",
      "median": 1075.9504400499316,
      "p2.5": 1056.0111472946921,
      "p97.5": 1095.8897328051708,
      "rank_p2.5": 10,
      "rank_p97.5": 34,
      "n_match": 1478,
      "useful": 160,
      "creative": 46,
      "complete": 148,
      "clear_formatting": 128,
      "incorrect": 48,
      "superficial": 53,
      "instructions_not_followed": 22,
      "total_prefs": 605,
      "positive_prefs_ratio": 0.7966942148760331,
      "win_rate": 0.5197313182199832,
      "mean_win_prob": 0.5194265144777229
    },
    {
      "rank": 19,
      "model_name": "claude-4-sonnet",
      "median": 1072.263193415274,
      "p2.5": 1057.9925813551677,
      "p97.5": 1086.53380547538,
      "rank_p2.5": 12,
      "rank_p97.5": 34,
      "n_match": 3077,
      "useful": 67,
      "creative": 24,
      "complete": 57,
      "clear_formatting": 69,
      "incorrect": 12,
      "superficial": 19,
      "instructions_not_followed": 7,
      "total_prefs": 255,
      "positive_prefs_ratio": 0.8509803921568627,
      "win_rate": 0.5642296571664601,
      "mean_win_prob": 0.5185614514406239
    },
    {
      "rank": 20,
      "model_name": "grok-4.1-fast",
      "median": 1071.7341825980056,
      "p2.5": 1055.5212370118388,
      "p97.5": 1087.9471281841727,
      "rank_p2.5": 12,
      "rank_p97.5": 34,
      "n_match": 2272,
      "useful": 174,
      "creative": 69,
      "complete": 142,
      "clear_formatting": 132,
      "incorrect": 44,
      "superficial": 64,
      "instructions_not_followed": 21,
      "total_prefs": 646,
      "positive_prefs_ratio": 0.8003095975232198,
      "win_rate": 0.5314379442318207,
      "mean_win_prob": 0.5184370874996067
    },
    {
      "rank": 21,
      "model_name": "mistral-small-2506",
      "median": 1069.3561924748833,
      "p2.5": 1058.2920284970505,
      "p97.5": 1080.4203564527165,
      "rank_p2.5": 14,
      "rank_p97.5": 34,
      "n_match": 5528,
      "useful": 212,
      "creative": 50,
      "complete": 140,
      "clear_formatting": 185,
      "incorrect": 58,
      "superficial": 74,
      "instructions_not_followed": 29,
      "total_prefs": 748,
      "positive_prefs_ratio": 0.7847593582887701,
      "win_rate": 0.5384986724595704,
      "mean_win_prob": 0.5178772646659009
    },
    {
      "rank": 22,
      "model_name": "gpt-oss-120b",
      "median": 1068.8333733259783,
      "p2.5": 1050.2439827801286,
      "p97.5": 1087.4227638718282,
      "rank_p2.5": 12,
      "rank_p97.5": 34,
      "n_match": 1832,
      "useful": 141,
      "creative": 62,
      "complete": 205,
      "clear_formatting": 139,
      "incorrect": 54,
      "superficial": 44,
      "instructions_not_followed": 33,
      "total_prefs": 678,
      "positive_prefs_ratio": 0.8067846607669616,
      "win_rate": 0.5612391930835735,
      "mean_win_prob": 0.5177540105184415
    },
    {
      "rank": 23,
      "model_name": "gemma-3-12b",
      "median": 1066.6194533709117,
      "p2.5": 1058.0950938445576,
      "p97.5": 1075.143812897266,
      "rank_p2.5": 14,
      "rank_p97.5": 34,
      "n_match": 9041,
      "useful": 905,
      "creative": 255,
      "complete": 911,
      "clear_formatting": 736,
      "incorrect": 242,
      "superficial": 238,
      "instructions_not_followed": 146,
      "total_prefs": 3433,
      "positive_prefs_ratio": 0.8176521992426449,
      "win_rate": 0.5822298163178129,
      "mean_win_prob": 0.5172313885823746
    },
    {
      "rank": 24,
      "model_name": "deepseek-r1-0528",
      "median": 1062.0886435922998,
      "p2.5": 1041.877298361011,
      "p97.5": 1082.2999888235886,
      "rank_p2.5": 13,
      "rank_p97.5": 35,
      "n_match": 1603,
      "useful": 104,
      "creative": 28,
      "complete": 70,
      "clear_formatting": 67,
      "incorrect": 25,
      "superficial": 32,
      "instructions_not_followed": 16,
      "total_prefs": 342,
      "positive_prefs_ratio": 0.7865497076023392,
      "win_rate": 0.5396551724137931,
      "mean_win_prob": 0.5161583334714487
    },
    {
      "rank": 25,
      "model_name": "kimi-k2-thinking",
      "median": 1062.0466685717765,
      "p2.5": 1033.717377459971,
      "p97.5": 1090.3759596835819,
      "rank_p2.5": 10,
      "rank_p97.5": 41,
      "n_match": 760,
      "useful": 90,
      "creative": 24,
      "complete": 81,
      "clear_formatting": 59,
      "incorrect": 19,
      "superficial": 29,
      "instructions_not_followed": 18,
      "total_prefs": 320,
      "positive_prefs_ratio": 0.79375,
      "win_rate": 0.5371621621621622,
      "mean_win_prob": 0.516148370228917
    },
    {
      "rank": 26,
      "model_name": "kimi-k2",
      "median": 1061.819133539126,
      "p2.5": 1041.6640472506963,
      "p97.5": 1081.9742198275555,
      "rank_p2.5": 13,
      "rank_p97.5": 35,
      "n_match": 1492,
      "useful": 113,
      "creative": 35,
      "complete": 72,
      "clear_formatting": 83,
      "incorrect": 22,
      "superficial": 37,
      "instructions_not_followed": 9,
      "total_prefs": 371,
      "positive_prefs_ratio": 0.816711590296496,
      "win_rate": 0.5300411522633744,
      "mean_win_prob": 0.5160943551623722
    },
    {
      "rank": 27,
      "model_name": "deepseek-v3-chat",
      "median": 1059.3161692782141,
      "p2.5": 1049.3077730644804,
      "p97.5": 1069.3245654919476,
      "rank_p2.5": 15,
      "rank_p97.5": 34,
      "n_match": 7560,
      "useful": 833,
      "creative": 209,
      "complete": 790,
      "clear_formatting": 769,
      "incorrect": 143,
      "superficial": 188,
      "instructions_not_followed": 81,
      "total_prefs": 3013,
      "positive_prefs_ratio": 0.8632592100896117,
      "win_rate": 0.6549130230371415,
      "mean_win_prob": 0.5154993815664578
    },
    {
      "rank": 28,
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1058.7168119772386,
      "p2.5": 1035.288792478206,
      "p97.5": 1082.144831476271,
      "rank_p2.5": 13,
      "rank_p97.5": 41,
      "n_match": 1091,
      "useful": 87,
      "creative": 23,
      "complete": 57,
      "clear_formatting": 75,
      "incorrect": 9,
      "superficial": 28,
      "instructions_not_followed": 10,
      "total_prefs": 289,
      "positive_prefs_ratio": 0.8373702422145328,
      "win_rate": 0.5394736842105263,
      "mean_win_prob": 0.5153566947428376
    },
    {
      "rank": 29,
      "model_name": "command-a",
      "median": 1057.4126651256167,
      "p2.5": 1048.815346940694,
      "p97.5": 1066.0099833105394,
      "rank_p2.5": 16,
      "rank_p97.5": 34,
      "n_match": 8914,
      "useful": 904,
      "creative": 207,
      "complete": 773,
      "clear_formatting": 793,
      "incorrect": 195,
      "superficial": 274,
      "instructions_not_followed": 105,
      "total_prefs": 3251,
      "positive_prefs_ratio": 0.8234389418640419,
      "win_rate": 0.5693313953488373,
      "mean_win_prob": 0.5150459335010895
    },
    {
      "rank": 30,
      "model_name": "claude-3-7-sonnet",
      "median": 1053.5364902874421,
      "p2.5": 1041.891640904188,
      "p97.5": 1065.1813396706962,
      "rank_p2.5": 16,
      "rank_p97.5": 35,
      "n_match": 4743,
      "useful": 734,
      "creative": 161,
      "complete": 511,
      "clear_formatting": 621,
      "incorrect": 84,
      "superficial": 245,
      "instructions_not_followed": 44,
      "total_prefs": 2400,
      "positive_prefs_ratio": 0.8445833333333334,
      "win_rate": 0.5711649973642594,
      "mean_win_prob": 0.5141199575624216
    },
    {
      "rank": 31,
      "model_name": "grok-3-mini-beta",
      "median": 1051.4440312512984,
      "p2.5": 1033.2189465082938,
      "p97.5": 1069.669115994303,
      "rank_p2.5": 14,
      "rank_p97.5": 41,
      "n_match": 2066,
      "useful": 247,
      "creative": 65,
      "complete": 231,
      "clear_formatting": 234,
      "incorrect": 65,
      "superficial": 65,
      "instructions_not_followed": 42,
      "total_prefs": 949,
      "positive_prefs_ratio": 0.8187565858798735,
      "win_rate": 0.574085554866708,
      "mean_win_prob": 0.5136186351311962
    },
    {
      "rank": 32,
      "model_name": "magistral-small-2506",
      "median": 1050.8421032180588,
      "p2.5": 1037.7654053015835,
      "p97.5": 1063.918801134534,
      "rank_p2.5": 16,
      "rank_p97.5": 38,
      "n_match": 3850,
      "useful": 83,
      "creative": 18,
      "complete": 59,
      "clear_formatting": 77,
      "incorrect": 20,
      "superficial": 28,
      "instructions_not_followed": 12,
      "total_prefs": 297,
      "positive_prefs_ratio": 0.797979797979798,
      "win_rate": 0.5278849526481936,
      "mean_win_prob": 0.5134742322388133
    },
    {
      "rank": 33,
      "model_name": "glm-4.6",
      "median": 1049.6108940778317,
      "p2.5": 1034.6545013517145,
      "p97.5": 1064.567286803949,
      "rank_p2.5": 16,
      "rank_p97.5": 41,
      "n_match": 2726,
      "useful": 113,
      "creative": 36,
      "complete": 117,
      "clear_formatting": 106,
      "incorrect": 27,
      "superficial": 46,
      "instructions_not_followed": 22,
      "total_prefs": 467,
      "positive_prefs_ratio": 0.7965738758029979,
      "win_rate": 0.5256701499318491,
      "mean_win_prob": 0.5131785998218239
    },
    {
      "rank": 34,
      "model_name": "gemma-3n-e4b-it",
      "median": 1032.6142978526254,
      "p2.5": 1020.7039866516334,
      "p97.5": 1044.5246090536177,
      "rank_p2.5": 26,
      "rank_p97.5": 42,
      "n_match": 4621,
      "useful": 207,
      "creative": 72,
      "complete": 157,
      "clear_formatting": 179,
      "incorrect": 82,
      "superficial": 90,
      "instructions_not_followed": 43,
      "total_prefs": 830,
      "positive_prefs_ratio": 0.7409638554216867,
      "win_rate": 0.5041702617198734,
      "mean_win_prob": 0.5090607926795105
    },
    {
      "rank": 35,
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1031.877014814858,
      "p2.5": 1022.8453749816133,
      "p97.5": 1040.9086546481028,
      "rank_p2.5": 29,
      "rank_p97.5": 42,
      "n_match": 8481,
      "useful": 1052,
      "creative": 320,
      "complete": 1060,
      "clear_formatting": 978,
      "incorrect": 211,
      "superficial": 285,
      "instructions_not_followed": 133,
      "total_prefs": 4039,
      "positive_prefs_ratio": 0.844268383263184,
      "win_rate": 0.5898007731192387,
      "mean_win_prob": 0.5088806076283188
    },
    {
      "rank": 36,
      "model_name": "gpt-4.1-mini",
      "median": 1029.1454879066346,
      "p2.5": 1020.33408445673,
      "p97.5": 1037.956891356539,
      "rank_p2.5": 29,
      "rank_p97.5": 43,
      "n_match": 8592,
      "useful": 870,
      "creative": 190,
      "complete": 569,
      "clear_formatting": 722,
      "incorrect": 139,
      "superficial": 324,
      "instructions_not_followed": 83,
      "total_prefs": 2897,
      "positive_prefs_ratio": 0.8115291681049361,
      "win_rate": 0.519074613145396,
      "mean_win_prob": 0.5082119049253223
    },
    {
      "rank": 37,
      "model_name": "gemma-3-4b",
      "median": 1028.7760649463219,
      "p2.5": 1020.7834192469169,
      "p97.5": 1036.768710645727,
      "rank_p2.5": 30,
      "rank_p97.5": 42,
      "n_match": 10317,
      "useful": 908,
      "creative": 252,
      "complete": 905,
      "clear_formatting": 787,
      "incorrect": 369,
      "superficial": 241,
      "instructions_not_followed": 181,
      "total_prefs": 3643,
      "positive_prefs_ratio": 0.7828712599505901,
      "win_rate": 0.5299913782485528,
      "mean_win_prob": 0.5081213283404625
    },
    {
      "rank": 38,
      "model_name": "deepseek-r1",
      "median": 1024.5625213853739,
      "p2.5": 1011.834143308442,
      "p97.5": 1037.2908994623058,
      "rank_p2.5": 30,
      "rank_p97.5": 47,
      "n_match": 4277,
      "useful": 576,
      "creative": 213,
      "complete": 554,
      "clear_formatting": 507,
      "incorrect": 119,
      "superficial": 148,
      "instructions_not_followed": 104,
      "total_prefs": 2221,
      "positive_prefs_ratio": 0.8329581269698334,
      "win_rate": 0.5648063433973772,
      "mean_win_prob": 0.5070858923361824
    },
    {
      "rank": 39,
      "model_name": "glm-4.7",
      "median": 1021.8449835879326,
      "p2.5": 974.5832121995214,
      "p97.5": 1069.1067549763436,
      "rank_p2.5": 15,
      "rank_p97.5": 60,
      "n_match": 239,
      "useful": 26,
      "creative": 13,
      "complete": 26,
      "clear_formatting": 28,
      "incorrect": 10,
      "superficial": 11,
      "instructions_not_followed": 8,
      "total_prefs": 122,
      "positive_prefs_ratio": 0.7622950819672131,
      "win_rate": 0.4787234042553192,
      "mean_win_prob": 0.5064157913695565
    },
    {
      "rank": 40,
      "model_name": "qwen3-32b",
      "median": 1013.5772365804189,
      "p2.5": 996.7480094246741,
      "p97.5": 1030.4064637361637,
      "rank_p2.5": 34,
      "rank_p97.5": 52,
      "n_match": 2225,
      "useful": 222,
      "creative": 61,
      "complete": 191,
      "clear_formatting": 173,
      "incorrect": 84,
      "superficial": 76,
      "instructions_not_followed": 52,
      "total_prefs": 859,
      "positive_prefs_ratio": 0.7532013969732246,
      "win_rate": 0.48821161587119033,
      "mean_win_prob": 0.5043659561965818
    },
    {
      "rank": 41,
      "model_name": "mistral-saba",
      "median": 1008.8283490554772,
      "p2.5": 998.3454200905196,
      "p97.5": 1019.3112780204348,
      "rank_p2.5": 38,
      "rank_p97.5": 52,
      "n_match": 6004,
      "useful": 506,
      "creative": 117,
      "complete": 359,
      "clear_formatting": 390,
      "incorrect": 134,
      "superficial": 239,
      "instructions_not_followed": 96,
      "total_prefs": 1841,
      "positive_prefs_ratio": 0.7452471482889734,
      "win_rate": 0.5004434589800444,
      "mean_win_prob": 0.5031809142540565
    },
    {
      "rank": 42,
      "model_name": "llama-maverick",
      "median": 1007.6847734970222,
      "p2.5": 994.8338978591778,
      "p97.5": 1020.5356491348665,
      "rank_p2.5": 37,
      "rank_p97.5": 52,
      "n_match": 3834,
      "useful": 140,
      "creative": 30,
      "complete": 91,
      "clear_formatting": 101,
      "incorrect": 36,
      "superficial": 72,
      "instructions_not_followed": 18,
      "total_prefs": 488,
      "positive_prefs_ratio": 0.7418032786885246,
      "win_rate": 0.4656978709023319,
      "mean_win_prob": 0.5028947062440391
    },
    {
      "rank": 43,
      "model_name": "mistral-large-2411",
      "median": 1007.6138315225666,
      "p2.5": 999.5642484085521,
      "p97.5": 1015.6634146365809,
      "rank_p2.5": 38,
      "rank_p97.5": 51,
      "n_match": 10960,
      "useful": 1392,
      "creative": 260,
      "complete": 1068,
      "clear_formatting": 1146,
      "incorrect": 213,
      "superficial": 465,
      "instructions_not_followed": 132,
      "total_prefs": 4676,
      "positive_prefs_ratio": 0.8267750213857998,
      "win_rate": 0.5500690607734806,
      "mean_win_prob": 0.5028769405032997
    },
    {
      "rank": 44,
      "model_name": "gemini-1.5-pro",
      "median": 1003.2399371603666,
      "p2.5": 993.5363172321483,
      "p97.5": 1012.9435570885847,
      "rank_p2.5": 38,
      "rank_p97.5": 52,
      "n_match": 10027,
      "useful": 1656,
      "creative": 524,
      "complete": 1435,
      "clear_formatting": 1532,
      "incorrect": 287,
      "superficial": 498,
      "instructions_not_followed": 205,
      "total_prefs": 6137,
      "positive_prefs_ratio": 0.8386833957959915,
      "win_rate": 0.5857166203628044,
      "mean_win_prob": 0.5017791672160634
    },
    {
      "rank": 45,
      "model_name": "llama-4-scout",
      "median": 1001.5931405177308,
      "p2.5": 992.1923344020381,
      "p97.5": 1010.9939466334235,
      "rank_p2.5": 39,
      "rank_p97.5": 53,
      "n_match": 7070,
      "useful": 619,
      "creative": 141,
      "complete": 465,
      "clear_formatting": 487,
      "incorrect": 167,
      "superficial": 289,
      "instructions_not_followed": 93,
      "total_prefs": 2261,
      "positive_prefs_ratio": 0.75718708536046,
      "win_rate": 0.48218484355217944,
      "mean_win_prob": 0.5013646036635898
    },
    {
      "rank": 46,
      "model_name": "gpt-oss-20b",
      "median": 999.5242611666135,
      "p2.5": 981.9641394618477,
      "p97.5": 1017.0843828713793,
      "rank_p2.5": 38,
      "rank_p97.5": 57,
      "n_match": 2183,
      "useful": 153,
      "creative": 65,
      "complete": 139,
      "clear_formatting": 120,
      "incorrect": 54,
      "superficial": 77,
      "instructions_not_followed": 40,
      "total_prefs": 648,
      "positive_prefs_ratio": 0.7361111111111112,
      "win_rate": 0.4470284237726098,
      "mean_win_prob": 0.5008428156556709
    },
    {
      "rank": 47,
      "model_name": "EuroLLM-22B-Instruct-2512",
      "median": 997.1557424487937,
      "p2.5": 956.6535648398205,
      "p97.5": 1037.657920057767,
      "rank_p2.5": 30,
      "rank_p97.5": 66,
      "n_match": 334,
      "useful": 52,
      "creative": 14,
      "complete": 31,
      "clear_formatting": 38,
      "incorrect": 9,
      "superficial": 20,
      "instructions_not_followed": 10,
      "total_prefs": 174,
      "positive_prefs_ratio": 0.7758620689655172,
      "win_rate": 0.4197080291970803,
      "mean_win_prob": 0.5002441264719631
    },
    {
      "rank": 48,
      "model_name": "gpt-5-mini",
      "median": 994.7246911732652,
      "p2.5": 981.9940743820868,
      "p97.5": 1007.4553079644437,
      "rank_p2.5": 39,
      "rank_p97.5": 57,
      "n_match": 4029,
      "useful": 169,
      "creative": 35,
      "complete": 169,
      "clear_formatting": 110,
      "incorrect": 32,
      "superficial": 71,
      "instructions_not_followed": 37,
      "total_prefs": 623,
      "positive_prefs_ratio": 0.7752808988764045,
      "win_rate": 0.4357901512713228,
      "mean_win_prob": 0.499628149646213
    },
    {
      "rank": 49,
      "model_name": "mistral-small-3.1-24b",
      "median": 994.2123883101494,
      "p2.5": 983.6092130199427,
      "p97.5": 1004.8155636003561,
      "rank_p2.5": 39,
      "rank_p97.5": 57,
      "n_match": 5935,
      "useful": 892,
      "creative": 192,
      "complete": 538,
      "clear_formatting": 722,
      "incorrect": 197,
      "superficial": 344,
      "instructions_not_followed": 124,
      "total_prefs": 3009,
      "positive_prefs_ratio": 0.7789963443004321,
      "win_rate": 0.5015695067264574,
      "mean_win_prob": 0.4994981510204338
    },
    {
      "rank": 50,
      "model_name": "o4-mini",
      "median": 988.4760558977366,
      "p2.5": 974.922410092272,
      "p97.5": 1002.0297017032012,
      "rank_p2.5": 39,
      "rank_p97.5": 60,
      "n_match": 3568,
      "useful": 321,
      "creative": 71,
      "complete": 206,
      "clear_formatting": 182,
      "incorrect": 75,
      "superficial": 125,
      "instructions_not_followed": 34,
      "total_prefs": 1014,
      "positive_prefs_ratio": 0.7692307692307693,
      "win_rate": 0.4513338139870223,
      "mean_win_prob": 0.498037954004643
    },
    {
      "rank": 51,
      "model_name": "lfm2-8b-a1b",
      "median": 981.4357810554619,
      "p2.5": 963.8864999460433,
      "p97.5": 998.9850621648806,
      "rank_p2.5": 40,
      "rank_p97.5": 65,
      "n_match": 2018,
      "useful": 125,
      "creative": 20,
      "complete": 83,
      "clear_formatting": 105,
      "incorrect": 68,
      "superficial": 77,
      "instructions_not_followed": 37,
      "total_prefs": 515,
      "positive_prefs_ratio": 0.6466019417475728,
      "win_rate": 0.4131922837585563,
      "mean_win_prob": 0.49623425742860694
    },
    {
      "rank": 52,
      "model_name": "gpt-5",
      "median": 980.7894857374262,
      "p2.5": 968.3585536080232,
      "p97.5": 993.220417866829,
      "rank_p2.5": 44,
      "rank_p97.5": 62,
      "n_match": 4654,
      "useful": 1,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 1,
      "positive_prefs_ratio": 1.0,
      "win_rate": 0.4053893988747409,
      "mean_win_prob": 0.49606803479874273
    },
    {
      "rank": 53,
      "model_name": "qwen-3-8b",
      "median": 974.264874134469,
      "p2.5": 959.9749246420662,
      "p97.5": 988.5548236268719,
      "rank_p2.5": 45,
      "rank_p97.5": 66,
      "n_match": 3092,
      "useful": 120,
      "creative": 39,
      "complete": 107,
      "clear_formatting": 82,
      "incorrect": 75,
      "superficial": 46,
      "instructions_not_followed": 34,
      "total_prefs": 503,
      "positive_prefs_ratio": 0.6918489065606361,
      "win_rate": 0.4082719082719083,
      "mean_win_prob": 0.4943838434157924
    },
    {
      "rank": 54,
      "model_name": "aya-expanse-32b",
      "median": 974.2075698166451,
      "p2.5": 963.9016085738823,
      "p97.5": 984.5135310594079,
      "rank_p2.5": 45,
      "rank_p97.5": 65,
      "n_match": 6060,
      "useful": 491,
      "creative": 88,
      "complete": 300,
      "clear_formatting": 348,
      "incorrect": 158,
      "superficial": 242,
      "instructions_not_followed": 87,
      "total_prefs": 1714,
      "positive_prefs_ratio": 0.7158693115519253,
      "win_rate": 0.44252631578947366,
      "mean_win_prob": 0.4943690020985522
    },
    {
      "rank": 55,
      "model_name": "qwen3-30b-a3b",
      "median": 973.9023104248073,
      "p2.5": 960.7245855123404,
      "p97.5": 987.0800353372744,
      "rank_p2.5": 45,
      "rank_p97.5": 66,
      "n_match": 3727,
      "useful": 136,
      "creative": 23,
      "complete": 120,
      "clear_formatting": 111,
      "incorrect": 74,
      "superficial": 76,
      "instructions_not_followed": 33,
      "total_prefs": 573,
      "positive_prefs_ratio": 0.680628272251309,
      "win_rate": 0.4075094729590079,
      "mean_win_prob": 0.49428992803531213
    },
    {
      "rank": 56,
      "model_name": "llama-3.3-70b",
      "median": 968.2567594901715,
      "p2.5": 960.6550129388247,
      "p97.5": 975.8585060415181,
      "rank_p2.5": 48,
      "rank_p97.5": 66,
      "n_match": 11351,
      "useful": 1089,
      "creative": 212,
      "complete": 780,
      "clear_formatting": 804,
      "incorrect": 255,
      "superficial": 502,
      "instructions_not_followed": 144,
      "total_prefs": 3786,
      "positive_prefs_ratio": 0.7620179609086106,
      "win_rate": 0.4657958822227142,
      "mean_win_prob": 0.492823080447444
    },
    {
      "rank": 57,
      "model_name": "o3-mini",
      "median": 967.5209028628822,
      "p2.5": 949.9077550858556,
      "p97.5": 985.1340506399089,
      "rank_p2.5": 45,
      "rank_p97.5": 68,
      "n_match": 1951,
      "useful": 252,
      "creative": 62,
      "complete": 165,
      "clear_formatting": 157,
      "incorrect": 35,
      "superficial": 97,
      "instructions_not_followed": 28,
      "total_prefs": 796,
      "positive_prefs_ratio": 0.7989949748743719,
      "win_rate": 0.5047679593134139,
      "mean_win_prob": 0.4926312661641942
    },
    {
      "rank": 58,
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 963.6282196117779,
      "p2.5": 951.469769460271,
      "p97.5": 975.7866697632849,
      "rank_p2.5": 48,
      "rank_p97.5": 68,
      "n_match": 4345,
      "useful": 524,
      "creative": 106,
      "complete": 415,
      "clear_formatting": 431,
      "incorrect": 89,
      "superficial": 189,
      "instructions_not_followed": 71,
      "total_prefs": 1825,
      "positive_prefs_ratio": 0.8087671232876712,
      "win_rate": 0.51994342291372,
      "mean_win_prob": 0.49161417161886223
    },
    {
      "rank": 59,
      "model_name": "gpt-4.1-nano",
      "median": 960.1851238655514,
      "p2.5": 950.4098486074593,
      "p97.5": 969.9603991236435,
      "rank_p2.5": 50,
      "rank_p97.5": 68,
      "n_match": 6875,
      "useful": 640,
      "creative": 108,
      "complete": 344,
      "clear_formatting": 465,
      "incorrect": 145,
      "superficial": 301,
      "instructions_not_followed": 83,
      "total_prefs": 2086,
      "positive_prefs_ratio": 0.7464046021093,
      "win_rate": 0.42656162070906023,
      "mean_win_prob": 0.49071117345168064
    },
    {
      "rank": 60,
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 959.6653655100035,
      "p2.5": 944.0387055388409,
      "p97.5": 975.2920254811663,
      "rank_p2.5": 48,
      "rank_p97.5": 70,
      "n_match": 2660,
      "useful": 98,
      "creative": 22,
      "complete": 62,
      "clear_formatting": 60,
      "incorrect": 35,
      "superficial": 63,
      "instructions_not_followed": 26,
      "total_prefs": 366,
      "positive_prefs_ratio": 0.6612021857923497,
      "win_rate": 0.38380450407283184,
      "mean_win_prob": 0.4905745836231059
    },
    {
      "rank": 61,
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 959.0744803217036,
      "p2.5": 950.0672587325391,
      "p97.5": 968.0817019108681,
      "rank_p2.5": 51,
      "rank_p97.5": 68,
      "n_match": 8990,
      "useful": 1033,
      "creative": 200,
      "complete": 653,
      "clear_formatting": 848,
      "incorrect": 192,
      "superficial": 364,
      "instructions_not_followed": 110,
      "total_prefs": 3400,
      "positive_prefs_ratio": 0.8041176470588235,
      "win_rate": 0.5002845759817871,
      "mean_win_prob": 0.4904192138176452
    },
    {
      "rank": 62,
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 958.0426513910603,
      "p2.5": 877.7964013205667,
      "p97.5": 1038.288901461554,
      "rank_p2.5": 29,
      "rank_p97.5": 82,
      "n_match": 97,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 0,
      "positive_prefs_ratio": 0.0,
      "win_rate": 0.38461538461538464,
      "mean_win_prob": 0.4901476751298332
    },
    {
      "rank": 63,
      "model_name": "claude-3-5-sonnet-v2",
      "median": 955.7005577810628,
      "p2.5": 946.1207880573041,
      "p97.5": 965.2803275048213,
      "rank_p2.5": 51,
      "rank_p97.5": 69,
      "n_match": 7388,
      "useful": 846,
      "creative": 179,
      "complete": 524,
      "clear_formatting": 565,
      "incorrect": 130,
      "superficial": 402,
      "instructions_not_followed": 94,
      "total_prefs": 2740,
      "positive_prefs_ratio": 0.7715328467153285,
      "win_rate": 0.5058297396582015,
      "mean_win_prob": 0.48953025843631204
    },
    {
      "rank": 64,
      "model_name": "minimax-m2",
      "median": 952.5481871138705,
      "p2.5": 933.4386913143978,
      "p97.5": 971.6576829133431,
      "rank_p2.5": 50,
      "rank_p97.5": 72,
      "n_match": 1811,
      "useful": 123,
      "creative": 29,
      "complete": 82,
      "clear_formatting": 68,
      "incorrect": 32,
      "superficial": 66,
      "instructions_not_followed": 23,
      "total_prefs": 423,
      "positive_prefs_ratio": 0.7139479905437353,
      "win_rate": 0.390296886314265,
      "mean_win_prob": 0.4886968953376641
    },
    {
      "rank": 65,
      "model_name": "gpt-4o-2024-08-06",
      "median": 942.5973019301298,
      "p2.5": 932.7654914469695,
      "p97.5": 952.4291124132902,
      "rank_p2.5": 57,
      "rank_p97.5": 72,
      "n_match": 7915,
      "useful": 768,
      "creative": 153,
      "complete": 491,
      "clear_formatting": 618,
      "incorrect": 131,
      "superficial": 367,
      "instructions_not_followed": 115,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7680665909950813,
      "win_rate": 0.48274765463507713,
      "mean_win_prob": 0.4860484910180199
    },
    {
      "rank": 66,
      "model_name": "phi-4",
      "median": 942.2233844086908,
      "p2.5": 934.8768727853867,
      "p97.5": 949.5698960319949,
      "rank_p2.5": 61,
      "rank_p97.5": 72,
      "n_match": 12674,
      "useful": 1117,
      "creative": 215,
      "complete": 803,
      "clear_formatting": 855,
      "incorrect": 318,
      "superficial": 497,
      "instructions_not_followed": 209,
      "total_prefs": 4014,
      "positive_prefs_ratio": 0.7448928749377179,
      "win_rate": 0.4413745296098237,
      "mean_win_prob": 0.48594844363252726
    },
    {
      "rank": 67,
      "model_name": "llama-3.1-70b",
      "median": 941.1871544367083,
      "p2.5": 930.2326934130458,
      "p97.5": 952.1416154603708,
      "rank_p2.5": 57,
      "rank_p97.5": 73,
      "n_match": 7727,
      "useful": 1241,
      "creative": 280,
      "complete": 934,
      "clear_formatting": 1057,
      "incorrect": 289,
      "superficial": 495,
      "instructions_not_followed": 178,
      "total_prefs": 4474,
      "positive_prefs_ratio": 0.7849798837729102,
      "win_rate": 0.5072528453470208,
      "mean_win_prob": 0.4856709825290869
    },
    {
      "rank": 68,
      "model_name": "gemma-2-27b-it-q8",
      "median": 932.6766226679839,
      "p2.5": 897.8495542951688,
      "p97.5": 967.503691040799,
      "rank_p2.5": 51,
      "rank_p97.5": 82,
      "n_match": 1107,
      "useful": 279,
      "creative": 81,
      "complete": 178,
      "clear_formatting": 257,
      "incorrect": 46,
      "superficial": 81,
      "instructions_not_followed": 29,
      "total_prefs": 951,
      "positive_prefs_ratio": 0.8359621451104101,
      "win_rate": 0.5354330708661418,
      "mean_win_prob": 0.4833809210099677
    },
    {
      "rank": 69,
      "model_name": "ministral-8b-instruct-2410",
      "median": 928.5630413149207,
      "p2.5": 920.4266630312185,
      "p97.5": 936.699419598623,
      "rank_p2.5": 63,
      "rank_p97.5": 79,
      "n_match": 11905,
      "useful": 1391,
      "creative": 306,
      "complete": 1065,
      "clear_formatting": 1267,
      "incorrect": 509,
      "superficial": 667,
      "instructions_not_followed": 261,
      "total_prefs": 5466,
      "positive_prefs_ratio": 0.7371020856201976,
      "win_rate": 0.4477699530516432,
      "mean_win_prob": 0.4822667544253134
    },
    {
      "rank": 70,
      "model_name": "aya-expanse-8b",
      "median": 927.5657108390969,
      "p2.5": 909.672993587545,
      "p97.5": 945.4584280906488,
      "rank_p2.5": 62,
      "rank_p97.5": 80,
      "n_match": 1801,
      "useful": 141,
      "creative": 33,
      "complete": 133,
      "clear_formatting": 127,
      "incorrect": 62,
      "superficial": 63,
      "instructions_not_followed": 31,
      "total_prefs": 590,
      "positive_prefs_ratio": 0.735593220338983,
      "win_rate": 0.480946123521682,
      "mean_win_prob": 0.48199590894342853
    },
    {
      "rank": 71,
      "model_name": "jamba-1.5-large",
      "median": 924.5804909253466,
      "p2.5": 885.9838538022506,
      "p97.5": 963.1771280484427,
      "rank_p2.5": 53,
      "rank_p97.5": 82,
      "n_match": 345,
      "useful": 19,
      "creative": 4,
      "complete": 18,
      "clear_formatting": 15,
      "incorrect": 5,
      "superficial": 9,
      "instructions_not_followed": 4,
      "total_prefs": 74,
      "positive_prefs_ratio": 0.7567567567567568,
      "win_rate": 0.49201277955271566,
      "mean_win_prob": 0.4811835302365976
    },
    {
      "rank": 72,
      "model_name": "qwq-32b",
      "median": 922.0500599384044,
      "p2.5": 903.9254970549503,
      "p97.5": 940.1746228218585,
      "rank_p2.5": 63,
      "rank_p97.5": 81,
      "n_match": 1952,
      "useful": 222,
      "creative": 106,
      "complete": 249,
      "clear_formatting": 170,
      "incorrect": 92,
      "superficial": 84,
      "instructions_not_followed": 88,
      "total_prefs": 1011,
      "positive_prefs_ratio": 0.7388724035608308,
      "win_rate": 0.43544303797468353,
      "mean_win_prob": 0.48049293450309066
    },
    {
      "rank": 73,
      "model_name": "llama-3.1-405b",
      "median": 920.5310447097941,
      "p2.5": 912.6383097754268,
      "p97.5": 928.4237796441613,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 13281,
      "useful": 1736,
      "creative": 426,
      "complete": 1350,
      "clear_formatting": 1415,
      "incorrect": 643,
      "superficial": 692,
      "instructions_not_followed": 430,
      "total_prefs": 6692,
      "positive_prefs_ratio": 0.736252241482367,
      "win_rate": 0.4449099762201336,
      "mean_win_prob": 0.4800774945514405
    },
    {
      "rank": 74,
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 918.6623013290647,
      "p2.5": 909.741349808238,
      "p97.5": 927.5832528498913,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 8877,
      "useful": 1088,
      "creative": 215,
      "complete": 616,
      "clear_formatting": 778,
      "incorrect": 246,
      "superficial": 545,
      "instructions_not_followed": 177,
      "total_prefs": 3665,
      "positive_prefs_ratio": 0.7358799454297408,
      "win_rate": 0.4260756569448455,
      "mean_win_prob": 0.4795655020215517
    },
    {
      "rank": 75,
      "model_name": "gpt-5-nano",
      "median": 918.5659962568451,
      "p2.5": 904.7665784842568,
      "p97.5": 932.3654140294333,
      "rank_p2.5": 66,
      "rank_p97.5": 81,
      "n_match": 3584,
      "useful": 113,
      "creative": 26,
      "complete": 102,
      "clear_formatting": 62,
      "incorrect": 56,
      "superficial": 76,
      "instructions_not_followed": 65,
      "total_prefs": 500,
      "positive_prefs_ratio": 0.606,
      "win_rate": 0.3339148639218423,
      "mean_win_prob": 0.47953908957199815
    },
    {
      "rank": 76,
      "model_name": "gemma-2-9b-it",
      "median": 918.2417780097353,
      "p2.5": 907.1961101424866,
      "p97.5": 929.2874458769841,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 7040,
      "useful": 983,
      "creative": 308,
      "complete": 665,
      "clear_formatting": 940,
      "incorrect": 278,
      "superficial": 467,
      "instructions_not_followed": 178,
      "total_prefs": 3819,
      "positive_prefs_ratio": 0.7583136946844724,
      "win_rate": 0.47050722974523757,
      "mean_win_prob": 0.4794501505440454
    },
    {
      "rank": 77,
      "model_name": "hermes-4-70b",
      "median": 914.6977104266705,
      "p2.5": 900.1883206454335,
      "p97.5": 929.2071002079075,
      "rank_p2.5": 67,
      "rank_p97.5": 81,
      "n_match": 3172,
      "useful": 112,
      "creative": 17,
      "complete": 42,
      "clear_formatting": 73,
      "incorrect": 43,
      "superficial": 92,
      "instructions_not_followed": 35,
      "total_prefs": 414,
      "positive_prefs_ratio": 0.5893719806763285,
      "win_rate": 0.3310104529616725,
      "mean_win_prob": 0.4784759791489478
    },
    {
      "rank": 78,
      "model_name": "llama-3.1-8b",
      "median": 914.3515091147176,
      "p2.5": 906.6474936421716,
      "p97.5": 922.0555245872636,
      "rank_p2.5": 67,
      "rank_p97.5": 80,
      "n_match": 13347,
      "useful": 1140,
      "creative": 269,
      "complete": 798,
      "clear_formatting": 820,
      "incorrect": 558,
      "superficial": 674,
      "instructions_not_followed": 367,
      "total_prefs": 4626,
      "positive_prefs_ratio": 0.6543450064850843,
      "win_rate": 0.4096063329054343,
      "mean_win_prob": 0.47838062366129214
    },
    {
      "rank": 79,
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 912.9648354813762,
      "p2.5": 899.5872863503383,
      "p97.5": 926.3423846124142,
      "rank_p2.5": 67,
      "rank_p97.5": 81,
      "n_match": 3752,
      "useful": 340,
      "creative": 101,
      "complete": 238,
      "clear_formatting": 263,
      "incorrect": 102,
      "superficial": 204,
      "instructions_not_followed": 97,
      "total_prefs": 1345,
      "positive_prefs_ratio": 0.7003717472118959,
      "win_rate": 0.3879051119278316,
      "mean_win_prob": 0.47799834034797234
    },
    {
      "rank": 80,
      "model_name": "c4ai-command-r-08-2024",
      "median": 909.1062139729972,
      "p2.5": 899.7827855165016,
      "p97.5": 918.429642429493,
      "rank_p2.5": 68,
      "rank_p97.5": 81,
      "n_match": 7987,
      "useful": 812,
      "creative": 187,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 250,
      "superficial": 456,
      "instructions_not_followed": 157,
      "total_prefs": 2902,
      "positive_prefs_ratio": 0.7026188835286009,
      "win_rate": 0.40279913872654566,
      "mean_win_prob": 0.47693165211126165
    },
    {
      "rank": 81,
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 896.4315490867114,
      "p2.5": 887.8622178566148,
      "p97.5": 905.0008803168078,
      "rank_p2.5": 73,
      "rank_p97.5": 82,
      "n_match": 9547,
      "useful": 988,
      "creative": 211,
      "complete": 671,
      "clear_formatting": 750,
      "incorrect": 436,
      "superficial": 530,
      "instructions_not_followed": 226,
      "total_prefs": 3812,
      "positive_prefs_ratio": 0.6873032528856243,
      "win_rate": 0.3990280086967643,
      "mean_win_prob": 0.47339721621851977
    },
    {
      "rank": 82,
      "model_name": "qwen2.5-7b-instruct",
      "median": 873.4518525046605,
      "p2.5": 848.9800324243752,
      "p97.5": 897.9236725849457,
      "rank_p2.5": 78,
      "rank_p97.5": 86,
      "n_match": 2199,
      "useful": 360,
      "creative": 92,
      "complete": 284,
      "clear_formatting": 325,
      "incorrect": 168,
      "superficial": 167,
      "instructions_not_followed": 88,
      "total_prefs": 1484,
      "positive_prefs_ratio": 0.7149595687331537,
      "win_rate": 0.45285359801488834,
      "mean_win_prob": 0.4668668972336018
    },
    {
      "rank": 83,
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 857.2449589084317,
      "p2.5": 839.1899436413831,
      "p97.5": 875.2999741754804,
      "rank_p2.5": 82,
      "rank_p97.5": 86,
      "n_match": 3938,
      "useful": 637,
      "creative": 143,
      "complete": 388,
      "clear_formatting": 452,
      "incorrect": 192,
      "superficial": 336,
      "instructions_not_followed": 162,
      "total_prefs": 2310,
      "positive_prefs_ratio": 0.7012987012987013,
      "win_rate": 0.40750158931977115,
      "mean_win_prob": 0.46216384779310465
    },
    {
      "rank": 84,
      "model_name": "lfm-40b",
      "median": 849.8466196414842,
      "p2.5": 837.0555114817672,
      "p97.5": 862.6377278012011,
      "rank_p2.5": 82,
      "rank_p97.5": 86,
      "n_match": 4974,
      "useful": 650,
      "creative": 130,
      "complete": 362,
      "clear_formatting": 480,
      "incorrect": 180,
      "superficial": 405,
      "instructions_not_followed": 121,
      "total_prefs": 2328,
      "positive_prefs_ratio": 0.6967353951890034,
      "win_rate": 0.3713458755426918,
      "mean_win_prob": 0.459989473035386
    },
    {
      "rank": 85,
      "model_name": "phi-3.5-mini-instruct",
      "median": 831.3854922962888,
      "p2.5": 808.4839275998058,
      "p97.5": 854.2870569927719,
      "rank_p2.5": 82,
      "rank_p97.5": 88,
      "n_match": 3651,
      "useful": 582,
      "creative": 207,
      "complete": 480,
      "clear_formatting": 491,
      "incorrect": 362,
      "superficial": 414,
      "instructions_not_followed": 300,
      "total_prefs": 2836,
      "positive_prefs_ratio": 0.6205923836389281,
      "win_rate": 0.3793103448275862,
      "mean_win_prob": 0.45448693237828514
    },
    {
      "rank": 86,
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 825.3142941232993,
      "p2.5": 814.3623042429094,
      "p97.5": 836.2662840036892,
      "rank_p2.5": 85,
      "rank_p97.5": 88,
      "n_match": 7556,
      "useful": 884,
      "creative": 156,
      "complete": 441,
      "clear_formatting": 557,
      "incorrect": 304,
      "superficial": 676,
      "instructions_not_followed": 282,
      "total_prefs": 3300,
      "positive_prefs_ratio": 0.6175757575757576,
      "win_rate": 0.33665934943125125,
      "mean_win_prob": 0.45265296109533204
    },
    {
      "rank": 87,
      "model_name": "mistral-nemo-2407",
      "median": 816.2943967744175,
      "p2.5": 805.5721952129721,
      "p97.5": 827.0165983358627,
      "rank_p2.5": 85,
      "rank_p97.5": 88,
      "n_match": 8676,
      "useful": 1207,
      "creative": 193,
      "complete": 598,
      "clear_formatting": 743,
      "incorrect": 376,
      "superficial": 821,
      "instructions_not_followed": 299,
      "total_prefs": 4237,
      "positive_prefs_ratio": 0.646919990559358,
      "win_rate": 0.32752084912812734,
      "mean_win_prob": 0.44990555553680966
    },
    {
      "rank": 88,
      "model_name": "olmo-3-32b-think",
      "median": 814.9445487447111,
      "p2.5": 776.5287637317535,
      "p97.5": 853.3603337576687,
      "rank_p2.5": 82,
      "rank_p97.5": 89,
      "n_match": 591,
      "useful": 45,
      "creative": 12,
      "complete": 38,
      "clear_formatting": 20,
      "incorrect": 33,
      "superficial": 29,
      "instructions_not_followed": 32,
      "total_prefs": 209,
      "positive_prefs_ratio": 0.5502392344497608,
      "win_rate": 0.21199143468950749,
      "mean_win_prob": 0.4494920446237121
    },
    {
      "rank": 89,
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 777.246067774269,
      "p2.5": 759.762986095032,
      "p97.5": 794.7291494535061,
      "rank_p2.5": 88,
      "rank_p97.5": 89,
      "n_match": 2338,
      "useful": 203,
      "creative": 24,
      "complete": 53,
      "clear_formatting": 134,
      "incorrect": 85,
      "superficial": 238,
      "instructions_not_followed": 50,
      "total_prefs": 787,
      "positive_prefs_ratio": 0.5260482846251588,
      "win_rate": 0.2738916256157635,
      "mean_win_prob": 0.4376890869131975
    }
  ]
}