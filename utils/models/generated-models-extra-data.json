{
  "timestamp": 1765254353.0,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1137.4045103553913,
      "p2.5": 1125.5155357116253,
      "p97.5": 1150.5831285071386,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 2,
      "total_output_tokens": 4610110,
      "conso_all_conv": 0.0,
      "n_match": 2491,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6812404469964893,
      "win_rate": 0.6043953394937726,
      "useful": 194,
      "creative": 78,
      "complete": 269,
      "clear_formatting": 186,
      "incorrect": 59,
      "superficial": 29,
      "instructions_not_followed": 25,
      "total_prefs": 840,
      "positive_prefs_ratio": 0.8654761904761905
    },
    {
      "model_name": "mistral-large-2512",
      "median": 1122.8921218092241,
      "p2.5": 1088.2599421276484,
      "p97.5": 1159.1779548118134,
      "rank": 2,
      "rank_p2.5": 1,
      "rank_p97.5": 9,
      "total_output_tokens": 560185,
      "conso_all_conv": 0.0,
      "n_match": 284,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.66325470289597,
      "win_rate": 0.6028268551236748,
      "useful": 47,
      "creative": 23,
      "complete": 91,
      "clear_formatting": 72,
      "incorrect": 21,
      "superficial": 9,
      "instructions_not_followed": 7,
      "total_prefs": 270,
      "positive_prefs_ratio": 0.8629629629629629
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1107.9185844911776,
      "p2.5": 1096.7150678018143,
      "p97.5": 1118.9151138361567,
      "rank": 3,
      "rank_p2.5": 2,
      "rank_p97.5": 7,
      "total_output_tokens": 4180781,
      "conso_all_conv": 0.0,
      "n_match": 3064,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6442288547907358,
      "win_rate": 0.5878485145282403,
      "useful": 273,
      "creative": 95,
      "complete": 336,
      "clear_formatting": 247,
      "incorrect": 74,
      "superficial": 53,
      "instructions_not_followed": 18,
      "total_prefs": 1096,
      "positive_prefs_ratio": 0.8677007299270073
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1103.1510760623146,
      "p2.5": 1095.67459300767,
      "p97.5": 1110.0588136091283,
      "rank": 4,
      "rank_p2.5": 3,
      "rank_p97.5": 7,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6380788360935652,
      "win_rate": 0.6317699216950714,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1100.6416209155873,
      "p2.5": 1086.562292590409,
      "p97.5": 1113.7256505524326,
      "rank": 5,
      "rank_p2.5": 2,
      "rank_p97.5": 10,
      "total_output_tokens": 1980654,
      "conso_all_conv": 0.0,
      "n_match": 1783,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6348247921489537,
      "win_rate": 0.5798989898989898,
      "useful": 142,
      "creative": 40,
      "complete": 150,
      "clear_formatting": 130,
      "incorrect": 60,
      "superficial": 27,
      "instructions_not_followed": 18,
      "total_prefs": 567,
      "positive_prefs_ratio": 0.8148148148148148
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1095.0610518897934,
      "p2.5": 1085.6787892088828,
      "p97.5": 1104.211018085244,
      "rank": 6,
      "rank_p2.5": 4,
      "rank_p97.5": 11,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6275481788104972,
      "win_rate": 0.5851128848346637,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gemini-3-pro-preview",
      "median": 1094.0567247799222,
      "p2.5": 1072.318216740222,
      "p97.5": 1116.5095174798048,
      "rank": 7,
      "rank_p2.5": 3,
      "rank_p97.5": 16,
      "total_output_tokens": 1217354,
      "conso_all_conv": 0.0,
      "n_match": 687,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6262328869399598,
      "win_rate": 0.5335568513119533,
      "useful": 87,
      "creative": 36,
      "complete": 81,
      "clear_formatting": 73,
      "incorrect": 18,
      "superficial": 18,
      "instructions_not_followed": 8,
      "total_prefs": 321,
      "positive_prefs_ratio": 0.8629283489096573
    },
    {
      "model_name": "magistral-medium",
      "median": 1090.5951283413997,
      "p2.5": 1078.166253923815,
      "p97.5": 1103.236884841022,
      "rank": 8,
      "rank_p2.5": 5,
      "rank_p97.5": 14,
      "total_output_tokens": 2039837,
      "conso_all_conv": 0.0,
      "n_match": 2201,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6216865219341065,
      "win_rate": 0.5804043616537937,
      "useful": 146,
      "creative": 41,
      "complete": 150,
      "clear_formatting": 104,
      "incorrect": 50,
      "superficial": 28,
      "instructions_not_followed": 3,
      "total_prefs": 522,
      "positive_prefs_ratio": 0.8448275862068966
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1087.4789625131925,
      "p2.5": 1069.3776121421365,
      "p97.5": 1105.3731522832725,
      "rank": 9,
      "rank_p2.5": 4,
      "rank_p97.5": 17,
      "total_output_tokens": 2378476,
      "conso_all_conv": 0.0,
      "n_match": 988,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.617577066674744,
      "win_rate": 0.5667617107942973,
      "useful": 118,
      "creative": 57,
      "complete": 189,
      "clear_formatting": 126,
      "incorrect": 55,
      "superficial": 28,
      "instructions_not_followed": 24,
      "total_prefs": 597,
      "positive_prefs_ratio": 0.8207705192629816
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1087.3793785612165,
      "p2.5": 1079.8039543504476,
      "p97.5": 1094.4463802063963,
      "rank": 10,
      "rank_p2.5": 7,
      "rank_p97.5": 14,
      "total_output_tokens": 9300084,
      "conso_all_conv": 0.0,
      "n_match": 6864,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6174454839519377,
      "win_rate": 0.5778001165501165,
      "useful": 1289,
      "creative": 430,
      "complete": 1475,
      "clear_formatting": 1082,
      "incorrect": 346,
      "superficial": 232,
      "instructions_not_followed": 113,
      "total_prefs": 4967,
      "positive_prefs_ratio": 0.8608818200120797
    },
    {
      "model_name": "gpt-5.1",
      "median": 1086.4332983684253,
      "p2.5": 1070.1603966872622,
      "p97.5": 1103.571873513514,
      "rank": 11,
      "rank_p2.5": 4,
      "rank_p97.5": 18,
      "total_output_tokens": 1362873,
      "conso_all_conv": 0.0,
      "n_match": 1138,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.616194625666883,
      "win_rate": 0.5648636763412489,
      "useful": 114,
      "creative": 33,
      "complete": 124,
      "clear_formatting": 100,
      "incorrect": 21,
      "superficial": 23,
      "instructions_not_followed": 9,
      "total_prefs": 424,
      "positive_prefs_ratio": 0.875
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1081.5396279410597,
      "p2.5": 1072.6095507449918,
      "p97.5": 1089.9003421059133,
      "rank": 12,
      "rank_p2.5": 9,
      "rank_p97.5": 17,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6097024402704819,
      "win_rate": 0.6141481069042317,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1080.0409077238965,
      "p2.5": 1067.7127562076835,
      "p97.5": 1092.6410152744913,
      "rank": 13,
      "rank_p2.5": 8,
      "rank_p97.5": 18,
      "total_output_tokens": 1970024,
      "conso_all_conv": 0.0,
      "n_match": 1920,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6077069814658759,
      "win_rate": 0.5526197916666666,
      "useful": 264,
      "creative": 32,
      "complete": 154,
      "clear_formatting": 150,
      "incorrect": 46,
      "superficial": 39,
      "instructions_not_followed": 19,
      "total_prefs": 704,
      "positive_prefs_ratio": 0.8522727272727273
    },
    {
      "model_name": "grok-4.1-fast",
      "median": 1079.4628825817372,
      "p2.5": 1054.4930167520213,
      "p97.5": 1103.0247753343529,
      "rank": 14,
      "rank_p2.5": 4,
      "rank_p97.5": 26,
      "total_output_tokens": 515110,
      "conso_all_conv": 0.0,
      "n_match": 538,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6069364989296978,
      "win_rate": 0.5550185873605948,
      "useful": 54,
      "creative": 16,
      "complete": 57,
      "clear_formatting": 45,
      "incorrect": 17,
      "superficial": 16,
      "instructions_not_followed": 5,
      "total_prefs": 210,
      "positive_prefs_ratio": 0.819047619047619
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1078.0676062908897,
      "p2.5": 1059.8632179234828,
      "p97.5": 1093.399851872553,
      "rank": 15,
      "rank_p2.5": 8,
      "rank_p97.5": 23,
      "total_output_tokens": 1584202,
      "conso_all_conv": 0.0,
      "n_match": 1065,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6050746754221238,
      "win_rate": 0.5132988721804511,
      "useful": 78,
      "creative": 31,
      "complete": 81,
      "clear_formatting": 63,
      "incorrect": 33,
      "superficial": 19,
      "instructions_not_followed": 11,
      "total_prefs": 316,
      "positive_prefs_ratio": 0.8006329113924051
    },
    {
      "model_name": "glm-4.5",
      "median": 1073.3182163795439,
      "p2.5": 1058.9318975120332,
      "p97.5": 1086.9482902145062,
      "rank": 16,
      "rank_p2.5": 10,
      "rank_p97.5": 23,
      "total_output_tokens": 3740614,
      "conso_all_conv": 0.0,
      "n_match": 1603,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5987167892361306,
      "win_rate": 0.5520911360799001,
      "useful": 143,
      "creative": 48,
      "complete": 171,
      "clear_formatting": 125,
      "incorrect": 32,
      "superficial": 33,
      "instructions_not_followed": 21,
      "total_prefs": 573,
      "positive_prefs_ratio": 0.849912739965096
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1067.0119216973542,
      "p2.5": 1056.8080592238312,
      "p97.5": 1076.4497663666118,
      "rank": 17,
      "rank_p2.5": 15,
      "rank_p97.5": 25,
      "total_output_tokens": 3731606,
      "conso_all_conv": 0.0,
      "n_match": 3531,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5902285418192071,
      "win_rate": 0.5309093484419264,
      "useful": 352,
      "creative": 72,
      "complete": 227,
      "clear_formatting": 211,
      "incorrect": 66,
      "superficial": 75,
      "instructions_not_followed": 41,
      "total_prefs": 1044,
      "positive_prefs_ratio": 0.8256704980842912
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1066.613601320979,
      "p2.5": 1058.6163445578377,
      "p97.5": 1074.056495165379,
      "rank": 18,
      "rank_p2.5": 15,
      "rank_p97.5": 24,
      "total_output_tokens": 8525491,
      "conso_all_conv": 0.0,
      "n_match": 6490,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5896907281320573,
      "win_rate": 0.5626548536209554,
      "useful": 1083,
      "creative": 323,
      "complete": 1266,
      "clear_formatting": 923,
      "incorrect": 363,
      "superficial": 283,
      "instructions_not_followed": 149,
      "total_prefs": 4390,
      "positive_prefs_ratio": 0.8189066059225513
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1065.6377911324998,
      "p2.5": 1048.8490317124863,
      "p97.5": 1080.3847257587488,
      "rank": 19,
      "rank_p2.5": 13,
      "rank_p97.5": 28,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5883723754755354,
      "win_rate": 0.5305338541666668,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1065.1319610279638,
      "p2.5": 1054.414258080486,
      "p97.5": 1076.2549165595224,
      "rank": 20,
      "rank_p2.5": 15,
      "rank_p97.5": 26,
      "total_output_tokens": 1853912,
      "conso_all_conv": 0.0,
      "n_match": 2540,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5876885334459893,
      "win_rate": 0.5356400157542339,
      "useful": 234,
      "creative": 62,
      "complete": 198,
      "clear_formatting": 215,
      "incorrect": 41,
      "superficial": 64,
      "instructions_not_followed": 20,
      "total_prefs": 834,
      "positive_prefs_ratio": 0.8501199040767387
    },
    {
      "model_name": "grok-4-fast",
      "median": 1061.4848670242436,
      "p2.5": 1048.934436442595,
      "p97.5": 1072.9869202617429,
      "rank": 21,
      "rank_p2.5": 16,
      "rank_p97.5": 28,
      "total_output_tokens": 3213403,
      "conso_all_conv": 0.0,
      "n_match": 2359,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.582749131168603,
      "win_rate": 0.5263937208315655,
      "useful": 203,
      "creative": 46,
      "complete": 157,
      "clear_formatting": 119,
      "incorrect": 54,
      "superficial": 42,
      "instructions_not_followed": 20,
      "total_prefs": 641,
      "positive_prefs_ratio": 0.8190327613104524
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1060.4645318564517,
      "p2.5": 1050.0253204772782,
      "p97.5": 1069.827715019693,
      "rank": 22,
      "rank_p2.5": 17,
      "rank_p97.5": 28,
      "total_output_tokens": 2395652,
      "conso_all_conv": 0.0,
      "n_match": 2954,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5813645541807736,
      "win_rate": 0.5370412999322952,
      "useful": 205,
      "creative": 48,
      "complete": 180,
      "clear_formatting": 161,
      "incorrect": 98,
      "superficial": 60,
      "instructions_not_followed": 17,
      "total_prefs": 769,
      "positive_prefs_ratio": 0.7724317295188556
    },
    {
      "model_name": "command-a",
      "median": 1059.6346893847945,
      "p2.5": 1051.9455179921556,
      "p97.5": 1067.0189437636582,
      "rank": 23,
      "rank_p2.5": 18,
      "rank_p97.5": 28,
      "total_output_tokens": 6282908,
      "conso_all_conv": 0.0,
      "n_match": 6381,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5802376292952407,
      "win_rate": 0.5551339915373765,
      "useful": 1193,
      "creative": 280,
      "complete": 1086,
      "clear_formatting": 1026,
      "incorrect": 294,
      "superficial": 315,
      "instructions_not_followed": 111,
      "total_prefs": 4305,
      "positive_prefs_ratio": 0.8327526132404182
    },
    {
      "model_name": "kimi-k2",
      "median": 1057.9005833285387,
      "p2.5": 1035.0203030851428,
      "p97.5": 1079.6380101228779,
      "rank": 24,
      "rank_p2.5": 14,
      "rank_p97.5": 34,
      "total_output_tokens": 974682,
      "conso_all_conv": 0.0,
      "n_match": 581,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5778803241766014,
      "win_rate": 0.4913715277777777,
      "useful": 128,
      "creative": 24,
      "complete": 62,
      "clear_formatting": 45,
      "incorrect": 23,
      "superficial": 28,
      "instructions_not_followed": 3,
      "total_prefs": 313,
      "positive_prefs_ratio": 0.8274760383386581
    },
    {
      "model_name": "kimi-k2-thinking",
      "median": 1056.8378353789212,
      "p2.5": 1002.8285059948354,
      "p97.5": 1109.285083529154,
      "rank": 25,
      "rank_p2.5": 3,
      "rank_p97.5": 48,
      "total_output_tokens": 171929,
      "conso_all_conv": 0.0,
      "n_match": 88,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5764340834634951,
      "win_rate": 0.45411764705882357,
      "useful": 12,
      "creative": 6,
      "complete": 13,
      "clear_formatting": 9,
      "incorrect": 8,
      "superficial": 2,
      "instructions_not_followed": 8,
      "total_prefs": 58,
      "positive_prefs_ratio": 0.6896551724137931
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1056.2077885078133,
      "p2.5": 1037.8510344059448,
      "p97.5": 1075.3999417074126,
      "rank": 26,
      "rank_p2.5": 15,
      "rank_p97.5": 33,
      "total_output_tokens": 1341521,
      "conso_all_conv": 0.0,
      "n_match": 754,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5755761341414355,
      "win_rate": 0.5394820717131474,
      "useful": 39,
      "creative": 15,
      "complete": 38,
      "clear_formatting": 23,
      "incorrect": 16,
      "superficial": 11,
      "instructions_not_followed": 5,
      "total_prefs": 147,
      "positive_prefs_ratio": 0.782312925170068
    },
    {
      "model_name": "DeepSeek-V3.2",
      "median": 1053.2972710052672,
      "p2.5": 1011.8706133352798,
      "p97.5": 1098.5036378121913,
      "rank": 27,
      "rank_p2.5": 7,
      "rank_p97.5": 44,
      "total_output_tokens": 226438,
      "conso_all_conv": 0.0,
      "n_match": 176,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5716076526447723,
      "win_rate": 0.535909090909091,
      "useful": 33,
      "creative": 13,
      "complete": 37,
      "clear_formatting": 28,
      "incorrect": 18,
      "superficial": 8,
      "instructions_not_followed": 3,
      "total_prefs": 140,
      "positive_prefs_ratio": 0.7928571428571428
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1051.902330036396,
      "p2.5": 1042.305034285326,
      "p97.5": 1061.8129603767716,
      "rank": 28,
      "rank_p2.5": 22,
      "rank_p97.5": 31,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5697027405246008,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1051.3497385607984,
      "p2.5": 1040.4097093856722,
      "p97.5": 1061.5239417897167,
      "rank": 29,
      "rank_p2.5": 22,
      "rank_p97.5": 31,
      "total_output_tokens": 2445603,
      "conso_all_conv": 0.0,
      "n_match": 3144,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5689476237217335,
      "win_rate": 0.5215781100859052,
      "useful": 247,
      "creative": 53,
      "complete": 198,
      "clear_formatting": 181,
      "incorrect": 88,
      "superficial": 83,
      "instructions_not_followed": 32,
      "total_prefs": 882,
      "positive_prefs_ratio": 0.7698412698412699
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1046.9591731540888,
      "p2.5": 1039.8807057613385,
      "p97.5": 1054.7043873068262,
      "rank": 30,
      "rank_p2.5": 26,
      "rank_p97.5": 33,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5629381597528949,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1041.9881376916094,
      "p2.5": 1035.2894042983044,
      "p97.5": 1049.133800627017,
      "rank": 31,
      "rank_p2.5": 28,
      "rank_p97.5": 34,
      "total_output_tokens": 9383512,
      "conso_all_conv": 0.0,
      "n_match": 7509,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5561147774234354,
      "win_rate": 0.5238793447862565,
      "useful": 1152,
      "creative": 346,
      "complete": 1270,
      "clear_formatting": 1014,
      "incorrect": 583,
      "superficial": 266,
      "instructions_not_followed": 203,
      "total_prefs": 4834,
      "positive_prefs_ratio": 0.7823748448489863
    },
    {
      "model_name": "glm-4.6",
      "median": 1036.225950942045,
      "p2.5": 1020.9361270496216,
      "p97.5": 1052.5435326818356,
      "rank": 32,
      "rank_p2.5": 27,
      "rank_p97.5": 40,
      "total_output_tokens": 4177198,
      "conso_all_conv": 0.0,
      "n_match": 1428,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5481826891535179,
      "win_rate": 0.5016537649542576,
      "useful": 109,
      "creative": 39,
      "complete": 120,
      "clear_formatting": 74,
      "incorrect": 26,
      "superficial": 34,
      "instructions_not_followed": 17,
      "total_prefs": 419,
      "positive_prefs_ratio": 0.8162291169451074
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1036.166989196659,
      "p2.5": 1006.4012775234331,
      "p97.5": 1063.4065837130547,
      "rank": 33,
      "rank_p2.5": 22,
      "rank_p97.5": 46,
      "total_output_tokens": 494540,
      "conso_all_conv": 0.0,
      "n_match": 371,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.548101409758411,
      "win_rate": 0.5497010869565218,
      "useful": 28,
      "creative": 11,
      "complete": 22,
      "clear_formatting": 20,
      "incorrect": 16,
      "superficial": 13,
      "instructions_not_followed": 6,
      "total_prefs": 116,
      "positive_prefs_ratio": 0.6982758620689655
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1033.9351628740137,
      "p2.5": 1027.1533947227274,
      "p97.5": 1040.8150044351503,
      "rank": 34,
      "rank_p2.5": 31,
      "rank_p97.5": 38,
      "total_output_tokens": 5550374,
      "conso_all_conv": 0.0,
      "n_match": 7173,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.545023250359379,
      "win_rate": 0.515105285176405,
      "useful": 1227,
      "creative": 286,
      "complete": 945,
      "clear_formatting": 1010,
      "incorrect": 244,
      "superficial": 455,
      "instructions_not_followed": 97,
      "total_prefs": 4264,
      "positive_prefs_ratio": 0.8133208255159474
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1033.304427636372,
      "p2.5": 1025.4184294081763,
      "p97.5": 1041.6399225700475,
      "rank": 35,
      "rank_p2.5": 31,
      "rank_p97.5": 39,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5441527989167146,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1032.9962619611933,
      "p2.5": 1021.5097410480275,
      "p97.5": 1043.76326270172,
      "rank": 36,
      "rank_p2.5": 30,
      "rank_p97.5": 40,
      "total_output_tokens": 3151355,
      "conso_all_conv": 0.0,
      "n_match": 2822,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5437274296958292,
      "win_rate": 0.4914280652019844,
      "useful": 202,
      "creative": 67,
      "complete": 235,
      "clear_formatting": 177,
      "incorrect": 96,
      "superficial": 73,
      "instructions_not_followed": 42,
      "total_prefs": 892,
      "positive_prefs_ratio": 0.7634529147982063
    },
    {
      "model_name": "qwen3-32b",
      "median": 1032.776646515004,
      "p2.5": 1016.0920415504145,
      "p97.5": 1049.9466367018433,
      "rank": 37,
      "rank_p2.5": 28,
      "rank_p97.5": 42,
      "total_output_tokens": 2312159,
      "conso_all_conv": 0.0,
      "n_match": 1092,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5434242560508654,
      "win_rate": 0.5366636029411764,
      "useful": 280,
      "creative": 70,
      "complete": 215,
      "clear_formatting": 184,
      "incorrect": 98,
      "superficial": 76,
      "instructions_not_followed": 42,
      "total_prefs": 965,
      "positive_prefs_ratio": 0.7761658031088083
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1030.9542975573436,
      "p2.5": 1016.9875745706463,
      "p97.5": 1045.5179326855007,
      "rank": 38,
      "rank_p2.5": 30,
      "rank_p97.5": 42,
      "total_output_tokens": 1944015,
      "conso_all_conv": 0.0,
      "n_match": 1310,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5409075298745962,
      "win_rate": 0.4939969372128637,
      "useful": 89,
      "creative": 38,
      "complete": 114,
      "clear_formatting": 109,
      "incorrect": 35,
      "superficial": 34,
      "instructions_not_followed": 26,
      "total_prefs": 445,
      "positive_prefs_ratio": 0.7865168539325843
    },
    {
      "model_name": "deepseek-r1",
      "median": 1027.9643641323896,
      "p2.5": 1018.2653169829114,
      "p97.5": 1038.4039570541877,
      "rank": 39,
      "rank_p2.5": 32,
      "rank_p97.5": 42,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5367746313562477,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1021.3790627845726,
      "p2.5": 1014.4274908118914,
      "p97.5": 1028.3572884598877,
      "rank": 40,
      "rank_p2.5": 37,
      "rank_p97.5": 43,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5276581564300465,
      "win_rate": 0.527441386340469,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "mistral-saba",
      "median": 1017.1070998042175,
      "p2.5": 1008.5091309488233,
      "p97.5": 1025.136485243333,
      "rank": 41,
      "rank_p2.5": 39,
      "rank_p97.5": 46,
      "total_output_tokens": 4349650,
      "conso_all_conv": 0.0,
      "n_match": 4934,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5217361942594363,
      "win_rate": 0.4829677680924387,
      "useful": 745,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 586,
      "incorrect": 241,
      "superficial": 337,
      "instructions_not_followed": 113,
      "total_prefs": 2779,
      "positive_prefs_ratio": 0.751349406261245
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1014.5445290674994,
      "p2.5": 1003.0107078412391,
      "p97.5": 1026.7814777505746,
      "rank": 42,
      "rank_p2.5": 38,
      "rank_p97.5": 48,
      "total_output_tokens": 2644654,
      "conso_all_conv": 0.0,
      "n_match": 2254,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5181816302672998,
      "win_rate": 0.47244562805148693,
      "useful": 132,
      "creative": 45,
      "complete": 158,
      "clear_formatting": 83,
      "incorrect": 35,
      "superficial": 64,
      "instructions_not_followed": 37,
      "total_prefs": 554,
      "positive_prefs_ratio": 0.7545126353790613
    },
    {
      "model_name": "lfm2-8b-a1b",
      "median": 1012.6429086434914,
      "p2.5": 990.7688176985562,
      "p97.5": 1036.8021521299836,
      "rank": 43,
      "rank_p2.5": 33,
      "rank_p97.5": 52,
      "total_output_tokens": 469376,
      "conso_all_conv": 0.0,
      "n_match": 624,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5155430463553515,
      "win_rate": 0.4758333333333334,
      "useful": 50,
      "creative": 12,
      "complete": 34,
      "clear_formatting": 47,
      "incorrect": 55,
      "superficial": 33,
      "instructions_not_followed": 17,
      "total_prefs": 248,
      "positive_prefs_ratio": 0.5766129032258065
    },
    {
      "model_name": "llama-4-scout",
      "median": 1012.3992358948403,
      "p2.5": 1003.7499280910095,
      "p97.5": 1020.4093572036098,
      "rank": 44,
      "rank_p2.5": 40,
      "rank_p97.5": 48,
      "total_output_tokens": 4020583,
      "conso_all_conv": 0.0,
      "n_match": 4975,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5152048966633165,
      "win_rate": 0.49095075376884423,
      "useful": 721,
      "creative": 184,
      "complete": 638,
      "clear_formatting": 581,
      "incorrect": 253,
      "superficial": 331,
      "instructions_not_followed": 88,
      "total_prefs": 2796,
      "positive_prefs_ratio": 0.759656652360515
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1008.294768735421,
      "p2.5": 998.9818295471072,
      "p97.5": 1016.2309207544666,
      "rank": 45,
      "rank_p2.5": 42,
      "rank_p97.5": 50,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5095079590389807,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "llama-maverick",
      "median": 1007.7821427071246,
      "p2.5": 995.8167044208247,
      "p97.5": 1019.9577357570502,
      "rank": 46,
      "rank_p2.5": 41,
      "rank_p97.5": 50,
      "total_output_tokens": 1762135,
      "conso_all_conv": 0.0,
      "n_match": 2226,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5087963370613582,
      "win_rate": 0.4789038634321653,
      "useful": 155,
      "creative": 33,
      "complete": 113,
      "clear_formatting": 103,
      "incorrect": 52,
      "superficial": 85,
      "instructions_not_followed": 25,
      "total_prefs": 566,
      "positive_prefs_ratio": 0.7137809187279152
    },
    {
      "model_name": "o4-mini",
      "median": 1005.9239709364044,
      "p2.5": 994.754309145475,
      "p97.5": 1016.9862696851949,
      "rank": 47,
      "rank_p2.5": 42,
      "rank_p97.5": 51,
      "total_output_tokens": 2897929,
      "conso_all_conv": 0.0,
      "n_match": 2791,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5062167439926156,
      "win_rate": 0.47113978494623654,
      "useful": 526,
      "creative": 111,
      "complete": 371,
      "clear_formatting": 285,
      "incorrect": 132,
      "superficial": 210,
      "instructions_not_followed": 45,
      "total_prefs": 1680,
      "positive_prefs_ratio": 0.7696428571428572
    },
    {
      "model_name": "gpt-5",
      "median": 1003.0705546114593,
      "p2.5": 992.5522103920212,
      "p97.5": 1012.9798687975447,
      "rank": 48,
      "rank_p2.5": 44,
      "rank_p97.5": 52,
      "total_output_tokens": 3722746,
      "conso_all_conv": 0.0,
      "n_match": 3564,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5022554456259027,
      "win_rate": 0.45610721302273366,
      "useful": 131,
      "creative": 48,
      "complete": 146,
      "clear_formatting": 63,
      "incorrect": 27,
      "superficial": 54,
      "instructions_not_followed": 31,
      "total_prefs": 500,
      "positive_prefs_ratio": 0.776
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1002.3440264593015,
      "p2.5": 980.1522913226013,
      "p97.5": 1025.8929852836068,
      "rank": 49,
      "rank_p2.5": 38,
      "rank_p97.5": 59,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5012468672105524,
      "win_rate": 0.5826115485564304,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 993.802927283996,
      "p2.5": 985.1077880551969,
      "p97.5": 1002.3804092462659,
      "rank": 50,
      "rank_p2.5": 48,
      "rank_p97.5": 55,
      "total_output_tokens": 4082212,
      "conso_all_conv": 0.0,
      "n_match": 5113,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4893942804580825,
      "win_rate": 0.46251711324075884,
      "useful": 722,
      "creative": 142,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 277,
      "superficial": 353,
      "instructions_not_followed": 109,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7203934922436626
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 988.1689157207213,
      "p2.5": 974.9135622564394,
      "p97.5": 1000.4489202131459,
      "rank": 51,
      "rank_p2.5": 49,
      "rank_p97.5": 61,
      "total_output_tokens": 3607564,
      "conso_all_conv": 0.0,
      "n_match": 2101,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4815842430325177,
      "win_rate": 0.4529285714285714,
      "useful": 124,
      "creative": 38,
      "complete": 140,
      "clear_formatting": 89,
      "incorrect": 79,
      "superficial": 61,
      "instructions_not_followed": 28,
      "total_prefs": 559,
      "positive_prefs_ratio": 0.6994633273703041
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 988.0233783576684,
      "p2.5": 977.2060539946641,
      "p97.5": 997.7999806461868,
      "rank": 52,
      "rank_p2.5": 49,
      "rank_p97.5": 60,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48138262304162166,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "qwen-3-8b",
      "median": 987.4210484610761,
      "p2.5": 972.8435058064492,
      "p97.5": 1002.2773393647158,
      "rank": 53,
      "rank_p2.5": 49,
      "rank_p97.5": 62,
      "total_output_tokens": 3370834,
      "conso_all_conv": 0.0,
      "n_match": 1615,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4805482644414227,
      "win_rate": 0.44024767801857584,
      "useful": 80,
      "creative": 23,
      "complete": 97,
      "clear_formatting": 60,
      "incorrect": 73,
      "superficial": 33,
      "instructions_not_followed": 30,
      "total_prefs": 396,
      "positive_prefs_ratio": 0.6565656565656566
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 987.188790932463,
      "p2.5": 980.3585152647038,
      "p97.5": 994.3392463305004,
      "rank": 54,
      "rank_p2.5": 51,
      "rank_p97.5": 59,
      "total_output_tokens": 6939669,
      "conso_all_conv": 0.0,
      "n_match": 8380,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48022657131657936,
      "win_rate": 0.4753419262441819,
      "useful": 1416,
      "creative": 287,
      "complete": 1117,
      "clear_formatting": 1052,
      "incorrect": 448,
      "superficial": 615,
      "instructions_not_followed": 144,
      "total_prefs": 5079,
      "positive_prefs_ratio": 0.7623547942508367
    },
    {
      "model_name": "o3-mini",
      "median": 984.3191131805361,
      "p2.5": 968.4598862264855,
      "p97.5": 998.7159131651049,
      "rank": 55,
      "rank_p2.5": 49,
      "rank_p97.5": 64,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47625355083866355,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 983.7564191226145,
      "p2.5": 975.5873878902913,
      "p97.5": 992.062326833387,
      "rank": 56,
      "rank_p2.5": 51,
      "rank_p97.5": 61,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47547489983506513,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 979.9675519012543,
      "p2.5": 972.5290085773429,
      "p97.5": 987.8342433438727,
      "rank": 57,
      "rank_p2.5": 54,
      "rank_p97.5": 62,
      "total_output_tokens": 3563423,
      "conso_all_conv": 0.0,
      "n_match": 5717,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47023561850893025,
      "win_rate": 0.4674846947699843,
      "useful": 928,
      "creative": 159,
      "complete": 529,
      "clear_formatting": 649,
      "incorrect": 276,
      "superficial": 448,
      "instructions_not_followed": 103,
      "total_prefs": 3092,
      "positive_prefs_ratio": 0.732535575679172
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 977.9682705969606,
      "p2.5": 962.9801776029684,
      "p97.5": 993.0176387691187,
      "rank": 58,
      "rank_p2.5": 51,
      "rank_p97.5": 67,
      "total_output_tokens": 1313975,
      "conso_all_conv": 0.0,
      "n_match": 1365,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4674738467302873,
      "win_rate": 0.42614369501466276,
      "useful": 97,
      "creative": 16,
      "complete": 71,
      "clear_formatting": 60,
      "incorrect": 57,
      "superficial": 65,
      "instructions_not_followed": 34,
      "total_prefs": 400,
      "positive_prefs_ratio": 0.61
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 975.2945556591378,
      "p2.5": 958.7268670225551,
      "p97.5": 990.9890396436741,
      "rank": 59,
      "rank_p2.5": 52,
      "rank_p97.5": 68,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46378385292345536,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 974.5416142726184,
      "p2.5": 926.0425583644549,
      "p97.5": 1021.0982873935814,
      "rank": 60,
      "rank_p2.5": 40,
      "rank_p97.5": 79,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4627454724786986,
      "win_rate": 0.5449418604651163,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 974.2341953127323,
      "p2.5": 965.0021517502291,
      "p97.5": 983.038741725419,
      "rank": 61,
      "rank_p2.5": 57,
      "rank_p97.5": 66,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4623216106397487,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 973.3687211152533,
      "p2.5": 964.3486478843133,
      "p97.5": 981.8179656124532,
      "rank": 62,
      "rank_p2.5": 57,
      "rank_p97.5": 66,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4611286311057055,
      "win_rate": 0.4921080415273623,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "minimax-m2",
      "median": 972.3005125927634,
      "p2.5": 948.1025610545371,
      "p97.5": 995.7808267921916,
      "rank": 63,
      "rank_p2.5": 51,
      "rank_p97.5": 72,
      "total_output_tokens": 1235885,
      "conso_all_conv": 0.0,
      "n_match": 655,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4596568571069835,
      "win_rate": 0.404104134762634,
      "useful": 63,
      "creative": 14,
      "complete": 53,
      "clear_formatting": 31,
      "incorrect": 29,
      "superficial": 35,
      "instructions_not_followed": 14,
      "total_prefs": 239,
      "positive_prefs_ratio": 0.6736401673640168
    },
    {
      "model_name": "phi-4",
      "median": 968.9561955749347,
      "p2.5": 962.8559950760964,
      "p97.5": 975.4531419506247,
      "rank": 64,
      "rank_p2.5": 60,
      "rank_p97.5": 67,
      "total_output_tokens": 8075286,
      "conso_all_conv": 0.0,
      "n_match": 9776,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4550540143310999,
      "win_rate": 0.4694895140664962,
      "useful": 1619,
      "creative": 336,
      "complete": 1186,
      "clear_formatting": 1303,
      "incorrect": 724,
      "superficial": 755,
      "instructions_not_followed": 251,
      "total_prefs": 6174,
      "positive_prefs_ratio": 0.7197926789763525
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 965.170710175277,
      "p2.5": 957.7733504718873,
      "p97.5": 971.8755250432445,
      "rank": 65,
      "rank_p2.5": 62,
      "rank_p97.5": 69,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.449853744093609,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 963.8256630640524,
      "p2.5": 954.985056260122,
      "p97.5": 972.9186328168923,
      "rank": 66,
      "rank_p2.5": 62,
      "rank_p97.5": 70,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4480086834786368,
      "win_rate": 0.48863297150610585,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 961.6600360622735,
      "p2.5": 903.1244313453656,
      "p97.5": 1017.7668300077244,
      "rank": 67,
      "rank_p2.5": 42,
      "rank_p97.5": 80,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44504113076710716,
      "win_rate": 0.46869047619047616,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 960.3320574778518,
      "p2.5": 909.0004049437944,
      "p97.5": 1013.4607597692486,
      "rank": 68,
      "rank_p2.5": 44,
      "rank_p97.5": 80,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4432233888781637,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 958.9178128359946,
      "p2.5": 952.302837860877,
      "p97.5": 966.2429314637644,
      "rank": 69,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4412892839211592,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 954.5349919050225,
      "p2.5": 945.5110905721341,
      "p97.5": 963.8075813226554,
      "rank": 70,
      "rank_p2.5": 66,
      "rank_p97.5": 73,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4353072130612997,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "olmo-3-32b-think",
      "median": 951.29266091904,
      "p2.5": 876.1079522257926,
      "p97.5": 1017.5814447659207,
      "rank": 71,
      "rank_p2.5": 42,
      "rank_p97.5": 82,
      "total_output_tokens": 133024,
      "conso_all_conv": 0.0,
      "n_match": 58,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4308939896841228,
      "win_rate": 0.4001754385964913,
      "useful": 4,
      "creative": 1,
      "complete": 1,
      "clear_formatting": 0,
      "incorrect": 3,
      "superficial": 2,
      "instructions_not_followed": 4,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4
    },
    {
      "model_name": "qwq-32b",
      "median": 951.0566328466599,
      "p2.5": 937.3931284396513,
      "p97.5": 966.6253746445636,
      "rank": 72,
      "rank_p2.5": 65,
      "rank_p97.5": 76,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4305731507829632,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "gpt-5-nano",
      "median": 950.1340571830261,
      "p2.5": 936.1368779411634,
      "p97.5": 964.5323688134915,
      "rank": 73,
      "rank_p2.5": 66,
      "rank_p97.5": 76,
      "total_output_tokens": 2005836,
      "conso_all_conv": 0.0,
      "n_match": 1871,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4293196385283253,
      "win_rate": 0.39220737573490116,
      "useful": 92,
      "creative": 27,
      "complete": 90,
      "clear_formatting": 55,
      "incorrect": 36,
      "superficial": 41,
      "instructions_not_followed": 49,
      "total_prefs": 390,
      "positive_prefs_ratio": 0.676923076923077
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 948.1305004654027,
      "p2.5": 937.4768887879991,
      "p97.5": 959.3783031348366,
      "rank": 74,
      "rank_p2.5": 68,
      "rank_p97.5": 76,
      "total_output_tokens": 2863519,
      "conso_all_conv": 0.0,
      "n_match": 2981,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.42660055952720255,
      "win_rate": 0.4396947333109695,
      "useful": 489,
      "creative": 130,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 190,
      "superficial": 342,
      "instructions_not_followed": 126,
      "total_prefs": 1995,
      "positive_prefs_ratio": 0.6701754385964912
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 943.4198348865468,
      "p2.5": 935.949358454898,
      "p97.5": 951.528779911219,
      "rank": 75,
      "rank_p2.5": 70,
      "rank_p97.5": 76,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.42022544051385075,
      "win_rate": 0.46634743875278395,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "hermes-4-70b",
      "median": 935.2860207783385,
      "p2.5": 919.4280916087439,
      "p97.5": 948.6971701498761,
      "rank": 76,
      "rank_p2.5": 72,
      "rank_p97.5": 80,
      "total_output_tokens": 849281,
      "conso_all_conv": 0.0,
      "n_match": 1851,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4092812429737181,
      "win_rate": 0.37784324324324325,
      "useful": 98,
      "creative": 25,
      "complete": 49,
      "clear_formatting": 58,
      "incorrect": 68,
      "superficial": 94,
      "instructions_not_followed": 38,
      "total_prefs": 430,
      "positive_prefs_ratio": 0.5348837209302325
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 933.7956312391402,
      "p2.5": 927.2495527980518,
      "p97.5": 940.7336903947785,
      "rank": 77,
      "rank_p2.5": 74,
      "rank_p97.5": 79,
      "total_output_tokens": 7216552,
      "conso_all_conv": 0.0,
      "n_match": 10219,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4072852330250823,
      "win_rate": 0.4425648302182209,
      "useful": 1531,
      "creative": 372,
      "complete": 1009,
      "clear_formatting": 1151,
      "incorrect": 1039,
      "superficial": 969,
      "instructions_not_followed": 401,
      "total_prefs": 6472,
      "positive_prefs_ratio": 0.6277812113720643
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 931.2646549311437,
      "p2.5": 923.1810345270505,
      "p97.5": 939.5337671201268,
      "rank": 78,
      "rank_p2.5": 74,
      "rank_p97.5": 80,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4039025963229872,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 928.144539795771,
      "p2.5": 909.3772933149259,
      "p97.5": 946.2844851727294,
      "rank": 79,
      "rank_p2.5": 72,
      "rank_p97.5": 80,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.39974503424729824,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 927.8551668244675,
      "p2.5": 920.9905899808923,
      "p97.5": 935.7634260367556,
      "rank": 80,
      "rank_p2.5": 76,
      "rank_p97.5": 80,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.39936015745743275,
      "win_rate": 0.43789423208658723,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 885.7230673020888,
      "p2.5": 871.8136882156339,
      "p97.5": 899.2880351061605,
      "rank": 81,
      "rank_p2.5": 80,
      "rank_p97.5": 82,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.34483128277256164,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 883.2864853548136,
      "p2.5": 872.3263884321789,
      "p97.5": 894.3096782657459,
      "rank": 82,
      "rank_p2.5": 81,
      "rank_p97.5": 82,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.34178145390436954,
      "win_rate": 0.416296813862493,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 862.933793534685,
      "p2.5": 848.413523690105,
      "p97.5": 877.686113321111,
      "rank": 83,
      "rank_p2.5": 82,
      "rank_p97.5": 84,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.31681560897809846,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 853.1658044326452,
      "p2.5": 844.5262500541337,
      "p97.5": 862.6714583094204,
      "rank": 84,
      "rank_p2.5": 83,
      "rank_p97.5": 85,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3051747290749978,
      "win_rate": 0.37482482802751566,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 842.691093131506,
      "p2.5": 833.4675856673107,
      "p97.5": 852.7009706706735,
      "rank": 85,
      "rank_p2.5": 84,
      "rank_p97.5": 86,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2929520625333005,
      "win_rate": 0.36710357470210814,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 819.8674241914347,
      "p2.5": 804.0558003381634,
      "p97.5": 836.8203801487045,
      "rank": 86,
      "rank_p2.5": 86,
      "rank_p97.5": 87,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.26729942630085524,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 763.8892583924917,
      "p2.5": 670.0792840157224,
      "p97.5": 852.5914987033851,
      "rank": 87,
      "rank_p2.5": 84,
      "rank_p97.5": 89,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2104545676127253,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 753.1852953036289,
      "p2.5": 704.4156854563375,
      "p97.5": 791.5271357329086,
      "rank": 88,
      "rank_p2.5": 87,
      "rank_p97.5": 89,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.20060324894704706,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 729.8728114466551,
      "p2.5": 636.7055905713299,
      "p97.5": 809.6581414282172,
      "rank": 89,
      "rank_p2.5": 87,
      "rank_p97.5": 89,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.1802887521592747,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}
