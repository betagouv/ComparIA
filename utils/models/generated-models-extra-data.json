{
  "timestamp": 1762251054.767453,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1154.1394525072606,
      "p2.5": 1135.6178155495718,
      "p97.5": 1173.660928550264,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 1,
      "total_output_tokens": 1710233,
      "conso_all_conv": 0.0,
      "n_match": 912,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.7009246184072193,
      "win_rate": 0.6183095499451152,
      "useful": 40,
      "creative": 19,
      "complete": 67,
      "clear_formatting": 47,
      "incorrect": 14,
      "superficial": 4,
      "instructions_not_followed": 4,
      "total_prefs": 195,
      "positive_prefs_ratio": 0.8871794871794871
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1122.1188268638914,
      "p2.5": 1105.9834859256273,
      "p97.5": 1138.6064558347312,
      "rank": 2,
      "rank_p2.5": 2,
      "rank_p97.5": 4,
      "total_output_tokens": 1949206,
      "conso_all_conv": 0.0,
      "n_match": 1316,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6618797805453361,
      "win_rate": 0.5947112462006079,
      "useful": 124,
      "creative": 47,
      "complete": 165,
      "clear_formatting": 113,
      "incorrect": 31,
      "superficial": 23,
      "instructions_not_followed": 9,
      "total_prefs": 512,
      "positive_prefs_ratio": 0.876953125
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1106.3087359506974,
      "p2.5": 1099.3836384733243,
      "p97.5": 1113.5523576719356,
      "rank": 3,
      "rank_p2.5": 3,
      "rank_p97.5": 7,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6417865586509349,
      "win_rate": 0.6317699216950714,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1105.6248200264354,
      "p2.5": 1080.1626623410918,
      "p97.5": 1133.3578228083543,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 14,
      "total_output_tokens": 485342,
      "conso_all_conv": 0.0,
      "n_match": 421,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.640906451630284,
      "win_rate": 0.5951558752997602,
      "useful": 23,
      "creative": 3,
      "complete": 22,
      "clear_formatting": 19,
      "incorrect": 13,
      "superficial": 3,
      "instructions_not_followed": 5,
      "total_prefs": 88,
      "positive_prefs_ratio": 0.7613636363636364
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1098.9387562151483,
      "p2.5": 1089.1916016315151,
      "p97.5": 1108.385770283691,
      "rank": 5,
      "rank_p2.5": 4,
      "rank_p97.5": 11,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6322575916889428,
      "win_rate": 0.5851128848346636,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1097.109065982859,
      "p2.5": 1089.0726590271038,
      "p97.5": 1104.7132499123938,
      "rank": 6,
      "rank_p2.5": 4,
      "rank_p97.5": 11,
      "total_output_tokens": 7552482,
      "conso_all_conv": 0.0,
      "n_match": 5454,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6298769734760558,
      "win_rate": 0.5942757609094244,
      "useful": 1165,
      "creative": 392,
      "complete": 1336,
      "clear_formatting": 990,
      "incorrect": 287,
      "superficial": 206,
      "instructions_not_followed": 97,
      "total_prefs": 4473,
      "positive_prefs_ratio": 0.8680974737312765
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1094.9979668058775,
      "p2.5": 1073.3689045003289,
      "p97.5": 1115.3877979355507,
      "rank": 7,
      "rank_p2.5": 3,
      "rank_p97.5": 17,
      "total_output_tokens": 1671560,
      "conso_all_conv": 0.0,
      "n_match": 736,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.627123080655859,
      "win_rate": 0.5771428571428572,
      "useful": 101,
      "creative": 45,
      "complete": 157,
      "clear_formatting": 106,
      "incorrect": 41,
      "superficial": 23,
      "instructions_not_followed": 20,
      "total_prefs": 493,
      "positive_prefs_ratio": 0.8296146044624746
    },
    {
      "model_name": "glm-4.5",
      "median": 1093.2812130764512,
      "p2.5": 1066.892469103343,
      "p97.5": 1120.276083431694,
      "rank": 8,
      "rank_p2.5": 3,
      "rank_p97.5": 20,
      "total_output_tokens": 1078700,
      "conso_all_conv": 0.0,
      "n_match": 465,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6248780707797803,
      "win_rate": 0.6022826086956521,
      "useful": 71,
      "creative": 17,
      "complete": 55,
      "clear_formatting": 46,
      "incorrect": 9,
      "superficial": 7,
      "instructions_not_followed": 6,
      "total_prefs": 211,
      "positive_prefs_ratio": 0.8957345971563981
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1092.0887480300107,
      "p2.5": 1060.1553024432353,
      "p97.5": 1120.9978972473086,
      "rank": 9,
      "rank_p2.5": 2,
      "rank_p97.5": 22,
      "total_output_tokens": 355251,
      "conso_all_conv": 0.0,
      "n_match": 362,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6233158022916394,
      "win_rate": 0.6172651933701657,
      "useful": 30,
      "creative": 4,
      "complete": 15,
      "clear_formatting": 14,
      "incorrect": 12,
      "superficial": 3,
      "instructions_not_followed": 2,
      "total_prefs": 80,
      "positive_prefs_ratio": 0.7875
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1090.713996044929,
      "p2.5": 1066.10411083166,
      "p97.5": 1117.1572368095676,
      "rank": 10,
      "rank_p2.5": 3,
      "rank_p97.5": 20,
      "total_output_tokens": 745768,
      "conso_all_conv": 0.0,
      "n_match": 470,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6215118337535737,
      "win_rate": 0.49090517241379306,
      "useful": 35,
      "creative": 15,
      "complete": 33,
      "clear_formatting": 23,
      "incorrect": 13,
      "superficial": 7,
      "instructions_not_followed": 4,
      "total_prefs": 130,
      "positive_prefs_ratio": 0.8153846153846154
    },
    {
      "model_name": "magistral-medium",
      "median": 1089.8563230580598,
      "p2.5": 1071.1266385466556,
      "p97.5": 1110.3490189578652,
      "rank": 11,
      "rank_p2.5": 4,
      "rank_p97.5": 18,
      "total_output_tokens": 858752,
      "conso_all_conv": 0.0,
      "n_match": 866,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6203848374239963,
      "win_rate": 0.575715935334873,
      "useful": 43,
      "creative": 18,
      "complete": 46,
      "clear_formatting": 28,
      "incorrect": 16,
      "superficial": 9,
      "instructions_not_followed": 1,
      "total_prefs": 161,
      "positive_prefs_ratio": 0.8385093167701864
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1084.5283253856242,
      "p2.5": 1076.0472314859742,
      "p97.5": 1092.3915780860714,
      "rank": 12,
      "rank_p2.5": 8,
      "rank_p97.5": 17,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6133578419260397,
      "win_rate": 0.6141481069042316,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1080.4657970170856,
      "p2.5": 1053.1704315199079,
      "p97.5": 1109.1427674290344,
      "rank": 13,
      "rank_p2.5": 4,
      "rank_p97.5": 24,
      "total_output_tokens": 609658,
      "conso_all_conv": 0.0,
      "n_match": 362,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6079709469941305,
      "win_rate": 0.531878453038674,
      "useful": 18,
      "creative": 9,
      "complete": 20,
      "clear_formatting": 9,
      "incorrect": 5,
      "superficial": 1,
      "instructions_not_followed": 0,
      "total_prefs": 62,
      "positive_prefs_ratio": 0.9032258064516129
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1078.3631911092132,
      "p2.5": 1061.5422507524684,
      "p97.5": 1093.6132607373156,
      "rank": 14,
      "rank_p2.5": 8,
      "rank_p97.5": 21,
      "total_output_tokens": 1724757,
      "conso_all_conv": 0.0,
      "n_match": 1165,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6051735045807526,
      "win_rate": 0.5310815450643778,
      "useful": 81,
      "creative": 16,
      "complete": 55,
      "clear_formatting": 42,
      "incorrect": 17,
      "superficial": 6,
      "instructions_not_followed": 12,
      "total_prefs": 229,
      "positive_prefs_ratio": 0.8471615720524017
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1076.6523508359292,
      "p2.5": 1068.4421896510341,
      "p97.5": 1086.3490412391009,
      "rank": 15,
      "rank_p2.5": 11,
      "rank_p97.5": 19,
      "total_output_tokens": 6802028,
      "conso_all_conv": 0.0,
      "n_match": 5096,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6028927119712855,
      "win_rate": 0.5755435635792778,
      "useful": 980,
      "creative": 293,
      "complete": 1141,
      "clear_formatting": 834,
      "incorrect": 308,
      "superficial": 245,
      "instructions_not_followed": 132,
      "total_prefs": 3933,
      "positive_prefs_ratio": 0.8258326976862446
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1074.2514807821026,
      "p2.5": 1059.403877695885,
      "p97.5": 1092.0245218586622,
      "rank": 16,
      "rank_p2.5": 9,
      "rank_p97.5": 23,
      "total_output_tokens": 887607,
      "conso_all_conv": 0.0,
      "n_match": 1071,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5996852605301951,
      "win_rate": 0.56103738317757,
      "useful": 51,
      "creative": 16,
      "complete": 48,
      "clear_formatting": 37,
      "incorrect": 35,
      "superficial": 16,
      "instructions_not_followed": 4,
      "total_prefs": 207,
      "positive_prefs_ratio": 0.7342995169082126
    },
    {
      "model_name": "kimi-k2",
      "median": 1072.439655268419,
      "p2.5": 1040.0465040017355,
      "p97.5": 1103.7142296498625,
      "rank": 17,
      "rank_p2.5": 6,
      "rank_p97.5": 29,
      "total_output_tokens": 491618,
      "conso_all_conv": 0.0,
      "n_match": 290,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5972596585186345,
      "win_rate": 0.5511228070175438,
      "useful": 78,
      "creative": 15,
      "complete": 36,
      "clear_formatting": 32,
      "incorrect": 11,
      "superficial": 19,
      "instructions_not_followed": 1,
      "total_prefs": 192,
      "positive_prefs_ratio": 0.8385416666666666
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1071.8570916035205,
      "p2.5": 1058.2146324724897,
      "p97.5": 1086.2219718739138,
      "rank": 18,
      "rank_p2.5": 11,
      "rank_p97.5": 23,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5964788349806834,
      "win_rate": 0.5305338541666665,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1067.490103480182,
      "p2.5": 1021.9985131365574,
      "p97.5": 1111.9183618903514,
      "rank": 19,
      "rank_p2.5": 3,
      "rank_p97.5": 36,
      "total_output_tokens": 258280,
      "conso_all_conv": 0.0,
      "n_match": 154,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.590611998690914,
      "win_rate": 0.5178807947019867,
      "useful": 15,
      "creative": 6,
      "complete": 9,
      "clear_formatting": 11,
      "incorrect": 7,
      "superficial": 3,
      "instructions_not_followed": 0,
      "total_prefs": 51,
      "positive_prefs_ratio": 0.803921568627451
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1067.2949370538918,
      "p2.5": 1052.3171100980235,
      "p97.5": 1083.0588801434578,
      "rank": 20,
      "rank_p2.5": 13,
      "rank_p97.5": 25,
      "total_output_tokens": 1126450,
      "conso_all_conv": 0.0,
      "n_match": 1388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5903492531071718,
      "win_rate": 0.5303749098774334,
      "useful": 136,
      "creative": 39,
      "complete": 122,
      "clear_formatting": 139,
      "incorrect": 26,
      "superficial": 40,
      "instructions_not_followed": 13,
      "total_prefs": 515,
      "positive_prefs_ratio": 0.8466019417475729
    },
    {
      "model_name": "command-a",
      "median": 1065.0525349767802,
      "p2.5": 1057.0322456405916,
      "p97.5": 1074.0462433516743,
      "rank": 21,
      "rank_p2.5": 16,
      "rank_p97.5": 24,
      "total_output_tokens": 5075444,
      "conso_all_conv": 0.0,
      "n_match": 5085,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5873271297606428,
      "win_rate": 0.5629577187807276,
      "useful": 1072,
      "creative": 248,
      "complete": 999,
      "clear_formatting": 938,
      "incorrect": 246,
      "superficial": 269,
      "instructions_not_followed": 92,
      "total_prefs": 3864,
      "positive_prefs_ratio": 0.8429089026915114
    },
    {
      "model_name": "grok-4-fast",
      "median": 1063.019707916078,
      "p2.5": 1045.4896258043527,
      "p97.5": 1082.07022872674,
      "rank": 22,
      "rank_p2.5": 13,
      "rank_p97.5": 27,
      "total_output_tokens": 1341367,
      "conso_all_conv": 0.0,
      "n_match": 899,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5845823931959614,
      "win_rate": 0.5484409799554566,
      "useful": 69,
      "creative": 8,
      "complete": 39,
      "clear_formatting": 20,
      "incorrect": 13,
      "superficial": 9,
      "instructions_not_followed": 3,
      "total_prefs": 161,
      "positive_prefs_ratio": 0.84472049689441
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1060.4313223565093,
      "p2.5": 1044.971248118649,
      "p97.5": 1075.8974193525883,
      "rank": 23,
      "rank_p2.5": 16,
      "rank_p97.5": 27,
      "total_output_tokens": 1191004,
      "conso_all_conv": 0.0,
      "n_match": 1488,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5810808238632614,
      "win_rate": 0.5367854741089442,
      "useful": 142,
      "creative": 28,
      "complete": 109,
      "clear_formatting": 95,
      "incorrect": 38,
      "superficial": 38,
      "instructions_not_followed": 16,
      "total_prefs": 466,
      "positive_prefs_ratio": 0.8025751072961373
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1055.4272781275083,
      "p2.5": 1045.9178639839329,
      "p97.5": 1065.3533806371847,
      "rank": 24,
      "rank_p2.5": 20,
      "rank_p97.5": 27,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5742911558641681,
      "win_rate": 0.5242257486562579,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "glm-4.6",
      "median": 1054.748980996634,
      "p2.5": 1023.1612783180889,
      "p97.5": 1085.9236095545386,
      "rank": 25,
      "rank_p2.5": 12,
      "rank_p97.5": 35,
      "total_output_tokens": 848727,
      "conso_all_conv": 0.0,
      "n_match": 306,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5733688684526963,
      "win_rate": 0.538235294117647,
      "useful": 28,
      "creative": 9,
      "complete": 26,
      "clear_formatting": 13,
      "incorrect": 2,
      "superficial": 3,
      "instructions_not_followed": 0,
      "total_prefs": 81,
      "positive_prefs_ratio": 0.9382716049382716
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1050.2293242679943,
      "p2.5": 1042.6236930069247,
      "p97.5": 1057.7184087264106,
      "rank": 26,
      "rank_p2.5": 22,
      "rank_p97.5": 29,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5672123172843191,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1047.124710892949,
      "p2.5": 1039.6809548489182,
      "p97.5": 1055.2074503245067,
      "rank": 27,
      "rank_p2.5": 24,
      "rank_p97.5": 29,
      "total_output_tokens": 7625485,
      "conso_all_conv": 0.0,
      "n_match": 6053,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5629727090544063,
      "win_rate": 0.5295588964150009,
      "useful": 1049,
      "creative": 317,
      "complete": 1162,
      "clear_formatting": 928,
      "incorrect": 501,
      "superficial": 224,
      "instructions_not_followed": 174,
      "total_prefs": 4355,
      "positive_prefs_ratio": 0.7935706084959816
    },
    {
      "model_name": "qwen3-32b",
      "median": 1046.767488471552,
      "p2.5": 1028.0736008082374,
      "p97.5": 1063.7880442309754,
      "rank": 28,
      "rank_p2.5": 21,
      "rank_p97.5": 33,
      "total_output_tokens": 1860504,
      "conso_all_conv": 0.0,
      "n_match": 882,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5624843702458366,
      "win_rate": 0.5532232346241458,
      "useful": 245,
      "creative": 60,
      "complete": 195,
      "clear_formatting": 166,
      "incorrect": 86,
      "superficial": 65,
      "instructions_not_followed": 35,
      "total_prefs": 852,
      "positive_prefs_ratio": 0.7816901408450704
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1038.5633948205582,
      "p2.5": 1030.5591303763044,
      "p97.5": 1045.9107406624553,
      "rank": 29,
      "rank_p2.5": 27,
      "rank_p97.5": 33,
      "total_output_tokens": 5066186,
      "conso_all_conv": 0.0,
      "n_match": 6297,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5512420059042422,
      "win_rate": 0.5130786338363781,
      "useful": 1175,
      "creative": 272,
      "complete": 894,
      "clear_formatting": 960,
      "incorrect": 228,
      "superficial": 439,
      "instructions_not_followed": 92,
      "total_prefs": 4060,
      "positive_prefs_ratio": 0.8130541871921182
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1036.0888883666269,
      "p2.5": 1028.1317380615453,
      "p97.5": 1043.625435762552,
      "rank": 30,
      "rank_p2.5": 28,
      "rank_p97.5": 34,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5478418837010287,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1033.8228835306415,
      "p2.5": 1017.3010742951756,
      "p97.5": 1052.0198083640962,
      "rank": 31,
      "rank_p2.5": 25,
      "rank_p97.5": 37,
      "total_output_tokens": 1383381,
      "conso_all_conv": 0.0,
      "n_match": 1065,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5447249606811596,
      "win_rate": 0.5099905926622764,
      "useful": 73,
      "creative": 32,
      "complete": 97,
      "clear_formatting": 91,
      "incorrect": 21,
      "superficial": 30,
      "instructions_not_followed": 21,
      "total_prefs": 365,
      "positive_prefs_ratio": 0.8027397260273973
    },
    {
      "model_name": "deepseek-r1",
      "median": 1033.6343038830175,
      "p2.5": 1023.5470026348598,
      "p97.5": 1043.8195045375303,
      "rank": 32,
      "rank_p2.5": 28,
      "rank_p97.5": 35,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5444654320850515,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1031.3560792079616,
      "p2.5": 1011.3092761767895,
      "p97.5": 1052.172885107704,
      "rank": 33,
      "rank_p2.5": 25,
      "rank_p97.5": 40,
      "total_output_tokens": 884385,
      "conso_all_conv": 0.0,
      "n_match": 740,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.541328529454703,
      "win_rate": 0.5003788903924222,
      "useful": 42,
      "creative": 11,
      "complete": 58,
      "clear_formatting": 26,
      "incorrect": 8,
      "superficial": 12,
      "instructions_not_followed": 6,
      "total_prefs": 163,
      "positive_prefs_ratio": 0.8404907975460123
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1024.6343390345824,
      "p2.5": 1017.913119705617,
      "p97.5": 1031.3274976973971,
      "rank": 34,
      "rank_p2.5": 32,
      "rank_p97.5": 38,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5320585072840655,
      "win_rate": 0.5274413863404689,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1024.107502056003,
      "p2.5": 1009.7891902154842,
      "p97.5": 1040.164565765533,
      "rank": 35,
      "rank_p2.5": 29,
      "rank_p97.5": 41,
      "total_output_tokens": 1493098,
      "conso_all_conv": 0.0,
      "n_match": 1391,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5313311257323982,
      "win_rate": 0.4867410071942446,
      "useful": 92,
      "creative": 37,
      "complete": 120,
      "clear_formatting": 96,
      "incorrect": 45,
      "superficial": 33,
      "instructions_not_followed": 21,
      "total_prefs": 444,
      "positive_prefs_ratio": 0.777027027027027
    },
    {
      "model_name": "llama-4-scout",
      "median": 1019.7569395881878,
      "p2.5": 1009.7348064260345,
      "p97.5": 1028.6759972759573,
      "rank": 36,
      "rank_p2.5": 33,
      "rank_p97.5": 41,
      "total_output_tokens": 3028677,
      "conso_all_conv": 0.0,
      "n_match": 3631,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5253207726506542,
      "win_rate": 0.4945035852178709,
      "useful": 620,
      "creative": 163,
      "complete": 550,
      "clear_formatting": 510,
      "incorrect": 208,
      "superficial": 278,
      "instructions_not_followed": 75,
      "total_prefs": 2404,
      "positive_prefs_ratio": 0.7666389351081531
    },
    {
      "model_name": "llama-maverick",
      "median": 1016.9398268234991,
      "p2.5": 996.0190835577241,
      "p97.5": 1036.3047528326902,
      "rank": 37,
      "rank_p2.5": 31,
      "rank_p97.5": 45,
      "total_output_tokens": 640412,
      "conso_all_conv": 0.0,
      "n_match": 771,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5214258781580698,
      "win_rate": 0.5068871595330741,
      "useful": 41,
      "creative": 7,
      "complete": 31,
      "clear_formatting": 27,
      "incorrect": 11,
      "superficial": 19,
      "instructions_not_followed": 4,
      "total_prefs": 140,
      "positive_prefs_ratio": 0.7571428571428571
    },
    {
      "model_name": "gpt-5",
      "median": 1015.2981931154989,
      "p2.5": 1001.5422151763232,
      "p97.5": 1030.3408279058062,
      "rank": 38,
      "rank_p2.5": 33,
      "rank_p97.5": 43,
      "total_output_tokens": 1588596,
      "conso_all_conv": 0.0,
      "n_match": 1682,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5191553013124753,
      "win_rate": 0.4637418203450327,
      "useful": 49,
      "creative": 21,
      "complete": 59,
      "clear_formatting": 30,
      "incorrect": 7,
      "superficial": 12,
      "instructions_not_followed": 9,
      "total_prefs": 187,
      "positive_prefs_ratio": 0.8502673796791443
    },
    {
      "model_name": "mistral-saba",
      "median": 1013.3807814163104,
      "p2.5": 1003.9468633865782,
      "p97.5": 1021.6711379232077,
      "rank": 39,
      "rank_p2.5": 36,
      "rank_p97.5": 43,
      "total_output_tokens": 3696731,
      "conso_all_conv": 0.0,
      "n_match": 4142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5165026139612843,
      "win_rate": 0.47356349589570257,
      "useful": 694,
      "creative": 157,
      "complete": 532,
      "clear_formatting": 536,
      "incorrect": 219,
      "superficial": 330,
      "instructions_not_followed": 111,
      "total_prefs": 2579,
      "positive_prefs_ratio": 0.7440868553702986
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1011.7913204110753,
      "p2.5": 1003.2964703917412,
      "p97.5": 1020.7917527601953,
      "rank": 40,
      "rank_p2.5": 36,
      "rank_p97.5": 43,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.514303180548091,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "o4-mini",
      "median": 1010.6902677871362,
      "p2.5": 998.9954488630333,
      "p97.5": 1022.4955825222708,
      "rank": 41,
      "rank_p2.5": 36,
      "rank_p97.5": 44,
      "total_output_tokens": 2687964,
      "conso_all_conv": 0.0,
      "n_match": 2576,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.512779389927578,
      "win_rate": 0.46645186335403727,
      "useful": 513,
      "creative": 108,
      "complete": 357,
      "clear_formatting": 273,
      "incorrect": 126,
      "superficial": 205,
      "instructions_not_followed": 44,
      "total_prefs": 1626,
      "positive_prefs_ratio": 0.7693726937269373
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1005.7575601433459,
      "p2.5": 980.192420617181,
      "p97.5": 1028.9051862230071,
      "rank": 42,
      "rank_p2.5": 33,
      "rank_p97.5": 51,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5059515905923715,
      "win_rate": 0.5826115485564304,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 995.9590845632836,
      "p2.5": 986.3056404729192,
      "p97.5": 1004.9721332826309,
      "rank": 43,
      "rank_p2.5": 42,
      "rank_p97.5": 49,
      "total_output_tokens": 3511472,
      "conso_all_conv": 0.0,
      "n_match": 4304,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4923902923952607,
      "win_rate": 0.46755576208178434,
      "useful": 680,
      "creative": 132,
      "complete": 478,
      "clear_formatting": 487,
      "incorrect": 246,
      "superficial": 332,
      "instructions_not_followed": 102,
      "total_prefs": 2457,
      "positive_prefs_ratio": 0.7232397232397232
    },
    {
      "model_name": "qwen-3-8b",
      "median": 995.3365896424434,
      "p2.5": 963.5295657771079,
      "p97.5": 1026.4700224104881,
      "rank": 44,
      "rank_p2.5": 34,
      "rank_p97.5": 60,
      "total_output_tokens": 725901,
      "conso_all_conv": 0.0,
      "n_match": 331,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.49152915089775145,
      "win_rate": 0.4854682779456193,
      "useful": 11,
      "creative": 3,
      "complete": 12,
      "clear_formatting": 8,
      "incorrect": 8,
      "superficial": 4,
      "instructions_not_followed": 5,
      "total_prefs": 51,
      "positive_prefs_ratio": 0.6666666666666666
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 993.8301023776314,
      "p2.5": 975.0594057877822,
      "p97.5": 1014.8648683339712,
      "rank": 45,
      "rank_p2.5": 39,
      "rank_p97.5": 54,
      "total_output_tokens": 1241265,
      "conso_all_conv": 0.0,
      "n_match": 770,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48944544678278,
      "win_rate": 0.4392847854356307,
      "useful": 45,
      "creative": 13,
      "complete": 51,
      "clear_formatting": 28,
      "incorrect": 23,
      "superficial": 18,
      "instructions_not_followed": 7,
      "total_prefs": 185,
      "positive_prefs_ratio": 0.7405405405405405
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 990.7046012031069,
      "p2.5": 980.643180571829,
      "p97.5": 1001.8926486336647,
      "rank": 46,
      "rank_p2.5": 43,
      "rank_p97.5": 52,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48512412366248997,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "o3-mini",
      "median": 988.7760820906083,
      "p2.5": 974.3186810037935,
      "p97.5": 1003.4724522166224,
      "rank": 47,
      "rank_p2.5": 42,
      "rank_p97.5": 54,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48245912344445163,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 986.9983807097507,
      "p2.5": 979.6203944198428,
      "p97.5": 993.8703483993992,
      "rank": 48,
      "rank_p2.5": 45,
      "rank_p97.5": 52,
      "total_output_tokens": 6147509,
      "conso_all_conv": 0.0,
      "n_match": 7276,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.480003613281855,
      "win_rate": 0.4728618556701032,
      "useful": 1348,
      "creative": 266,
      "complete": 1036,
      "clear_formatting": 999,
      "incorrect": 414,
      "superficial": 570,
      "instructions_not_followed": 134,
      "total_prefs": 4767,
      "positive_prefs_ratio": 0.7654709460876862
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 986.8495885586356,
      "p2.5": 979.2066919664836,
      "p97.5": 994.6183640944529,
      "rank": 49,
      "rank_p2.5": 45,
      "rank_p97.5": 53,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47979813956096196,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 983.0205363699652,
      "p2.5": 974.8685932923121,
      "p97.5": 991.1447536005663,
      "rank": 50,
      "rank_p2.5": 46,
      "rank_p97.5": 55,
      "total_output_tokens": 3162989,
      "conso_all_conv": 0.0,
      "n_match": 4858,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.474513399657923,
      "win_rate": 0.467863318237958,
      "useful": 880,
      "creative": 149,
      "complete": 490,
      "clear_formatting": 616,
      "incorrect": 249,
      "superficial": 420,
      "instructions_not_followed": 97,
      "total_prefs": 2901,
      "positive_prefs_ratio": 0.7359531196139263
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 978.1469597207455,
      "p2.5": 928.4812563260587,
      "p97.5": 1021.7918691562706,
      "rank": 51,
      "rank_p2.5": 36,
      "rank_p97.5": 69,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46779653276375927,
      "win_rate": 0.5449418604651162,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 977.787077253414,
      "p2.5": 962.4046115999691,
      "p97.5": 993.7694699862818,
      "rank": 52,
      "rank_p2.5": 45,
      "rank_p97.5": 60,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4673010122123688,
      "win_rate": 0.5014285714285713,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 977.1540822019613,
      "p2.5": 967.4720616900968,
      "p97.5": 985.9152671123966,
      "rank": 53,
      "rank_p2.5": 49,
      "rank_p97.5": 58,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4664296153289293,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 976.1627817984197,
      "p2.5": 966.9976887103535,
      "p97.5": 986.0777539723514,
      "rank": 54,
      "rank_p2.5": 50,
      "rank_p97.5": 58,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4650654151226361,
      "win_rate": 0.49210804152736226,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 971.7997900969835,
      "p2.5": 938.1012694835626,
      "p97.5": 1006.9106217978377,
      "rank": 55,
      "rank_p2.5": 41,
      "rank_p97.5": 66,
      "total_output_tokens": 305431,
      "conso_all_conv": 0.0,
      "n_match": 278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45906814878870716,
      "win_rate": 0.4155311355311355,
      "useful": 15,
      "creative": 1,
      "complete": 9,
      "clear_formatting": 6,
      "incorrect": 8,
      "superficial": 8,
      "instructions_not_followed": 5,
      "total_prefs": 52,
      "positive_prefs_ratio": 0.5961538461538461
    },
    {
      "model_name": "phi-4",
      "median": 971.4848174790986,
      "p2.5": 964.667602535506,
      "p97.5": 978.240283591858,
      "rank": 56,
      "rank_p2.5": 52,
      "rank_p97.5": 59,
      "total_output_tokens": 7169033,
      "conso_all_conv": 0.0,
      "n_match": 8558,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45863566074952494,
      "win_rate": 0.4759086128315998,
      "useful": 1564,
      "creative": 321,
      "complete": 1125,
      "clear_formatting": 1266,
      "incorrect": 654,
      "superficial": 711,
      "instructions_not_followed": 228,
      "total_prefs": 5869,
      "positive_prefs_ratio": 0.728573862668257
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 968.2900779001927,
      "p2.5": 961.5502585583288,
      "p97.5": 974.9456531655384,
      "rank": 57,
      "rank_p2.5": 54,
      "rank_p97.5": 60,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4542528006237047,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 966.7528713150671,
      "p2.5": 957.7656277605722,
      "p97.5": 975.1585698134279,
      "rank": 58,
      "rank_p2.5": 54,
      "rank_p97.5": 62,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45214650853408084,
      "win_rate": 0.4886329715061059,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 966.1068174806937,
      "p2.5": 902.524460816031,
      "p97.5": 1021.2470080134358,
      "rank": 59,
      "rank_p2.5": 36,
      "rank_p97.5": 71,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45126180892529405,
      "win_rate": 0.46869047619047627,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 962.2997896698585,
      "p2.5": 906.3431895865274,
      "p97.5": 1020.2094511087347,
      "rank": 60,
      "rank_p2.5": 36,
      "rank_p97.5": 71,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44605517257814836,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 961.9649997356048,
      "p2.5": 954.8437606338871,
      "p97.5": 969.0302521967704,
      "rank": 61,
      "rank_p2.5": 57,
      "rank_p97.5": 63,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4455978674425245,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 957.1834575610162,
      "p2.5": 945.0983579813094,
      "p97.5": 968.4565563310703,
      "rank": 62,
      "rank_p2.5": 57,
      "rank_p97.5": 65,
      "total_output_tokens": 2623950,
      "conso_all_conv": 0.0,
      "n_match": 2813,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43907720962273766,
      "win_rate": 0.44434921763869134,
      "useful": 485,
      "creative": 128,
      "complete": 350,
      "clear_formatting": 362,
      "incorrect": 184,
      "superficial": 334,
      "instructions_not_followed": 123,
      "total_prefs": 1966,
      "positive_prefs_ratio": 0.6739572736520855
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 957.1618277138105,
      "p2.5": 948.0838740688682,
      "p97.5": 965.4678521121149,
      "rank": 63,
      "rank_p2.5": 58,
      "rank_p97.5": 65,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43904775966965925,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "qwq-32b",
      "median": 956.0776594529359,
      "p2.5": 941.5748235958783,
      "p97.5": 971.7105454440391,
      "rank": 64,
      "rank_p2.5": 56,
      "rank_p97.5": 66,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43757218259408587,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "gpt-5-nano",
      "median": 947.6699540163336,
      "p2.5": 925.1074109860391,
      "p97.5": 970.4580705668867,
      "rank": 65,
      "rank_p2.5": 56,
      "rank_p97.5": 70,
      "total_output_tokens": 647180,
      "conso_all_conv": 0.0,
      "n_match": 689,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4261689667530167,
      "win_rate": 0.38153846153846155,
      "useful": 19,
      "creative": 3,
      "complete": 17,
      "clear_formatting": 13,
      "incorrect": 8,
      "superficial": 11,
      "instructions_not_followed": 8,
      "total_prefs": 79,
      "positive_prefs_ratio": 0.6582278481012658
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 946.5640541998258,
      "p2.5": 938.7618342513997,
      "p97.5": 954.556630592245,
      "rank": 66,
      "rank_p2.5": 62,
      "rank_p97.5": 66,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4246746475835632,
      "win_rate": 0.466347438752784,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 934.2950778288592,
      "p2.5": 926.7166063828357,
      "p97.5": 942.7728852087994,
      "rank": 67,
      "rank_p2.5": 65,
      "rank_p97.5": 70,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4081943390648887,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 931.0859946304819,
      "p2.5": 924.0396308252239,
      "p97.5": 938.3483986427273,
      "rank": 68,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6364466,
      "conso_all_conv": 0.0,
      "n_match": 8913,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40391573879456655,
      "win_rate": 0.4444468132854578,
      "useful": 1489,
      "creative": 363,
      "complete": 971,
      "clear_formatting": 1119,
      "incorrect": 959,
      "superficial": 933,
      "instructions_not_followed": 387,
      "total_prefs": 6221,
      "positive_prefs_ratio": 0.6336601832502813
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 930.9270167594692,
      "p2.5": 923.2470941807496,
      "p97.5": 939.1227654445793,
      "rank": 69,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40370414593271087,
      "win_rate": 0.4378942320865873,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 930.0583909454788,
      "p2.5": 912.355020810576,
      "p97.5": 948.5065286049003,
      "rank": 70,
      "rank_p2.5": 64,
      "rank_p97.5": 71,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4025486669621937,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "hermes-4-70b",
      "median": 914.9922704456199,
      "p2.5": 891.8712370014201,
      "p97.5": 938.8353240481771,
      "rank": 71,
      "rank_p2.5": 67,
      "rank_p97.5": 72,
      "total_output_tokens": 304511,
      "conso_all_conv": 0.0,
      "n_match": 661,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3826859420126865,
      "win_rate": 0.36136157337367625,
      "useful": 21,
      "creative": 6,
      "complete": 13,
      "clear_formatting": 16,
      "incorrect": 10,
      "superficial": 20,
      "instructions_not_followed": 9,
      "total_prefs": 95,
      "positive_prefs_ratio": 0.5894736842105263
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 888.3885014329381,
      "p2.5": 874.8614135282439,
      "p97.5": 901.57645830055,
      "rank": 72,
      "rank_p2.5": 71,
      "rank_p97.5": 73,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3485599789048892,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 886.1194325201757,
      "p2.5": 875.3080456982847,
      "p97.5": 896.0986703496354,
      "rank": 73,
      "rank_p2.5": 72,
      "rank_p97.5": 73,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.34571230789898105,
      "win_rate": 0.41629681386249295,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 864.6677973247023,
      "p2.5": 851.8845945786707,
      "p97.5": 879.1578927828494,
      "rank": 74,
      "rank_p2.5": 74,
      "rank_p97.5": 75,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3193351086956484,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 856.0158565938327,
      "p2.5": 846.9363341713417,
      "p97.5": 864.4869124882063,
      "rank": 75,
      "rank_p2.5": 74,
      "rank_p97.5": 76,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3089914344123285,
      "win_rate": 0.37482482802751566,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 845.6429103037099,
      "p2.5": 836.4655329071472,
      "p97.5": 854.7042850211794,
      "rank": 76,
      "rank_p2.5": 75,
      "rank_p97.5": 77,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.29682748905971884,
      "win_rate": 0.36710357470210814,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 823.3000271532585,
      "p2.5": 806.9870966572407,
      "p97.5": 838.1523123374474,
      "rank": 77,
      "rank_p2.5": 77,
      "rank_p97.5": 78,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2715497168628755,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 770.6387197499538,
      "p2.5": 664.9626563159571,
      "p97.5": 856.81478446138,
      "rank": 78,
      "rank_p2.5": 75,
      "rank_p97.5": 80,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2173126608538249,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 753.5341350150945,
      "p2.5": 706.8469384706539,
      "p97.5": 796.9879717405495,
      "rank": 79,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.20137941165742362,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 733.1603521666498,
      "p2.5": 644.9399408738719,
      "p97.5": 811.2120380750905,
      "rank": 80,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.18349190959315526,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}