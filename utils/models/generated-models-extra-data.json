{
  "timestamp": 1763526225.0,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1151.584394621197,
      "p2.5": 1135.1773901225804,
      "p97.5": 1167.74856348718,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 1,
      "total_output_tokens": 2841712,
      "conso_all_conv": 0.0,
      "n_match": 1563,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6981566340034481,
      "win_rate": 0.6209026888604353,
      "useful": 92,
      "creative": 37,
      "complete": 142,
      "clear_formatting": 97,
      "incorrect": 30,
      "superficial": 9,
      "instructions_not_followed": 7,
      "total_prefs": 414,
      "positive_prefs_ratio": 0.8888888888888888
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1117.9624814002561,
      "p2.5": 1104.7669321014591,
      "p97.5": 1130.480299107385,
      "rank": 2,
      "rank_p2.5": 2,
      "rank_p97.5": 4,
      "total_output_tokens": 2935956,
      "conso_all_conv": 0.0,
      "n_match": 2067,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6568611538036818,
      "win_rate": 0.5953775411423039,
      "useful": 176,
      "creative": 63,
      "complete": 232,
      "clear_formatting": 159,
      "incorrect": 52,
      "superficial": 32,
      "instructions_not_followed": 11,
      "total_prefs": 725,
      "positive_prefs_ratio": 0.8689655172413793
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1111.7291088798024,
      "p2.5": 1093.1310864040909,
      "p97.5": 1130.3021059136622,
      "rank": 3,
      "rank_p2.5": 2,
      "rank_p97.5": 8,
      "total_output_tokens": 1086849,
      "conso_all_conv": 0.0,
      "n_match": 994,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6489363996895328,
      "win_rate": 0.599778449144008,
      "useful": 55,
      "creative": 19,
      "complete": 73,
      "clear_formatting": 55,
      "incorrect": 31,
      "superficial": 9,
      "instructions_not_followed": 8,
      "total_prefs": 250,
      "positive_prefs_ratio": 0.808
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1107.459713543028,
      "p2.5": 1100.5365593058903,
      "p97.5": 1114.2615281369742,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 6,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6434645448375013,
      "win_rate": 0.6317699216950713,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1099.9273647408054,
      "p2.5": 1090.4992348953092,
      "p97.5": 1108.7514672556197,
      "rank": 5,
      "rank_p2.5": 4,
      "rank_p97.5": 9,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6337275309735171,
      "win_rate": 0.5851128848346636,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1094.970471901183,
      "p2.5": 1087.4010421152266,
      "p97.5": 1102.6212280421528,
      "rank": 6,
      "rank_p2.5": 5,
      "rank_p97.5": 10,
      "total_output_tokens": 8284358,
      "conso_all_conv": 0.0,
      "n_match": 6035,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6272645253975595,
      "win_rate": 0.5831068765534383,
      "useful": 1206,
      "creative": 404,
      "complete": 1384,
      "clear_formatting": 1022,
      "incorrect": 303,
      "superficial": 211,
      "instructions_not_followed": 100,
      "total_prefs": 4630,
      "positive_prefs_ratio": 0.8673866090712743
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1093.539381722313,
      "p2.5": 1074.0799638951444,
      "p97.5": 1113.0606561277573,
      "rank": 7,
      "rank_p2.5": 3,
      "rank_p97.5": 15,
      "total_output_tokens": 1985242,
      "conso_all_conv": 0.0,
      "n_match": 850,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6253907863705386,
      "win_rate": 0.5655135773317591,
      "useful": 105,
      "creative": 46,
      "complete": 168,
      "clear_formatting": 113,
      "incorrect": 47,
      "superficial": 23,
      "instructions_not_followed": 23,
      "total_prefs": 525,
      "positive_prefs_ratio": 0.8228571428571428
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1093.0360716260006,
      "p2.5": 1075.4512735706,
      "p97.5": 1111.4167880720288,
      "rank": 8,
      "rank_p2.5": 3,
      "rank_p97.5": 14,
      "total_output_tokens": 864058,
      "conso_all_conv": 0.0,
      "n_match": 973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6247309829599959,
      "win_rate": 0.5590628218331616,
      "useful": 86,
      "creative": 12,
      "complete": 64,
      "clear_formatting": 59,
      "incorrect": 27,
      "superficial": 12,
      "instructions_not_followed": 7,
      "total_prefs": 267,
      "positive_prefs_ratio": 0.8277153558052435
    },
    {
      "model_name": "magistral-medium",
      "median": 1089.5487092423743,
      "p2.5": 1075.2545702061116,
      "p97.5": 1104.8473443204366,
      "rank": 9,
      "rank_p2.5": 4,
      "rank_p97.5": 15,
      "total_output_tokens": 1420911,
      "conso_all_conv": 0.0,
      "n_match": 1484,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6201478574677715,
      "win_rate": 0.5747506738544474,
      "useful": 87,
      "creative": 27,
      "complete": 94,
      "clear_formatting": 57,
      "incorrect": 35,
      "superficial": 18,
      "instructions_not_followed": 3,
      "total_prefs": 321,
      "positive_prefs_ratio": 0.8255451713395638
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1085.878651936775,
      "p2.5": 1066.0543729216915,
      "p97.5": 1106.2442249209807,
      "rank": 10,
      "rank_p2.5": 5,
      "rank_p97.5": 19,
      "total_output_tokens": 1120941,
      "conso_all_conv": 0.0,
      "n_match": 742,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6153035903879175,
      "win_rate": 0.49710027100271004,
      "useful": 56,
      "creative": 24,
      "complete": 58,
      "clear_formatting": 43,
      "incorrect": 19,
      "superficial": 12,
      "instructions_not_followed": 7,
      "total_prefs": 219,
      "positive_prefs_ratio": 0.8264840182648402
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1085.3527618800842,
      "p2.5": 1077.1544648085533,
      "p97.5": 1094.034884624787,
      "rank": 11,
      "rank_p2.5": 7,
      "rank_p97.5": 15,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.614607725255981,
      "win_rate": 0.6141481069042317,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1080.7186569566566,
      "p2.5": 1059.973148522755,
      "p97.5": 1103.0623974350028,
      "rank": 12,
      "rank_p2.5": 5,
      "rank_p97.5": 21,
      "total_output_tokens": 925134,
      "conso_all_conv": 0.0,
      "n_match": 539,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6084577716072538,
      "win_rate": 0.5503538175046555,
      "useful": 28,
      "creative": 12,
      "complete": 28,
      "clear_formatting": 16,
      "incorrect": 11,
      "superficial": 5,
      "instructions_not_followed": 3,
      "total_prefs": 103,
      "positive_prefs_ratio": 0.8155339805825242
    },
    {
      "model_name": "glm-4.5",
      "median": 1077.9372375749767,
      "p2.5": 1059.0102652118012,
      "p97.5": 1097.8138935536845,
      "rank": 13,
      "rank_p2.5": 7,
      "rank_p97.5": 22,
      "total_output_tokens": 2180123,
      "conso_all_conv": 0.0,
      "n_match": 920,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6047514400101299,
      "win_rate": 0.5507399347116431,
      "useful": 94,
      "creative": 26,
      "complete": 101,
      "clear_formatting": 76,
      "incorrect": 21,
      "superficial": 13,
      "instructions_not_followed": 9,
      "total_prefs": 340,
      "positive_prefs_ratio": 0.8735294117647059
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1076.4910790588924,
      "p2.5": 1064.0100036973045,
      "p97.5": 1088.9916472830446,
      "rank": 14,
      "rank_p2.5": 9,
      "rank_p97.5": 20,
      "total_output_tokens": 2349675,
      "conso_all_conv": 0.0,
      "n_match": 2082,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6028200713550799,
      "win_rate": 0.5370687169629986,
      "useful": 154,
      "creative": 36,
      "complete": 114,
      "clear_formatting": 104,
      "incorrect": 29,
      "superficial": 28,
      "instructions_not_followed": 18,
      "total_prefs": 483,
      "positive_prefs_ratio": 0.84472049689441
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1073.996024132129,
      "p2.5": 1066.0242966152512,
      "p97.5": 1081.942705946501,
      "rank": 15,
      "rank_p2.5": 11,
      "rank_p97.5": 20,
      "total_output_tokens": 7536331,
      "conso_all_conv": 0.0,
      "n_match": 5671,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5994811373345804,
      "win_rate": 0.5701745723858226,
      "useful": 1006,
      "creative": 302,
      "complete": 1177,
      "clear_formatting": 855,
      "incorrect": 327,
      "superficial": 253,
      "instructions_not_followed": 138,
      "total_prefs": 4058,
      "positive_prefs_ratio": 0.823065549531789
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1071.8459857438997,
      "p2.5": 1058.5128449795648,
      "p97.5": 1085.0404375273502,
      "rank": 16,
      "rank_p2.5": 11,
      "rank_p97.5": 22,
      "total_output_tokens": 1487007,
      "conso_all_conv": 0.0,
      "n_match": 1825,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5965972397726631,
      "win_rate": 0.5436767123287671,
      "useful": 94,
      "creative": 30,
      "complete": 99,
      "clear_formatting": 70,
      "incorrect": 55,
      "superficial": 29,
      "instructions_not_followed": 8,
      "total_prefs": 385,
      "positive_prefs_ratio": 0.7610389610389611
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1071.601598748551,
      "p2.5": 1056.5104296241502,
      "p97.5": 1086.696970376102,
      "rank": 17,
      "rank_p2.5": 10,
      "rank_p97.5": 23,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5962690553128182,
      "win_rate": 0.5305338541666668,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1067.8668769662168,
      "p2.5": 1056.0572771280035,
      "p97.5": 1079.9372955842068,
      "rank": 18,
      "rank_p2.5": 13,
      "rank_p97.5": 23,
      "total_output_tokens": 1656622,
      "conso_all_conv": 0.0,
      "n_match": 2218,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5912443038430872,
      "win_rate": 0.5286332882273342,
      "useful": 208,
      "creative": 59,
      "complete": 183,
      "clear_formatting": 201,
      "incorrect": 37,
      "superficial": 56,
      "instructions_not_followed": 16,
      "total_prefs": 760,
      "positive_prefs_ratio": 0.8565789473684211
    },
    {
      "model_name": "command-a",
      "median": 1065.975858102083,
      "p2.5": 1058.8379439040286,
      "p97.5": 1073.6907865648661,
      "rank": 19,
      "rank_p2.5": 15,
      "rank_p97.5": 23,
      "total_output_tokens": 5585006,
      "conso_all_conv": 0.0,
      "n_match": 5627,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5886935318813801,
      "win_rate": 0.5596676737160121,
      "useful": 1118,
      "creative": 257,
      "complete": 1036,
      "clear_formatting": 972,
      "incorrect": 260,
      "superficial": 280,
      "instructions_not_followed": 99,
      "total_prefs": 4022,
      "positive_prefs_ratio": 0.8411238189955246
    },
    {
      "model_name": "kimi-k2",
      "median": 1065.00703438124,
      "p2.5": 1039.0199556777957,
      "p97.5": 1090.509985784863,
      "rank": 20,
      "rank_p2.5": 9,
      "rank_p97.5": 29,
      "total_output_tokens": 747540,
      "conso_all_conv": 0.0,
      "n_match": 455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5873850430352502,
      "win_rate": 0.5074778761061947,
      "useful": 102,
      "creative": 19,
      "complete": 50,
      "clear_formatting": 41,
      "incorrect": 17,
      "superficial": 23,
      "instructions_not_followed": 1,
      "total_prefs": 253,
      "positive_prefs_ratio": 0.8379446640316206
    },
    {
      "model_name": "grok-4-fast",
      "median": 1063.558336010411,
      "p2.5": 1048.9272777407245,
      "p97.5": 1078.4920128700007,
      "rank": 21,
      "rank_p2.5": 13,
      "rank_p97.5": 25,
      "total_output_tokens": 2132960,
      "conso_all_conv": 0.0,
      "n_match": 1544,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5854263926124064,
      "win_rate": 0.522230869001297,
      "useful": 108,
      "creative": 18,
      "complete": 78,
      "clear_formatting": 53,
      "incorrect": 28,
      "superficial": 20,
      "instructions_not_followed": 6,
      "total_prefs": 311,
      "positive_prefs_ratio": 0.8263665594855305
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1056.4660790687062,
      "p2.5": 1046.7442542300375,
      "p97.5": 1066.6074218719789,
      "rank": 22,
      "rank_p2.5": 19,
      "rank_p97.5": 26,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5758040222144468,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1056.413125422096,
      "p2.5": 1043.6820936138402,
      "p97.5": 1067.1523399214393,
      "rank": 23,
      "rank_p2.5": 18,
      "rank_p97.5": 27,
      "total_output_tokens": 1713519,
      "conso_all_conv": 0.0,
      "n_match": 2185,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5757319780512868,
      "win_rate": 0.5228663003663004,
      "useful": 188,
      "creative": 42,
      "complete": 144,
      "clear_formatting": 126,
      "incorrect": 59,
      "superficial": 59,
      "instructions_not_followed": 19,
      "total_prefs": 637,
      "positive_prefs_ratio": 0.7849293563579278
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1055.9392256915403,
      "p2.5": 1023.433731309059,
      "p97.5": 1087.4128292779117,
      "rank": 24,
      "rank_p2.5": 10,
      "rank_p97.5": 34,
      "total_output_tokens": 377104,
      "conso_all_conv": 0.0,
      "n_match": 272,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5750871051713041,
      "win_rate": 0.5474626865671641,
      "useful": 18,
      "creative": 8,
      "complete": 14,
      "clear_formatting": 14,
      "incorrect": 9,
      "superficial": 6,
      "instructions_not_followed": 2,
      "total_prefs": 71,
      "positive_prefs_ratio": 0.7605633802816901
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1051.3592244539932,
      "p2.5": 1044.0376551074294,
      "p97.5": 1058.9933704972605,
      "rank": 25,
      "rank_p2.5": 22,
      "rank_p97.5": 28,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5688434470204046,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1047.272371560694,
      "p2.5": 1040.4250029401276,
      "p97.5": 1054.7290445150588,
      "rank": 26,
      "rank_p2.5": 23,
      "rank_p97.5": 29,
      "total_output_tokens": 8382374,
      "conso_all_conv": 0.0,
      "n_match": 6672,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5632558271803396,
      "win_rate": 0.5280935251798562,
      "useful": 1080,
      "creative": 329,
      "complete": 1200,
      "clear_formatting": 952,
      "incorrect": 527,
      "superficial": 236,
      "instructions_not_followed": 183,
      "total_prefs": 4507,
      "positive_prefs_ratio": 0.7901042822276458
    },
    {
      "model_name": "glm-4.6",
      "median": 1044.8237768406534,
      "p2.5": 1024.2046911865561,
      "p97.5": 1066.735313930842,
      "rank": 27,
      "rank_p2.5": 19,
      "rank_p97.5": 34,
      "total_output_tokens": 2154242,
      "conso_all_conv": 0.0,
      "n_match": 743,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5599012950350477,
      "win_rate": 0.514331983805668,
      "useful": 50,
      "creative": 17,
      "complete": 64,
      "clear_formatting": 32,
      "incorrect": 12,
      "superficial": 7,
      "instructions_not_followed": 6,
      "total_prefs": 188,
      "positive_prefs_ratio": 0.8670212765957447
    },
    {
      "model_name": "qwen3-32b",
      "median": 1041.2651704878588,
      "p2.5": 1025.8752041067933,
      "p97.5": 1057.1676669521082,
      "rank": 28,
      "rank_p2.5": 22,
      "rank_p97.5": 34,
      "total_output_tokens": 2020707,
      "conso_all_conv": 0.0,
      "n_match": 969,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5550177209383357,
      "win_rate": 0.5478031088082903,
      "useful": 251,
      "creative": 63,
      "complete": 201,
      "clear_formatting": 170,
      "incorrect": 89,
      "superficial": 69,
      "instructions_not_followed": 37,
      "total_prefs": 880,
      "positive_prefs_ratio": 0.7784090909090909
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1038.8407311062083,
      "p2.5": 1031.335159113601,
      "p97.5": 1045.8553069732704,
      "rank": 29,
      "rank_p2.5": 26,
      "rank_p97.5": 32,
      "total_output_tokens": 5407570,
      "conso_all_conv": 0.0,
      "n_match": 6885,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5516853302623169,
      "win_rate": 0.515891326456487,
      "useful": 1214,
      "creative": 279,
      "complete": 927,
      "clear_formatting": 994,
      "incorrect": 243,
      "superficial": 453,
      "instructions_not_followed": 95,
      "total_prefs": 4205,
      "positive_prefs_ratio": 0.8118906064209275
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1037.2141259140371,
      "p2.5": 1029.8835546986024,
      "p97.5": 1044.6812850378951,
      "rank": 30,
      "rank_p2.5": 27,
      "rank_p97.5": 33,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5494473340415201,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1036.2006911800786,
      "p2.5": 1020.9372246544954,
      "p97.5": 1051.442224667362,
      "rank": 31,
      "rank_p2.5": 24,
      "rank_p97.5": 36,
      "total_output_tokens": 1647974,
      "conso_all_conv": 0.0,
      "n_match": 1179,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5480521199217409,
      "win_rate": 0.5032285471537807,
      "useful": 80,
      "creative": 33,
      "complete": 104,
      "clear_formatting": 97,
      "incorrect": 27,
      "superficial": 32,
      "instructions_not_followed": 21,
      "total_prefs": 394,
      "positive_prefs_ratio": 0.7969543147208121
    },
    {
      "model_name": "deepseek-r1",
      "median": 1034.0733236638116,
      "p2.5": 1024.3719685293474,
      "p97.5": 1043.7511533454986,
      "rank": 32,
      "rank_p2.5": 27,
      "rank_p97.5": 34,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5451212902791982,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1029.1507001034356,
      "p2.5": 1015.9769937993984,
      "p97.5": 1041.080709600212,
      "rank": 33,
      "rank_p2.5": 28,
      "rank_p97.5": 38,
      "total_output_tokens": 2198270,
      "conso_all_conv": 0.0,
      "n_match": 2011,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5383298013976966,
      "win_rate": 0.4882487562189055,
      "useful": 122,
      "creative": 45,
      "complete": 162,
      "clear_formatting": 117,
      "incorrect": 64,
      "superficial": 45,
      "instructions_not_followed": 31,
      "total_prefs": 586,
      "positive_prefs_ratio": 0.7610921501706485
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1025.5401902141198,
      "p2.5": 1019.1246125765699,
      "p97.5": 1032.151751406639,
      "rank": 34,
      "rank_p2.5": 31,
      "rank_p97.5": 37,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5333410490328281,
      "win_rate": 0.5274413863404689,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "gpt-5",
      "median": 1020.4319595003212,
      "p2.5": 1008.3990548267761,
      "p97.5": 1031.7812571544248,
      "rank": 35,
      "rank_p2.5": 32,
      "rank_p97.5": 41,
      "total_output_tokens": 2761323,
      "conso_all_conv": 0.0,
      "n_match": 2727,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5262739994426174,
      "win_rate": 0.4623587674247982,
      "useful": 95,
      "creative": 37,
      "complete": 112,
      "clear_formatting": 53,
      "incorrect": 20,
      "superficial": 38,
      "instructions_not_followed": 21,
      "total_prefs": 376,
      "positive_prefs_ratio": 0.7898936170212766
    },
    {
      "model_name": "mistral-saba",
      "median": 1019.5143517465519,
      "p2.5": 1011.0211169973412,
      "p97.5": 1027.9216532712524,
      "rank": 36,
      "rank_p2.5": 33,
      "rank_p97.5": 41,
      "total_output_tokens": 4175657,
      "conso_all_conv": 0.0,
      "n_match": 4737,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5250036016514408,
      "win_rate": 0.480390542537471,
      "useful": 735,
      "creative": 166,
      "complete": 573,
      "clear_formatting": 569,
      "incorrect": 239,
      "superficial": 336,
      "instructions_not_followed": 113,
      "total_prefs": 2731,
      "positive_prefs_ratio": 0.7480776272427682
    },
    {
      "model_name": "llama-4-scout",
      "median": 1018.1931610235137,
      "p2.5": 1008.5835850942975,
      "p97.5": 1027.5021188438711,
      "rank": 37,
      "rank_p2.5": 33,
      "rank_p97.5": 41,
      "total_output_tokens": 3448228,
      "conso_all_conv": 0.0,
      "n_match": 4190,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5231740363128631,
      "win_rate": 0.4956532123238596,
      "useful": 650,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 533,
      "incorrect": 224,
      "superficial": 298,
      "instructions_not_followed": 78,
      "total_prefs": 2540,
      "positive_prefs_ratio": 0.7637795275590551
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1014.2278645613512,
      "p2.5": 999.0266052837992,
      "p97.5": 1029.2726372312172,
      "rank": 38,
      "rank_p2.5": 33,
      "rank_p97.5": 44,
      "total_output_tokens": 1533340,
      "conso_all_conv": 0.0,
      "n_match": 1326,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5176804097182585,
      "win_rate": 0.477577358490566,
      "useful": 61,
      "creative": 17,
      "complete": 89,
      "clear_formatting": 40,
      "incorrect": 20,
      "superficial": 26,
      "instructions_not_followed": 24,
      "total_prefs": 277,
      "positive_prefs_ratio": 0.7472924187725631
    },
    {
      "model_name": "llama-maverick",
      "median": 1013.3194994323128,
      "p2.5": 998.0994168216736,
      "p97.5": 1028.3666597965064,
      "rank": 39,
      "rank_p2.5": 33,
      "rank_p97.5": 44,
      "total_output_tokens": 1113044,
      "conso_all_conv": 0.0,
      "n_match": 1377,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5164214914347722,
      "win_rate": 0.5013798111837329,
      "useful": 76,
      "creative": 13,
      "complete": 61,
      "clear_formatting": 47,
      "incorrect": 23,
      "superficial": 43,
      "instructions_not_followed": 8,
      "total_prefs": 271,
      "positive_prefs_ratio": 0.7269372693726938
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1012.4976417574219,
      "p2.5": 1004.8745758762713,
      "p97.5": 1020.7758069515163,
      "rank": 40,
      "rank_p2.5": 36,
      "rank_p97.5": 43,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5152823493882525,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "o4-mini",
      "median": 1012.0482178866096,
      "p2.5": 1000.9744114565877,
      "p97.5": 1024.054700924645,
      "rank": 41,
      "rank_p2.5": 35,
      "rank_p97.5": 44,
      "total_output_tokens": 2807115,
      "conso_all_conv": 0.0,
      "n_match": 2698,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5146593796470548,
      "win_rate": 0.46999258435298485,
      "useful": 521,
      "creative": 110,
      "complete": 367,
      "clear_formatting": 282,
      "incorrect": 130,
      "superficial": 208,
      "instructions_not_followed": 44,
      "total_prefs": 1662,
      "positive_prefs_ratio": 0.7701564380264742
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1006.6509191294921,
      "p2.5": 981.4256819569547,
      "p97.5": 1030.9410303037153,
      "rank": 42,
      "rank_p2.5": 32,
      "rank_p97.5": 51,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5071762680105302,
      "win_rate": 0.5826115485564305,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 997.3692404932378,
      "p2.5": 988.975141468991,
      "p97.5": 1005.9188272748037,
      "rank": 43,
      "rank_p2.5": 42,
      "rank_p97.5": 48,
      "total_output_tokens": 3907887,
      "conso_all_conv": 0.0,
      "n_match": 4880,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4943074807540957,
      "win_rate": 0.466266393442623,
      "useful": 712,
      "creative": 137,
      "complete": 514,
      "clear_formatting": 505,
      "incorrect": 271,
      "superficial": 349,
      "instructions_not_followed": 109,
      "total_prefs": 2597,
      "positive_prefs_ratio": 0.719291490180978
    },
    {
      "model_name": "qwen-3-8b",
      "median": 996.3729039101411,
      "p2.5": 977.2707564006251,
      "p97.5": 1016.1560459394506,
      "rank": 44,
      "rank_p2.5": 39,
      "rank_p97.5": 54,
      "total_output_tokens": 1803955,
      "conso_all_conv": 0.0,
      "n_match": 865,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.49292661328585474,
      "win_rate": 0.4610982658959537,
      "useful": 30,
      "creative": 7,
      "complete": 45,
      "clear_formatting": 27,
      "incorrect": 35,
      "superficial": 14,
      "instructions_not_followed": 9,
      "total_prefs": 167,
      "positive_prefs_ratio": 0.6526946107784432
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 992.5016660574709,
      "p2.5": 976.603955496741,
      "p97.5": 1007.7116169273999,
      "rank": 45,
      "rank_p2.5": 42,
      "rank_p97.5": 54,
      "total_output_tokens": 2212489,
      "conso_all_conv": 0.0,
      "n_match": 1328,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48756314803916145,
      "win_rate": 0.44205727204220047,
      "useful": 74,
      "creative": 22,
      "complete": 85,
      "clear_formatting": 40,
      "incorrect": 44,
      "superficial": 35,
      "instructions_not_followed": 15,
      "total_prefs": 315,
      "positive_prefs_ratio": 0.7015873015873015
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 991.8811322865533,
      "p2.5": 982.0006553291121,
      "p97.5": 1001.9483299414347,
      "rank": 46,
      "rank_p2.5": 43,
      "rank_p97.5": 52,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.486703745653505,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 989.5390538451161,
      "p2.5": 982.2020757766045,
      "p97.5": 996.6285579764968,
      "rank": 47,
      "rank_p2.5": 44,
      "rank_p97.5": 52,
      "total_output_tokens": 6489661,
      "conso_all_conv": 0.0,
      "n_match": 7739,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4834610742259626,
      "win_rate": 0.47415740501421566,
      "useful": 1366,
      "creative": 272,
      "complete": 1059,
      "clear_formatting": 1016,
      "incorrect": 429,
      "superficial": 589,
      "instructions_not_followed": 139,
      "total_prefs": 4870,
      "positive_prefs_ratio": 0.762422997946612
    },
    {
      "model_name": "o3-mini",
      "median": 989.4625395427029,
      "p2.5": 975.0375625040598,
      "p97.5": 1003.6621400006862,
      "rank": 48,
      "rank_p2.5": 42,
      "rank_p97.5": 54,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48335516577157617,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 987.61078664452,
      "p2.5": 980.530492831213,
      "p97.5": 995.6561721993095,
      "rank": 49,
      "rank_p2.5": 45,
      "rank_p97.5": 53,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4807926180233405,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 984.4236263194875,
      "p2.5": 976.9134725681917,
      "p97.5": 993.1594603436192,
      "rank": 50,
      "rank_p2.5": 46,
      "rank_p97.5": 54,
      "total_output_tokens": 3446522,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4763849682077385,
      "win_rate": 0.46637946837763516,
      "useful": 920,
      "creative": 157,
      "complete": 520,
      "clear_formatting": 644,
      "incorrect": 274,
      "superficial": 443,
      "instructions_not_followed": 101,
      "total_prefs": 3059,
      "positive_prefs_ratio": 0.7325923504413206
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 978.72559296457,
      "p2.5": 962.5368611824332,
      "p97.5": 995.9344086024686,
      "rank": 51,
      "rank_p2.5": 46,
      "rank_p97.5": 60,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46851586913633675,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 978.0037933865797,
      "p2.5": 969.8775373558344,
      "p97.5": 986.8147192222788,
      "rank": 52,
      "rank_p2.5": 49,
      "rank_p97.5": 58,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46752019631212854,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 977.5529665085362,
      "p2.5": 932.9975501217299,
      "p97.5": 1022.2517203524849,
      "rank": 53,
      "rank_p2.5": 35,
      "rank_p97.5": 68,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46689845438822625,
      "win_rate": 0.5449418604651162,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 977.2505979558603,
      "p2.5": 968.8688931149867,
      "p97.5": 985.7654413752673,
      "rank": 54,
      "rank_p2.5": 50,
      "rank_p97.5": 58,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46648151608248445,
      "win_rate": 0.49210804152736226,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 975.6433116695764,
      "p2.5": 956.6355317947294,
      "p97.5": 996.097680415711,
      "rank": 55,
      "rank_p2.5": 44,
      "rank_p97.5": 63,
      "total_output_tokens": 824946,
      "conso_all_conv": 0.0,
      "n_match": 817,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4642660836704916,
      "win_rate": 0.41535539215686285,
      "useful": 52,
      "creative": 5,
      "complete": 38,
      "clear_formatting": 28,
      "incorrect": 28,
      "superficial": 30,
      "instructions_not_followed": 14,
      "total_prefs": 195,
      "positive_prefs_ratio": 0.6307692307692307
    },
    {
      "model_name": "phi-4",
      "median": 972.809092889466,
      "p2.5": 966.3535710980806,
      "p97.5": 979.2319798598558,
      "rank": 56,
      "rank_p2.5": 53,
      "rank_p97.5": 59,
      "total_output_tokens": 7582246,
      "conso_all_conv": 0.0,
      "n_match": 9122,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4603632356151831,
      "win_rate": 0.4748459598728209,
      "useful": 1593,
      "creative": 330,
      "complete": 1152,
      "clear_formatting": 1283,
      "incorrect": 685,
      "superficial": 735,
      "instructions_not_followed": 238,
      "total_prefs": 6016,
      "positive_prefs_ratio": 0.7244015957446809
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 969.4724142468599,
      "p2.5": 963.0292691110003,
      "p97.5": 975.9569932010085,
      "rank": 57,
      "rank_p2.5": 54,
      "rank_p97.5": 60,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45577512743333387,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 967.8550383306674,
      "p2.5": 959.8751681515814,
      "p97.5": 976.487075831368,
      "rank": 58,
      "rank_p2.5": 54,
      "rank_p97.5": 62,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4535539391210218,
      "win_rate": 0.48863297150610585,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 965.877749324324,
      "p2.5": 907.2906166874329,
      "p97.5": 1019.662057993343,
      "rank": 59,
      "rank_p2.5": 36,
      "rank_p97.5": 71,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45084110300084,
      "win_rate": 0.46869047619047627,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 965.0090035631301,
      "p2.5": 906.2521703969419,
      "p97.5": 1022.2629208619318,
      "rank": 60,
      "rank_p2.5": 36,
      "rank_p97.5": 71,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44965013612128574,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 963.0308873538231,
      "p2.5": 956.8318820494078,
      "p97.5": 969.71607877113,
      "rank": 61,
      "rank_p2.5": 57,
      "rank_p97.5": 63,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4469405798094596,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 958.2528447965401,
      "p2.5": 949.2949739962119,
      "p97.5": 967.2535315888705,
      "rank": 62,
      "rank_p2.5": 58,
      "rank_p97.5": 65,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4404094572915863,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "qwq-32b",
      "median": 956.233152310089,
      "p2.5": 941.2900898439335,
      "p97.5": 972.9604207960189,
      "rank": 63,
      "rank_p2.5": 56,
      "rank_p97.5": 66,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4376549271804181,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 956.2276786911169,
      "p2.5": 945.0253457709568,
      "p97.5": 967.1422710542905,
      "rank": 64,
      "rank_p2.5": 58,
      "rank_p97.5": 66,
      "total_output_tokens": 2790055,
      "conso_all_conv": 0.0,
      "n_match": 2939,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.437647467295605,
      "win_rate": 0.44376318475672,
      "useful": 488,
      "creative": 129,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 188,
      "superficial": 341,
      "instructions_not_followed": 125,
      "total_prefs": 1989,
      "positive_prefs_ratio": 0.6711915535444947
    },
    {
      "model_name": "gpt-5-nano",
      "median": 954.7880684171623,
      "p2.5": 936.7475240831316,
      "p97.5": 972.7331373486932,
      "rank": 65,
      "rank_p2.5": 56,
      "rank_p97.5": 67,
      "total_output_tokens": 1174720,
      "conso_all_conv": 0.0,
      "n_match": 1110,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4356864564461595,
      "win_rate": 0.3856396396396397,
      "useful": 38,
      "creative": 13,
      "complete": 42,
      "clear_formatting": 24,
      "incorrect": 14,
      "superficial": 21,
      "instructions_not_followed": 17,
      "total_prefs": 169,
      "positive_prefs_ratio": 0.6923076923076923
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 947.7361181655984,
      "p2.5": 940.6155115878174,
      "p97.5": 955.1892966967919,
      "rank": 66,
      "rank_p2.5": 62,
      "rank_p97.5": 67,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4261109056029866,
      "win_rate": 0.46634743875278395,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 935.6558430660068,
      "p2.5": 927.9412349633546,
      "p97.5": 943.5139997240319,
      "rank": 67,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4098396010447268,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 934.7723621263253,
      "p2.5": 928.2495191410162,
      "p97.5": 941.4154206922307,
      "rank": 68,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6744680,
      "conso_all_conv": 0.0,
      "n_match": 9526,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40865683144218895,
      "win_rate": 0.44287664041994745,
      "useful": 1510,
      "creative": 367,
      "complete": 988,
      "clear_formatting": 1137,
      "incorrect": 994,
      "superficial": 951,
      "instructions_not_followed": 395,
      "total_prefs": 6342,
      "positive_prefs_ratio": 0.631031220435194
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 932.3002901510553,
      "p2.5": 925.377128514397,
      "p97.5": 939.9138007364932,
      "rank": 69,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40535291698521203,
      "win_rate": 0.4378942320865872,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 931.6397650249105,
      "p2.5": 913.0028637122878,
      "p97.5": 949.3236241490197,
      "rank": 70,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40447154786058404,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "hermes-4-70b",
      "median": 927.6550434088838,
      "p2.5": 910.2274617304201,
      "p97.5": 944.4614491709198,
      "rank": 71,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 516144,
      "conso_all_conv": 0.0,
      "n_match": 1130,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3991676249453575,
      "win_rate": 0.3737787610619469,
      "useful": 43,
      "creative": 10,
      "complete": 27,
      "clear_formatting": 30,
      "incorrect": 26,
      "superficial": 44,
      "instructions_not_followed": 23,
      "total_prefs": 203,
      "positive_prefs_ratio": 0.541871921182266
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 889.3869489756387,
      "p2.5": 876.5356150545811,
      "p97.5": 903.272948327003,
      "rank": 72,
      "rank_p2.5": 72,
      "rank_p97.5": 73,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3495570816781032,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 887.3347331987626,
      "p2.5": 876.992484391168,
      "p97.5": 898.4511442312889,
      "rank": 73,
      "rank_p2.5": 72,
      "rank_p97.5": 73,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3469736477550769,
      "win_rate": 0.41629681386249306,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 866.6267477562806,
      "p2.5": 851.6076591260457,
      "p97.5": 879.4997018121754,
      "rank": 74,
      "rank_p2.5": 74,
      "rank_p97.5": 75,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.32140865866806106,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 857.4943054728071,
      "p2.5": 848.5962348483006,
      "p97.5": 865.44106261131,
      "rank": 75,
      "rank_p2.5": 74,
      "rank_p97.5": 76,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.31044214017271765,
      "win_rate": 0.37482482802751566,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 846.969105487327,
      "p2.5": 837.5386447077789,
      "p97.5": 855.9311340161872,
      "rank": 76,
      "rank_p2.5": 75,
      "rank_p97.5": 77,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2980516470148595,
      "win_rate": 0.3671035747021082,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 824.2977582755154,
      "p2.5": 807.922088040558,
      "p97.5": 840.3438628328956,
      "rank": 77,
      "rank_p2.5": 77,
      "rank_p97.5": 78,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.27231363317841545,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 772.9123006491258,
      "p2.5": 669.646285814946,
      "p97.5": 857.566882789718,
      "rank": 78,
      "rank_p2.5": 75,
      "rank_p97.5": 80,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.21913331139756345,
      "win_rate": 0.326,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 755.5681546262115,
      "p2.5": 708.9112728197626,
      "p97.5": 799.5893291225177,
      "rank": 79,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2028693426919429,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 732.9362062217442,
      "p2.5": 626.3411058063755,
      "p97.5": 812.8623796689697,
      "rank": 80,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.18294417263205862,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}
