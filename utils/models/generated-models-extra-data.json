{
  "timestamp": 1764735979.0,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1137.3641961403066,
      "p2.5": 1124.1993120058364,
      "p97.5": 1150.8032905377179,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 1,
      "total_output_tokens": 3970287,
      "conso_all_conv": 0.0,
      "n_match": 2194,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6813046697967455,
      "win_rate": 0.6146259124087591,
      "useful": 125,
      "creative": 47,
      "complete": 193,
      "clear_formatting": 130,
      "incorrect": 41,
      "superficial": 17,
      "instructions_not_followed": 11,
      "total_prefs": 564,
      "positive_prefs_ratio": 0.8776595744680851
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1109.1019181223032,
      "p2.5": 1097.9969557548545,
      "p97.5": 1120.9272921997751,
      "rank": 2,
      "rank_p2.5": 2,
      "rank_p97.5": 6,
      "total_output_tokens": 3815962,
      "conso_all_conv": 0.0,
      "n_match": 2782,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6458328940744001,
      "win_rate": 0.5867889248471773,
      "useful": 224,
      "creative": 78,
      "complete": 289,
      "clear_formatting": 198,
      "incorrect": 60,
      "superficial": 41,
      "instructions_not_followed": 14,
      "total_prefs": 904,
      "positive_prefs_ratio": 0.8727876106194691
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1105.483748034473,
      "p2.5": 1097.8857686905847,
      "p97.5": 1112.2704917247495,
      "rank": 3,
      "rank_p2.5": 2,
      "rank_p97.5": 6,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6411734853217008,
      "win_rate": 0.6317699216950714,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1101.2441105678272,
      "p2.5": 1086.0576681850926,
      "p97.5": 1115.308648568666,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 11,
      "total_output_tokens": 1662335,
      "conso_all_conv": 0.0,
      "n_match": 1524,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6356825459786938,
      "win_rate": 0.5854629021667761,
      "useful": 87,
      "creative": 27,
      "complete": 116,
      "clear_formatting": 83,
      "incorrect": 44,
      "superficial": 16,
      "instructions_not_followed": 14,
      "total_prefs": 387,
      "positive_prefs_ratio": 0.8087855297157622
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1097.4167209432323,
      "p2.5": 1088.3868341972793,
      "p97.5": 1106.7786610154926,
      "rank": 5,
      "rank_p2.5": 3,
      "rank_p97.5": 10,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6306975348444221,
      "win_rate": 0.5851128848346636,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gemini-3-pro-preview",
      "median": 1095.4222359772648,
      "p2.5": 1066.9947865592435,
      "p97.5": 1121.7029786735086,
      "rank": 6,
      "rank_p2.5": 2,
      "rank_p97.5": 19,
      "total_output_tokens": 694593,
      "conso_all_conv": 0.0,
      "n_match": 386,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6280895869189296,
      "win_rate": 0.562109375,
      "useful": 32,
      "creative": 14,
      "complete": 30,
      "clear_formatting": 33,
      "incorrect": 5,
      "superficial": 1,
      "instructions_not_followed": 3,
      "total_prefs": 118,
      "positive_prefs_ratio": 0.923728813559322
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1092.3625245915964,
      "p2.5": 1072.4794444047525,
      "p97.5": 1110.3114967048677,
      "rank": 7,
      "rank_p2.5": 2,
      "rank_p97.5": 15,
      "total_output_tokens": 2280143,
      "conso_all_conv": 0.0,
      "n_match": 948,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.624075535837766,
      "win_rate": 0.5676246023329798,
      "useful": 110,
      "creative": 49,
      "complete": 180,
      "clear_formatting": 121,
      "incorrect": 52,
      "superficial": 26,
      "instructions_not_followed": 23,
      "total_prefs": 561,
      "positive_prefs_ratio": 0.8199643493761141
    },
    {
      "model_name": "magistral-medium",
      "median": 1092.3597934903023,
      "p2.5": 1080.1063679703848,
      "p97.5": 1104.6348016362526,
      "rank": 8,
      "rank_p2.5": 4,
      "rank_p97.5": 13,
      "total_output_tokens": 1928334,
      "conso_all_conv": 0.0,
      "n_match": 2070,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6240719458498238,
      "win_rate": 0.5812946859903382,
      "useful": 122,
      "creative": 35,
      "complete": 134,
      "clear_formatting": 86,
      "incorrect": 42,
      "superficial": 21,
      "instructions_not_followed": 3,
      "total_prefs": 443,
      "positive_prefs_ratio": 0.8510158013544018
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1090.5667761327502,
      "p2.5": 1083.3096067242452,
      "p97.5": 1097.6095631434569,
      "rank": 9,
      "rank_p2.5": 6,
      "rank_p97.5": 12,
      "total_output_tokens": 8927714,
      "conso_all_conv": 0.0,
      "n_match": 6573,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6217123860966446,
      "win_rate": 0.5789396013996653,
      "useful": 1239,
      "creative": 417,
      "complete": 1423,
      "clear_formatting": 1047,
      "incorrect": 320,
      "superficial": 215,
      "instructions_not_followed": 106,
      "total_prefs": 4767,
      "positive_prefs_ratio": 0.8655338787497378
    },
    {
      "model_name": "gpt-5.1",
      "median": 1086.1945136768554,
      "p2.5": 1063.831414667935,
      "p97.5": 1108.381291678861,
      "rank": 10,
      "rank_p2.5": 3,
      "rank_p97.5": 20,
      "total_output_tokens": 862866,
      "conso_all_conv": 0.0,
      "n_match": 709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6159367434086138,
      "win_rate": 0.573319209039548,
      "useful": 35,
      "creative": 11,
      "complete": 42,
      "clear_formatting": 41,
      "incorrect": 6,
      "superficial": 7,
      "instructions_not_followed": 2,
      "total_prefs": 144,
      "positive_prefs_ratio": 0.8958333333333334
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1085.934638184376,
      "p2.5": 1071.3800353991119,
      "p97.5": 1100.6058051093783,
      "rank": 11,
      "rank_p2.5": 5,
      "rank_p97.5": 16,
      "total_output_tokens": 1569361,
      "conso_all_conv": 0.0,
      "n_match": 1610,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6155924999344969,
      "win_rate": 0.5579539800995025,
      "useful": 176,
      "creative": 24,
      "complete": 108,
      "clear_formatting": 104,
      "incorrect": 33,
      "superficial": 19,
      "instructions_not_followed": 11,
      "total_prefs": 475,
      "positive_prefs_ratio": 0.8673684210526316
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1083.9091732784655,
      "p2.5": 1075.1269664386798,
      "p97.5": 1092.4408409421997,
      "rank": 12,
      "rank_p2.5": 7,
      "rank_p97.5": 15,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.612905888570779,
      "win_rate": 0.6141481069042316,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1079.6102889531878,
      "p2.5": 1063.5444394659958,
      "p97.5": 1098.089737318729,
      "rank": 13,
      "rank_p2.5": 6,
      "rank_p97.5": 21,
      "total_output_tokens": 1473031,
      "conso_all_conv": 0.0,
      "n_match": 996,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6071832405893217,
      "win_rate": 0.5106559031281533,
      "useful": 69,
      "creative": 29,
      "complete": 74,
      "clear_formatting": 54,
      "incorrect": 26,
      "superficial": 15,
      "instructions_not_followed": 10,
      "total_prefs": 277,
      "positive_prefs_ratio": 0.8158844765342961
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1071.4673478027785,
      "p2.5": 1060.8322176897886,
      "p97.5": 1081.9100500180557,
      "rank": 14,
      "rank_p2.5": 12,
      "rank_p97.5": 22,
      "total_output_tokens": 3261097,
      "conso_all_conv": 0.0,
      "n_match": 3109,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5962711824260629,
      "win_rate": 0.5347940797940798,
      "useful": 248,
      "creative": 50,
      "complete": 168,
      "clear_formatting": 151,
      "incorrect": 40,
      "superficial": 38,
      "instructions_not_followed": 23,
      "total_prefs": 718,
      "positive_prefs_ratio": 0.8593314763231198
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1070.9244617040904,
      "p2.5": 1062.3633050445462,
      "p97.5": 1078.2631755478108,
      "rank": 15,
      "rank_p2.5": 13,
      "rank_p97.5": 21,
      "total_output_tokens": 8151916,
      "conso_all_conv": 0.0,
      "n_match": 6209,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5955405011369268,
      "win_rate": 0.5650362377194396,
      "useful": 1033,
      "creative": 308,
      "complete": 1210,
      "clear_formatting": 882,
      "incorrect": 338,
      "superficial": 260,
      "instructions_not_followed": 140,
      "total_prefs": 4171,
      "positive_prefs_ratio": 0.823064013426037
    },
    {
      "model_name": "glm-4.5",
      "median": 1070.6590224224447,
      "p2.5": 1055.3851937632094,
      "p97.5": 1085.470395602764,
      "rank": 16,
      "rank_p2.5": 11,
      "rank_p97.5": 24,
      "total_output_tokens": 3289280,
      "conso_all_conv": 0.0,
      "n_match": 1391,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5951831022007389,
      "win_rate": 0.5512733812949641,
      "useful": 107,
      "creative": 35,
      "complete": 135,
      "clear_formatting": 98,
      "incorrect": 23,
      "superficial": 17,
      "instructions_not_followed": 12,
      "total_prefs": 427,
      "positive_prefs_ratio": 0.8782201405152225
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1067.9887686453085,
      "p2.5": 1053.765967214966,
      "p97.5": 1083.1828320195632,
      "rank": 17,
      "rank_p2.5": 12,
      "rank_p97.5": 25,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5915827659124199,
      "win_rate": 0.5305338541666667,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1067.219227540718,
      "p2.5": 1054.3444100331046,
      "p97.5": 1079.0934920639197,
      "rank": 18,
      "rank_p2.5": 13,
      "rank_p97.5": 25,
      "total_output_tokens": 1853912,
      "conso_all_conv": 0.0,
      "n_match": 2540,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5905435287317268,
      "win_rate": 0.5356400157542339,
      "useful": 234,
      "creative": 62,
      "complete": 198,
      "clear_formatting": 215,
      "incorrect": 41,
      "superficial": 64,
      "instructions_not_followed": 20,
      "total_prefs": 834,
      "positive_prefs_ratio": 0.8501199040767387
    },
    {
      "model_name": "grok-4.1-fast",
      "median": 1066.5253902692675,
      "p2.5": 1035.5561944067583,
      "p97.5": 1097.1911514044018,
      "rank": 19,
      "rank_p2.5": 6,
      "rank_p97.5": 33,
      "total_output_tokens": 318897,
      "conso_all_conv": 0.0,
      "n_match": 329,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5896059047443454,
      "win_rate": 0.5537386018237083,
      "useful": 17,
      "creative": 2,
      "complete": 15,
      "clear_formatting": 11,
      "incorrect": 10,
      "superficial": 2,
      "instructions_not_followed": 0,
      "total_prefs": 57,
      "positive_prefs_ratio": 0.7894736842105263
    },
    {
      "model_name": "kimi-k2-thinking",
      "median": 1064.1429364155179,
      "p2.5": 1004.7225810731265,
      "p97.5": 1126.3613130368026,
      "rank": 20,
      "rank_p2.5": 2,
      "rank_p97.5": 47,
      "total_output_tokens": 115228,
      "conso_all_conv": 0.0,
      "n_match": 55,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5863819484169025,
      "win_rate": 0.39622641509433965,
      "useful": 6,
      "creative": 1,
      "complete": 6,
      "clear_formatting": 4,
      "incorrect": 1,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 18,
      "positive_prefs_ratio": 0.9444444444444444
    },
    {
      "model_name": "command-a",
      "median": 1062.7284465913856,
      "p2.5": 1054.7976433771603,
      "p97.5": 1070.8824021711305,
      "rank": 21,
      "rank_p2.5": 16,
      "rank_p97.5": 25,
      "total_output_tokens": 6050178,
      "conso_all_conv": 0.0,
      "n_match": 6141,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5844646971101254,
      "win_rate": 0.559070184009119,
      "useful": 1150,
      "creative": 268,
      "complete": 1056,
      "clear_formatting": 990,
      "incorrect": 269,
      "superficial": 288,
      "instructions_not_followed": 101,
      "total_prefs": 4122,
      "positive_prefs_ratio": 0.8403687530325085
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1061.5554596624913,
      "p2.5": 1041.2289082249713,
      "p97.5": 1081.729349036386,
      "rank": 22,
      "rank_p2.5": 12,
      "rank_p97.5": 30,
      "total_output_tokens": 1256581,
      "conso_all_conv": 0.0,
      "n_match": 716,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5828730534522852,
      "win_rate": 0.5422019635343618,
      "useful": 31,
      "creative": 14,
      "complete": 36,
      "clear_formatting": 20,
      "incorrect": 11,
      "superficial": 8,
      "instructions_not_followed": 3,
      "total_prefs": 123,
      "positive_prefs_ratio": 0.8211382113821138
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1060.6572295933433,
      "p2.5": 1049.8052851909158,
      "p97.5": 1071.6584337204154,
      "rank": 23,
      "rank_p2.5": 16,
      "rank_p97.5": 27,
      "total_output_tokens": 2132079,
      "conso_all_conv": 0.0,
      "n_match": 2627,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5816531887792202,
      "win_rate": 0.5327826417967263,
      "useful": 140,
      "creative": 38,
      "complete": 141,
      "clear_formatting": 109,
      "incorrect": 80,
      "superficial": 38,
      "instructions_not_followed": 9,
      "total_prefs": 555,
      "positive_prefs_ratio": 0.7711711711711712
    },
    {
      "model_name": "grok-4-fast",
      "median": 1060.5821475692705,
      "p2.5": 1048.8428445819704,
      "p97.5": 1072.240853017702,
      "rank": 24,
      "rank_p2.5": 16,
      "rank_p97.5": 27,
      "total_output_tokens": 2855944,
      "conso_all_conv": 0.0,
      "n_match": 2107,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5815511812193754,
      "win_rate": 0.5289026128266033,
      "useful": 145,
      "creative": 28,
      "complete": 107,
      "clear_formatting": 74,
      "incorrect": 41,
      "superficial": 26,
      "instructions_not_followed": 12,
      "total_prefs": 433,
      "positive_prefs_ratio": 0.8175519630484989
    },
    {
      "model_name": "kimi-k2",
      "median": 1057.6919370277928,
      "p2.5": 1032.0755390418906,
      "p97.5": 1079.6830457546837,
      "rank": 25,
      "rank_p2.5": 13,
      "rank_p97.5": 34,
      "total_output_tokens": 941864,
      "conso_all_conv": 0.0,
      "n_match": 558,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5776198564558469,
      "win_rate": 0.4957685352622061,
      "useful": 117,
      "creative": 23,
      "complete": 57,
      "clear_formatting": 43,
      "incorrect": 22,
      "superficial": 26,
      "instructions_not_followed": 3,
      "total_prefs": 291,
      "positive_prefs_ratio": 0.8247422680412371
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1054.4428336345936,
      "p2.5": 1044.9044456556185,
      "p97.5": 1064.1398153397226,
      "rank": 26,
      "rank_p2.5": 20,
      "rank_p97.5": 29,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5731899474750835,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1051.5123316211802,
      "p2.5": 1041.433403955698,
      "p97.5": 1062.0053441248908,
      "rank": 27,
      "rank_p2.5": 21,
      "rank_p97.5": 30,
      "total_output_tokens": 2296189,
      "conso_all_conv": 0.0,
      "n_match": 2959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5691854994248501,
      "win_rate": 0.518448275862069,
      "useful": 218,
      "creative": 49,
      "complete": 178,
      "clear_formatting": 154,
      "incorrect": 79,
      "superficial": 71,
      "instructions_not_followed": 29,
      "total_prefs": 778,
      "positive_prefs_ratio": 0.7699228791773779
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1049.555058792414,
      "p2.5": 1041.8028863414202,
      "p97.5": 1057.2743568664482,
      "rank": 28,
      "rank_p2.5": 23,
      "rank_p97.5": 30,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5665064747707291,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1044.304239387317,
      "p2.5": 1037.4874552690776,
      "p97.5": 1051.575450346651,
      "rank": 29,
      "rank_p2.5": 26,
      "rank_p97.5": 33,
      "total_output_tokens": 9057533,
      "conso_all_conv": 0.0,
      "n_match": 7239,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5593029654931768,
      "win_rate": 0.5242934106920846,
      "useful": 1105,
      "creative": 339,
      "complete": 1223,
      "clear_formatting": 978,
      "incorrect": 554,
      "superficial": 246,
      "instructions_not_followed": 191,
      "total_prefs": 4636,
      "positive_prefs_ratio": 0.7862381363244176
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1043.0666969236381,
      "p2.5": 1012.5877577807966,
      "p97.5": 1073.4377502121251,
      "rank": 30,
      "rank_p2.5": 16,
      "rank_p97.5": 43,
      "total_output_tokens": 463129,
      "conso_all_conv": 0.0,
      "n_match": 340,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.55760193726197,
      "win_rate": 0.5588724035608309,
      "useful": 22,
      "creative": 9,
      "complete": 18,
      "clear_formatting": 18,
      "incorrect": 14,
      "superficial": 9,
      "instructions_not_followed": 4,
      "total_prefs": 94,
      "positive_prefs_ratio": 0.7127659574468085
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1036.33771116607,
      "p2.5": 1028.7448173056987,
      "p97.5": 1043.6928470124044,
      "rank": 31,
      "rank_p2.5": 29,
      "rank_p97.5": 36,
      "total_output_tokens": 5550374,
      "conso_all_conv": 0.0,
      "n_match": 7173,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5483333322136569,
      "win_rate": 0.515105285176405,
      "useful": 1227,
      "creative": 286,
      "complete": 945,
      "clear_formatting": 1010,
      "incorrect": 244,
      "superficial": 455,
      "instructions_not_followed": 97,
      "total_prefs": 4264,
      "positive_prefs_ratio": 0.8133208255159474
    },
    {
      "model_name": "qwen3-32b",
      "median": 1035.9309455713706,
      "p2.5": 1018.1814615347074,
      "p97.5": 1051.4575089422206,
      "rank": 32,
      "rank_p2.5": 26,
      "rank_p97.5": 40,
      "total_output_tokens": 2226439,
      "conso_all_conv": 0.0,
      "n_match": 1057,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5477720731453233,
      "win_rate": 0.538727445394112,
      "useful": 272,
      "creative": 68,
      "complete": 207,
      "clear_formatting": 176,
      "incorrect": 93,
      "superficial": 72,
      "instructions_not_followed": 39,
      "total_prefs": 927,
      "positive_prefs_ratio": 0.7799352750809061
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1035.6908617139743,
      "p2.5": 1027.8318995614266,
      "p97.5": 1044.038581780172,
      "rank": 33,
      "rank_p2.5": 29,
      "rank_p97.5": 37,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5474407548150687,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1033.536763796591,
      "p2.5": 1022.5516516909806,
      "p97.5": 1044.5309012510959,
      "rank": 34,
      "rank_p2.5": 29,
      "rank_p97.5": 39,
      "total_output_tokens": 2833695,
      "conso_all_conv": 0.0,
      "n_match": 2564,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5444665196331753,
      "win_rate": 0.4908970358814353,
      "useful": 148,
      "creative": 52,
      "complete": 197,
      "clear_formatting": 142,
      "incorrect": 80,
      "superficial": 53,
      "instructions_not_followed": 34,
      "total_prefs": 706,
      "positive_prefs_ratio": 0.7634560906515581
    },
    {
      "model_name": "glm-4.6",
      "median": 1032.4474033714112,
      "p2.5": 1014.7458079336803,
      "p97.5": 1049.2136168128518,
      "rank": 35,
      "rank_p2.5": 27,
      "rank_p97.5": 41,
      "total_output_tokens": 3668496,
      "conso_all_conv": 0.0,
      "n_match": 1221,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5429613777035968,
      "win_rate": 0.49517269736842107,
      "useful": 69,
      "creative": 25,
      "complete": 90,
      "clear_formatting": 44,
      "incorrect": 16,
      "superficial": 14,
      "instructions_not_followed": 9,
      "total_prefs": 267,
      "positive_prefs_ratio": 0.8539325842696629
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1032.245500899342,
      "p2.5": 1016.8023536268263,
      "p97.5": 1047.1121269576454,
      "rank": 36,
      "rank_p2.5": 28,
      "rank_p97.5": 41,
      "total_output_tokens": 1867186,
      "conso_all_conv": 0.0,
      "n_match": 1271,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5426823414594262,
      "win_rate": 0.49532754538279405,
      "useful": 82,
      "creative": 34,
      "complete": 111,
      "clear_formatting": 104,
      "incorrect": 32,
      "superficial": 34,
      "instructions_not_followed": 22,
      "total_prefs": 419,
      "positive_prefs_ratio": 0.7899761336515513
    },
    {
      "model_name": "deepseek-r1",
      "median": 1031.046191512198,
      "p2.5": 1021.1719558783017,
      "p97.5": 1040.9169425530556,
      "rank": 37,
      "rank_p2.5": 30,
      "rank_p97.5": 39,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5410243996940861,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1023.4964201628804,
      "p2.5": 1017.1668638703264,
      "p97.5": 1030.5517104886483,
      "rank": 38,
      "rank_p2.5": 35,
      "rank_p97.5": 41,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5305716102063102,
      "win_rate": 0.527441386340469,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "mistral-saba",
      "median": 1019.1756939821655,
      "p2.5": 1011.4408830836769,
      "p97.5": 1027.6033834744348,
      "rank": 39,
      "rank_p2.5": 36,
      "rank_p97.5": 44,
      "total_output_tokens": 4349650,
      "conso_all_conv": 0.0,
      "n_match": 4934,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5245794111331372,
      "win_rate": 0.4829677680924387,
      "useful": 745,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 586,
      "incorrect": 241,
      "superficial": 337,
      "instructions_not_followed": 113,
      "total_prefs": 2779,
      "positive_prefs_ratio": 0.751349406261245
    },
    {
      "model_name": "llama-4-scout",
      "median": 1016.4203568111818,
      "p2.5": 1007.8583963703495,
      "p97.5": 1024.752225917562,
      "rank": 40,
      "rank_p2.5": 37,
      "rank_p97.5": 45,
      "total_output_tokens": 3836169,
      "conso_all_conv": 0.0,
      "n_match": 4730,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5207553022277508,
      "win_rate": 0.49488997037663984,
      "useful": 681,
      "creative": 174,
      "complete": 616,
      "clear_formatting": 559,
      "incorrect": 242,
      "superficial": 314,
      "instructions_not_followed": 80,
      "total_prefs": 2666,
      "positive_prefs_ratio": 0.7614403600900225
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1013.1550300889467,
      "p2.5": 998.443017010195,
      "p97.5": 1027.663585026919,
      "rank": 41,
      "rank_p2.5": 37,
      "rank_p97.5": 48,
      "total_output_tokens": 2276055,
      "conso_all_conv": 0.0,
      "n_match": 1929,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5162211772558087,
      "win_rate": 0.4716960580912863,
      "useful": 90,
      "creative": 29,
      "complete": 114,
      "clear_formatting": 49,
      "incorrect": 27,
      "superficial": 33,
      "instructions_not_followed": 26,
      "total_prefs": 368,
      "positive_prefs_ratio": 0.7663043478260869
    },
    {
      "model_name": "llama-maverick",
      "median": 1012.434402497975,
      "p2.5": 999.2638207404477,
      "p97.5": 1025.1880604868652,
      "rank": 42,
      "rank_p2.5": 38,
      "rank_p97.5": 48,
      "total_output_tokens": 1574127,
      "conso_all_conv": 0.0,
      "n_match": 1984,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5152202808112765,
      "win_rate": 0.4857661290322581,
      "useful": 115,
      "creative": 23,
      "complete": 87,
      "clear_formatting": 76,
      "incorrect": 37,
      "superficial": 62,
      "instructions_not_followed": 16,
      "total_prefs": 416,
      "positive_prefs_ratio": 0.7235576923076923
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1010.7044754854692,
      "p2.5": 1002.4952933955132,
      "p97.5": 1019.4645204362189,
      "rank": 43,
      "rank_p2.5": 40,
      "rank_p97.5": 48,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5128172480576227,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "lfm2-8b-a1b",
      "median": 1010.457430102178,
      "p2.5": 979.5270743768108,
      "p97.5": 1041.739679983256,
      "rank": 44,
      "rank_p2.5": 30,
      "rank_p97.5": 58,
      "total_output_tokens": 246326,
      "conso_all_conv": 0.0,
      "n_match": 344,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5124740488470226,
      "win_rate": 0.4455263157894737,
      "useful": 14,
      "creative": 2,
      "complete": 11,
      "clear_formatting": 10,
      "incorrect": 21,
      "superficial": 4,
      "instructions_not_followed": 2,
      "total_prefs": 64,
      "positive_prefs_ratio": 0.578125
    },
    {
      "model_name": "o4-mini",
      "median": 1008.5759899381951,
      "p2.5": 996.6160842574626,
      "p97.5": 1019.7519102852183,
      "rank": 45,
      "rank_p2.5": 40,
      "rank_p97.5": 49,
      "total_output_tokens": 2895053,
      "conso_all_conv": 0.0,
      "n_match": 2787,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5098601320633922,
      "win_rate": 0.47116337522441654,
      "useful": 526,
      "creative": 111,
      "complete": 371,
      "clear_formatting": 285,
      "incorrect": 132,
      "superficial": 210,
      "instructions_not_followed": 45,
      "total_prefs": 1680,
      "positive_prefs_ratio": 0.7696428571428572
    },
    {
      "model_name": "gpt-5",
      "median": 1005.585706315178,
      "p2.5": 996.0316342580711,
      "p97.5": 1015.358822815014,
      "rank": 46,
      "rank_p2.5": 42,
      "rank_p97.5": 50,
      "total_output_tokens": 3687528,
      "conso_all_conv": 0.0,
      "n_match": 3524,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5057052364915993,
      "win_rate": 0.4586119784274766,
      "useful": 131,
      "creative": 48,
      "complete": 146,
      "clear_formatting": 63,
      "incorrect": 27,
      "superficial": 52,
      "instructions_not_followed": 31,
      "total_prefs": 498,
      "positive_prefs_ratio": 0.7791164658634538
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1005.4722492509813,
      "p2.5": 981.6863123209273,
      "p97.5": 1028.7159649876544,
      "rank": 47,
      "rank_p2.5": 36,
      "rank_p97.5": 57,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5055475870504987,
      "win_rate": 0.5826115485564304,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 995.7675963292204,
      "p2.5": 987.4413900772741,
      "p97.5": 1003.7123921342334,
      "rank": 48,
      "rank_p2.5": 46,
      "rank_p97.5": 54,
      "total_output_tokens": 4082212,
      "conso_all_conv": 0.0,
      "n_match": 5113,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.49206573716924984,
      "win_rate": 0.46251711324075884,
      "useful": 722,
      "creative": 142,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 277,
      "superficial": 353,
      "instructions_not_followed": 109,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7203934922436626
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 993.2620516505747,
      "p2.5": 979.1886891846457,
      "p97.5": 1007.3761080124289,
      "rank": 49,
      "rank_p2.5": 45,
      "rank_p97.5": 58,
      "total_output_tokens": 3089159,
      "conso_all_conv": 0.0,
      "n_match": 1834,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4885871820272468,
      "win_rate": 0.45579378068739773,
      "useful": 91,
      "creative": 28,
      "complete": 108,
      "clear_formatting": 56,
      "incorrect": 57,
      "superficial": 39,
      "instructions_not_followed": 19,
      "total_prefs": 398,
      "positive_prefs_ratio": 0.7110552763819096
    },
    {
      "model_name": "qwen-3-8b",
      "median": 992.3987497378616,
      "p2.5": 975.6962409500969,
      "p97.5": 1007.7976687843934,
      "rank": 50,
      "rank_p2.5": 45,
      "rank_p97.5": 60,
      "total_output_tokens": 2904416,
      "conso_all_conv": 0.0,
      "n_match": 1387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48738895890111605,
      "win_rate": 0.44903388608507566,
      "useful": 49,
      "creative": 15,
      "complete": 72,
      "clear_formatting": 45,
      "incorrect": 54,
      "superficial": 19,
      "instructions_not_followed": 16,
      "total_prefs": 270,
      "positive_prefs_ratio": 0.6703703703703704
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 990.3308048547036,
      "p2.5": 979.3389644710029,
      "p97.5": 1000.0195134065841,
      "rank": 51,
      "rank_p2.5": 47,
      "rank_p97.5": 58,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48451955802755736,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 988.8155755996759,
      "p2.5": 981.7278270893077,
      "p97.5": 995.8427076263418,
      "rank": 52,
      "rank_p2.5": 49,
      "rank_p97.5": 57,
      "total_output_tokens": 6802625,
      "conso_all_conv": 0.0,
      "n_match": 8189,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48241789152070963,
      "win_rate": 0.47505495847581825,
      "useful": 1389,
      "creative": 278,
      "complete": 1092,
      "clear_formatting": 1028,
      "incorrect": 439,
      "superficial": 599,
      "instructions_not_followed": 141,
      "total_prefs": 4966,
      "positive_prefs_ratio": 0.7625855819573097
    },
    {
      "model_name": "o3-mini",
      "median": 986.6898443505481,
      "p2.5": 971.7710918548428,
      "p97.5": 1001.1929297137258,
      "rank": 53,
      "rank_p2.5": 47,
      "rank_p97.5": 62,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47947074018949987,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 986.0381064722355,
      "p2.5": 978.3091726880994,
      "p97.5": 993.8480151202681,
      "rank": 54,
      "rank_p2.5": 50,
      "rank_p97.5": 59,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4785674890786876,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 982.6394216816295,
      "p2.5": 973.8635064766798,
      "p97.5": 990.26572529754,
      "rank": 55,
      "rank_p2.5": 52,
      "rank_p97.5": 61,
      "total_output_tokens": 3563423,
      "conso_all_conv": 0.0,
      "n_match": 5717,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47386000382163573,
      "win_rate": 0.4674846947699843,
      "useful": 928,
      "creative": 159,
      "complete": 529,
      "clear_formatting": 649,
      "incorrect": 276,
      "superficial": 448,
      "instructions_not_followed": 103,
      "total_prefs": 3092,
      "positive_prefs_ratio": 0.732535575679172
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 978.9127615050463,
      "p2.5": 961.3689609891715,
      "p97.5": 995.6908575110251,
      "rank": 56,
      "rank_p2.5": 49,
      "rank_p97.5": 66,
      "total_output_tokens": 1150238,
      "conso_all_conv": 0.0,
      "n_match": 1172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46870431405104185,
      "win_rate": 0.42377245508982037,
      "useful": 61,
      "creative": 10,
      "complete": 51,
      "clear_formatting": 34,
      "incorrect": 41,
      "superficial": 43,
      "instructions_not_followed": 21,
      "total_prefs": 261,
      "positive_prefs_ratio": 0.5977011494252874
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 977.4927640183598,
      "p2.5": 961.9325893182245,
      "p97.5": 993.1468911771912,
      "rank": 57,
      "rank_p2.5": 51,
      "rank_p97.5": 67,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4667416803242094,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "minimax-m2",
      "median": 977.3712961450801,
      "p2.5": 949.5040945113119,
      "p97.5": 1002.7390950177585,
      "rank": 58,
      "rank_p2.5": 47,
      "rank_p97.5": 70,
      "total_output_tokens": 823184,
      "conso_all_conv": 0.0,
      "n_match": 425,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4665738462166318,
      "win_rate": 0.4048349056603774,
      "useful": 24,
      "creative": 4,
      "complete": 22,
      "clear_formatting": 11,
      "incorrect": 15,
      "superficial": 12,
      "instructions_not_followed": 6,
      "total_prefs": 94,
      "positive_prefs_ratio": 0.648936170212766
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 976.657375944204,
      "p2.5": 968.0521716810197,
      "p97.5": 985.71727624641,
      "rank": 59,
      "rank_p2.5": 55,
      "rank_p97.5": 64,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4655875792585868,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 976.5293021864244,
      "p2.5": 929.524690428531,
      "p97.5": 1021.1326696776378,
      "rank": 60,
      "rank_p2.5": 39,
      "rank_p97.5": 76,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46541067856094914,
      "win_rate": 0.5449418604651163,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 975.6576849700989,
      "p2.5": 967.5654787960702,
      "p97.5": 984.4181674451704,
      "rank": 61,
      "rank_p2.5": 55,
      "rank_p97.5": 64,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46420701846755685,
      "win_rate": 0.4921080415273623,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "phi-4",
      "median": 970.7965993301607,
      "p2.5": 964.2492451483702,
      "p97.5": 976.8408015321598,
      "rank": 62,
      "rank_p2.5": 59,
      "rank_p97.5": 66,
      "total_output_tokens": 8039322,
      "conso_all_conv": 0.0,
      "n_match": 9734,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45750273161862337,
      "win_rate": 0.46910099660947296,
      "useful": 1617,
      "creative": 336,
      "complete": 1185,
      "clear_formatting": 1303,
      "incorrect": 722,
      "superficial": 754,
      "instructions_not_followed": 251,
      "total_prefs": 6168,
      "positive_prefs_ratio": 0.7200064850843061
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 967.5741866592439,
      "p2.5": 960.6485281361796,
      "p97.5": 974.6418579468177,
      "rank": 63,
      "rank_p2.5": 60,
      "rank_p97.5": 67,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45306728913912186,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 966.1999031024015,
      "p2.5": 957.304122011961,
      "p97.5": 974.5853731360108,
      "rank": 64,
      "rank_p2.5": 60,
      "rank_p97.5": 68,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45117801129294205,
      "win_rate": 0.48863297150610585,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 964.4152088986928,
      "p2.5": 906.7072198086769,
      "p97.5": 1022.1338207088273,
      "rank": 65,
      "rank_p2.5": 39,
      "rank_p97.5": 78,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44872672621987236,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 962.8460121464216,
      "p2.5": 905.1028132285609,
      "p97.5": 1017.9386026334995,
      "rank": 66,
      "rank_p2.5": 41,
      "rank_p97.5": 78,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4465735605807999,
      "win_rate": 0.46869047619047616,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 961.4072210735795,
      "p2.5": 954.6849802446994,
      "p97.5": 967.7045646111405,
      "rank": 67,
      "rank_p2.5": 63,
      "rank_p97.5": 70,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4446011504890056,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 957.0515995495307,
      "p2.5": 948.5197592768988,
      "p97.5": 965.8022510875334,
      "rank": 68,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43864131852399113,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "gpt-5-nano",
      "median": 953.5278838142817,
      "p2.5": 938.1951590161398,
      "p97.5": 968.5557035446823,
      "rank": 69,
      "rank_p2.5": 63,
      "rank_p97.5": 74,
      "total_output_tokens": 1745488,
      "conso_all_conv": 0.0,
      "n_match": 1600,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4338329284955073,
      "win_rate": 0.3968625,
      "useful": 59,
      "creative": 19,
      "complete": 61,
      "clear_formatting": 37,
      "incorrect": 21,
      "superficial": 28,
      "instructions_not_followed": 26,
      "total_prefs": 251,
      "positive_prefs_ratio": 0.701195219123506
    },
    {
      "model_name": "qwq-32b",
      "median": 953.399211700462,
      "p2.5": 937.8242945454755,
      "p97.5": 969.0909256438898,
      "rank": 70,
      "rank_p2.5": 63,
      "rank_p97.5": 74,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4336575793265315,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "olmo-3-32b-think",
      "median": 952.9102665165929,
      "p2.5": 852.1640968422278,
      "p97.5": 1051.211342563464,
      "rank": 71,
      "rank_p2.5": 26,
      "rank_p97.5": 82,
      "total_output_tokens": 70813,
      "conso_all_conv": 0.0,
      "n_match": 34,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4329914188831151,
      "win_rate": 0.39249999999999996,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 1,
      "superficial": 2,
      "instructions_not_followed": 2,
      "total_prefs": 5,
      "positive_prefs_ratio": 0.0
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 950.7457094608324,
      "p2.5": 939.897312146471,
      "p97.5": 961.9375164294361,
      "rank": 72,
      "rank_p2.5": 66,
      "rank_p97.5": 74,
      "total_output_tokens": 2863519,
      "conso_all_conv": 0.0,
      "n_match": 2981,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4300453159509802,
      "win_rate": 0.4396947333109695,
      "useful": 489,
      "creative": 130,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 190,
      "superficial": 342,
      "instructions_not_followed": 126,
      "total_prefs": 1995,
      "positive_prefs_ratio": 0.6701754385964912
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 946.1205239943788,
      "p2.5": 938.5217085191724,
      "p97.5": 953.7621910843568,
      "rank": 73,
      "rank_p2.5": 68,
      "rank_p97.5": 74,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.42376715939221454,
      "win_rate": 0.46634743875278395,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "hermes-4-70b",
      "median": 935.833219459052,
      "p2.5": 920.742039869997,
      "p97.5": 950.4232986498146,
      "rank": 74,
      "rank_p2.5": 70,
      "rank_p97.5": 78,
      "total_output_tokens": 732947,
      "conso_all_conv": 0.0,
      "n_match": 1624,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4098943963572038,
      "win_rate": 0.3794944512946979,
      "useful": 62,
      "creative": 14,
      "complete": 33,
      "clear_formatting": 40,
      "incorrect": 52,
      "superficial": 65,
      "instructions_not_followed": 28,
      "total_prefs": 294,
      "positive_prefs_ratio": 0.5068027210884354
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 935.6854561318962,
      "p2.5": 928.969313932638,
      "p97.5": 941.9828385994605,
      "rank": 75,
      "rank_p2.5": 72,
      "rank_p97.5": 77,
      "total_output_tokens": 7185946,
      "conso_all_conv": 0.0,
      "n_match": 10173,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40969611708969667,
      "win_rate": 0.44238179494740987,
      "useful": 1530,
      "creative": 372,
      "complete": 1008,
      "clear_formatting": 1151,
      "incorrect": 1038,
      "superficial": 968,
      "instructions_not_followed": 401,
      "total_prefs": 6468,
      "positive_prefs_ratio": 0.6278602350030922
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 933.9944656132332,
      "p2.5": 925.1850089168751,
      "p97.5": 941.9361099891343,
      "rank": 76,
      "rank_p2.5": 72,
      "rank_p97.5": 78,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40742912178667534,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 930.6022682424011,
      "p2.5": 913.169500979036,
      "p97.5": 949.6763494932363,
      "rank": 77,
      "rank_p2.5": 71,
      "rank_p97.5": 78,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40289331335932216,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 930.4090213313852,
      "p2.5": 922.8694511076362,
      "p97.5": 937.5258950081765,
      "rank": 78,
      "rank_p2.5": 73,
      "rank_p97.5": 78,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4026354048990172,
      "win_rate": 0.43789423208658723,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 888.55058400758,
      "p2.5": 874.9360656046721,
      "p97.5": 901.7854105730477,
      "rank": 79,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3482254563661572,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 885.4847034441714,
      "p2.5": 874.4076662688612,
      "p97.5": 897.0571005796796,
      "rank": 80,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.34436967868177826,
      "win_rate": 0.416296813862493,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 864.807522278224,
      "p2.5": 850.0097506887909,
      "p97.5": 878.4205003172275,
      "rank": 81,
      "rank_p2.5": 80,
      "rank_p97.5": 82,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.31889889728261933,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 855.2753285676868,
      "p2.5": 846.941712721452,
      "p97.5": 864.0470732574172,
      "rank": 82,
      "rank_p2.5": 81,
      "rank_p97.5": 83,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3074883451796356,
      "win_rate": 0.37482482802751554,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 845.0035420803563,
      "p2.5": 835.7183554514102,
      "p97.5": 854.8198492189488,
      "rank": 83,
      "rank_p2.5": 82,
      "rank_p97.5": 84,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.29544090221463737,
      "win_rate": 0.36710357470210814,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 823.2443767353159,
      "p2.5": 806.5411230906215,
      "p97.5": 839.246904760383,
      "rank": 84,
      "rank_p2.5": 84,
      "rank_p97.5": 85,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.27081359467712807,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 765.7877460899763,
      "p2.5": 666.263777701147,
      "p97.5": 854.0305496203804,
      "rank": 85,
      "rank_p2.5": 83,
      "rank_p97.5": 87,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.21203113874330212,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 751.5053744439356,
      "p2.5": 706.5832944590343,
      "p97.5": 795.424578935628,
      "rank": 86,
      "rank_p2.5": 85,
      "rank_p97.5": 87,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.19888140305555044,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 730.8038452480748,
      "p2.5": 630.3691594989367,
      "p97.5": 810.319999411834,
      "rank": 87,
      "rank_p2.5": 85,
      "rank_p97.5": 87,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.18086483773701548,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}
