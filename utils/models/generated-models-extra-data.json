[
    {
        "model_name": "mistral-medium-3.1",
        "median": 1148.3430805038013,
        "p2.5": 1126.8534290270225,
        "p97.5": 1172.531870584977,
        "rank": 1,
        "rank_p2.5": 1,
        "rank_p97.5": 3,
        "total_output_tokens": 1267622,
        "conso_all_conv": 20.2529868373,
        "n_match": 675,
        "mean_conso_per_match": 0.030004424944148147,
        "mean_conso_per_token": 0.00001597715,
        "mean_win_prob": 0.6939629623472354,
        "win_rate": null,
        "useful": 28,
        "creative": 12,
        "complete": 44,
        "clear_formatting": 36,
        "incorrect": 9,
        "superficial": 3,
        "instructions_not_followed": 2,
        "total_prefs": 134,
        "positive_prefs_ratio": 0.8955223880597015
    },
    {
        "model_name": "gemini-2.5-flash",
        "median": 1128.435571266882,
        "p2.5": 1109.9872733345064,
        "p97.5": 1145.6979398257768,
        "rank": 2,
        "rank_p2.5": 1,
        "rank_p97.5": 4,
        "total_output_tokens": 1574894,
        "conso_all_conv": 137.77985357304007,
        "n_match": 1042,
        "mean_conso_per_match": 0.13222634699907876,
        "mean_conso_per_token": 0.00008748516000000005,
        "mean_win_prob": 0.6696964574098164,
        "win_rate": 0.5973416506717851,
        "useful": 109,
        "creative": 45,
        "complete": 142,
        "clear_formatting": 97,
        "incorrect": 27,
        "superficial": 18,
        "instructions_not_followed": 9,
        "total_prefs": 447,
        "positive_prefs_ratio": 0.8791946308724832
    },
    {
        "model_name": "qwen3-max-2025-09-23",
        "median": 1110.6373010950742,
        "p2.5": 1071.9922420312848,
        "p97.5": 1149.6880175226945,
        "rank": 3,
        "rank_p2.5": 1,
        "rank_p97.5": 17,
        "total_output_tokens": 221734,
        "conso_all_conv": 12.658244159679995,
        "n_match": 199,
        "mean_conso_per_match": 0.06360926713407032,
        "mean_conso_per_token": 0.00005708751999999998,
        "mean_win_prob": 0.6472710700387323,
        "win_rate": null,
        "useful": 8,
        "creative": 0,
        "complete": 10,
        "clear_formatting": 7,
        "incorrect": 6,
        "superficial": 2,
        "instructions_not_followed": 1,
        "total_prefs": 34,
        "positive_prefs_ratio": 0.7352941176470589
    },
    {
        "model_name": "gemini-2.0-flash",
        "median": 1106.2622661149305,
        "p2.5": 1099.1980290088582,
        "p97.5": 1113.6039905601906,
        "rank": 4,
        "rank_p2.5": 3,
        "rank_p97.5": 8,
        "total_output_tokens": 12296785,
        "conso_all_conv": 0.0,
        "n_match": 8684,
        "mean_conso_per_match": 0.0,
        "mean_conso_per_token": 0.0,
        "mean_win_prob": 0.641662322501461,
        "win_rate": 0.6317699216950715,
        "useful": 2128,
        "creative": 677,
        "complete": 2179,
        "clear_formatting": 1909,
        "incorrect": 376,
        "superficial": 397,
        "instructions_not_followed": 112,
        "total_prefs": 7778,
        "positive_prefs_ratio": 0.8862175366418102
    },
    {
        "model_name": "deepseek-chat-v3.1",
        "median": 1099.5280766708136,
        "p2.5": 1070.3196603218776,
        "p97.5": 1133.9191013255677,
        "rank": 5,
        "rank_p2.5": 2,
        "rank_p97.5": 18,
        "total_output_tokens": 509605,
        "conso_all_conv": 23.964552232700008,
        "n_match": 314,
        "mean_conso_per_match": 0.07632023004044589,
        "mean_conso_per_token": 0.000047025740000000016,
        "mean_win_prob": 0.6329602020497992,
        "win_rate": null,
        "useful": 29,
        "creative": 12,
        "complete": 26,
        "clear_formatting": 20,
        "incorrect": 7,
        "superficial": 3,
        "instructions_not_followed": 3,
        "total_prefs": 100,
        "positive_prefs_ratio": 0.87
    },
    {
        "model_name": "deepseek-v3-0324",
        "median": 1099.33554606697,
        "p2.5": 1090.7474562719835,
        "p97.5": 1108.203660337849,
        "rank": 6,
        "rank_p2.5": 4,
        "rank_p97.5": 11,
        "total_output_tokens": 4435392,
        "conso_all_conv": 208.57759099007896,
        "n_match": 4385,
        "mean_conso_per_match": 0.04756615529990398,
        "mean_conso_per_token": 0.000047025739999999765,
        "mean_win_prob": 0.6327102176280684,
        "win_rate": 0.5851128848346637,
        "useful": 1141,
        "creative": 336,
        "complete": 1012,
        "clear_formatting": 1043,
        "incorrect": 230,
        "superficial": 288,
        "instructions_not_followed": 79,
        "total_prefs": 4129,
        "positive_prefs_ratio": 0.8554129329135384
    },
    {
        "model_name": "gemma-3-27b",
        "median": 1098.2999923343737,
        "p2.5": 1089.404714768361,
        "p97.5": 1107.3169431723206,
        "rank": 7,
        "rank_p2.5": 4,
        "rank_p97.5": 11,
        "total_output_tokens": 7212658,
        "conso_all_conv": 46.40809282088655,
        "n_match": 5223,
        "mean_conso_per_match": 0.008885332724657581,
        "mean_conso_per_token": 6.434256666666651e-6,
        "mean_win_prob": 0.6313645312751852,
        "win_rate": 0.5971108558299828,
        "useful": 1143,
        "creative": 386,
        "complete": 1317,
        "clear_formatting": 976,
        "incorrect": 281,
        "superficial": 203,
        "instructions_not_followed": 94,
        "total_prefs": 4400,
        "positive_prefs_ratio": 0.8686363636363637
    },
    {
        "model_name": "magistral-medium",
        "median": 1094.3490985915344,
        "p2.5": 1071.933385094883,
        "p97.5": 1116.9218851778962,
        "rank": 8,
        "rank_p2.5": 3,
        "rank_p97.5": 17,
        "total_output_tokens": 566459,
        "conso_all_conv": 9.050400411849989,
        "n_match": 643,
        "mean_conso_per_match": 0.014075272802255036,
        "mean_conso_per_token": 0.00001597714999999998,
        "mean_win_prob": 0.6262134970487602,
        "win_rate": 0.5984136858475895,
        "useful": 26,
        "creative": 13,
        "complete": 36,
        "clear_formatting": 18,
        "incorrect": 13,
        "superficial": 6,
        "instructions_not_followed": 0,
        "total_prefs": 112,
        "positive_prefs_ratio": 0.8303571428571429
    },
    {
        "model_name": "gpt-oss-120b",
        "median": 1093.7396708461292,
        "p2.5": 1070.7487170337051,
        "p97.5": 1116.2114080755287,
        "rank": 9,
        "rank_p2.5": 3,
        "rank_p97.5": 17,
        "total_output_tokens": 1534455,
        "conso_all_conv": 5.135590716749995,
        "n_match": 684,
        "mean_conso_per_match": 0.007508173562499993,
        "mean_conso_per_token": 3.3468499999999966e-6,
        "mean_win_prob": 0.6254166033817264,
        "win_rate": null,
        "useful": 96,
        "creative": 43,
        "complete": 150,
        "clear_formatting": 101,
        "incorrect": 37,
        "superficial": 23,
        "instructions_not_followed": 19,
        "total_prefs": 469,
        "positive_prefs_ratio": 0.8315565031982942
    },
    {
        "model_name": "glm-4.5",
        "median": 1092.6679183547478,
        "p2.5": 1061.3394270487681,
        "p97.5": 1129.3784137905686,
        "rank": 10,
        "rank_p2.5": 2,
        "rank_p97.5": 21,
        "total_output_tokens": 603261,
        "conso_all_conv": 12.914502901020002,
        "n_match": 274,
        "mean_conso_per_match": 0.04713322226649636,
        "mean_conso_per_token": 0.000021407820000000003,
        "mean_win_prob": 0.6240136774082448,
        "win_rate": null,
        "useful": 60,
        "creative": 15,
        "complete": 39,
        "clear_formatting": 39,
        "incorrect": 6,
        "superficial": 6,
        "instructions_not_followed": 6,
        "total_prefs": 171,
        "positive_prefs_ratio": 0.8947368421052632
    },
    {
        "model_name": "deepseek-v3-chat",
        "median": 1084.298781678911,
        "p2.5": 1074.7616316756514,
        "p97.5": 1093.1113631584421,
        "rank": 11,
        "rank_p2.5": 8,
        "rank_p97.5": 17,
        "total_output_tokens": 5638296,
        "conso_all_conv": 265.1450417390392,
        "n_match": 5388,
        "mean_conso_per_match": 0.04921028985505553,
        "mean_conso_per_token": 0.00004702573999999985,
        "mean_win_prob": 0.6129952199141866,
        "win_rate": 0.6141481069042316,
        "useful": 1330,
        "creative": 390,
        "complete": 1245,
        "clear_formatting": 1399,
        "incorrect": 314,
        "superficial": 285,
        "instructions_not_followed": 81,
        "total_prefs": 5044,
        "positive_prefs_ratio": 0.8651863600317209
    },
    {
        "model_name": "claude-4-5-sonnet",
        "median": 1082.9508679728847,
        "p2.5": 1064.0466777254167,
        "p97.5": 1102.5521347385086,
        "rank": 12,
        "rank_p2.5": 6,
        "rank_p97.5": 20,
        "total_output_tokens": 1423558,
        "conso_all_conv": 124.54019939928008,
        "n_match": 790,
        "mean_conso_per_match": 0.15764582202440516,
        "mean_conso_per_token": 0.00008748516000000006,
        "mean_win_prob": 0.6112105228288874,
        "win_rate": 0.5251392405063291,
        "useful": 54,
        "creative": 13,
        "complete": 39,
        "clear_formatting": 26,
        "incorrect": 11,
        "superficial": 3,
        "instructions_not_followed": 11,
        "total_prefs": 157,
        "positive_prefs_ratio": 0.8407643312101911
    },
    {
        "model_name": "deepseek-r1-0528",
        "median": 1081.3912069769553,
        "p2.5": 1025.6784633150296,
        "p97.5": 1132.5058548115715,
        "rank": 13,
        "rank_p2.5": 2,
        "rank_p97.5": 34,
        "total_output_tokens": 170822,
        "conso_all_conv": 8.03303095828,
        "n_match": 103,
        "mean_conso_per_match": 0.07799059182796116,
        "mean_conso_per_token": 0.000047025739999999995,
        "mean_win_prob": 0.6091421140776446,
        "win_rate": null,
        "useful": 8,
        "creative": 4,
        "complete": 8,
        "clear_formatting": 4,
        "incorrect": 5,
        "superficial": 1,
        "instructions_not_followed": 0,
        "total_prefs": 30,
        "positive_prefs_ratio": 0.8
    },
    {
        "model_name": "gemma-3-12b",
        "median": 1077.2689202378363,
        "p2.5": 1068.8327423249161,
        "p97.5": 1086.0676151924747,
        "rank": 14,
        "rank_p2.5": 11,
        "rank_p97.5": 19,
        "total_output_tokens": 6469153,
        "conso_all_conv": 28.006300295286675,
        "n_match": 4863,
        "mean_conso_per_match": 0.005759058255251219,
        "mean_conso_per_token": 4.329206666666668e-6,
        "mean_win_prob": 0.603658349279164,
        "win_rate": null,
        "useful": 972,
        "creative": 289,
        "complete": 1129,
        "clear_formatting": 826,
        "incorrect": 299,
        "superficial": 241,
        "instructions_not_followed": 130,
        "total_prefs": 3886,
        "positive_prefs_ratio": 0.8275862068965517
    },
    {
        "model_name": "grok-3-mini-beta",
        "median": 1073.2061010344119,
        "p2.5": 1058.427330040799,
        "p97.5": 1086.7428947598055,
        "rank": 15,
        "rank_p2.5": 11,
        "rank_p97.5": 22,
        "total_output_tokens": 3474752,
        "conso_all_conv": 1630.7706086399996,
        "n_match": 1537,
        "mean_conso_per_match": 1.0610088540273257,
        "mean_conso_per_token": 0.0000115,
        "mean_win_prob": 0.5982307479688509,
        "win_rate": null,
        "useful": 404,
        "creative": 102,
        "complete": 350,
        "clear_formatting": 320,
        "incorrect": 102,
        "superficial": 106,
        "instructions_not_followed": 47,
        "total_prefs": 1431,
        "positive_prefs_ratio": 0.8218029350104822
    },
    {
        "model_name": "grok-4-fast",
        "median": 1072.28628765042,
        "p2.5": 1051.1955535637783,
        "p97.5": 1095.2760671092074,
        "rank": 16,
        "rank_p2.5": 9,
        "rank_p97.5": 25,
        "total_output_tokens": 926120,
        "conso_all_conv": 2904.152564299999,
        "n_match": 637,
        "mean_conso_per_match": 4.559109206122447,
        "mean_conso_per_token": 0.000255827499999999,
        "mean_win_prob": 0.596998911297566,
        "win_rate": null,
        "useful": 46,
        "creative": 4,
        "complete": 28,
        "clear_formatting": 11,
        "incorrect": 9,
        "superficial": 5,
        "instructions_not_followed": 2,
        "total_prefs": 105,
        "positive_prefs_ratio": 0.8476190476190476
    },
    {
        "model_name": "mistral-small-2506",
        "median": 1069.371726649631,
        "p2.5": 1050.2667962977487,
        "p97.5": 1088.3938433963733,
        "rank": 17,
        "rank_p2.5": 10,
        "rank_p97.5": 26,
        "total_output_tokens": 645842,
        "conso_all_conv": 3.8836072536933326,
        "n_match": 742,
        "mean_conso_per_match": 0.005233972040017968,
        "mean_conso_per_token": 6.013246666666666e-6,
        "mean_win_prob": 0.5930885099965143,
        "win_rate": null,
        "useful": 33,
        "creative": 12,
        "complete": 27,
        "clear_formatting": 21,
        "incorrect": 26,
        "superficial": 11,
        "instructions_not_followed": 3,
        "total_prefs": 133,
        "positive_prefs_ratio": 0.6992481203007519
    },
    {
        "model_name": "magistral-small-2506",
        "median": 1067.8253715025812,
        "p2.5": 1050.2653974827654,
        "p97.5": 1086.4601213692074,
        "rank": 18,
        "rank_p2.5": 11,
        "rank_p97.5": 26,
        "total_output_tokens": 924646,
        "conso_all_conv": 5.560124477346667,
        "n_match": 1053,
        "mean_conso_per_match": 0.00528027015892371,
        "mean_conso_per_token": 6.013246666666667e-6,
        "mean_win_prob": 0.5910094979393204,
        "win_rate": null,
        "useful": 124,
        "creative": 27,
        "complete": 90,
        "clear_formatting": 86,
        "incorrect": 35,
        "superficial": 33,
        "instructions_not_followed": 14,
        "total_prefs": 409,
        "positive_prefs_ratio": 0.7995110024449877
    },
    {
        "model_name": "command-a",
        "median": 1066.7923170794215,
        "p2.5": 1057.6066358305368,
        "p97.5": 1075.613595991807,
        "rank": 19,
        "rank_p2.5": 15,
        "rank_p97.5": 23,
        "total_output_tokens": 4819688,
        "conso_all_conv": 87.8269413018931,
        "n_match": 4831,
        "mean_conso_per_match": 0.018179867791739412,
        "mean_conso_per_token": 0.000018222536666666618,
        "mean_win_prob": 0.5896189803028824,
        "win_rate": null,
        "useful": 1031,
        "creative": 245,
        "complete": 983,
        "clear_formatting": 921,
        "incorrect": 241,
        "superficial": 266,
        "instructions_not_followed": 90,
        "total_prefs": 3777,
        "positive_prefs_ratio": 0.8419380460683081
    },
    {
        "model_name": "claude-4-sonnet",
        "median": 1065.2060425610134,
        "p2.5": 1048.919617256564,
        "p97.5": 1082.029539008038,
        "rank": 20,
        "rank_p2.5": 13,
        "rank_p97.5": 26,
        "total_output_tokens": 950171,
        "conso_all_conv": 83.12586196235993,
        "n_match": 1127,
        "mean_conso_per_match": 0.07375852880422354,
        "mean_conso_per_token": 0.00008748515999999994,
        "mean_win_prob": 0.5874813443363986,
        "win_rate": null,
        "useful": 119,
        "creative": 37,
        "complete": 105,
        "clear_formatting": 128,
        "incorrect": 23,
        "superficial": 33,
        "instructions_not_followed": 11,
        "total_prefs": 456,
        "positive_prefs_ratio": 0.8530701754385965
    },
    {
        "model_name": "mistral-medium-2508",
        "median": 1064.8577178053847,
        "p2.5": 1020.7164894339575,
        "p97.5": 1111.5661794582727,
        "rank": 21,
        "rank_p2.5": 4,
        "rank_p97.5": 36,
        "total_output_tokens": 134880,
        "conso_all_conv": 2.154997991999999,
        "n_match": 154,
        "mean_conso_per_match": 0.013993493454545447,
        "mean_conso_per_token": 0.000015977149999999993,
        "mean_win_prob": 0.587011553448694,
        "win_rate": 0.5632467532467531,
        "useful": 10,
        "creative": 1,
        "complete": 7,
        "clear_formatting": 4,
        "incorrect": 4,
        "superficial": 0,
        "instructions_not_followed": 1,
        "total_prefs": 27,
        "positive_prefs_ratio": 0.8148148148148148
    },
    {
        "model_name": "kimi-k2",
        "median": 1059.7722727986625,
        "p2.5": 1023.0855045673152,
        "p97.5": 1098.3800159278449,
        "rank": 22,
        "rank_p2.5": 8,
        "rank_p97.5": 35,
        "total_output_tokens": 376507,
        "conso_all_conv": 21.49385089263999,
        "n_match": 207,
        "mean_conso_per_match": 0.10383502846685985,
        "mean_conso_per_token": 0.00005708751999999997,
        "mean_win_prob": 0.5801371532332226,
        "win_rate": null,
        "useful": 66,
        "creative": 15,
        "complete": 28,
        "clear_formatting": 28,
        "incorrect": 8,
        "superficial": 17,
        "instructions_not_followed": 0,
        "total_prefs": 162,
        "positive_prefs_ratio": 0.845679012345679
    },
    {
        "model_name": "glm-4.6",
        "median": 1057.9977674713277,
        "p2.5": 1009.7292063012651,
        "p97.5": 1116.2490947177282,
        "rank": 23,
        "rank_p2.5": 4,
        "rank_p97.5": 41,
        "total_output_tokens": 287292,
        "conso_all_conv": 6.150295423440003,
        "n_match": 121,
        "mean_conso_per_match": 0.05082888779702482,
        "mean_conso_per_token": 0.00002140782000000001,
        "mean_win_prob": 0.5777318189952823,
        "win_rate": null,
        "useful": 10,
        "creative": 0,
        "complete": 8,
        "clear_formatting": 4,
        "incorrect": 0,
        "superficial": 1,
        "instructions_not_followed": 0,
        "total_prefs": 23,
        "positive_prefs_ratio": 0.9565217391304348
    },
    {
        "model_name": "claude-3-7-sonnet",
        "median": 1056.033823752251,
        "p2.5": 1046.1944676122923,
        "p97.5": 1064.862735796739,
        "rank": 24,
        "rank_p2.5": 19,
        "rank_p97.5": 28,
        "total_output_tokens": 3546728,
        "conso_all_conv": 310.2860665564794,
        "n_match": 3907,
        "mean_conso_per_match": 0.07941798478537994,
        "mean_conso_per_token": 0.00008748515999999983,
        "mean_win_prob": 0.5750659255070321,
        "win_rate": 0.524225748656258,
        "useful": 1003,
        "creative": 230,
        "complete": 774,
        "clear_formatting": 822,
        "incorrect": 140,
        "superficial": 318,
        "instructions_not_followed": 46,
        "total_prefs": 3333,
        "positive_prefs_ratio": 0.8487848784878488
    },
    {
        "model_name": "Qwen3-Coder-480B-A35B-Instruct",
        "median": 1051.8687393824375,
        "p2.5": 996.6070409997003,
        "p97.5": 1101.5472284720781,
        "rank": 25,
        "rank_p2.5": 6,
        "rank_p97.5": 45,
        "total_output_tokens": 196474,
        "conso_all_conv": 5.938976777199999,
        "n_match": 98,
        "mean_conso_per_match": 0.06060180384897958,
        "mean_conso_per_token": 0.000030227799999999995,
        "mean_win_prob": 0.5693996646204437,
        "win_rate": null,
        "useful": 11,
        "creative": 5,
        "complete": 4,
        "clear_formatting": 4,
        "incorrect": 5,
        "superficial": 3,
        "instructions_not_followed": 0,
        "total_prefs": 32,
        "positive_prefs_ratio": 0.75
    },
    {
        "model_name": "llama-3.1-nemotron-70b-instruct",
        "median": 1050.609371283052,
        "p2.5": 1042.3864937847754,
        "p97.5": 1058.8244219832156,
        "rank": 26,
        "rank_p2.5": 21,
        "rank_p97.5": 29,
        "total_output_tokens": 7426282,
        "conso_all_conv": 92.596329916133,
        "n_match": 6709,
        "mean_conso_per_match": 0.01380180800657818,
        "mean_conso_per_token": 0.000012468733333333289,
        "mean_win_prob": 0.5676832103439327,
        "win_rate": null,
        "useful": 1544,
        "creative": 487,
        "complete": 1473,
        "clear_formatting": 1431,
        "incorrect": 381,
        "superficial": 395,
        "instructions_not_followed": 136,
        "total_prefs": 5847,
        "positive_prefs_ratio": 0.8440225756798359
    },
    {
        "model_name": "qwen3-32b",
        "median": 1049.2303603517776,
        "p2.5": 1031.2103739951585,
        "p97.5": 1066.69278098778,
        "rank": 27,
        "rank_p2.5": 19,
        "rank_p97.5": 33,
        "total_output_tokens": 1715714,
        "conso_all_conv": 12.243232161159993,
        "n_match": 829,
        "mean_conso_per_match": 0.01476867570706875,
        "mean_conso_per_token": 7.135939999999996e-6,
        "mean_win_prob": 0.5658020706953724,
        "win_rate": null,
        "useful": 232,
        "creative": 60,
        "complete": 184,
        "clear_formatting": 162,
        "incorrect": 84,
        "superficial": 63,
        "instructions_not_followed": 34,
        "total_prefs": 819,
        "positive_prefs_ratio": 0.778998778998779
    },
    {
        "model_name": "gemma-3-4b",
        "median": 1048.324586588787,
        "p2.5": 1040.5874602176982,
        "p97.5": 1056.4456628760972,
        "rank": 28,
        "rank_p2.5": 22,
        "rank_p97.5": 30,
        "total_output_tokens": 7331151,
        "conso_all_conv": 23.507433430179955,
        "n_match": 5794,
        "mean_conso_per_match": 0.004057202870241621,
        "mean_conso_per_token": 3.206513333333327e-6,
        "mean_win_prob": 0.5645655872769223,
        "win_rate": 0.5317897825336555,
        "useful": 1036,
        "creative": 315,
        "complete": 1150,
        "clear_formatting": 921,
        "incorrect": 495,
        "superficial": 222,
        "instructions_not_followed": 172,
        "total_prefs": 4311,
        "positive_prefs_ratio": 0.7937833449315704
    },
    {
        "model_name": "gemma-3n-e4b-it",
        "median": 1043.606240387357,
        "p2.5": 1024.2770384748028,
        "p97.5": 1061.729484569863,
        "rank": 29,
        "rank_p2.5": 21,
        "rank_p97.5": 35,
        "total_output_tokens": 1078939,
        "conso_all_conv": 3.4596322893533356,
        "n_match": 933,
        "mean_conso_per_match": 0.0037080731933047542,
        "mean_conso_per_token": 3.2065133333333355e-6,
        "mean_win_prob": 0.5581136016360454,
        "win_rate": null,
        "useful": 82,
        "creative": 33,
        "complete": 107,
        "clear_formatting": 86,
        "incorrect": 37,
        "superficial": 31,
        "instructions_not_followed": 17,
        "total_prefs": 393,
        "positive_prefs_ratio": 0.7837150127226463
    },
    {
        "model_name": "gpt-4.1-mini",
        "median": 1038.9517381057094,
        "p2.5": 1031.6707749677205,
        "p97.5": 1047.0127555958068,
        "rank": 30,
        "rank_p2.5": 26,
        "rank_p97.5": 33,
        "total_output_tokens": 4924632,
        "conso_all_conv": 64.8594553700001,
        "n_match": 6078,
        "mean_conso_per_match": 0.010671183838433712,
        "mean_conso_per_token": 0.000013170416666666689,
        "mean_win_prob": 0.5517324218667737,
        "win_rate": null,
        "useful": 1163,
        "creative": 270,
        "complete": 882,
        "clear_formatting": 949,
        "incorrect": 224,
        "superficial": 435,
        "instructions_not_followed": 92,
        "total_prefs": 4015,
        "positive_prefs_ratio": 0.8129514321295144
    },
    {
        "model_name": "gemini-1.5-pro",
        "median": 1035.676144962163,
        "p2.5": 1028.0007873308316,
        "p97.5": 1044.048455314055,
        "rank": 31,
        "rank_p2.5": 27,
        "rank_p97.5": 35,
        "total_output_tokens": 6034223,
        "conso_all_conv": 0.0,
        "n_match": 7387,
        "mean_conso_per_match": 0.0,
        "mean_conso_per_token": 0.0,
        "mean_win_prob": 0.5472329246370836,
        "win_rate": 0.5830743197509137,
        "useful": 2118,
        "creative": 683,
        "complete": 1630,
        "clear_formatting": 1959,
        "incorrect": 461,
        "superficial": 655,
        "instructions_not_followed": 202,
        "total_prefs": 7708,
        "positive_prefs_ratio": 0.8290088220031137
    },
    {
        "model_name": "gpt-oss-20b",
        "median": 1035.128743935128,
        "p2.5": 1014.5376975510546,
        "p97.5": 1057.7101409220095,
        "rank": 32,
        "rank_p2.5": 22,
        "rank_p97.5": 39,
        "total_output_tokens": 1023293,
        "conso_all_conv": 3.1375971197633326,
        "n_match": 758,
        "mean_conso_per_match": 0.004139310184384344,
        "mean_conso_per_token": 3.066176666666666e-6,
        "mean_win_prob": 0.5464803427582682,
        "win_rate": null,
        "useful": 70,
        "creative": 32,
        "complete": 93,
        "clear_formatting": 88,
        "incorrect": 19,
        "superficial": 29,
        "instructions_not_followed": 21,
        "total_prefs": 352,
        "positive_prefs_ratio": 0.8039772727272727
    },
    {
        "model_name": "deepseek-r1",
        "median": 1033.9828253463074,
        "p2.5": 1023.4723808971734,
        "p97.5": 1045.1049722726248,
        "rank": 33,
        "rank_p2.5": 28,
        "rank_p97.5": 36,
        "total_output_tokens": 3693582,
        "conso_all_conv": 173.69342680067993,
        "n_match": 3510,
        "mean_conso_per_match": 0.04948530678082049,
        "mean_conso_per_token": 0.00004702573999999998,
        "mean_win_prob": 0.5449043332061375,
        "win_rate": 0.5313019943019942,
        "useful": 826,
        "creative": 284,
        "complete": 783,
        "clear_formatting": 667,
        "incorrect": 171,
        "superficial": 207,
        "instructions_not_followed": 112,
        "total_prefs": 3050,
        "positive_prefs_ratio": 0.839344262295082
    },
    {
        "model_name": "gpt-5-mini",
        "median": 1029.6309367161562,
        "p2.5": 1004.0396029846328,
        "p97.5": 1052.5975244269748,
        "rank": 34,
        "rank_p2.5": 25,
        "rank_p97.5": 42,
        "total_output_tokens": 620324,
        "conso_all_conv": 5.732407880760001,
        "n_match": 539,
        "mean_conso_per_match": 0.010635265084897962,
        "mean_conso_per_token": 9.240990000000001e-6,
        "mean_win_prob": 0.5389125283446132,
        "win_rate": null,
        "useful": 27,
        "creative": 7,
        "complete": 44,
        "clear_formatting": 18,
        "incorrect": 5,
        "superficial": 9,
        "instructions_not_followed": 3,
        "total_prefs": 113,
        "positive_prefs_ratio": 0.8495575221238938
    },
    {
        "model_name": "mistral-large-2411",
        "median": 1024.4857675656017,
        "p2.5": 1017.7271440583022,
        "p97.5": 1031.7926446180625,
        "rank": 35,
        "rank_p2.5": 32,
        "rank_p97.5": 38,
        "total_output_tokens": 7677266,
        "conso_all_conv": 152.82808421939313,
        "n_match": 8829,
        "mean_conso_per_match": 0.017309784145361096,
        "mean_conso_per_token": 0.00001990657666666664,
        "mean_win_prob": 0.5318168492804626,
        "win_rate": 0.527441386340469,
        "useful": 1898,
        "creative": 355,
        "complete": 1451,
        "clear_formatting": 1608,
        "incorrect": 393,
        "superficial": 666,
        "instructions_not_followed": 140,
        "total_prefs": 6511,
        "positive_prefs_ratio": 0.8158500998310552
    },
    {
        "model_name": "llama-4-scout",
        "median": 1019.4900610179484,
        "p2.5": 1009.278276269848,
        "p97.5": 1029.5311188366222,
        "rank": 36,
        "rank_p2.5": 33,
        "rank_p97.5": 41,
        "total_output_tokens": 2856478,
        "conso_all_conv": 14.370626605420021,
        "n_match": 3406,
        "mean_conso_per_match": 0.004219209220616566,
        "mean_conso_per_token": 5.030890000000007e-6,
        "mean_win_prob": 0.5249176232873758,
        "win_rate": null,
        "useful": 611,
        "creative": 160,
        "complete": 537,
        "clear_formatting": 500,
        "incorrect": 203,
        "superficial": 272,
        "instructions_not_followed": 74,
        "total_prefs": 2357,
        "positive_prefs_ratio": 0.7670767925328807
    },
    {
        "model_name": "gpt-5",
        "median": 1013.5998786849659,
        "p2.5": 995.9031590332073,
        "p97.5": 1030.4122799129213,
        "rank": 37,
        "rank_p2.5": 33,
        "rank_p97.5": 45,
        "total_output_tokens": 967868,
        "conso_all_conv": 45.98383454192,
        "n_match": 1092,
        "mean_conso_per_match": 0.04210973859150183,
        "mean_conso_per_token": 0.000047510439999999994,
        "mean_win_prob": 0.5167742630115164,
        "win_rate": null,
        "useful": 33,
        "creative": 12,
        "complete": 42,
        "clear_formatting": 18,
        "incorrect": 6,
        "superficial": 10,
        "instructions_not_followed": 5,
        "total_prefs": 126,
        "positive_prefs_ratio": 0.8333333333333334
    },
    {
        "model_name": "llama-maverick",
        "median": 1012.401872544021,
        "p2.5": 990.4729261675026,
        "p97.5": 1037.4677757958552,
        "rank": 38,
        "rank_p2.5": 31,
        "rank_p97.5": 47,
        "total_output_tokens": 456481,
        "conso_all_conv": 6.889517094269991,
        "n_match": 559,
        "mean_conso_per_match": 0.012324717521055439,
        "mean_conso_per_token": 0.00001509266999999998,
        "mean_win_prob": 0.5151171527109812,
        "win_rate": null,
        "useful": 22,
        "creative": 6,
        "complete": 18,
        "clear_formatting": 19,
        "incorrect": 9,
        "superficial": 6,
        "instructions_not_followed": 3,
        "total_prefs": 83,
        "positive_prefs_ratio": 0.7831325301204819
    },
    {
        "model_name": "mistral-small-3.1-24b",
        "median": 1012.0016915120586,
        "p2.5": 1003.1176385236788,
        "p97.5": 1020.6017832603335,
        "rank": 39,
        "rank_p2.5": 36,
        "rank_p97.5": 43,
        "total_output_tokens": 4470466,
        "conso_all_conv": 26.882014772946658,
        "n_match": 5079,
        "mean_conso_per_match": 0.005292777076776266,
        "mean_conso_per_token": 6.013246666666665e-6,
        "mean_win_prob": 0.5145635663190378,
        "win_rate": 0.4945107304587519,
        "useful": 1108,
        "creative": 247,
        "complete": 791,
        "clear_formatting": 916,
        "incorrect": 268,
        "superficial": 430,
        "instructions_not_followed": 130,
        "total_prefs": 3890,
        "positive_prefs_ratio": 0.787146529562982
    },
    {
        "model_name": "o4-mini",
        "median": 1011.7818491032731,
        "p2.5": 1000.2218915008879,
        "p97.5": 1023.4827477465494,
        "rank": 40,
        "rank_p2.5": 35,
        "rank_p97.5": 44,
        "total_output_tokens": 2626142,
        "conso_all_conv": 13.580375540853344,
        "n_match": 2520,
        "mean_conso_per_match": 0.005389037913037041,
        "mean_conso_per_token": 5.1712266666666705e-6,
        "mean_win_prob": 0.514259440440636,
        "win_rate": 0.4673531746031746,
        "useful": 508,
        "creative": 108,
        "complete": 356,
        "clear_formatting": 270,
        "incorrect": 124,
        "superficial": 203,
        "instructions_not_followed": 43,
        "total_prefs": 1612,
        "positive_prefs_ratio": 0.7704714640198511
    },
    {
        "model_name": "mistral-saba",
        "median": 1009.8899555758059,
        "p2.5": 1000.8248209293266,
        "p97.5": 1018.7790121116099,
        "rank": 41,
        "rank_p2.5": 37,
        "rank_p97.5": 44,
        "total_output_tokens": 3490208,
        "conso_all_conv": 20.987481621973373,
        "n_match": 3944,
        "mean_conso_per_match": 0.005321369579607853,
        "mean_conso_per_token": 6.013246666666678e-6,
        "mean_win_prob": 0.5116419969148767,
        "win_rate": null,
        "useful": 681,
        "creative": 156,
        "complete": 516,
        "clear_formatting": 521,
        "incorrect": 217,
        "superficial": 327,
        "instructions_not_followed": 109,
        "total_prefs": 2527,
        "positive_prefs_ratio": 0.7415908191531461
    },
    {
        "model_name": "Apertus-70B-Instruct-2509",
        "median": 1009.7510612313058,
        "p2.5": 944.9983187396476,
        "p97.5": 1068.7789595274846,
        "rank": 42,
        "rank_p2.5": 18,
        "rank_p97.5": 65,
        "total_output_tokens": 92469,
        "conso_all_conv": 1.1529713026000001,
        "n_match": 89,
        "mean_conso_per_match": 0.012954733737078653,
        "mean_conso_per_token": 0.000012468733333333335,
        "mean_win_prob": 0.5114498215736502,
        "win_rate": null,
        "useful": 3,
        "creative": 1,
        "complete": 2,
        "clear_formatting": 0,
        "incorrect": 1,
        "superficial": 1,
        "instructions_not_followed": 1,
        "total_prefs": 9,
        "positive_prefs_ratio": 0.6666666666666666
    },
    {
        "model_name": "gemma-2-27b-it-q8",
        "median": 1004.1900451961408,
        "p2.5": 981.1132573880055,
        "p97.5": 1028.9835646359472,
        "rank": 43,
        "rank_p2.5": 34,
        "rank_p97.5": 52,
        "total_output_tokens": 449540,
        "conso_all_conv": 2.892455741933335,
        "n_match": 762,
        "mean_conso_per_match": 0.0037958736770778677,
        "mean_conso_per_token": 6.4342566666666705e-6,
        "mean_win_prob": 0.5037547400009338,
        "win_rate": 0.5826115485564305,
        "useful": 318,
        "creative": 97,
        "complete": 178,
        "clear_formatting": 305,
        "incorrect": 63,
        "superficial": 91,
        "instructions_not_followed": 29,
        "total_prefs": 1081,
        "positive_prefs_ratio": 0.8307123034227567
    },
    {
        "model_name": "qwen3-30b-a3b",
        "median": 999.6463004815873,
        "p2.5": 976.5339491516471,
        "p97.5": 1021.419553899018,
        "rank": 44,
        "rank_p2.5": 36,
        "rank_p97.5": 54,
        "total_output_tokens": 827675,
        "conso_all_conv": 2.537797772583333,
        "n_match": 570,
        "mean_conso_per_match": 0.004452276794005848,
        "mean_conso_per_token": 3.0661766666666664e-6,
        "mean_win_prob": 0.49746769463087526,
        "win_rate": null,
        "useful": 37,
        "creative": 12,
        "complete": 35,
        "clear_formatting": 19,
        "incorrect": 19,
        "superficial": 15,
        "instructions_not_followed": 5,
        "total_prefs": 142,
        "positive_prefs_ratio": 0.7253521126760564
    },
    {
        "model_name": "aya-expanse-32b",
        "median": 995.6851656083768,
        "p2.5": 986.5339839437395,
        "p97.5": 1005.6080713558855,
        "rank": 45,
        "rank_p2.5": 42,
        "rank_p97.5": 49,
        "total_output_tokens": 3307611,
        "conso_all_conv": 23.60291363934002,
        "n_match": 4054,
        "mean_conso_per_match": 0.005822129659432664,
        "mean_conso_per_token": 7.135940000000006e-6,
        "mean_win_prob": 0.4919886420059455,
        "win_rate": 0.4701578687715836,
        "useful": 661,
        "creative": 130,
        "complete": 471,
        "clear_formatting": 480,
        "incorrect": 241,
        "superficial": 321,
        "instructions_not_followed": 99,
        "total_prefs": 2403,
        "positive_prefs_ratio": 0.7249271743653766
    },
    {
        "model_name": "mistral-small-24b-instruct-2501",
        "median": 990.8747647778005,
        "p2.5": 978.8089668893247,
        "p97.5": 1002.1921725183706,
        "rank": 46,
        "rank_p2.5": 43,
        "rank_p97.5": 52,
        "total_output_tokens": 2818040,
        "conso_all_conv": 16.945569636533296,
        "n_match": 3318,
        "mean_conso_per_match": 0.005107163844645358,
        "mean_conso_per_token": 6.013246666666654e-6,
        "mean_win_prob": 0.4853390619102505,
        "win_rate": 0.4952350813743219,
        "useful": 745,
        "creative": 155,
        "complete": 536,
        "clear_formatting": 656,
        "incorrect": 204,
        "superficial": 307,
        "instructions_not_followed": 71,
        "total_prefs": 2674,
        "positive_prefs_ratio": 0.7823485415108452
    },
    {
        "model_name": "o3-mini",
        "median": 988.8995840712469,
        "p2.5": 973.9195260728519,
        "p97.5": 1003.8665096694517,
        "rank": 47,
        "rank_p2.5": 43,
        "rank_p97.5": 55,
        "total_output_tokens": 1655945,
        "conso_all_conv": 8.563266942533337,
        "n_match": 1619,
        "mean_conso_per_match": 0.005289232206629609,
        "mean_conso_per_token": 5.171226666666669e-6,
        "mean_win_prob": 0.48261050542005857,
        "win_rate": 0.51142063001853,
        "useful": 350,
        "creative": 85,
        "complete": 254,
        "clear_formatting": 230,
        "incorrect": 49,
        "superficial": 133,
        "instructions_not_followed": 28,
        "total_prefs": 1129,
        "positive_prefs_ratio": 0.8139946855624446
    },
    {
        "model_name": "llama-3.3-70b",
        "median": 987.1141486813386,
        "p2.5": 979.7281532608658,
        "p97.5": 994.3348747197437,
        "rank": 48,
        "rank_p2.5": 45,
        "rank_p97.5": 53,
        "total_output_tokens": 6044909,
        "conso_all_conv": 75.37235834526692,
        "n_match": 7120,
        "mean_conso_per_match": 0.010586005385571196,
        "mean_conso_per_token": 0.000012468733333333375,
        "mean_win_prob": 0.48014515950283504,
        "win_rate": null,
        "useful": 1344,
        "creative": 266,
        "complete": 1031,
        "clear_formatting": 994,
        "incorrect": 411,
        "superficial": 563,
        "instructions_not_followed": 132,
        "total_prefs": 4741,
        "positive_prefs_ratio": 0.7667158827251634
    },
    {
        "model_name": "gpt-4o-mini-2024-07-18",
        "median": 986.6797696725512,
        "p2.5": 978.4820354178424,
        "p97.5": 995.6560155710395,
        "rank": 49,
        "rank_p2.5": 45,
        "rank_p97.5": 53,
        "total_output_tokens": 5207480,
        "conso_all_conv": 26.929059442133166,
        "n_match": 6990,
        "mean_conso_per_match": 0.003852512080419623,
        "mean_conso_per_token": 5.171226666666635e-6,
        "mean_win_prob": 0.4795455350403343,
        "win_rate": 0.49947067238912735,
        "useful": 1491,
        "creative": 319,
        "complete": 856,
        "clear_formatting": 1282,
        "incorrect": 369,
        "superficial": 599,
        "instructions_not_followed": 110,
        "total_prefs": 5026,
        "positive_prefs_ratio": 0.7855153203342619
    },
    {
        "model_name": "gpt-4.1-nano",
        "median": 983.791189598166,
        "p2.5": 975.2448068096926,
        "p97.5": 992.51272562966,
        "rank": 50,
        "rank_p2.5": 45,
        "rank_p97.5": 54,
        "total_output_tokens": 3046683,
        "conso_all_conv": 17.89289505752993,
        "n_match": 4646,
        "mean_conso_per_match": 0.00385124732189624,
        "mean_conso_per_token": 5.872909999999977e-6,
        "mean_win_prob": 0.47555994223659903,
        "win_rate": 0.4719436074042187,
        "useful": 861,
        "creative": 149,
        "complete": 481,
        "clear_formatting": 609,
        "incorrect": 245,
        "superficial": 415,
        "instructions_not_followed": 94,
        "total_prefs": 2854,
        "positive_prefs_ratio": 0.7358093903293623
    },
    {
        "model_name": "aya-expanse-8b",
        "median": 978.0332856982579,
        "p2.5": 962.0332580301781,
        "p97.5": 992.6457072494991,
        "rank": 51,
        "rank_p2.5": 46,
        "rank_p97.5": 60,
        "total_output_tokens": 1057810,
        "conso_all_conv": 3.985679986599994,
        "n_match": 1302,
        "mean_conso_per_match": 0.003061198146390164,
        "mean_conso_per_token": 3.767859999999994e-6,
        "mean_win_prob": 0.46762641722555415,
        "win_rate": 0.5014285714285716,
        "useful": 243,
        "creative": 55,
        "complete": 133,
        "clear_formatting": 223,
        "incorrect": 149,
        "superficial": 104,
        "instructions_not_followed": 31,
        "total_prefs": 938,
        "positive_prefs_ratio": 0.697228144989339
    },
    {
        "model_name": "jamba-1.5-large",
        "median": 976.7164930361517,
        "p2.5": 929.8478850678641,
        "p97.5": 1023.0342714559367,
        "rank": 52,
        "rank_p2.5": 35,
        "rank_p97.5": 68,
        "total_output_tokens": 143976,
        "conso_all_conv": 6.840363109439998,
        "n_match": 172,
        "mean_conso_per_match": 0.03976955296186046,
        "mean_conso_per_token": 0.00004751043999999999,
        "mean_win_prob": 0.4658144519443739,
        "win_rate": 0.5449418604651162,
        "useful": 44,
        "creative": 11,
        "complete": 18,
        "clear_formatting": 57,
        "incorrect": 15,
        "superficial": 13,
        "instructions_not_followed": 4,
        "total_prefs": 162,
        "positive_prefs_ratio": 0.8024691358024691
    },
    {
        "model_name": "llama-3.1-70b",
        "median": 976.3331788387665,
        "p2.5": 968.2893315972233,
        "p97.5": 985.8100300408108,
        "rank": 53,
        "rank_p2.5": 49,
        "rank_p97.5": 58,
        "total_output_tokens": 4057254,
        "conso_all_conv": 50.588818191600005,
        "n_match": 5583,
        "mean_conso_per_match": 0.009061224823858141,
        "mean_conso_per_token": 0.000012468733333333335,
        "mean_win_prob": 0.46528717653896534,
        "win_rate": 0.530445996775927,
        "useful": 1532,
        "creative": 365,
        "complete": 1047,
        "clear_formatting": 1305,
        "incorrect": 463,
        "superficial": 621,
        "instructions_not_followed": 177,
        "total_prefs": 5510,
        "positive_prefs_ratio": 0.7711433756805808
    },
    {
        "model_name": "claude-3-5-sonnet-v2",
        "median": 976.2762566225798,
        "p2.5": 967.1396901511647,
        "p97.5": 984.98818152699,
        "rank": 54,
        "rank_p2.5": 50,
        "rank_p97.5": 58,
        "total_output_tokens": 3224219,
        "conso_all_conv": 282.07131509004046,
        "n_match": 5683,
        "mean_conso_per_match": 0.049634227536519526,
        "mean_conso_per_token": 0.00008748516000000014,
        "mean_win_prob": 0.4652088832014115,
        "win_rate": 0.4921080415273623,
        "useful": 1301,
        "creative": 291,
        "complete": 721,
        "clear_formatting": 905,
        "incorrect": 250,
        "superficial": 606,
        "instructions_not_followed": 94,
        "total_prefs": 4168,
        "positive_prefs_ratio": 0.7720729366602687
    },
    {
        "model_name": "phi-4",
        "median": 972.2933903697576,
        "p2.5": 965.9419847685373,
        "p97.5": 979.3795270913535,
        "rank": 55,
        "rank_p2.5": 51,
        "rank_p97.5": 59,
        "total_output_tokens": 6999198,
        "conso_all_conv": 32.265462876240115,
        "n_match": 8355,
        "mean_conso_per_match": 0.0038618148265996545,
        "mean_conso_per_token": 4.609880000000016e-6,
        "mean_win_prob": 0.45973548553731897,
        "win_rate": null,
        "useful": 1560,
        "creative": 319,
        "complete": 1125,
        "clear_formatting": 1265,
        "incorrect": 644,
        "superficial": 701,
        "instructions_not_followed": 225,
        "total_prefs": 5839,
        "positive_prefs_ratio": 0.7311183421818804
    },
    {
        "model_name": "ministral-8b-instruct-2410",
        "median": 968.2932151168529,
        "p2.5": 961.8099229441501,
        "p97.5": 975.6303009983944,
        "rank": 56,
        "rank_p2.5": 53,
        "rank_p97.5": 60,
        "total_output_tokens": 7488476,
        "conso_all_conv": 28.21552918135989,
        "n_match": 9278,
        "mean_conso_per_match": 0.00304112192081913,
        "mean_conso_per_token": 3.7678599999999853e-6,
        "mean_win_prob": 0.4542486752479652,
        "win_rate": null,
        "useful": 1796,
        "creative": 412,
        "complete": 1332,
        "clear_formatting": 1695,
        "incorrect": 876,
        "superficial": 927,
        "instructions_not_followed": 274,
        "total_prefs": 7312,
        "positive_prefs_ratio": 0.7159463894967177
    },
    {
        "model_name": "qwen-3-8b",
        "median": 967.8964137587277,
        "p2.5": 912.5324678786417,
        "p97.5": 1016.9423743881198,
        "rank": 57,
        "rank_p2.5": 38,
        "rank_p97.5": 70,
        "total_output_tokens": 265806,
        "conso_all_conv": 1.0015197951599997,
        "n_match": 142,
        "mean_conso_per_match": 0.007052956303943659,
        "mean_conso_per_token": 3.767859999999999e-6,
        "mean_win_prob": 0.45370501903105737,
        "win_rate": null,
        "useful": 7,
        "creative": 0,
        "complete": 6,
        "clear_formatting": 6,
        "incorrect": 1,
        "superficial": 1,
        "instructions_not_followed": 2,
        "total_prefs": 23,
        "positive_prefs_ratio": 0.8260869565217391
    },
    {
        "model_name": "gpt-4o-2024-08-06",
        "median": 966.6529549712683,
        "p2.5": 957.0701281571473,
        "p97.5": 975.1543807116649,
        "rank": 58,
        "rank_p2.5": 54,
        "rank_p97.5": 62,
        "total_output_tokens": 3903917,
        "conso_all_conv": 341.5348033717197,
        "n_match": 5896,
        "mean_conso_per_match": 0.05792652703048163,
        "mean_conso_per_token": 0.00008748515999999992,
        "mean_win_prob": 0.45200211379994937,
        "win_rate": 0.48863297150610585,
        "useful": 1230,
        "creative": 243,
        "complete": 593,
        "clear_formatting": 1002,
        "incorrect": 275,
        "superficial": 590,
        "instructions_not_followed": 115,
        "total_prefs": 4048,
        "positive_prefs_ratio": 0.7579051383399209
    },
    {
        "model_name": "Apertus-8B-Instruct-2509",
        "median": 966.5186667085936,
        "p2.5": 906.6889557887498,
        "p97.5": 1017.8416436706628,
        "rank": 59,
        "rank_p2.5": 37,
        "rank_p97.5": 70,
        "total_output_tokens": 52304,
        "conso_all_conv": 0.19707414944000007,
        "n_match": 86,
        "mean_conso_per_match": 0.0022915598772093033,
        "mean_conso_per_token": 3.7678600000000014e-6,
        "mean_win_prob": 0.45181827666337715,
        "win_rate": null,
        "useful": 2,
        "creative": 0,
        "complete": 3,
        "clear_formatting": 2,
        "incorrect": 3,
        "superficial": 5,
        "instructions_not_followed": 0,
        "total_prefs": 15,
        "positive_prefs_ratio": 0.4666666666666667
    },
    {
        "model_name": "qwen2.5-32b-instruct",
        "median": 965.0585352401331,
        "p2.5": 910.3603461263605,
        "p97.5": 1020.4127231381761,
        "rank": 60,
        "rank_p2.5": 36,
        "rank_p97.5": 70,
        "total_output_tokens": 75812,
        "conso_all_conv": 0.5409898832799998,
        "n_match": 142,
        "mean_conso_per_match": 0.0038097879104225336,
        "mean_conso_per_token": 7.135939999999997e-6,
        "mean_win_prob": 0.44982028863653145,
        "win_rate": 0.5557042253521126,
        "useful": 52,
        "creative": 16,
        "complete": 43,
        "clear_formatting": 57,
        "incorrect": 13,
        "superficial": 21,
        "instructions_not_followed": 15,
        "total_prefs": 217,
        "positive_prefs_ratio": 0.7741935483870968
    },
    {
        "model_name": "llama-3.1-405b",
        "median": 961.6302533809792,
        "p2.5": 954.2786325632679,
        "p97.5": 968.6204903714167,
        "rank": 61,
        "rank_p2.5": 57,
        "rank_p97.5": 63,
        "total_output_tokens": 10383810,
        "conso_all_conv": 2470.5790703140174,
        "n_match": 9973,
        "mean_conso_per_match": 0.2477267693085348,
        "mean_conso_per_token": 0.00023792606666666835,
        "mean_win_prob": 0.4451358584770793,
        "win_rate": null,
        "useful": 2224,
        "creative": 556,
        "complete": 1579,
        "clear_formatting": 1788,
        "incorrect": 947,
        "superficial": 890,
        "instructions_not_followed": 437,
        "total_prefs": 8421,
        "positive_prefs_ratio": 0.7299608122550766
    },
    {
        "model_name": "deepseek-r1-distill-llama-70b",
        "median": 957.9494948871952,
        "p2.5": 946.1580332711412,
        "p97.5": 969.115537819754,
        "rank": 62,
        "rank_p2.5": 57,
        "rank_p97.5": 65,
        "total_output_tokens": 2499440,
        "conso_all_conv": 31.16485084266665,
        "n_match": 2721,
        "mean_conso_per_match": 0.011453454921964958,
        "mean_conso_per_token": 0.000012468733333333326,
        "mean_win_prob": 0.4401176028974063,
        "win_rate": null,
        "useful": 483,
        "creative": 128,
        "complete": 348,
        "clear_formatting": 360,
        "incorrect": 178,
        "superficial": 328,
        "instructions_not_followed": 119,
        "total_prefs": 1944,
        "positive_prefs_ratio": 0.6784979423868313
    },
    {
        "model_name": "gemma-2-9b-it",
        "median": 957.4371655492878,
        "p2.5": 948.1025580956925,
        "p97.5": 966.3111087572506,
        "rank": 63,
        "rank_p2.5": 58,
        "rank_p97.5": 65,
        "total_output_tokens": 3108609,
        "conso_all_conv": 12.149055331769983,
        "n_match": 5115,
        "mean_conso_per_match": 0.002375181883043985,
        "mean_conso_per_token": 3.908196666666661e-6,
        "mean_win_prob": 0.4394200727547841,
        "win_rate": 0.5068660801564029,
        "useful": 1250,
        "creative": 403,
        "complete": 771,
        "clear_formatting": 1197,
        "incorrect": 472,
        "superficial": 595,
        "instructions_not_followed": 177,
        "total_prefs": 4865,
        "positive_prefs_ratio": 0.7442959917780062
    },
    {
        "model_name": "qwq-32b",
        "median": 956.6539436149421,
        "p2.5": 941.5478889849782,
        "p97.5": 972.2598341064717,
        "rank": 64,
        "rank_p2.5": 56,
        "rank_p97.5": 66,
        "total_output_tokens": 1895013,
        "conso_all_conv": 13.522699067219987,
        "n_match": 1566,
        "mean_conso_per_match": 0.008635184589540221,
        "mean_conso_per_token": 7.135939999999993e-6,
        "mean_win_prob": 0.43835419801335707,
        "win_rate": null,
        "useful": 291,
        "creative": 139,
        "complete": 343,
        "clear_formatting": 233,
        "incorrect": 140,
        "superficial": 105,
        "instructions_not_followed": 92,
        "total_prefs": 1343,
        "positive_prefs_ratio": 0.7490692479523455
    },
    {
        "model_name": "gpt-5-nano",
        "median": 948.6623290754503,
        "p2.5": 922.815742996209,
        "p97.5": 976.0273778520401,
        "rank": 65,
        "rank_p2.5": 54,
        "rank_p97.5": 70,
        "total_output_tokens": 414724,
        "conso_all_conv": 1.7372249218800013,
        "n_match": 524,
        "mean_conso_per_match": 0.003315314736412216,
        "mean_conso_per_token": 4.188870000000003e-6,
        "mean_win_prob": 0.4275131664338935,
        "win_rate": 0.3965648854961832,
        "useful": 13,
        "creative": 2,
        "complete": 14,
        "clear_formatting": 9,
        "incorrect": 5,
        "superficial": 9,
        "instructions_not_followed": 8,
        "total_prefs": 60,
        "positive_prefs_ratio": 0.6333333333333333
    },
    {
        "model_name": "hermes-3-llama-3.1-405b",
        "median": 946.491096523165,
        "p2.5": 938.5086233084112,
        "p97.5": 954.243088800919,
        "rank": 66,
        "rank_p2.5": 62,
        "rank_p97.5": 66,
        "total_output_tokens": 4092822,
        "conso_all_conv": 973.7890400268024,
        "n_match": 6735,
        "mean_conso_per_match": 0.14458634595795136,
        "mean_conso_per_token": 0.00023792606666666723,
        "mean_win_prob": 0.42457940843665554,
        "win_rate": 0.46634743875278395,
        "useful": 1450,
        "creative": 285,
        "complete": 830,
        "clear_formatting": 1109,
        "incorrect": 426,
        "superficial": 753,
        "instructions_not_followed": 184,
        "total_prefs": 5037,
        "positive_prefs_ratio": 0.7294024220766329
    },
    {
        "model_name": "c4ai-command-r-08-2024",
        "median": 934.6429545251469,
        "p2.5": 926.5599810324668,
        "p97.5": 942.7417227662972,
        "rank": 67,
        "rank_p2.5": 65,
        "rank_p97.5": 69,
        "total_output_tokens": 4319206,
        "conso_all_conv": 32.64002378170008,
        "n_match": 5959,
        "mean_conso_per_match": 0.005477433089729834,
        "mean_conso_per_token": 7.556950000000018e-6,
        "mean_win_prob": 0.4086686552027624,
        "win_rate": 0.4296878670917938,
        "useful": 1075,
        "creative": 255,
        "complete": 843,
        "clear_formatting": 796,
        "incorrect": 471,
        "superficial": 660,
        "instructions_not_followed": 163,
        "total_prefs": 4263,
        "positive_prefs_ratio": 0.6964578935022285
    },
    {
        "model_name": "qwen2.5-coder-32b-instruct",
        "median": 931.0360390377439,
        "p2.5": 923.2155149202197,
        "p97.5": 939.1456660167105,
        "rank": 68,
        "rank_p2.5": 66,
        "rank_p97.5": 70,
        "total_output_tokens": 6149486,
        "conso_all_conv": 43.88236312684011,
        "n_match": 7299,
        "mean_conso_per_match": 0.006012106196306359,
        "mean_conso_per_token": 7.135940000000018e-6,
        "mean_win_prob": 0.40386083220332325,
        "win_rate": 0.4378942320865873,
        "useful": 1379,
        "creative": 293,
        "complete": 923,
        "clear_formatting": 1142,
        "incorrect": 790,
        "superficial": 742,
        "instructions_not_followed": 234,
        "total_prefs": 5503,
        "positive_prefs_ratio": 0.6790841359258586
    },
    {
        "model_name": "qwen2.5-7b-instruct",
        "median": 930.0635119939136,
        "p2.5": 912.4427358391822,
        "p97.5": 947.6950498889964,
        "rank": 69,
        "rank_p2.5": 64,
        "rank_p97.5": 70,
        "total_output_tokens": 1186919,
        "conso_all_conv": 4.305576367276668,
        "n_match": 1417,
        "mean_conso_per_match": 0.003038515432093626,
        "mean_conso_per_token": 3.6275233333333344e-6,
        "mean_win_prob": 0.40256757991373376,
        "win_rate": 0.49383909668313336,
        "useful": 402,
        "creative": 104,
        "complete": 284,
        "clear_formatting": 380,
        "incorrect": 231,
        "superficial": 203,
        "instructions_not_followed": 88,
        "total_prefs": 1692,
        "positive_prefs_ratio": 0.6914893617021277
    },
    {
        "model_name": "llama-3.1-8b",
        "median": 927.9363322143909,
        "p2.5": 920.8677891826636,
        "p97.5": 935.3331661705075,
        "rank": 70,
        "rank_p2.5": 66,
        "rank_p97.5": 70,
        "total_output_tokens": 6093696,
        "conso_all_conv": 22.96019341055991,
        "n_match": 8402,
        "mean_conso_per_match": 0.0027327057141823268,
        "mean_conso_per_token": 3.7678599999999853e-6,
        "mean_win_prob": 0.3997435608934462,
        "win_rate": null,
        "useful": 1481,
        "creative": 361,
        "complete": 962,
        "clear_formatting": 1115,
        "incorrect": 953,
        "superficial": 928,
        "instructions_not_followed": 384,
        "total_prefs": 6184,
        "positive_prefs_ratio": 0.633732212160414
    },
    {
        "model_name": "hermes-4-70b",
        "median": 899.1162899720732,
        "p2.5": 870.7694053595712,
        "p97.5": 924.6837909498881,
        "rank": 71,
        "rank_p2.5": 70,
        "rank_p97.5": 73,
        "total_output_tokens": 218566,
        "conso_all_conv": 2.725241169733336,
        "n_match": 478,
        "mean_conso_per_match": 0.005701341359274761,
        "mean_conso_per_token": 0.000012468733333333345,
        "mean_win_prob": 0.3621905168876112,
        "win_rate": null,
        "useful": 13,
        "creative": 6,
        "complete": 4,
        "clear_formatting": 8,
        "incorrect": 8,
        "superficial": 12,
        "instructions_not_followed": 5,
        "total_prefs": 56,
        "positive_prefs_ratio": 0.5535714285714286
    },
    {
        "model_name": "mixtral-8x7b-instruct-v0.1",
        "median": 888.0985664623535,
        "p2.5": 873.5946336225421,
        "p97.5": 901.7263949763012,
        "rank": 72,
        "rank_p2.5": 71,
        "rank_p97.5": 73,
        "total_output_tokens": 1575791,
        "conso_all_conv": 7.264207415080017,
        "n_match": 2560,
        "mean_conso_per_match": 0.0028375810215156318,
        "mean_conso_per_token": 4.609880000000011e-6,
        "mean_win_prob": 0.34822898026868915,
        "win_rate": 0.43488671874999996,
        "useful": 727,
        "creative": 164,
        "complete": 375,
        "clear_formatting": 503,
        "incorrect": 256,
        "superficial": 389,
        "instructions_not_followed": 159,
        "total_prefs": 2573,
        "positive_prefs_ratio": 0.687524290711232
    },
    {
        "model_name": "lfm-40b",
        "median": 886.2135879706576,
        "p2.5": 874.9297212725892,
        "p97.5": 897.5230117081456,
        "rank": 73,
        "rank_p2.5": 71,
        "rank_p97.5": 73,
        "total_output_tokens": 2000824,
        "conso_all_conv": 16.52407178053337,
        "n_match": 3578,
        "mean_conso_per_match": 0.00461824253228993,
        "mean_conso_per_token": 8.258633333333351e-6,
        "mean_win_prob": 0.3458646125096044,
        "win_rate": 0.416296813862493,
        "useful": 841,
        "creative": 166,
        "complete": 419,
        "clear_formatting": 647,
        "incorrect": 357,
        "superficial": 572,
        "instructions_not_followed": 121,
        "total_prefs": 3123,
        "positive_prefs_ratio": 0.6637848222862632
    },
    {
        "model_name": "phi-3.5-mini-instruct",
        "median": 864.2437798957969,
        "p2.5": 850.7885092434617,
        "p97.5": 877.9621285183813,
        "rank": 74,
        "rank_p2.5": 74,
        "rank_p97.5": 75,
        "total_output_tokens": 2149046,
        "conso_all_conv": 6.5893547007933355,
        "n_match": 2535,
        "mean_conso_per_match": 0.002599350966782381,
        "mean_conso_per_token": 3.0661766666666677e-6,
        "mean_win_prob": 0.31886832342210925,
        "win_rate": 0.40211045364891523,
        "useful": 636,
        "creative": 226,
        "complete": 477,
        "clear_formatting": 533,
        "incorrect": 428,
        "superficial": 449,
        "instructions_not_followed": 300,
        "total_prefs": 3049,
        "positive_prefs_ratio": 0.6139717940308298
    },
    {
        "model_name": "mistral-nemo-2407",
        "median": 855.6393896754043,
        "p2.5": 846.5677628723561,
        "p97.5": 864.6277256259796,
        "rank": 75,
        "rank_p2.5": 74,
        "rank_p97.5": 76,
        "total_output_tokens": 3305342,
        "conso_all_conv": 14.309508622013325,
        "n_match": 6251,
        "mean_conso_per_match": 0.0022891551147037796,
        "mean_conso_per_token": 4.329206666666664e-6,
        "mean_win_prob": 0.3085931771145354,
        "win_rate": 0.37482482802751554,
        "useful": 1413,
        "creative": 225,
        "complete": 661,
        "clear_formatting": 900,
        "incorrect": 638,
        "superficial": 1088,
        "instructions_not_followed": 292,
        "total_prefs": 5217,
        "positive_prefs_ratio": 0.6131876557408472
    },
    {
        "model_name": "mixtral-8x22b-instruct-v0.1",
        "median": 845.3760171414476,
        "p2.5": 835.5039183767558,
        "p97.5": 855.1316724540478,
        "rank": 76,
        "rank_p2.5": 75,
        "rank_p97.5": 77,
        "total_output_tokens": 3041056,
        "conso_all_conv": 53.644106197760216,
        "n_match": 5455,
        "mean_conso_per_match": 0.009833933308480333,
        "mean_conso_per_token": 0.000017639960000000072,
        "mean_win_prob": 0.29656979665067656,
        "win_rate": 0.3671035747021082,
        "useful": 1117,
        "creative": 203,
        "complete": 516,
        "clear_formatting": 702,
        "incorrect": 535,
        "superficial": 934,
        "instructions_not_followed": 276,
        "total_prefs": 4283,
        "positive_prefs_ratio": 0.5925752976885361
    },
    {
        "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
        "median": 823.6276623780707,
        "p2.5": 806.1437762575083,
        "p97.5": 839.4017431144941,
        "rank": 77,
        "rank_p2.5": 77,
        "rank_p97.5": 78,
        "total_output_tokens": 533384,
        "conso_all_conv": 1.9348629056266697,
        "n_match": 1796,
        "mean_conso_per_match": 0.0010773178761841146,
        "mean_conso_per_token": 3.627523333333339e-6,
        "mean_win_prob": 0.27197026019630677,
        "win_rate": 0.33173162583518934,
        "useful": 280,
        "creative": 37,
        "complete": 100,
        "clear_formatting": 199,
        "incorrect": 161,
        "superficial": 370,
        "instructions_not_followed": 50,
        "total_prefs": 1197,
        "positive_prefs_ratio": 0.5146198830409356
    },
    {
        "model_name": "Yi-1.5-9B-Chat",
        "median": 770.0984500948958,
        "p2.5": 671.1171245159724,
        "p97.5": 861.7321529755118,
        "rank": 78,
        "rank_p2.5": 75,
        "rank_p97.5": 80,
        "total_output_tokens": 47531,
        "conso_all_conv": 0.1857604957633333,
        "n_match": 65,
        "mean_conso_per_match": 0.0028578537809743586,
        "mean_conso_per_token": 3.908196666666666e-6,
        "mean_win_prob": 0.2168682956648327,
        "win_rate": 0.32599999999999996,
        "useful": 16,
        "creative": 5,
        "complete": 14,
        "clear_formatting": 19,
        "incorrect": 12,
        "superficial": 17,
        "instructions_not_followed": 14,
        "total_prefs": 97,
        "positive_prefs_ratio": 0.5567010309278351
    },
    {
        "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
        "median": 755.99099120546,
        "p2.5": 707.4318042167406,
        "p97.5": 797.5909622221349,
        "rank": 79,
        "rank_p2.5": 78,
        "rank_p97.5": 80,
        "total_output_tokens": 131187,
        "conso_all_conv": 0.6047563275600004,
        "n_match": 309,
        "mean_conso_per_match": 0.0019571402186407782,
        "mean_conso_per_token": 4.609880000000003e-6,
        "mean_win_prob": 0.20369010270635504,
        "win_rate": 0.24022653721682855,
        "useful": 83,
        "creative": 17,
        "complete": 39,
        "clear_formatting": 43,
        "incorrect": 52,
        "superficial": 107,
        "instructions_not_followed": 50,
        "total_prefs": 391,
        "positive_prefs_ratio": 0.46547314578005117
    },
    {
        "model_name": "qwen2-7b-instruct",
        "median": 730.7119659497538,
        "p2.5": 646.1694648395073,
        "p97.5": 807.4148246219326,
        "rank": 80,
        "rank_p2.5": 78,
        "rank_p97.5": 80,
        "total_output_tokens": 43550,
        "conso_all_conv": 0.15797864116666668,
        "n_match": 80,
        "mean_conso_per_match": 0.0019747330145833335,
        "mean_conso_per_token": 3.6275233333333336e-6,
        "mean_win_prob": 0.18149734163969775,
        "win_rate": 0.2525,
        "useful": 19,
        "creative": 4,
        "complete": 12,
        "clear_formatting": 22,
        "incorrect": 11,
        "superficial": 14,
        "instructions_not_followed": 7,
        "total_prefs": 89,
        "positive_prefs_ratio": 0.6404494382022472
    }
]