[
  {
    "model_name": "gemini-2.5-flash",
    "median": 1116.0567944919221,
    "p2.5": 1094.5703057166515,
    "p97.5": 1139.1473197545888,
    "rank": 4,
    "rank_p2.5": 3,
    "rank_p97.5": 6,
    "total_output_tokens": 617870.0,
    "conso_all_conv": 7.704056264666662,
    "n_match": 812,
    "mean_conso_per_match": 0.009487754020525447,
    "mean_conso_per_token": 0.000012468733333333326
  },
  {
    "model_name": "gemini-2.0-flash-exp",
    "median": 1103.732316309426,
    "p2.5": 1090.11100486507,
    "p97.5": 1119.1015121271535,
    "rank": 5,
    "rank_p2.5": 4,
    "rank_p97.5": 7,
    "total_output_tokens": 3936799.0,
    "conso_all_conv": 32.512579448033314,
    "n_match": 2816,
    "mean_conso_per_match": 0.01154566031535274,
    "mean_conso_per_token": 8.25863333333333e-6
  },
  {
    "model_name": "gemini-2.0-flash-001",
    "median": 1090.5742793733104,
    "p2.5": 1079.3762180000565,
    "p97.5": 1105.892630844956,
    "rank": 6,
    "rank_p2.5": 5,
    "rank_p97.5": 10,
    "total_output_tokens": 8359986.0,
    "conso_all_conv": 69.04205904580017,
    "n_match": 5903,
    "mean_conso_per_match": 0.011696096738234825,
    "mean_conso_per_token": 8.258633333333355e-6
  },
  {
    "model_name": "gemma-3-27b",
    "median": 1089.3951415163854,
    "p2.5": 1078.2443906264195,
    "p97.5": 1105.0543942445292,
    "rank": 7,
    "rank_p2.5": 5,
    "rank_p97.5": 10,
    "total_output_tokens": 6498374.0,
    "conso_all_conv": 41.81220623199337,
    "n_match": 4859,
    "mean_conso_per_match": 0.008605105213417034,
    "mean_conso_per_token": 6.434256666666672e-6
  },
  {
    "model_name": "deepseek-v3-0324",
    "median": 1085.7929057711044,
    "p2.5": 1074.191492865011,
    "p97.5": 1100.6481731866063,
    "rank": 8,
    "rank_p2.5": 6,
    "rank_p97.5": 11,
    "total_output_tokens": 4428337.0,
    "conso_all_conv": 208.2458243943796,
    "n_match": 4832,
    "mean_conso_per_match": 0.043097231869697765,
    "mean_conso_per_token": 0.000047025739999999914
  },
  {
    "model_name": "glm-4.5",
    "median": 1081.715725721778,
    "p2.5": 1034.035548998358,
    "p97.5": 1130.3560211789475,
    "rank": 9,
    "rank_p2.5": 4,
    "rank_p97.5": 23,
    "total_output_tokens": 122648.0,
    "conso_all_conv": 19.304065444399995,
    "n_match": 127,
    "mean_conso_per_match": 0.15200051531023617,
    "mean_conso_per_token": 0.00015739404999999995
  },
  {
    "model_name": "deepseek-v3-chat",
    "median": 1073.6307321621175,
    "p2.5": 1062.5926816958793,
    "p97.5": 1088.5721924923405,
    "rank": 10,
    "rank_p2.5": 9,
    "rank_p97.5": 14,
    "total_output_tokens": 5638296.0,
    "conso_all_conv": 265.14504173903896,
    "n_match": 5394,
    "mean_conso_per_match": 0.049155550934193354,
    "mean_conso_per_token": 0.00004702573999999981
  },
  {
    "model_name": "gpt-oss-120b",
    "median": 1072.6305276145147,
    "p2.5": 1052.7852768516648,
    "p97.5": 1096.6963610129858,
    "rank": 11,
    "rank_p2.5": 7,
    "rank_p97.5": 16,
    "total_output_tokens": 948315.0,
    "conso_all_conv": 3.17386805775,
    "n_match": 842,
    "mean_conso_per_match": 0.0037694394985154393,
    "mean_conso_per_token": 3.34685e-6
  },
  {
    "model_name": "gemma-3-12b",
    "median": 1065.7633554971526,
    "p2.5": 1054.8235170545374,
    "p97.5": 1081.7014986179631,
    "rank": 12,
    "rank_p2.5": 10,
    "rank_p97.5": 16,
    "total_output_tokens": 5733918.0,
    "conso_all_conv": 24.823316031720033,
    "n_match": 4522,
    "mean_conso_per_match": 0.005489455115373736,
    "mean_conso_per_token": 4.329206666666673e-6
  },
  {
    "model_name": "grok-3-mini-beta",
    "median": 1059.0176878742866,
    "p2.5": 1043.6023771179907,
    "p97.5": 1077.3458413154196,
    "rank": 14,
    "rank_p2.5": 11,
    "rank_p97.5": 19,
    "total_output_tokens": 3468359.0,
    "conso_all_conv": 213.0439515750001,
    "n_match": 1572,
    "mean_conso_per_match": 0.13552414222328252,
    "mean_conso_per_token": 0.00006142500000000003
  },
  {
    "model_name": "command-a",
    "median": 1056.9015862452275,
    "p2.5": 1045.811181581172,
    "p97.5": 1071.8516773126248,
    "rank": 15,
    "rank_p2.5": 12,
    "rank_p97.5": 19,
    "total_output_tokens": 4137991.0,
    "conso_all_conv": 75.40469272383653,
    "n_match": 4415,
    "mean_conso_per_match": 0.017079205599962975,
    "mean_conso_per_token": 0.000018222536666666635
  },
  {
    "model_name": "gemini-1.5-pro-001",
    "median": 1051.4164172033502,
    "p2.5": 1034.9970322802387,
    "p97.5": 1071.3260438563243,
    "rank": 16,
    "rank_p2.5": 12,
    "rank_p97.5": 23,
    "total_output_tokens": 1731076.0,
    "conso_all_conv": 229.93791835866662,
    "n_match": 2360,
    "mean_conso_per_match": 0.09743132133841806,
    "mean_conso_per_token": 0.0001328294762094019
  },
  {
    "model_name": "magistral-small-2506",
    "median": 1049.086473152375,
    "p2.5": 1030.648880433442,
    "p97.5": 1067.9544169227834,
    "rank": 17,
    "rank_p2.5": 12,
    "rank_p97.5": 24,
    "total_output_tokens": 334874.0,
    "conso_all_conv": 2.0136799642533343,
    "n_match": 1037,
    "mean_conso_per_match": 0.001941832173821923,
    "mean_conso_per_token": 6.013246666666669e-6
  },
  {
    "model_name": "claude-4-sonnet",
    "median": 1044.2002176625238,
    "p2.5": 1025.7432188407126,
    "p97.5": 1064.2146439841265,
    "rank": 18,
    "rank_p2.5": 14,
    "rank_p97.5": 25,
    "total_output_tokens": 385602.0,
    "conso_all_conv": 23.68560284999998,
    "n_match": 1092,
    "mean_conso_per_match": 0.021690112499999983,
    "mean_conso_per_token": 0.00006142499999999995
  },
  {
    "model_name": "claude-3-7-sonnet",
    "median": 1044.0264349650583,
    "p2.5": 1031.47779448328,
    "p97.5": 1059.9912997914494,
    "rank": 19,
    "rank_p2.5": 15,
    "rank_p97.5": 24,
    "total_output_tokens": 3546728.0,
    "conso_all_conv": 476.107446628,
    "n_match": 3909,
    "mean_conso_per_match": 0.12179776071322589,
    "mean_conso_per_token": 0.0001342385
  },
  {
    "model_name": "gemma-3-4b",
    "median": 1039.2127103407252,
    "p2.5": 1028.403915575931,
    "p97.5": 1054.4641978382153,
    "rank": 20,
    "rank_p2.5": 17,
    "rank_p97.5": 25,
    "total_output_tokens": 6678174.0,
    "conso_all_conv": 21.41365397331993,
    "n_match": 5382,
    "mean_conso_per_match": 0.003978753989840196,
    "mean_conso_per_token": 3.2065133333333232e-6
  },
  {
    "model_name": "llama-3.1-nemotron-70b-instruct",
    "median": 1038.5561002170753,
    "p2.5": 1028.003190710665,
    "p97.5": 1054.5097386130221,
    "rank": 21,
    "rank_p2.5": 18,
    "rank_p97.5": 25,
    "total_output_tokens": 7403970.0,
    "conso_all_conv": 92.31812753799991,
    "n_match": 6838,
    "mean_conso_per_match": 0.013500749859315576,
    "mean_conso_per_token": 0.000012468733333333321
  },
  {
    "model_name": "qwen3-32b",
    "median": 1035.4528058365167,
    "p2.5": 1015.0737759400776,
    "p97.5": 1057.2863473903074,
    "rank": 22,
    "rank_p2.5": 16,
    "rank_p97.5": 29,
    "total_output_tokens": 1412357.0,
    "conso_all_conv": 10.078494810579995,
    "n_match": 748,
    "mean_conso_per_match": 0.013473923543556143,
    "mean_conso_per_token": 7.135939999999997e-6
  },
  {
    "model_name": "kimi-k2",
    "median": 1031.778697112863,
    "p2.5": 984.7848114803654,
    "p97.5": 1078.4213235620362,
    "rank": 23,
    "rank_p2.5": 10,
    "rank_p97.5": 40,
    "total_output_tokens": 105815.0,
    "conso_all_conv": 6.040715928799998,
    "n_match": 154,
    "mean_conso_per_match": 0.039225428109090896,
    "mean_conso_per_token": 0.000057087519999999986
  },
  {
    "model_name": "gpt-4.1-mini",
    "median": 1028.3401654273396,
    "p2.5": 1018.5229077917741,
    "p97.5": 1045.1576482579699,
    "rank": 24,
    "rank_p2.5": 22,
    "rank_p97.5": 29,
    "total_output_tokens": 4581959.0,
    "conso_all_conv": 57.13122491526669,
    "n_match": 5695,
    "mean_conso_per_match": 0.01003182175860697,
    "mean_conso_per_token": 0.000012468733333333338
  },
  {
    "model_name": "deepseek-r1",
    "median": 1023.0178310662632,
    "p2.5": 1011.4375094557337,
    "p97.5": 1039.6317038939846,
    "rank": 26,
    "rank_p2.5": 23,
    "rank_p97.5": 30,
    "total_output_tokens": 3662507.0,
    "conso_all_conv": 172.23210193018016,
    "n_match": 3799,
    "mean_conso_per_match": 0.04533616792055282,
    "mean_conso_per_token": 0.00004702574000000004
  },
  {
    "model_name": "gpt-oss-20b",
    "median": 1019.7836076620567,
    "p2.5": 1000.6890829115487,
    "p97.5": 1040.729982971314,
    "rank": 28,
    "rank_p2.5": 22,
    "rank_p97.5": 35,
    "total_output_tokens": 537902.0,
    "conso_all_conv": 1.6493025613533343,
    "n_match": 1085,
    "mean_conso_per_match": 0.0015200945265929349,
    "mean_conso_per_token": 3.0661766666666685e-6
  },
  {
    "model_name": "gemini-1.5-pro-002",
    "median": 1015.2123024007701,
    "p2.5": 1003.5619419246377,
    "p97.5": 1031.0006052620015,
    "rank": 29,
    "rank_p2.5": 25,
    "rank_p97.5": 34,
    "total_output_tokens": 4303147.0,
    "conso_all_conv": 576.952753442532,
    "n_match": 5035,
    "mean_conso_per_match": 0.11458843166683852,
    "mean_conso_per_token": 0.00013407693333333302
  },
  {
    "model_name": "mistral-large-2411",
    "median": 1013.6087582533988,
    "p2.5": 1003.5656718979211,
    "p97.5": 1028.4618367705473,
    "rank": 31,
    "rank_p2.5": 26,
    "rank_p97.5": 33,
    "total_output_tokens": 7677266.0,
    "conso_all_conv": 152.82808421939282,
    "n_match": 8880,
    "mean_conso_per_match": 0.01721036984452622,
    "mean_conso_per_token": 0.0000199065766666666
  },
  {
    "model_name": "llama-4-scout",
    "median": 1006.4131402748285,
    "p2.5": 993.2397554497549,
    "p97.5": 1022.139889008972,
    "rank": 33,
    "rank_p2.5": 28,
    "rank_p97.5": 37,
    "total_output_tokens": 2450373.0,
    "conso_all_conv": 12.327557021970016,
    "n_match": 3273,
    "mean_conso_per_match": 0.003766439664518795,
    "mean_conso_per_token": 5.0308900000000065e-6
  },
  {
    "model_name": "gemma-3n-e4b-it",
    "median": 1004.2521853443941,
    "p2.5": 984.0957364683101,
    "p97.5": 1024.5289792214523,
    "rank": 34,
    "rank_p2.5": 27,
    "rank_p97.5": 40,
    "total_output_tokens": 360025.0,
    "conso_all_conv": 1.1544249628333325,
    "n_match": 1087,
    "mean_conso_per_match": 0.0010620284846672792,
    "mean_conso_per_token": 3.206513333333331e-6
  },
  {
    "model_name": "mistral-small-3.1-24b",
    "median": 1000.4611481408454,
    "p2.5": 989.3442003490856,
    "p97.5": 1016.2105052122631,
    "rank": 36,
    "rank_p2.5": 31,
    "rank_p97.5": 39,
    "total_output_tokens": 4470466.0,
    "conso_all_conv": 26.882014772946622,
    "n_match": 5104,
    "mean_conso_per_match": 0.005266852424166658,
    "mean_conso_per_token": 6.013246666666656e-6
  },
  {
    "model_name": "o4-mini",
    "median": 999.1557531677574,
    "p2.5": 985.4675638907465,
    "p97.5": 1016.0011463444325,
    "rank": 37,
    "rank_p2.5": 30,
    "rank_p97.5": 40,
    "total_output_tokens": 2288087.0,
    "conso_all_conv": 140.54574397499988,
    "n_match": 2088,
    "mean_conso_per_match": 0.06731118006465511,
    "mean_conso_per_token": 0.00006142499999999995
  },
  {
    "model_name": "gemma-2-27b-it-q8",
    "median": 998.1716928494227,
    "p2.5": 973.0562806394264,
    "p97.5": 1023.3890549574822,
    "rank": 38,
    "rank_p2.5": 27,
    "rank_p97.5": 45,
    "total_output_tokens": 449540.0,
    "conso_all_conv": 2.8924557419333334,
    "n_match": 762,
    "mean_conso_per_match": 0.0037958736770778655,
    "mean_conso_per_token": 6.434256666666667e-6
  },
  {
    "model_name": "mistral-saba",
    "median": 989.1087255782224,
    "p2.5": 976.8045704331498,
    "p97.5": 1005.4036057775389,
    "rank": 39,
    "rank_p2.5": 35,
    "rank_p97.5": 43,
    "total_output_tokens": 3043927.0,
    "conso_all_conv": 18.303883886326705,
    "n_match": 3572,
    "mean_conso_per_match": 0.005124267605354621,
    "mean_conso_per_token": 6.013246666666679e-6
  },
  {
    "model_name": "aya-expanse-32b",
    "median": 985.1941050805667,
    "p2.5": 973.0490423302962,
    "p97.5": 1001.2134477786732,
    "rank": 40,
    "rank_p2.5": 36,
    "rank_p97.5": 45,
    "total_output_tokens": 2836974.0,
    "conso_all_conv": 20.24447624556002,
    "n_match": 3552,
    "mean_conso_per_match": 0.005699458402466222,
    "mean_conso_per_token": 7.135940000000007e-6
  },
  {
    "model_name": "qwen3-30b-a3b",
    "median": 982.6556255021542,
    "p2.5": 955.537491437331,
    "p97.5": 1011.9940648797602,
    "rank": 41,
    "rank_p2.5": 31,
    "rank_p97.5": 53,
    "total_output_tokens": 134107.0,
    "conso_all_conv": 0.4111957542366667,
    "n_match": 416,
    "mean_conso_per_match": 0.0009884513322996795,
    "mean_conso_per_token": 3.066176666666667e-6
  },
  {
    "model_name": "mistral-small-24b-instruct-2501",
    "median": 980.3247563978589,
    "p2.5": 968.3830600873669,
    "p97.5": 996.7751422537467,
    "rank": 42,
    "rank_p2.5": 38,
    "rank_p97.5": 47,
    "total_output_tokens": 2818040.0,
    "conso_all_conv": 16.945569636533293,
    "n_match": 3319,
    "mean_conso_per_match": 0.005105625078798822,
    "mean_conso_per_token": 6.013246666666652e-6
  },
  {
    "model_name": "o3-mini",
    "median": 977.7982030330671,
    "p2.5": 961.8808520117818,
    "p97.5": 996.2999007483538,
    "rank": 43,
    "rank_p2.5": 37,
    "rank_p97.5": 50,
    "total_output_tokens": 1655945.0,
    "conso_all_conv": 101.71642162499992,
    "n_match": 1620,
    "mean_conso_per_match": 0.06278791458333328,
    "mean_conso_per_token": 0.00006142499999999995
  },
  {
    "model_name": "llama-3.3-70b",
    "median": 976.6580923837242,
    "p2.5": 966.7950467734502,
    "p97.5": 992.3328441415223,
    "rank": 44,
    "rank_p2.5": 40,
    "rank_p97.5": 48,
    "total_output_tokens": 5635427.0,
    "conso_all_conv": 70.26663648246686,
    "n_match": 6738,
    "mean_conso_per_match": 0.01042841146964483,
    "mean_conso_per_token": 0.000012468733333333369
  },
  {
    "model_name": "gpt-4o-mini-2024-07-18",
    "median": 976.0478222203029,
    "p2.5": 965.844023655209,
    "p97.5": 992.082132605765,
    "rank": 45,
    "rank_p2.5": 40,
    "rank_p97.5": 48,
    "total_output_tokens": 5207480.0,
    "conso_all_conv": 39.344663175950004,
    "n_match": 6994,
    "mean_conso_per_match": 0.00562548801486274,
    "mean_conso_per_token": 7.555413208682511e-6
  },
  {
    "model_name": "gpt-4.1-nano",
    "median": 969.7475262088315,
    "p2.5": 958.4116448025771,
    "p97.5": 984.1062616983318,
    "rank": 46,
    "rank_p2.5": 42,
    "rank_p97.5": 52,
    "total_output_tokens": 2762342.0,
    "conso_all_conv": 20.874880376900077,
    "n_match": 4300,
    "mean_conso_per_match": 0.004854623343465134,
    "mean_conso_per_token": 7.556950000000028e-6
  },
  {
    "model_name": "llama-3.1-70b",
    "median": 967.5717508952348,
    "p2.5": 955.8697460834545,
    "p97.5": 983.5052671591003,
    "rank": 48,
    "rank_p2.5": 44,
    "rank_p97.5": 52,
    "total_output_tokens": 4057254.0,
    "conso_all_conv": 50.5888181916,
    "n_match": 5584,
    "mean_conso_per_match": 0.009059602111676217,
    "mean_conso_per_token": 0.000012468733333333333
  },
  {
    "model_name": "jamba-1.5-large",
    "median": 967.4601711731407,
    "p2.5": 920.4348687785879,
    "p97.5": 1008.9531375871454,
    "rank": 49,
    "rank_p2.5": 33,
    "rank_p97.5": 62,
    "total_output_tokens": 143976.0,
    "conso_all_conv": 6.840363109439998,
    "n_match": 172,
    "mean_conso_per_match": 0.03976955296186046,
    "mean_conso_per_token": 0.00004751043999999999
  },
  {
    "model_name": "aya-expanse-8b",
    "median": 967.11928977077,
    "p2.5": 949.1691899841665,
    "p97.5": 987.5854276235409,
    "rank": 50,
    "rank_p2.5": 41,
    "rank_p97.5": 55,
    "total_output_tokens": 1057810.0,
    "conso_all_conv": 3.9856799865999943,
    "n_match": 1302,
    "mean_conso_per_match": 0.0030611981463901645,
    "mean_conso_per_token": 3.7678599999999946e-6
  },
  {
    "model_name": "claude-3-5-sonnet-v2",
    "median": 965.4104845270176,
    "p2.5": 953.9564010430701,
    "p97.5": 980.7194675198482,
    "rank": 51,
    "rank_p2.5": 45,
    "rank_p97.5": 53,
    "total_output_tokens": 3224219.0,
    "conso_all_conv": 432.8143222314999,
    "n_match": 5685,
    "mean_conso_per_match": 0.0761326864083553,
    "mean_conso_per_token": 0.00013423849999999997
  },
  {
    "model_name": "phi-4",
    "median": 961.2418112911515,
    "p2.5": 951.4331275513211,
    "p97.5": 976.3107307194541,
    "rank": 52,
    "rank_p2.5": 47,
    "rank_p97.5": 54,
    "total_output_tokens": 6547451.0,
    "conso_all_conv": 30.182963415880106,
    "n_match": 8003,
    "mean_conso_per_match": 0.003771456130935912,
    "mean_conso_per_token": 4.609880000000016e-6
  },
  {
    "model_name": "qwen2.5-32b-instruct",
    "median": 958.163425749537,
    "p2.5": 901.3907804431892,
    "p97.5": 1016.7922422050394,
    "rank": 53,
    "rank_p2.5": 28,
    "rank_p97.5": 64,
    "total_output_tokens": 75812.0,
    "conso_all_conv": 0.5310851985600001,
    "n_match": 142,
    "mean_conso_per_match": 0.0037400366095774657,
    "mean_conso_per_token": 7.005292019205404e-6
  },
  {
    "model_name": "ministral-8b-instruct-2410",
    "median": 957.9416646707011,
    "p2.5": 947.9232536166567,
    "p97.5": 972.1135924200302,
    "rank": 54,
    "rank_p2.5": 49,
    "rank_p97.5": 56,
    "total_output_tokens": 7488476.0,
    "conso_all_conv": 28.215529181359937,
    "n_match": 9388,
    "mean_conso_per_match": 0.0030054888348274326,
    "mean_conso_per_token": 3.7678599999999917e-6
  },
  {
    "model_name": "gpt-4o-2024-08-06",
    "median": 955.9089314068067,
    "p2.5": 944.6202414310749,
    "p97.5": 970.5323551265548,
    "rank": 55,
    "rank_p2.5": 49,
    "rank_p97.5": 57,
    "total_output_tokens": 3903917.0,
    "conso_all_conv": 239.79810172500032,
    "n_match": 5900,
    "mean_conso_per_match": 0.0406437460550848,
    "mean_conso_per_token": 0.00006142500000000008
  },
  {
    "model_name": "llama-3.1-405b",
    "median": 952.3264126369595,
    "p2.5": 943.1480873808299,
    "p97.5": 967.4191006355786,
    "rank": 56,
    "rank_p2.5": 52,
    "rank_p97.5": 58,
    "total_output_tokens": 10356576.0,
    "conso_all_conv": 2464.0993918144145,
    "n_match": 10102,
    "mean_conso_per_match": 0.2439219354399539,
    "mean_conso_per_token": 0.00023792606666666807
  },
  {
    "model_name": "gemma-2-9b-it",
    "median": 947.8343111759082,
    "p2.5": 936.4982727443894,
    "p97.5": 964.0412699667112,
    "rank": 57,
    "rank_p2.5": 53,
    "rank_p97.5": 59,
    "total_output_tokens": 3108609.0,
    "conso_all_conv": 12.132379056593328,
    "n_match": 5116,
    "mean_conso_per_match": 0.00237145798604248,
    "mean_conso_per_token": 3.902832120923965e-6
  },
  {
    "model_name": "deepseek-r1-distill-llama-70b",
    "median": 946.8330435571322,
    "p2.5": 933.0279200402888,
    "p97.5": 963.9875736162766,
    "rank": 58,
    "rank_p2.5": 52,
    "rank_p97.5": 60,
    "total_output_tokens": 2339110.0,
    "conso_all_conv": 29.16573882733335,
    "n_match": 2559,
    "mean_conso_per_match": 0.011397318807086108,
    "mean_conso_per_token": 0.00001246873333333334
  },
  {
    "model_name": "qwq-32b",
    "median": 946.0964885012581,
    "p2.5": 930.2059386581877,
    "p97.5": 965.8910820798208,
    "rank": 59,
    "rank_p2.5": 51,
    "rank_p97.5": 60,
    "total_output_tokens": 1895013.0,
    "conso_all_conv": 13.522699067219992,
    "n_match": 1596,
    "mean_conso_per_match": 0.008472869089736837,
    "mean_conso_per_token": 7.135939999999996e-6
  },
  {
    "model_name": "hermes-3-llama-3.1-405b",
    "median": 935.9965971654875,
    "p2.5": 926.3894150257095,
    "p97.5": 951.7006406567779,
    "rank": 60,
    "rank_p2.5": 57,
    "rank_p97.5": 61,
    "total_output_tokens": 4093318.0,
    "conso_all_conv": 973.907051355867,
    "n_match": 6782,
    "mean_conso_per_match": 0.1436017474721125,
    "mean_conso_per_token": 0.00023792606666666675
  },
  {
    "model_name": "c4ai-command-r-08-2024",
    "median": 923.3945643377816,
    "p2.5": 912.8666371378592,
    "p97.5": 937.8761715344359,
    "rank": 61,
    "rank_p2.5": 59,
    "rank_p97.5": 64,
    "total_output_tokens": 4319206.0,
    "conso_all_conv": 32.640023781700016,
    "n_match": 5999,
    "mean_conso_per_match": 0.0054409107820803495,
    "mean_conso_per_token": 7.556950000000004e-6
  },
  {
    "model_name": "qwen2.5-7b-instruct",
    "median": 922.4252205771093,
    "p2.5": 902.0074536778437,
    "p97.5": 943.2699526755739,
    "rank": 62,
    "rank_p2.5": 58,
    "rank_p97.5": 65,
    "total_output_tokens": 1186919.0,
    "conso_all_conv": 4.305576367276672,
    "n_match": 1420,
    "mean_conso_per_match": 0.0030320960332934307,
    "mean_conso_per_token": 3.6275233333333374e-6
  },
  {
    "model_name": "qwen2.5-coder-32b-instruct",
    "median": 919.4958479594295,
    "p2.5": 909.6053608726511,
    "p97.5": 934.1445483171677,
    "rank": 63,
    "rank_p2.5": 60,
    "rank_p97.5": 64,
    "total_output_tokens": 6149486.0,
    "conso_all_conv": 43.882363126840055,
    "n_match": 7330,
    "mean_conso_per_match": 0.005986679826308329,
    "mean_conso_per_token": 7.135940000000009e-6
  },
  {
    "model_name": "llama-3.1-8b",
    "median": 918.6258134867364,
    "p2.5": 908.1594499605558,
    "p97.5": 933.613620453441,
    "rank": 64,
    "rank_p2.5": 60,
    "rank_p97.5": 65,
    "total_output_tokens": 5670436.0,
    "conso_all_conv": 21.36540898695992,
    "n_match": 8257,
    "mean_conso_per_match": 0.0025875510460167035,
    "mean_conso_per_token": 3.767859999999986e-6
  },
  {
    "model_name": "mixtral-8x7b-instruct-v0.1",
    "median": 880.603158770764,
    "p2.5": 865.2399188353081,
    "p97.5": 898.0910204992772,
    "rank": 65,
    "rank_p2.5": 64,
    "rank_p97.5": 67,
    "total_output_tokens": 1575791.0,
    "conso_all_conv": 7.264207415080013,
    "n_match": 2560,
    "mean_conso_per_match": 0.00283758102151563,
    "mean_conso_per_token": 4.6098800000000085e-6
  },
  {
    "model_name": "lfm-40b",
    "median": 876.4066548757382,
    "p2.5": 863.5118775443749,
    "p97.5": 893.3302864610661,
    "rank": 67,
    "rank_p2.5": 65,
    "rank_p97.5": 67,
    "total_output_tokens": 2000824.0,
    "conso_all_conv": 16.524071780533337,
    "n_match": 3578,
    "mean_conso_per_match": 0.004618242532289921,
    "mean_conso_per_token": 8.258633333333334e-6
  },
  {
    "model_name": "phi-3.5-mini-instruct",
    "median": 857.816167952481,
    "p2.5": 841.4545453215026,
    "p97.5": 875.369065283321,
    "rank": 68,
    "rank_p2.5": 67,
    "rank_p97.5": 69,
    "total_output_tokens": 2149046.0,
    "conso_all_conv": 6.589354700793325,
    "n_match": 2535,
    "mean_conso_per_match": 0.0025993509667823766,
    "mean_conso_per_token": 3.0661766666666626e-6
  },
  {
    "model_name": "mistral-nemo-2407",
    "median": 846.9437102294826,
    "p2.5": 834.4372534698823,
    "p97.5": 861.8854426609249,
    "rank": 69,
    "rank_p2.5": 67,
    "rank_p97.5": 70,
    "total_output_tokens": 3305342.0,
    "conso_all_conv": 14.309508622013325,
    "n_match": 6253,
    "mean_conso_per_match": 0.002288422936512606,
    "mean_conso_per_token": 4.329206666666664e-6
  },
  {
    "model_name": "mixtral-8x22b-instruct-v0.1",
    "median": 835.9053287744059,
    "p2.5": 823.8875598125117,
    "p97.5": 851.3557821031085,
    "rank": 70,
    "rank_p2.5": 69,
    "rank_p97.5": 71,
    "total_output_tokens": 3041056.0,
    "conso_all_conv": 53.62491392128014,
    "n_match": 5458,
    "mean_conso_per_match": 0.009825011711484085,
    "mean_conso_per_token": 0.0000176336489434197
  },
  {
    "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
    "median": 812.1079530157476,
    "p2.5": 794.5977064723941,
    "p97.5": 832.4503981800221,
    "rank": 71,
    "rank_p2.5": 70,
    "rank_p97.5": 72,
    "total_output_tokens": 533384.0,
    "conso_all_conv": 1.9348629056266673,
    "n_match": 1796,
    "mean_conso_per_match": 0.0010773178761841133,
    "mean_conso_per_token": 3.6275233333333344e-6
  },
  {
    "model_name": "Yi-1.5-9B-Chat",
    "median": 763.4534684441455,
    "p2.5": 666.1534497904763,
    "p97.5": 852.5946603824437,
    "rank": 72,
    "rank_p2.5": 69,
    "rank_p97.5": 74,
    "total_output_tokens": 47531.0,
    "conso_all_conv": 0.18274336793666665,
    "n_match": 65,
    "mean_conso_per_match": 0.0028114364297948716,
    "mean_conso_per_token": 3.844719613234871e-6
  },
  {
    "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
    "median": 746.0280953571157,
    "p2.5": 698.1165879190097,
    "p97.5": 786.3676656698326,
    "rank": 73,
    "rank_p2.5": 72,
    "rank_p97.5": 74,
    "total_output_tokens": 131187.0,
    "conso_all_conv": 0.6022485528400002,
    "n_match": 309,
    "mean_conso_per_match": 0.0019490244428478972,
    "mean_conso_per_token": 4.59076396929574e-6
  },
  {
    "model_name": "qwen2-7b-instruct",
    "median": 720.7963754119712,
    "p2.5": 631.6453822521042,
    "p97.5": 804.6623109006914,
    "rank": 74,
    "rank_p2.5": 72,
    "rank_p97.5": 74,
    "total_output_tokens": 43550.0,
    "conso_all_conv": 0.15307785714333336,
    "n_match": 80,
    "mean_conso_per_match": 0.001913473214291667,
    "mean_conso_per_token": 3.5149909791810184e-6
  }
]
