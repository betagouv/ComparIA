{
  "timestamp": 1764131045.0,
  "models": [
    {
      "model_name": "kimi-k2-thinking",
      "median": 1171.2615041660483,
      "p2.5": 1064.7196240761218,
      "p97.5": 1324.1742037181002,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 22,
      "total_output_tokens": 31476,
      "conso_all_conv": 0.0,
      "n_match": 9,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.7183311520242905,
      "win_rate": 0.375,
      "useful": 6,
      "creative": 1,
      "complete": 4,
      "clear_formatting": 3,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 14,
      "positive_prefs_ratio": 1.0
    },
    {
      "model_name": "mistral-medium-3.1",
      "median": 1146.1350261594516,
      "p2.5": 1131.098951953418,
      "p97.5": 1162.6287982418517,
      "rank": 2,
      "rank_p2.5": 1,
      "rank_p97.5": 3,
      "total_output_tokens": 3331983,
      "conso_all_conv": 0.0,
      "n_match": 1852,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.688882223390123,
      "win_rate": 0.6171756756756758,
      "useful": 106,
      "creative": 43,
      "complete": 165,
      "clear_formatting": 114,
      "incorrect": 35,
      "superficial": 13,
      "instructions_not_followed": 9,
      "total_prefs": 485,
      "positive_prefs_ratio": 0.8824742268041237
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1112.8694388575564,
      "p2.5": 1098.4241713440383,
      "p97.5": 1127.925823190354,
      "rank": 3,
      "rank_p2.5": 2,
      "rank_p97.5": 8,
      "total_output_tokens": 3445940,
      "conso_all_conv": 0.0,
      "n_match": 2452,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6476499702395127,
      "win_rate": 0.5896164830681355,
      "useful": 199,
      "creative": 71,
      "complete": 262,
      "clear_formatting": 183,
      "incorrect": 57,
      "superficial": 36,
      "instructions_not_followed": 12,
      "total_prefs": 820,
      "positive_prefs_ratio": 0.8719512195121951
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1111.2912468158215,
      "p2.5": 1093.5233440706527,
      "p97.5": 1131.7171018629217,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 9,
      "total_output_tokens": 1337355,
      "conso_all_conv": 0.0,
      "n_match": 1230,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.645637149235712,
      "win_rate": 0.6010089503661514,
      "useful": 68,
      "creative": 21,
      "complete": 90,
      "clear_formatting": 71,
      "incorrect": 34,
      "superficial": 11,
      "instructions_not_followed": 8,
      "total_prefs": 303,
      "positive_prefs_ratio": 0.8250825082508251
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1107.1821924434385,
      "p2.5": 1096.798430875877,
      "p97.5": 1119.6627545759268,
      "rank": 5,
      "rank_p2.5": 3,
      "rank_p97.5": 8,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6403745299223749,
      "win_rate": 0.6317699216950714,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1099.873390505905,
      "p2.5": 1087.5641935481538,
      "p97.5": 1113.8859480092767,
      "rank": 6,
      "rank_p2.5": 5,
      "rank_p97.5": 11,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6309382578283439,
      "win_rate": 0.5851128848346636,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1095.303566317686,
      "p2.5": 1072.317925158677,
      "p97.5": 1116.1477231201259,
      "rank": 7,
      "rank_p2.5": 4,
      "rank_p97.5": 16,
      "total_output_tokens": 2098702,
      "conso_all_conv": 0.0,
      "n_match": 890,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6249913017941814,
      "win_rate": 0.571354401805869,
      "useful": 106,
      "creative": 49,
      "complete": 172,
      "clear_formatting": 115,
      "incorrect": 47,
      "superficial": 24,
      "instructions_not_followed": 23,
      "total_prefs": 536,
      "positive_prefs_ratio": 0.8246268656716418
    },
    {
      "model_name": "grok-4.1-fast",
      "median": 1093.6919747693842,
      "p2.5": 1002.4121783518382,
      "p97.5": 1186.8700413984432,
      "rank": 8,
      "rank_p2.5": 1,
      "rank_p97.5": 48,
      "total_output_tokens": 44307,
      "conso_all_conv": 0.0,
      "n_match": 38,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.622885796472421,
      "win_rate": 0.6108333333333333,
      "useful": 1,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 0,
      "instructions_not_followed": 0,
      "total_prefs": 1,
      "positive_prefs_ratio": 1.0
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1093.6404085499528,
      "p2.5": 1083.185836056576,
      "p97.5": 1105.996590508301,
      "rank": 9,
      "rank_p2.5": 6,
      "rank_p97.5": 13,
      "total_output_tokens": 8629588,
      "conso_all_conv": 0.0,
      "n_match": 6298,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6228183568336086,
      "win_rate": 0.5808939345824071,
      "useful": 1224,
      "creative": 413,
      "complete": 1404,
      "clear_formatting": 1038,
      "incorrect": 309,
      "superficial": 212,
      "instructions_not_followed": 101,
      "total_prefs": 4701,
      "positive_prefs_ratio": 0.8676877260157413
    },
    {
      "model_name": "magistral-medium",
      "median": 1091.4004651923879,
      "p2.5": 1076.4509980455352,
      "p97.5": 1107.9036469040873,
      "rank": 10,
      "rank_p2.5": 6,
      "rank_p97.5": 16,
      "total_output_tokens": 1655059,
      "conso_all_conv": 0.0,
      "n_match": 1757,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6198847888217095,
      "win_rate": 0.5766533864541833,
      "useful": 103,
      "creative": 31,
      "complete": 111,
      "clear_formatting": 70,
      "incorrect": 37,
      "superficial": 19,
      "instructions_not_followed": 3,
      "total_prefs": 374,
      "positive_prefs_ratio": 0.8422459893048129
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1090.4644909634503,
      "p2.5": 1073.5260046891078,
      "p97.5": 1109.9300930466582,
      "rank": 11,
      "rank_p2.5": 6,
      "rank_p97.5": 17,
      "total_output_tokens": 1152761,
      "conso_all_conv": 0.0,
      "n_match": 1235,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6186566242626815,
      "win_rate": 0.5665990259740259,
      "useful": 117,
      "creative": 14,
      "complete": 78,
      "clear_formatting": 75,
      "incorrect": 29,
      "superficial": 12,
      "instructions_not_followed": 8,
      "total_prefs": 333,
      "positive_prefs_ratio": 0.8528528528528528
    },
    {
      "model_name": "gpt-5.1",
      "median": 1087.7529160090285,
      "p2.5": 1052.3641012581113,
      "p97.5": 1120.2904816339037,
      "rank": 12,
      "rank_p2.5": 3,
      "rank_p97.5": 26,
      "total_output_tokens": 354428,
      "conso_all_conv": 0.0,
      "n_match": 298,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6150908817603039,
      "win_rate": 0.593030303030303,
      "useful": 12,
      "creative": 3,
      "complete": 18,
      "clear_formatting": 17,
      "incorrect": 2,
      "superficial": 3,
      "instructions_not_followed": 1,
      "total_prefs": 56,
      "positive_prefs_ratio": 0.8928571428571429
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1085.7433061494946,
      "p2.5": 1074.0220889630693,
      "p97.5": 1099.326260124126,
      "rank": 13,
      "rank_p2.5": 9,
      "rank_p97.5": 17,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6124410175031708,
      "win_rate": 0.6141481069042317,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1085.0651226160564,
      "p2.5": 1066.7309641585614,
      "p97.5": 1105.8179868901173,
      "rank": 14,
      "rank_p2.5": 6,
      "rank_p97.5": 21,
      "total_output_tokens": 1290379,
      "conso_all_conv": 0.0,
      "n_match": 856,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6115454094181124,
      "win_rate": 0.5063732394366197,
      "useful": 63,
      "creative": 26,
      "complete": 66,
      "clear_formatting": 49,
      "incorrect": 21,
      "superficial": 14,
      "instructions_not_followed": 9,
      "total_prefs": 248,
      "positive_prefs_ratio": 0.8225806451612904
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1078.468668476035,
      "p2.5": 1055.2753738387323,
      "p97.5": 1102.016321196825,
      "rank": 15,
      "rank_p2.5": 8,
      "rank_p97.5": 25,
      "total_output_tokens": 1052625,
      "conso_all_conv": 0.0,
      "n_match": 608,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6027996712751523,
      "win_rate": 0.5484297520661157,
      "useful": 29,
      "creative": 14,
      "complete": 32,
      "clear_formatting": 19,
      "incorrect": 11,
      "superficial": 8,
      "instructions_not_followed": 3,
      "total_prefs": 116,
      "positive_prefs_ratio": 0.8103448275862069
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1076.0409557642952,
      "p2.5": 1062.3374709710922,
      "p97.5": 1092.6974264649498,
      "rank": 16,
      "rank_p2.5": 11,
      "rank_p97.5": 22,
      "total_output_tokens": 2650890,
      "conso_all_conv": 0.0,
      "n_match": 2480,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5995658277507352,
      "win_rate": 0.5426432606941082,
      "useful": 183,
      "creative": 43,
      "complete": 132,
      "clear_formatting": 123,
      "incorrect": 36,
      "superficial": 32,
      "instructions_not_followed": 20,
      "total_prefs": 569,
      "positive_prefs_ratio": 0.8453427065026362
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1073.004696756823,
      "p2.5": 1062.8402684973353,
      "p97.5": 1085.2031995886227,
      "rank": 17,
      "rank_p2.5": 14,
      "rank_p97.5": 23,
      "total_output_tokens": 7822076,
      "conso_all_conv": 0.0,
      "n_match": 5902,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5955105011924244,
      "win_rate": 0.5664689935615046,
      "useful": 1016,
      "creative": 305,
      "complete": 1191,
      "clear_formatting": 865,
      "incorrect": 329,
      "superficial": 254,
      "instructions_not_followed": 138,
      "total_prefs": 4098,
      "positive_prefs_ratio": 0.8240605173255247
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1071.2539671089248,
      "p2.5": 1054.517877579282,
      "p97.5": 1089.1704716419515,
      "rank": 18,
      "rank_p2.5": 12,
      "rank_p97.5": 26,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.593166859330881,
      "win_rate": 0.5305338541666667,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "glm-4.5",
      "median": 1070.3869753516105,
      "p2.5": 1052.6533626757548,
      "p97.5": 1090.658231249636,
      "rank": 19,
      "rank_p2.5": 12,
      "rank_p97.5": 26,
      "total_output_tokens": 2668283,
      "conso_all_conv": 0.0,
      "n_match": 1126,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5920048474152976,
      "win_rate": 0.550088888888889,
      "useful": 98,
      "creative": 32,
      "complete": 117,
      "clear_formatting": 86,
      "incorrect": 22,
      "superficial": 15,
      "instructions_not_followed": 10,
      "total_prefs": 380,
      "positive_prefs_ratio": 0.8763157894736842
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1070.3134337570561,
      "p2.5": 1056.2376703977698,
      "p97.5": 1085.907145155146,
      "rank": 20,
      "rank_p2.5": 14,
      "rank_p97.5": 25,
      "total_output_tokens": 1853912,
      "conso_all_conv": 0.0,
      "n_match": 2540,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5919062389693509,
      "win_rate": 0.5356400157542339,
      "useful": 234,
      "creative": 62,
      "complete": 198,
      "clear_formatting": 215,
      "incorrect": 41,
      "superficial": 64,
      "instructions_not_followed": 20,
      "total_prefs": 834,
      "positive_prefs_ratio": 0.8501199040767387
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1066.6578528581217,
      "p2.5": 1052.9555494541944,
      "p97.5": 1082.4502622285054,
      "rank": 21,
      "rank_p2.5": 15,
      "rank_p97.5": 26,
      "total_output_tokens": 1759206,
      "conso_all_conv": 0.0,
      "n_match": 2155,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.586996535071098,
      "win_rate": 0.5415313225058005,
      "useful": 119,
      "creative": 33,
      "complete": 121,
      "clear_formatting": 91,
      "incorrect": 63,
      "superficial": 31,
      "instructions_not_followed": 8,
      "total_prefs": 466,
      "positive_prefs_ratio": 0.7811158798283262
    },
    {
      "model_name": "command-a",
      "median": 1065.9048595950362,
      "p2.5": 1055.7360783788035,
      "p97.5": 1078.3559629828353,
      "rank": 22,
      "rank_p2.5": 17,
      "rank_p97.5": 26,
      "total_output_tokens": 5798886,
      "conso_all_conv": 0.0,
      "n_match": 5862,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5859832823958055,
      "win_rate": 0.5582019788468099,
      "useful": 1133,
      "creative": 262,
      "complete": 1046,
      "clear_formatting": 985,
      "incorrect": 264,
      "superficial": 283,
      "instructions_not_followed": 99,
      "total_prefs": 4072,
      "positive_prefs_ratio": 0.8413555992141454
    },
    {
      "model_name": "grok-4-fast",
      "median": 1064.2971350672412,
      "p2.5": 1049.1189288689777,
      "p97.5": 1081.8401588780127,
      "rank": 23,
      "rank_p2.5": 15,
      "rank_p97.5": 28,
      "total_output_tokens": 2551269,
      "conso_all_conv": 0.0,
      "n_match": 1834,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5838177386045795,
      "win_rate": 0.5259443231441048,
      "useful": 131,
      "creative": 20,
      "complete": 93,
      "clear_formatting": 66,
      "incorrect": 32,
      "superficial": 21,
      "instructions_not_followed": 7,
      "total_prefs": 370,
      "positive_prefs_ratio": 0.8378378378378378
    },
    {
      "model_name": "kimi-k2",
      "median": 1058.936549079387,
      "p2.5": 1034.4300000833887,
      "p97.5": 1084.8063207750802,
      "rank": 24,
      "rank_p2.5": 15,
      "rank_p97.5": 35,
      "total_output_tokens": 898110,
      "conso_all_conv": 0.0,
      "n_match": 522,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5765770923281459,
      "win_rate": 0.49446808510638296,
      "useful": 114,
      "creative": 22,
      "complete": 55,
      "clear_formatting": 42,
      "incorrect": 21,
      "superficial": 25,
      "instructions_not_followed": 2,
      "total_prefs": 281,
      "positive_prefs_ratio": 0.8291814946619217
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1057.1674869144554,
      "p2.5": 1045.1814891939812,
      "p97.5": 1070.0760576993296,
      "rank": 25,
      "rank_p2.5": 21,
      "rank_p97.5": 30,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5741811262809435,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1055.760505787221,
      "p2.5": 1041.8524035978585,
      "p97.5": 1071.1599599267938,
      "rank": 26,
      "rank_p2.5": 21,
      "rank_p97.5": 31,
      "total_output_tokens": 2013064,
      "conso_all_conv": 0.0,
      "n_match": 2553,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5722733822878725,
      "win_rate": 0.5179310344827586,
      "useful": 198,
      "creative": 44,
      "complete": 161,
      "clear_formatting": 139,
      "incorrect": 71,
      "superficial": 64,
      "instructions_not_followed": 24,
      "total_prefs": 701,
      "positive_prefs_ratio": 0.7731811697574893
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1051.6930001104474,
      "p2.5": 1041.5754473343918,
      "p97.5": 1064.6194697257447,
      "rank": 27,
      "rank_p2.5": 24,
      "rank_p97.5": 31,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5667478919541155,
      "win_rate": 0.5452863107664778,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1047.7687546542802,
      "p2.5": 1037.3816262465928,
      "p97.5": 1060.300688605221,
      "rank": 28,
      "rank_p2.5": 26,
      "rank_p97.5": 33,
      "total_output_tokens": 8684892,
      "conso_all_conv": 0.0,
      "n_match": 6937,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.56140342861683,
      "win_rate": 0.5260083609629523,
      "useful": 1092,
      "creative": 333,
      "complete": 1210,
      "clear_formatting": 964,
      "incorrect": 539,
      "superficial": 242,
      "instructions_not_followed": 187,
      "total_prefs": 4567,
      "positive_prefs_ratio": 0.7880446682723888
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1046.2402254872036,
      "p2.5": 1012.7279424650758,
      "p97.5": 1077.2850493239011,
      "rank": 29,
      "rank_p2.5": 17,
      "rank_p97.5": 43,
      "total_output_tokens": 426314,
      "conso_all_conv": 0.0,
      "n_match": 311,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5593183537288451,
      "win_rate": 0.548474025974026,
      "useful": 21,
      "creative": 9,
      "complete": 17,
      "clear_formatting": 17,
      "incorrect": 12,
      "superficial": 8,
      "instructions_not_followed": 3,
      "total_prefs": 87,
      "positive_prefs_ratio": 0.735632183908046
    },
    {
      "model_name": "lfm2-8b-a1b",
      "median": 1043.4606797508952,
      "p2.5": 928.8771503857732,
      "p97.5": 1164.9558416157959,
      "rank": 30,
      "rank_p2.5": 2,
      "rank_p97.5": 76,
      "total_output_tokens": 27688,
      "conso_all_conv": 0.0,
      "n_match": 28,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5555222513089504,
      "win_rate": 0.4782608695652174,
      "useful": 2,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 0,
      "superficial": 1,
      "instructions_not_followed": 0,
      "total_prefs": 3,
      "positive_prefs_ratio": 0.6666666666666666
    },
    {
      "model_name": "qwen3-32b",
      "median": 1043.3786330264263,
      "p2.5": 1025.3798851379297,
      "p97.5": 1062.1729093425972,
      "rank": 31,
      "rank_p2.5": 25,
      "rank_p97.5": 38,
      "total_output_tokens": 2137463,
      "conso_all_conv": 0.0,
      "n_match": 1016,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5554101127307471,
      "win_rate": 0.546304347826087,
      "useful": 270,
      "creative": 65,
      "complete": 207,
      "clear_formatting": 175,
      "incorrect": 89,
      "superficial": 70,
      "instructions_not_followed": 38,
      "total_prefs": 914,
      "positive_prefs_ratio": 0.7844638949671773
    },
    {
      "model_name": "glm-4.6",
      "median": 1040.2413450850242,
      "p2.5": 1019.3275871820989,
      "p97.5": 1060.0049262859482,
      "rank": 32,
      "rank_p2.5": 25,
      "rank_p97.5": 40,
      "total_output_tokens": 2922936,
      "conso_all_conv": 0.0,
      "n_match": 973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.551118712297955,
      "win_rate": 0.5045407636738906,
      "useful": 57,
      "creative": 22,
      "complete": 73,
      "clear_formatting": 37,
      "incorrect": 15,
      "superficial": 9,
      "instructions_not_followed": 9,
      "total_prefs": 222,
      "positive_prefs_ratio": 0.8513513513513513
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1038.9583240791385,
      "p2.5": 1028.6776515131958,
      "p97.5": 1052.0146075092089,
      "rank": 33,
      "rank_p2.5": 29,
      "rank_p97.5": 37,
      "total_output_tokens": 5550374,
      "conso_all_conv": 0.0,
      "n_match": 7173,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5493618435505639,
      "win_rate": 0.515105285176405,
      "useful": 1227,
      "creative": 286,
      "complete": 945,
      "clear_formatting": 1010,
      "incorrect": 244,
      "superficial": 455,
      "instructions_not_followed": 97,
      "total_prefs": 4264,
      "positive_prefs_ratio": 0.8133208255159474
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1037.4934412437008,
      "p2.5": 1026.6564044802,
      "p97.5": 1050.4806928240448,
      "rank": 34,
      "rank_p2.5": 29,
      "rank_p97.5": 38,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5473547015311243,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1036.4538978598166,
      "p2.5": 1019.2995121791114,
      "p97.5": 1054.5739973718476,
      "rank": 35,
      "rank_p2.5": 28,
      "rank_p97.5": 40,
      "total_output_tokens": 1761279,
      "conso_all_conv": 0.0,
      "n_match": 1231,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5459295741155128,
      "win_rate": 0.5008964955175224,
      "useful": 80,
      "creative": 33,
      "complete": 107,
      "clear_formatting": 100,
      "incorrect": 29,
      "superficial": 32,
      "instructions_not_followed": 21,
      "total_prefs": 402,
      "positive_prefs_ratio": 0.7960199004975125
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1034.2211614776247,
      "p2.5": 1020.0476994942112,
      "p97.5": 1049.3442700323635,
      "rank": 36,
      "rank_p2.5": 30,
      "rank_p97.5": 40,
      "total_output_tokens": 2512794,
      "conso_all_conv": 0.0,
      "n_match": 2279,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5428666196589524,
      "win_rate": 0.4947608600263273,
      "useful": 131,
      "creative": 47,
      "complete": 177,
      "clear_formatting": 126,
      "incorrect": 70,
      "superficial": 49,
      "instructions_not_followed": 32,
      "total_prefs": 632,
      "positive_prefs_ratio": 0.7610759493670886
    },
    {
      "model_name": "deepseek-r1",
      "median": 1034.0776191889104,
      "p2.5": 1021.6500678378413,
      "p97.5": 1047.2668975496435,
      "rank": 37,
      "rank_p2.5": 30,
      "rank_p97.5": 39,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5426696104547338,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1025.6502663260503,
      "p2.5": 1016.2182743933423,
      "p97.5": 1038.1353912799761,
      "rank": 39,
      "rank_p2.5": 35,
      "rank_p97.5": 41,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5310862237565296,
      "win_rate": 0.527441386340469,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "mistral-saba",
      "median": 1021.776338132438,
      "p2.5": 1011.9662292880496,
      "p97.5": 1035.3603509020993,
      "rank": 40,
      "rank_p2.5": 36,
      "rank_p97.5": 43,
      "total_output_tokens": 4349650,
      "conso_all_conv": 0.0,
      "n_match": 4934,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5257522471320063,
      "win_rate": 0.4829677680924386,
      "useful": 745,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 586,
      "incorrect": 241,
      "superficial": 337,
      "instructions_not_followed": 113,
      "total_prefs": 2779,
      "positive_prefs_ratio": 0.751349406261245
    },
    {
      "model_name": "llama-4-scout",
      "median": 1018.0139101606304,
      "p2.5": 1006.9894729499599,
      "p97.5": 1031.5006547116295,
      "rank": 41,
      "rank_p2.5": 37,
      "rank_p97.5": 46,
      "total_output_tokens": 3641901,
      "conso_all_conv": 0.0,
      "n_match": 4450,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5205677269856235,
      "win_rate": 0.4949898808185294,
      "useful": 665,
      "creative": 174,
      "complete": 600,
      "clear_formatting": 545,
      "incorrect": 231,
      "superficial": 306,
      "instructions_not_followed": 79,
      "total_prefs": 2600,
      "positive_prefs_ratio": 0.7630769230769231
    },
    {
      "model_name": "gpt-5",
      "median": 1015.9501018060416,
      "p2.5": 1002.6904506746816,
      "p97.5": 1030.1993721801125,
      "rank": 42,
      "rank_p2.5": 38,
      "rank_p97.5": 47,
      "total_output_tokens": 3197770,
      "conso_all_conv": 0.0,
      "n_match": 3078,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5177225399481482,
      "win_rate": 0.46313617159571013,
      "useful": 109,
      "creative": 44,
      "complete": 127,
      "clear_formatting": 56,
      "incorrect": 25,
      "superficial": 43,
      "instructions_not_followed": 26,
      "total_prefs": 430,
      "positive_prefs_ratio": 0.7813953488372093
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1015.3044981202803,
      "p2.5": 998.3356579096035,
      "p97.5": 1033.2107371891536,
      "rank": 43,
      "rank_p2.5": 37,
      "rank_p97.5": 48,
      "total_output_tokens": 1859305,
      "conso_all_conv": 0.0,
      "n_match": 1588,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5168323500026357,
      "win_rate": 0.4734826498422713,
      "useful": 75,
      "creative": 24,
      "complete": 102,
      "clear_formatting": 47,
      "incorrect": 23,
      "superficial": 28,
      "instructions_not_followed": 24,
      "total_prefs": 323,
      "positive_prefs_ratio": 0.7678018575851393
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1012.9481581877612,
      "p2.5": 1002.508390653334,
      "p97.5": 1026.68337138343,
      "rank": 44,
      "rank_p2.5": 40,
      "rank_p97.5": 48,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5135828081921219,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "o4-mini",
      "median": 1011.7376825658313,
      "p2.5": 998.8047228883418,
      "p97.5": 1027.8325583712294,
      "rank": 45,
      "rank_p2.5": 39,
      "rank_p97.5": 49,
      "total_output_tokens": 2858639,
      "conso_all_conv": 0.0,
      "n_match": 2746,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5119132376227988,
      "win_rate": 0.4710495626822157,
      "useful": 524,
      "creative": 111,
      "complete": 369,
      "clear_formatting": 283,
      "incorrect": 131,
      "superficial": 209,
      "instructions_not_followed": 45,
      "total_prefs": 1672,
      "positive_prefs_ratio": 0.7697368421052632
    },
    {
      "model_name": "llama-maverick",
      "median": 1011.0952744999548,
      "p2.5": 993.6167027800142,
      "p97.5": 1027.5156401030374,
      "rank": 46,
      "rank_p2.5": 39,
      "rank_p97.5": 50,
      "total_output_tokens": 1320498,
      "conso_all_conv": 0.0,
      "n_match": 1640,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5110271327495479,
      "win_rate": 0.4932967032967033,
      "useful": 88,
      "creative": 16,
      "complete": 69,
      "clear_formatting": 56,
      "incorrect": 25,
      "superficial": 57,
      "instructions_not_followed": 11,
      "total_prefs": 322,
      "positive_prefs_ratio": 0.7111801242236024
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1007.2688759589964,
      "p2.5": 982.373055461445,
      "p97.5": 1031.565354712259,
      "rank": 47,
      "rank_p2.5": 37,
      "rank_p97.5": 57,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5057487704653598,
      "win_rate": 0.5826115485564304,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 999.9763986012988,
      "p2.5": 983.4192435716716,
      "p97.5": 1017.9493102066118,
      "rank": 48,
      "rank_p2.5": 43,
      "rank_p97.5": 56,
      "total_output_tokens": 2626410,
      "conso_all_conv": 0.0,
      "n_match": 1558,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.49569023441226895,
      "win_rate": 0.45365446371226714,
      "useful": 81,
      "creative": 25,
      "complete": 98,
      "clear_formatting": 48,
      "incorrect": 47,
      "superficial": 36,
      "instructions_not_followed": 16,
      "total_prefs": 351,
      "positive_prefs_ratio": 0.717948717948718
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 998.1412930810608,
      "p2.5": 987.264017673344,
      "p97.5": 1011.7064130497475,
      "rank": 49,
      "rank_p2.5": 46,
      "rank_p97.5": 53,
      "total_output_tokens": 4082212,
      "conso_all_conv": 0.0,
      "n_match": 5113,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.49316001958060335,
      "win_rate": 0.4625171132407587,
      "useful": 722,
      "creative": 142,
      "complete": 527,
      "clear_formatting": 513,
      "incorrect": 277,
      "superficial": 353,
      "instructions_not_followed": 109,
      "total_prefs": 2643,
      "positive_prefs_ratio": 0.7203934922436626
    },
    {
      "model_name": "qwen-3-8b",
      "median": 994.7526372784341,
      "p2.5": 974.7685726470976,
      "p97.5": 1015.5295551049855,
      "rank": 50,
      "rank_p2.5": 44,
      "rank_p97.5": 60,
      "total_output_tokens": 2262142,
      "conso_all_conv": 0.0,
      "n_match": 1089,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48848952886643904,
      "win_rate": 0.4532047750229569,
      "useful": 41,
      "creative": 12,
      "complete": 55,
      "clear_formatting": 36,
      "incorrect": 37,
      "superficial": 18,
      "instructions_not_followed": 10,
      "total_prefs": 209,
      "positive_prefs_ratio": 0.6889952153110048
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 992.4576401930972,
      "p2.5": 980.5463694941932,
      "p97.5": 1005.2976287203837,
      "rank": 51,
      "rank_p2.5": 47,
      "rank_p97.5": 57,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48532799805640603,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 989.5911009813302,
      "p2.5": 979.230780286483,
      "p97.5": 1002.0353519336982,
      "rank": 52,
      "rank_p2.5": 49,
      "rank_p97.5": 58,
      "total_output_tokens": 6642606,
      "conso_all_conv": 0.0,
      "n_match": 7945,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4813813542974932,
      "win_rate": 0.4750094410876133,
      "useful": 1376,
      "creative": 275,
      "complete": 1076,
      "clear_formatting": 1021,
      "incorrect": 431,
      "superficial": 594,
      "instructions_not_followed": 141,
      "total_prefs": 4914,
      "positive_prefs_ratio": 0.7627187627187627
    },
    {
      "model_name": "o3-mini",
      "median": 989.2390848023467,
      "p2.5": 972.8129616580454,
      "p97.5": 1006.4704860436826,
      "rank": 53,
      "rank_p2.5": 46,
      "rank_p97.5": 61,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4808968904325321,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 988.122008729391,
      "p2.5": 977.7600987849952,
      "p97.5": 1001.275221789584,
      "rank": 54,
      "rank_p2.5": 49,
      "rank_p97.5": 58,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4793598093945357,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 984.8830412829,
      "p2.5": 974.5570101108663,
      "p97.5": 998.0217961520683,
      "rank": 55,
      "rank_p2.5": 51,
      "rank_p97.5": 60,
      "total_output_tokens": 3563423,
      "conso_all_conv": 0.0,
      "n_match": 5717,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.47490583045404344,
      "win_rate": 0.4674846947699842,
      "useful": 928,
      "creative": 159,
      "complete": 529,
      "clear_formatting": 649,
      "incorrect": 276,
      "superficial": 448,
      "instructions_not_followed": 103,
      "total_prefs": 3092,
      "positive_prefs_ratio": 0.732535575679172
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 981.2310459720993,
      "p2.5": 962.2910816702009,
      "p97.5": 1001.225761758667,
      "rank": 56,
      "rank_p2.5": 48,
      "rank_p97.5": 66,
      "total_output_tokens": 1020555,
      "conso_all_conv": 0.0,
      "n_match": 1032,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4698895254627368,
      "win_rate": 0.41951503394762363,
      "useful": 57,
      "creative": 5,
      "complete": 47,
      "clear_formatting": 31,
      "incorrect": 35,
      "superficial": 39,
      "instructions_not_followed": 20,
      "total_prefs": 234,
      "positive_prefs_ratio": 0.5982905982905983
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 979.7390038972092,
      "p2.5": 961.3733334344543,
      "p97.5": 997.6927278715041,
      "rank": 57,
      "rank_p2.5": 50,
      "rank_p97.5": 66,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4678420146234273,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 979.6320676680316,
      "p2.5": 928.9787746817861,
      "p97.5": 1024.9139999564604,
      "rank": 58,
      "rank_p2.5": 38,
      "rank_p97.5": 75,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46769531318452606,
      "win_rate": 0.5449418604651162,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 978.7431271791091,
      "p2.5": 967.0075579174015,
      "p97.5": 991.2046329958289,
      "rank": 59,
      "rank_p2.5": 54,
      "rank_p97.5": 63,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46647605639240497,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 977.3222088017961,
      "p2.5": 966.5016599301886,
      "p97.5": 990.69080973325,
      "rank": 60,
      "rank_p2.5": 55,
      "rank_p97.5": 63,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46452807824310993,
      "win_rate": 0.4921080415273624,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "phi-4",
      "median": 972.7990240568905,
      "p2.5": 963.706506935102,
      "p97.5": 985.3458619427779,
      "rank": 61,
      "rank_p2.5": 58,
      "rank_p97.5": 65,
      "total_output_tokens": 7793705,
      "conso_all_conv": 0.0,
      "n_match": 9395,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45833532718397235,
      "win_rate": 0.47082818820523736,
      "useful": 1604,
      "creative": 333,
      "complete": 1166,
      "clear_formatting": 1291,
      "incorrect": 690,
      "superficial": 740,
      "instructions_not_followed": 242,
      "total_prefs": 6066,
      "positive_prefs_ratio": 0.724365314869766
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 969.5497706660524,
      "p2.5": 959.851222177084,
      "p97.5": 981.8376662047153,
      "rank": 62,
      "rank_p2.5": 59,
      "rank_p97.5": 66,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4538951844366856,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 968.2845558470564,
      "p2.5": 957.2735022771856,
      "p97.5": 981.0157895517661,
      "rank": 63,
      "rank_p2.5": 59,
      "rank_p97.5": 67,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4521683281356197,
      "win_rate": 0.48863297150610585,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "minimax-m2",
      "median": 966.105212179157,
      "p2.5": 926.1246701147577,
      "p97.5": 1004.707207414101,
      "rank": 64,
      "rank_p2.5": 48,
      "rank_p97.5": 76,
      "total_output_tokens": 364139,
      "conso_all_conv": 0.0,
      "n_match": 196,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4491966870102278,
      "win_rate": 0.3159487179487179,
      "useful": 18,
      "creative": 1,
      "complete": 10,
      "clear_formatting": 4,
      "incorrect": 5,
      "superficial": 8,
      "instructions_not_followed": 4,
      "total_prefs": 50,
      "positive_prefs_ratio": 0.66
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 966.065660951527,
      "p2.5": 911.8459300514016,
      "p97.5": 1024.430307166774,
      "rank": 65,
      "rank_p2.5": 39,
      "rank_p97.5": 77,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4491427916580291,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 963.8932965680258,
      "p2.5": 910.3884032693801,
      "p97.5": 1019.0346901236051,
      "rank": 66,
      "rank_p2.5": 41,
      "rank_p97.5": 77,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44618453540238057,
      "win_rate": 0.46869047619047627,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 963.1878143145843,
      "p2.5": 953.2590393560745,
      "p97.5": 975.4247110739698,
      "rank": 67,
      "rank_p2.5": 62,
      "rank_p97.5": 69,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44522468044359353,
      "win_rate": 0.4950747166783672,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 958.5326886587193,
      "p2.5": 946.8254985199243,
      "p97.5": 971.3924905201233,
      "rank": 68,
      "rank_p2.5": 64,
      "rank_p97.5": 71,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4389020809507277,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "gpt-5-nano",
      "median": 958.1727039747215,
      "p2.5": 940.0340459910211,
      "p97.5": 976.6657274462456,
      "rank": 69,
      "rank_p2.5": 60,
      "rank_p97.5": 72,
      "total_output_tokens": 1421421,
      "conso_all_conv": 0.0,
      "n_match": 1305,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.438413976980206,
      "win_rate": 0.3908665644171779,
      "useful": 44,
      "creative": 15,
      "complete": 48,
      "clear_formatting": 29,
      "incorrect": 16,
      "superficial": 24,
      "instructions_not_followed": 22,
      "total_prefs": 198,
      "positive_prefs_ratio": 0.6868686868686869
    },
    {
      "model_name": "qwq-32b",
      "median": 956.6043273717632,
      "p2.5": 939.4822827922272,
      "p97.5": 973.7905646662226,
      "rank": 70,
      "rank_p2.5": 63,
      "rank_p97.5": 72,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43628885946706675,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 954.0309296510379,
      "p2.5": 940.8999786764981,
      "p97.5": 967.9714175884868,
      "rank": 71,
      "rank_p2.5": 65,
      "rank_p97.5": 72,
      "total_output_tokens": 2863519,
      "conso_all_conv": 0.0,
      "n_match": 2981,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43280720599252986,
      "win_rate": 0.4396947333109695,
      "useful": 489,
      "creative": 130,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 190,
      "superficial": 342,
      "instructions_not_followed": 126,
      "total_prefs": 1995,
      "positive_prefs_ratio": 0.6701754385964912
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 947.5140183437354,
      "p2.5": 937.7213021440901,
      "p97.5": 959.7354927022706,
      "rank": 72,
      "rank_p2.5": 68,
      "rank_p97.5": 73,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4240212706407704,
      "win_rate": 0.466347438752784,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 935.8481840398165,
      "p2.5": 925.4503977566264,
      "p97.5": 948.5061002155628,
      "rank": 73,
      "rank_p2.5": 71,
      "rank_p97.5": 77,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40841784490157684,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 935.1374814595608,
      "p2.5": 926.4840274564197,
      "p97.5": 947.0657647595476,
      "rank": 74,
      "rank_p2.5": 71,
      "rank_p97.5": 77,
      "total_output_tokens": 6935760,
      "conso_all_conv": 0.0,
      "n_match": 9790,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4074728852430371,
      "win_rate": 0.4414525025536261,
      "useful": 1515,
      "creative": 369,
      "complete": 995,
      "clear_formatting": 1142,
      "incorrect": 1010,
      "superficial": 957,
      "instructions_not_followed": 397,
      "total_prefs": 6385,
      "positive_prefs_ratio": 0.6297572435395458
    },
    {
      "model_name": "hermes-4-70b",
      "median": 932.6834800067297,
      "p2.5": 914.1195872494659,
      "p97.5": 951.4278638622753,
      "rank": 75,
      "rank_p2.5": 70,
      "rank_p97.5": 78,
      "total_output_tokens": 625300,
      "conso_all_conv": 0.0,
      "n_match": 1351,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40421530579513626,
      "win_rate": 0.373765752409192,
      "useful": 48,
      "creative": 12,
      "complete": 28,
      "clear_formatting": 33,
      "incorrect": 34,
      "superficial": 49,
      "instructions_not_followed": 24,
      "total_prefs": 228,
      "positive_prefs_ratio": 0.5307017543859649
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 932.1794970272508,
      "p2.5": 922.5745515724019,
      "p97.5": 944.9806721351795,
      "rank": 76,
      "rank_p2.5": 72,
      "rank_p97.5": 77,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4035473217984619,
      "win_rate": 0.4378942320865872,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 932.1525999639153,
      "p2.5": 912.5281535262266,
      "p97.5": 953.1046213190593,
      "rank": 77,
      "rank_p2.5": 70,
      "rank_p97.5": 78,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4035116821624208,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 890.0324437722678,
      "p2.5": 874.5380090093562,
      "p97.5": 906.248322463214,
      "rank": 78,
      "rank_p2.5": 77,
      "rank_p97.5": 80,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3491411350761286,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 887.4893138380394,
      "p2.5": 874.0470791840711,
      "p97.5": 902.5102092275018,
      "rank": 79,
      "rank_p2.5": 77,
      "rank_p97.5": 80,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3459629109970086,
      "win_rate": 0.41629681386249306,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 866.6022852038396,
      "p2.5": 849.7529951247482,
      "p97.5": 882.3662158350543,
      "rank": 80,
      "rank_p2.5": 79,
      "rank_p97.5": 82,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3203805107817397,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 857.3188242605102,
      "p2.5": 846.5336802190249,
      "p97.5": 870.1616727011449,
      "rank": 81,
      "rank_p2.5": 80,
      "rank_p97.5": 82,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3093254096055386,
      "win_rate": 0.37482482802751566,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 846.8552218323905,
      "p2.5": 835.0740337944503,
      "p97.5": 860.9373053866489,
      "rank": 82,
      "rank_p2.5": 81,
      "rank_p97.5": 83,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.29711177949722894,
      "win_rate": 0.36710357470210814,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 824.331881853821,
      "p2.5": 807.1353586296148,
      "p97.5": 842.6340161575346,
      "rank": 83,
      "rank_p2.5": 83,
      "rank_p97.5": 85,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.27175371884348054,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 770.9362898073972,
      "p2.5": 673.5998021795648,
      "p97.5": 859.4708388597875,
      "rank": 84,
      "rank_p2.5": 81,
      "rank_p97.5": 87,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2170815004104922,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 755.1142119101992,
      "p2.5": 712.0826751083723,
      "p97.5": 799.3343962822526,
      "rank": 85,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.202412752167643,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "olmo-3-32b-think",
      "median": 746.7773463534186,
      "p2.5": -123.33241164448191,
      "p97.5": 1021.2257031425167,
      "rank": 86,
      "rank_p2.5": 39,
      "rank_p97.5": 87,
      "total_output_tokens": 6734,
      "conso_all_conv": 0.0,
      "n_match": 3,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.19496889534629605,
      "win_rate": 0.0,
      "useful": 0,
      "creative": 0,
      "complete": 0,
      "clear_formatting": 0,
      "incorrect": 1,
      "superficial": 1,
      "instructions_not_followed": 2,
      "total_prefs": 4,
      "positive_prefs_ratio": 0.0
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 733.0874181439995,
      "p2.5": 633.8236254679772,
      "p97.5": 812.2075372163588,
      "rank": 87,
      "rank_p2.5": 84,
      "rank_p97.5": 87,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.18317166834307042,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}