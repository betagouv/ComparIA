{
  "timestamp": 1763526225.0,
  "models": [
    {
      "model_name": "mistral-medium-3.1",
      "median": 1151.4380355018097,
      "p2.5": 1136.1270280731076,
      "p97.5": 1165.7344882103137,
      "rank": 1,
      "rank_p2.5": 1,
      "rank_p97.5": 1,
      "total_output_tokens": 2841712,
      "conso_all_conv": 0.0,
      "n_match": 1563,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6980200318807589,
      "win_rate": 0.6209026888604353,
      "useful": 92,
      "creative": 37,
      "complete": 142,
      "clear_formatting": 97,
      "incorrect": 30,
      "superficial": 9,
      "instructions_not_followed": 7,
      "total_prefs": 414,
      "positive_prefs_ratio": 0.8888888888888888
    },
    {
      "model_name": "gemini-2.5-flash",
      "median": 1117.2036292959415,
      "p2.5": 1104.9971979396896,
      "p97.5": 1130.000125480013,
      "rank": 2,
      "rank_p2.5": 2,
      "rank_p97.5": 4,
      "total_output_tokens": 2935956,
      "conso_all_conv": 0.0,
      "n_match": 2067,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6559393379121377,
      "win_rate": 0.5953775411423039,
      "useful": 176,
      "creative": 63,
      "complete": 232,
      "clear_formatting": 159,
      "incorrect": 52,
      "superficial": 32,
      "instructions_not_followed": 11,
      "total_prefs": 725,
      "positive_prefs_ratio": 0.8689655172413793
    },
    {
      "model_name": "qwen3-max-2025-09-23",
      "median": 1111.5445902020433,
      "p2.5": 1093.5091134819008,
      "p97.5": 1129.9689336067302,
      "rank": 3,
      "rank_p2.5": 2,
      "rank_p97.5": 8,
      "total_output_tokens": 1086849,
      "conso_all_conv": 0.0,
      "n_match": 994,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6487396060447758,
      "win_rate": 0.599778449144008,
      "useful": 55,
      "creative": 19,
      "complete": 73,
      "clear_formatting": 55,
      "incorrect": 31,
      "superficial": 9,
      "instructions_not_followed": 8,
      "total_prefs": 250,
      "positive_prefs_ratio": 0.808
    },
    {
      "model_name": "gemini-2.0-flash",
      "median": 1107.1953727223358,
      "p2.5": 1100.7522080479816,
      "p97.5": 1114.0136366235167,
      "rank": 4,
      "rank_p2.5": 2,
      "rank_p97.5": 6,
      "total_output_tokens": 12296785,
      "conso_all_conv": 0.0,
      "n_match": 8684,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.643163722229072,
      "win_rate": 0.6317699216950713,
      "useful": 2128,
      "creative": 677,
      "complete": 2179,
      "clear_formatting": 1909,
      "incorrect": 376,
      "superficial": 397,
      "instructions_not_followed": 112,
      "total_prefs": 7778,
      "positive_prefs_ratio": 0.8862175366418102
    },
    {
      "model_name": "deepseek-v3-0324",
      "median": 1099.7777667406267,
      "p2.5": 1090.8258628587744,
      "p97.5": 1108.974066584073,
      "rank": 5,
      "rank_p2.5": 4,
      "rank_p97.5": 9,
      "total_output_tokens": 4435392,
      "conso_all_conv": 0.0,
      "n_match": 4385,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6335724622836741,
      "win_rate": 0.5851128848346637,
      "useful": 1141,
      "creative": 336,
      "complete": 1012,
      "clear_formatting": 1043,
      "incorrect": 230,
      "superficial": 288,
      "instructions_not_followed": 79,
      "total_prefs": 4129,
      "positive_prefs_ratio": 0.8554129329135384
    },
    {
      "model_name": "gemma-3-27b",
      "median": 1095.0391240626764,
      "p2.5": 1087.907999766515,
      "p97.5": 1102.4483183340992,
      "rank": 6,
      "rank_p2.5": 5,
      "rank_p97.5": 10,
      "total_output_tokens": 8284358,
      "conso_all_conv": 0.0,
      "n_match": 6035,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6273938334985597,
      "win_rate": 0.5831068765534383,
      "useful": 1206,
      "creative": 404,
      "complete": 1384,
      "clear_formatting": 1022,
      "incorrect": 303,
      "superficial": 211,
      "instructions_not_followed": 100,
      "total_prefs": 4630,
      "positive_prefs_ratio": 0.8673866090712743
    },
    {
      "model_name": "gpt-oss-120b",
      "median": 1093.044721515147,
      "p2.5": 1073.5335393447863,
      "p97.5": 1112.5694605486726,
      "rank": 7,
      "rank_p2.5": 3,
      "rank_p97.5": 15,
      "total_output_tokens": 1985242,
      "conso_all_conv": 0.0,
      "n_match": 850,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6247818971442443,
      "win_rate": 0.5655135773317591,
      "useful": 105,
      "creative": 46,
      "complete": 168,
      "clear_formatting": 113,
      "incorrect": 47,
      "superficial": 23,
      "instructions_not_followed": 23,
      "total_prefs": 525,
      "positive_prefs_ratio": 0.8228571428571428
    },
    {
      "model_name": "mistral-medium-2508",
      "median": 1092.4946247226776,
      "p2.5": 1075.0140897568099,
      "p97.5": 1111.615109354731,
      "rank": 8,
      "rank_p2.5": 3,
      "rank_p97.5": 14,
      "total_output_tokens": 864058,
      "conso_all_conv": 0.0,
      "n_match": 973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6240603038244049,
      "win_rate": 0.5590628218331616,
      "useful": 86,
      "creative": 12,
      "complete": 64,
      "clear_formatting": 59,
      "incorrect": 27,
      "superficial": 12,
      "instructions_not_followed": 7,
      "total_prefs": 267,
      "positive_prefs_ratio": 0.8277153558052435
    },
    {
      "model_name": "magistral-medium",
      "median": 1089.6787635677674,
      "p2.5": 1074.9360265850846,
      "p97.5": 1104.8098348251572,
      "rank": 9,
      "rank_p2.5": 4,
      "rank_p97.5": 15,
      "total_output_tokens": 1420911,
      "conso_all_conv": 0.0,
      "n_match": 1484,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6203588077140676,
      "win_rate": 0.5747506738544474,
      "useful": 87,
      "creative": 27,
      "complete": 94,
      "clear_formatting": 57,
      "incorrect": 35,
      "superficial": 18,
      "instructions_not_followed": 3,
      "total_prefs": 321,
      "positive_prefs_ratio": 0.8255451713395638
    },
    {
      "model_name": "deepseek-chat-v3.1",
      "median": 1085.8712291119502,
      "p2.5": 1064.8627318333115,
      "p97.5": 1105.2579427663907,
      "rank": 10,
      "rank_p2.5": 5,
      "rank_p97.5": 18,
      "total_output_tokens": 1120941,
      "conso_all_conv": 0.0,
      "n_match": 742,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6153335673545313,
      "win_rate": 0.49710027100271004,
      "useful": 56,
      "creative": 24,
      "complete": 58,
      "clear_formatting": 43,
      "incorrect": 19,
      "superficial": 12,
      "instructions_not_followed": 7,
      "total_prefs": 219,
      "positive_prefs_ratio": 0.8264840182648402
    },
    {
      "model_name": "deepseek-v3-chat",
      "median": 1085.640289614861,
      "p2.5": 1076.773503924689,
      "p97.5": 1095.2394980568408,
      "rank": 11,
      "rank_p2.5": 7,
      "rank_p97.5": 14,
      "total_output_tokens": 5638296,
      "conso_all_conv": 0.0,
      "n_match": 5388,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.615028041841167,
      "win_rate": 0.6141481069042316,
      "useful": 1330,
      "creative": 390,
      "complete": 1245,
      "clear_formatting": 1399,
      "incorrect": 314,
      "superficial": 285,
      "instructions_not_followed": 81,
      "total_prefs": 5044,
      "positive_prefs_ratio": 0.8651863600317209
    },
    {
      "model_name": "deepseek-r1-0528",
      "median": 1080.6296474893786,
      "p2.5": 1057.262056233649,
      "p97.5": 1103.663502661562,
      "rank": 12,
      "rank_p2.5": 5,
      "rank_p97.5": 22,
      "total_output_tokens": 925134,
      "conso_all_conv": 0.0,
      "n_match": 539,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6083792937022423,
      "win_rate": 0.5503538175046555,
      "useful": 28,
      "creative": 12,
      "complete": 28,
      "clear_formatting": 16,
      "incorrect": 11,
      "superficial": 5,
      "instructions_not_followed": 3,
      "total_prefs": 103,
      "positive_prefs_ratio": 0.8155339805825242
    },
    {
      "model_name": "glm-4.5",
      "median": 1077.9100145243196,
      "p2.5": 1058.2444496709222,
      "p97.5": 1096.2845397940073,
      "rank": 13,
      "rank_p2.5": 7,
      "rank_p97.5": 22,
      "total_output_tokens": 2180123,
      "conso_all_conv": 0.0,
      "n_match": 920,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6047551517359461,
      "win_rate": 0.5507399347116431,
      "useful": 94,
      "creative": 26,
      "complete": 101,
      "clear_formatting": 76,
      "incorrect": 21,
      "superficial": 13,
      "instructions_not_followed": 9,
      "total_prefs": 340,
      "positive_prefs_ratio": 0.8735294117647059
    },
    {
      "model_name": "claude-4-5-sonnet",
      "median": 1076.139815226733,
      "p2.5": 1063.1919549405627,
      "p97.5": 1089.2374349178417,
      "rank": 14,
      "rank_p2.5": 10,
      "rank_p97.5": 20,
      "total_output_tokens": 2349675,
      "conso_all_conv": 0.0,
      "n_match": 2082,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.6023906113788734,
      "win_rate": 0.5370687169629985,
      "useful": 154,
      "creative": 36,
      "complete": 114,
      "clear_formatting": 104,
      "incorrect": 29,
      "superficial": 28,
      "instructions_not_followed": 18,
      "total_prefs": 483,
      "positive_prefs_ratio": 0.84472049689441
    },
    {
      "model_name": "gemma-3-12b",
      "median": 1074.0891515025842,
      "p2.5": 1066.1301158279077,
      "p97.5": 1081.7070259430695,
      "rank": 15,
      "rank_p2.5": 11,
      "rank_p97.5": 20,
      "total_output_tokens": 7536331,
      "conso_all_conv": 0.0,
      "n_match": 5671,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.599646071859579,
      "win_rate": 0.5701745723858226,
      "useful": 1006,
      "creative": 302,
      "complete": 1177,
      "clear_formatting": 855,
      "incorrect": 327,
      "superficial": 253,
      "instructions_not_followed": 138,
      "total_prefs": 4058,
      "positive_prefs_ratio": 0.823065549531789
    },
    {
      "model_name": "mistral-small-2506",
      "median": 1071.7242238575761,
      "p2.5": 1058.1228135042652,
      "p97.5": 1084.0473156110183,
      "rank": 16,
      "rank_p2.5": 11,
      "rank_p97.5": 22,
      "total_output_tokens": 1487007,
      "conso_all_conv": 0.0,
      "n_match": 1825,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5964739677204061,
      "win_rate": 0.5436767123287671,
      "useful": 94,
      "creative": 30,
      "complete": 99,
      "clear_formatting": 70,
      "incorrect": 55,
      "superficial": 29,
      "instructions_not_followed": 8,
      "total_prefs": 385,
      "positive_prefs_ratio": 0.7610389610389611
    },
    {
      "model_name": "grok-3-mini-beta",
      "median": 1071.4259604044273,
      "p2.5": 1056.278668647006,
      "p97.5": 1086.3631693420332,
      "rank": 17,
      "rank_p2.5": 10,
      "rank_p97.5": 23,
      "total_output_tokens": 3474752,
      "conso_all_conv": 0.0,
      "n_match": 1537,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5960733850094879,
      "win_rate": 0.5305338541666665,
      "useful": 404,
      "creative": 102,
      "complete": 350,
      "clear_formatting": 320,
      "incorrect": 102,
      "superficial": 106,
      "instructions_not_followed": 47,
      "total_prefs": 1431,
      "positive_prefs_ratio": 0.8218029350104822
    },
    {
      "model_name": "claude-4-sonnet",
      "median": 1067.920156646158,
      "p2.5": 1056.6167040110297,
      "p97.5": 1080.3472181813195,
      "rank": 18,
      "rank_p2.5": 13,
      "rank_p97.5": 23,
      "total_output_tokens": 1656622,
      "conso_all_conv": 0.0,
      "n_match": 2218,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5913564564813779,
      "win_rate": 0.5286332882273342,
      "useful": 208,
      "creative": 59,
      "complete": 183,
      "clear_formatting": 201,
      "incorrect": 37,
      "superficial": 56,
      "instructions_not_followed": 16,
      "total_prefs": 760,
      "positive_prefs_ratio": 0.8565789473684211
    },
    {
      "model_name": "command-a",
      "median": 1066.0138630765134,
      "p2.5": 1058.5683477468951,
      "p97.5": 1073.3769542387495,
      "rank": 19,
      "rank_p2.5": 15,
      "rank_p97.5": 23,
      "total_output_tokens": 5585006,
      "conso_all_conv": 0.0,
      "n_match": 5627,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5887852439669655,
      "win_rate": 0.5596676737160121,
      "useful": 1118,
      "creative": 257,
      "complete": 1036,
      "clear_formatting": 972,
      "incorrect": 260,
      "superficial": 280,
      "instructions_not_followed": 99,
      "total_prefs": 4022,
      "positive_prefs_ratio": 0.8411238189955246
    },
    {
      "model_name": "kimi-k2",
      "median": 1065.7111299342905,
      "p2.5": 1039.263741111779,
      "p97.5": 1090.4552967963436,
      "rank": 20,
      "rank_p2.5": 9,
      "rank_p97.5": 28,
      "total_output_tokens": 747540,
      "conso_all_conv": 0.0,
      "n_match": 455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5883765159451815,
      "win_rate": 0.5074778761061947,
      "useful": 102,
      "creative": 19,
      "complete": 50,
      "clear_formatting": 41,
      "incorrect": 17,
      "superficial": 23,
      "instructions_not_followed": 1,
      "total_prefs": 253,
      "positive_prefs_ratio": 0.8379446640316206
    },
    {
      "model_name": "grok-4-fast",
      "median": 1063.17250385313,
      "p2.5": 1049.5945403134872,
      "p97.5": 1078.6165706952866,
      "rank": 21,
      "rank_p2.5": 14,
      "rank_p97.5": 25,
      "total_output_tokens": 2132960,
      "conso_all_conv": 0.0,
      "n_match": 1544,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5849448299160213,
      "win_rate": 0.522230869001297,
      "useful": 108,
      "creative": 18,
      "complete": 78,
      "clear_formatting": 53,
      "incorrect": 28,
      "superficial": 20,
      "instructions_not_followed": 6,
      "total_prefs": 311,
      "positive_prefs_ratio": 0.8263665594855305
    },
    {
      "model_name": "claude-3-7-sonnet",
      "median": 1056.450099584883,
      "p2.5": 1047.6921830009317,
      "p97.5": 1066.2176359432183,
      "rank": 22,
      "rank_p2.5": 19,
      "rank_p97.5": 26,
      "total_output_tokens": 3546728,
      "conso_all_conv": 0.0,
      "n_match": 3907,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5758229812216207,
      "win_rate": 0.524225748656258,
      "useful": 1003,
      "creative": 230,
      "complete": 774,
      "clear_formatting": 822,
      "incorrect": 140,
      "superficial": 318,
      "instructions_not_followed": 46,
      "total_prefs": 3333,
      "positive_prefs_ratio": 0.8487848784878488
    },
    {
      "model_name": "magistral-small-2506",
      "median": 1055.840867808663,
      "p2.5": 1044.0717813048254,
      "p97.5": 1068.6234010701296,
      "rank": 23,
      "rank_p2.5": 19,
      "rank_p97.5": 27,
      "total_output_tokens": 1713519,
      "conso_all_conv": 0.0,
      "n_match": 2185,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5749939514148915,
      "win_rate": 0.5228663003663004,
      "useful": 188,
      "creative": 42,
      "complete": 144,
      "clear_formatting": 126,
      "incorrect": 59,
      "superficial": 59,
      "instructions_not_followed": 19,
      "total_prefs": 637,
      "positive_prefs_ratio": 0.7849293563579278
    },
    {
      "model_name": "Qwen3-Coder-480B-A35B-Instruct",
      "median": 1054.8870448026096,
      "p2.5": 1024.3484539335348,
      "p97.5": 1088.607755150978,
      "rank": 24,
      "rank_p2.5": 10,
      "rank_p97.5": 34,
      "total_output_tokens": 377104,
      "conso_all_conv": 0.0,
      "n_match": 272,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5736952657294563,
      "win_rate": 0.5474626865671642,
      "useful": 18,
      "creative": 8,
      "complete": 14,
      "clear_formatting": 14,
      "incorrect": 9,
      "superficial": 6,
      "instructions_not_followed": 2,
      "total_prefs": 71,
      "positive_prefs_ratio": 0.7605633802816901
    },
    {
      "model_name": "llama-3.1-nemotron-70b-instruct",
      "median": 1051.6120268096845,
      "p2.5": 1043.1884919999734,
      "p97.5": 1058.8872995018,
      "rank": 25,
      "rank_p2.5": 21,
      "rank_p97.5": 28,
      "total_output_tokens": 7426282,
      "conso_all_conv": 0.0,
      "n_match": 6709,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5692294442435268,
      "win_rate": 0.5452863107664777,
      "useful": 1544,
      "creative": 487,
      "complete": 1473,
      "clear_formatting": 1431,
      "incorrect": 381,
      "superficial": 395,
      "instructions_not_followed": 136,
      "total_prefs": 5847,
      "positive_prefs_ratio": 0.8440225756798359
    },
    {
      "model_name": "gemma-3-4b",
      "median": 1047.3473279004518,
      "p2.5": 1039.9051348855614,
      "p97.5": 1054.754496934169,
      "rank": 26,
      "rank_p2.5": 23,
      "rank_p97.5": 29,
      "total_output_tokens": 8382374,
      "conso_all_conv": 0.0,
      "n_match": 6672,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5633994214242668,
      "win_rate": 0.5280935251798562,
      "useful": 1080,
      "creative": 329,
      "complete": 1200,
      "clear_formatting": 952,
      "incorrect": 527,
      "superficial": 236,
      "instructions_not_followed": 183,
      "total_prefs": 4507,
      "positive_prefs_ratio": 0.7901042822276458
    },
    {
      "model_name": "glm-4.6",
      "median": 1044.067558169611,
      "p2.5": 1022.9186757967254,
      "p97.5": 1064.8949077912569,
      "rank": 27,
      "rank_p2.5": 20,
      "rank_p97.5": 35,
      "total_output_tokens": 2154242,
      "conso_all_conv": 0.0,
      "n_match": 743,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5589054073542108,
      "win_rate": 0.514331983805668,
      "useful": 50,
      "creative": 17,
      "complete": 64,
      "clear_formatting": 32,
      "incorrect": 12,
      "superficial": 7,
      "instructions_not_followed": 6,
      "total_prefs": 188,
      "positive_prefs_ratio": 0.8670212765957447
    },
    {
      "model_name": "qwen3-32b",
      "median": 1042.13489493205,
      "p2.5": 1025.1201878275865,
      "p97.5": 1060.168067972955,
      "rank": 28,
      "rank_p2.5": 22,
      "rank_p97.5": 34,
      "total_output_tokens": 2020707,
      "conso_all_conv": 0.0,
      "n_match": 969,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5562532904566734,
      "win_rate": 0.5478031088082901,
      "useful": 251,
      "creative": 63,
      "complete": 201,
      "clear_formatting": 170,
      "incorrect": 89,
      "superficial": 69,
      "instructions_not_followed": 37,
      "total_prefs": 880,
      "positive_prefs_ratio": 0.7784090909090909
    },
    {
      "model_name": "gpt-4.1-mini",
      "median": 1038.892382690939,
      "p2.5": 1031.5686605345454,
      "p97.5": 1046.2636264404168,
      "rank": 29,
      "rank_p2.5": 26,
      "rank_p97.5": 32,
      "total_output_tokens": 5407570,
      "conso_all_conv": 0.0,
      "n_match": 6885,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.551797620543627,
      "win_rate": 0.515891326456487,
      "useful": 1214,
      "creative": 279,
      "complete": 927,
      "clear_formatting": 994,
      "incorrect": 243,
      "superficial": 453,
      "instructions_not_followed": 95,
      "total_prefs": 4205,
      "positive_prefs_ratio": 0.8118906064209275
    },
    {
      "model_name": "gemini-1.5-pro",
      "median": 1037.363276376157,
      "p2.5": 1029.174628176997,
      "p97.5": 1045.487364855282,
      "rank": 30,
      "rank_p2.5": 26,
      "rank_p97.5": 33,
      "total_output_tokens": 6034223,
      "conso_all_conv": 0.0,
      "n_match": 7387,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5496939200820659,
      "win_rate": 0.5830743197509137,
      "useful": 2118,
      "creative": 683,
      "complete": 1630,
      "clear_formatting": 1959,
      "incorrect": 461,
      "superficial": 655,
      "instructions_not_followed": 202,
      "total_prefs": 7708,
      "positive_prefs_ratio": 0.8290088220031137
    },
    {
      "model_name": "gpt-oss-20b",
      "median": 1036.2458851777412,
      "p2.5": 1021.067732310249,
      "p97.5": 1052.7367251535543,
      "rank": 31,
      "rank_p2.5": 24,
      "rank_p97.5": 36,
      "total_output_tokens": 1647974,
      "conso_all_conv": 0.0,
      "n_match": 1179,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5481556923172227,
      "win_rate": 0.5032285471537807,
      "useful": 80,
      "creative": 33,
      "complete": 104,
      "clear_formatting": 97,
      "incorrect": 27,
      "superficial": 32,
      "instructions_not_followed": 21,
      "total_prefs": 394,
      "positive_prefs_ratio": 0.7969543147208121
    },
    {
      "model_name": "deepseek-r1",
      "median": 1033.4443373864833,
      "p2.5": 1023.5293408195337,
      "p97.5": 1043.2750800009783,
      "rank": 32,
      "rank_p2.5": 27,
      "rank_p97.5": 35,
      "total_output_tokens": 3693582,
      "conso_all_conv": 0.0,
      "n_match": 3510,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5442956717717372,
      "win_rate": 0.5313019943019943,
      "useful": 826,
      "creative": 284,
      "complete": 783,
      "clear_formatting": 667,
      "incorrect": 171,
      "superficial": 207,
      "instructions_not_followed": 112,
      "total_prefs": 3050,
      "positive_prefs_ratio": 0.839344262295082
    },
    {
      "model_name": "gemma-3n-e4b-it",
      "median": 1028.883468100606,
      "p2.5": 1015.9557200948516,
      "p97.5": 1041.7389579609398,
      "rank": 33,
      "rank_p2.5": 28,
      "rank_p97.5": 38,
      "total_output_tokens": 2198270,
      "conso_all_conv": 0.0,
      "n_match": 2011,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5380023454311385,
      "win_rate": 0.4882487562189055,
      "useful": 122,
      "creative": 45,
      "complete": 162,
      "clear_formatting": 117,
      "incorrect": 64,
      "superficial": 45,
      "instructions_not_followed": 31,
      "total_prefs": 586,
      "positive_prefs_ratio": 0.7610921501706485
    },
    {
      "model_name": "mistral-large-2411",
      "median": 1025.7309148102722,
      "p2.5": 1019.0300158142544,
      "p97.5": 1032.2897453071894,
      "rank": 34,
      "rank_p2.5": 31,
      "rank_p97.5": 37,
      "total_output_tokens": 7677266,
      "conso_all_conv": 0.0,
      "n_match": 8829,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5336464114660205,
      "win_rate": 0.527441386340469,
      "useful": 1898,
      "creative": 355,
      "complete": 1451,
      "clear_formatting": 1608,
      "incorrect": 393,
      "superficial": 666,
      "instructions_not_followed": 140,
      "total_prefs": 6511,
      "positive_prefs_ratio": 0.8158500998310552
    },
    {
      "model_name": "gpt-5",
      "median": 1020.233947931496,
      "p2.5": 1008.9643732870509,
      "p97.5": 1032.150304113321,
      "rank": 35,
      "rank_p2.5": 32,
      "rank_p97.5": 41,
      "total_output_tokens": 2761323,
      "conso_all_conv": 0.0,
      "n_match": 2727,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5260417623608954,
      "win_rate": 0.4623587674247982,
      "useful": 95,
      "creative": 37,
      "complete": 112,
      "clear_formatting": 53,
      "incorrect": 20,
      "superficial": 38,
      "instructions_not_followed": 21,
      "total_prefs": 376,
      "positive_prefs_ratio": 0.7898936170212766
    },
    {
      "model_name": "mistral-saba",
      "median": 1019.6160296918632,
      "p2.5": 1010.5782241133205,
      "p97.5": 1027.7563453191267,
      "rank": 36,
      "rank_p2.5": 33,
      "rank_p97.5": 40,
      "total_output_tokens": 4175657,
      "conso_all_conv": 0.0,
      "n_match": 4737,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5251862876157758,
      "win_rate": 0.480390542537471,
      "useful": 735,
      "creative": 166,
      "complete": 573,
      "clear_formatting": 569,
      "incorrect": 239,
      "superficial": 336,
      "instructions_not_followed": 113,
      "total_prefs": 2731,
      "positive_prefs_ratio": 0.7480776272427682
    },
    {
      "model_name": "llama-4-scout",
      "median": 1018.3118684208971,
      "p2.5": 1009.2499330340011,
      "p97.5": 1027.566863505243,
      "rank": 37,
      "rank_p2.5": 34,
      "rank_p97.5": 41,
      "total_output_tokens": 3448228,
      "conso_all_conv": 0.0,
      "n_match": 4190,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5233803900544021,
      "win_rate": 0.4956532123238596,
      "useful": 650,
      "creative": 170,
      "complete": 587,
      "clear_formatting": 533,
      "incorrect": 224,
      "superficial": 298,
      "instructions_not_followed": 78,
      "total_prefs": 2540,
      "positive_prefs_ratio": 0.7637795275590551
    },
    {
      "model_name": "gpt-5-mini",
      "median": 1014.3331716953793,
      "p2.5": 998.4442955643649,
      "p97.5": 1029.5107243965008,
      "rank": 38,
      "rank_p2.5": 33,
      "rank_p97.5": 44,
      "total_output_tokens": 1533340,
      "conso_all_conv": 0.0,
      "n_match": 1326,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5178684404697785,
      "win_rate": 0.477577358490566,
      "useful": 61,
      "creative": 17,
      "complete": 89,
      "clear_formatting": 40,
      "incorrect": 20,
      "superficial": 26,
      "instructions_not_followed": 24,
      "total_prefs": 277,
      "positive_prefs_ratio": 0.7472924187725631
    },
    {
      "model_name": "llama-maverick",
      "median": 1013.6193368718916,
      "p2.5": 998.6197162088276,
      "p97.5": 1028.8787562710397,
      "rank": 39,
      "rank_p2.5": 33,
      "rank_p97.5": 44,
      "total_output_tokens": 1113044,
      "conso_all_conv": 0.0,
      "n_match": 1377,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5168791758426762,
      "win_rate": 0.5013798111837328,
      "useful": 76,
      "creative": 13,
      "complete": 61,
      "clear_formatting": 47,
      "incorrect": 23,
      "superficial": 43,
      "instructions_not_followed": 8,
      "total_prefs": 271,
      "positive_prefs_ratio": 0.7269372693726938
    },
    {
      "model_name": "mistral-small-3.1-24b",
      "median": 1012.9110344898805,
      "p2.5": 1004.5560184843082,
      "p97.5": 1021.0378979648124,
      "rank": 40,
      "rank_p2.5": 36,
      "rank_p97.5": 43,
      "total_output_tokens": 4470466,
      "conso_all_conv": 0.0,
      "n_match": 5079,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5158974935761487,
      "win_rate": 0.4945107304587517,
      "useful": 1108,
      "creative": 247,
      "complete": 791,
      "clear_formatting": 916,
      "incorrect": 268,
      "superficial": 430,
      "instructions_not_followed": 130,
      "total_prefs": 3890,
      "positive_prefs_ratio": 0.787146529562982
    },
    {
      "model_name": "o4-mini",
      "median": 1012.2512190303784,
      "p2.5": 1000.9626744326856,
      "p97.5": 1022.9395748483489,
      "rank": 41,
      "rank_p2.5": 35,
      "rank_p97.5": 44,
      "total_output_tokens": 2807115,
      "conso_all_conv": 0.0,
      "n_match": 2698,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5149829428343278,
      "win_rate": 0.46999258435298485,
      "useful": 521,
      "creative": 110,
      "complete": 367,
      "clear_formatting": 282,
      "incorrect": 130,
      "superficial": 208,
      "instructions_not_followed": 44,
      "total_prefs": 1662,
      "positive_prefs_ratio": 0.7701564380264742
    },
    {
      "model_name": "gemma-2-27b-it-q8",
      "median": 1006.9012870097957,
      "p2.5": 983.6568160703771,
      "p97.5": 1029.8231606171555,
      "rank": 42,
      "rank_p2.5": 33,
      "rank_p97.5": 51,
      "total_output_tokens": 449540,
      "conso_all_conv": 0.0,
      "n_match": 762,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.5075658030873519,
      "win_rate": 0.5826115485564305,
      "useful": 318,
      "creative": 97,
      "complete": 178,
      "clear_formatting": 305,
      "incorrect": 63,
      "superficial": 91,
      "instructions_not_followed": 29,
      "total_prefs": 1081,
      "positive_prefs_ratio": 0.8307123034227567
    },
    {
      "model_name": "aya-expanse-32b",
      "median": 997.3106749268684,
      "p2.5": 988.2001675772766,
      "p97.5": 1006.0963712064391,
      "rank": 43,
      "rank_p2.5": 42,
      "rank_p97.5": 48,
      "total_output_tokens": 3907887,
      "conso_all_conv": 0.0,
      "n_match": 4880,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4942690491498805,
      "win_rate": 0.466266393442623,
      "useful": 712,
      "creative": 137,
      "complete": 514,
      "clear_formatting": 505,
      "incorrect": 271,
      "superficial": 349,
      "instructions_not_followed": 109,
      "total_prefs": 2597,
      "positive_prefs_ratio": 0.719291490180978
    },
    {
      "model_name": "qwen-3-8b",
      "median": 996.2172068086368,
      "p2.5": 976.0181225301637,
      "p97.5": 1015.3888205925051,
      "rank": 44,
      "rank_p2.5": 39,
      "rank_p97.5": 54,
      "total_output_tokens": 1803955,
      "conso_all_conv": 0.0,
      "n_match": 865,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4927536265579864,
      "win_rate": 0.46109826589595376,
      "useful": 30,
      "creative": 7,
      "complete": 45,
      "clear_formatting": 27,
      "incorrect": 35,
      "superficial": 14,
      "instructions_not_followed": 9,
      "total_prefs": 167,
      "positive_prefs_ratio": 0.6526946107784432
    },
    {
      "model_name": "qwen3-30b-a3b",
      "median": 992.2507282851775,
      "p2.5": 974.9083537157758,
      "p97.5": 1008.8703311842094,
      "rank": 45,
      "rank_p2.5": 41,
      "rank_p97.5": 55,
      "total_output_tokens": 2212489,
      "conso_all_conv": 0.0,
      "n_match": 1328,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48725854879897884,
      "win_rate": 0.44205727204220036,
      "useful": 74,
      "creative": 22,
      "complete": 85,
      "clear_formatting": 40,
      "incorrect": 44,
      "superficial": 35,
      "instructions_not_followed": 15,
      "total_prefs": 315,
      "positive_prefs_ratio": 0.7015873015873015
    },
    {
      "model_name": "mistral-small-24b-instruct-2501",
      "median": 992.0147173792743,
      "p2.5": 981.356807981266,
      "p97.5": 1002.7681662517668,
      "rank": 46,
      "rank_p2.5": 43,
      "rank_p97.5": 52,
      "total_output_tokens": 2818040,
      "conso_all_conv": 0.0,
      "n_match": 3318,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4869317010055289,
      "win_rate": 0.4952350813743219,
      "useful": 745,
      "creative": 155,
      "complete": 536,
      "clear_formatting": 656,
      "incorrect": 204,
      "superficial": 307,
      "instructions_not_followed": 71,
      "total_prefs": 2674,
      "positive_prefs_ratio": 0.7823485415108452
    },
    {
      "model_name": "o3-mini",
      "median": 989.4306192184059,
      "p2.5": 974.6381603622422,
      "p97.5": 1003.6818898235277,
      "rank": 47,
      "rank_p2.5": 42,
      "rank_p97.5": 55,
      "total_output_tokens": 1655945,
      "conso_all_conv": 0.0,
      "n_match": 1619,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48335404706431007,
      "win_rate": 0.51142063001853,
      "useful": 350,
      "creative": 85,
      "complete": 254,
      "clear_formatting": 230,
      "incorrect": 49,
      "superficial": 133,
      "instructions_not_followed": 28,
      "total_prefs": 1129,
      "positive_prefs_ratio": 0.8139946855624446
    },
    {
      "model_name": "llama-3.3-70b",
      "median": 989.3615528442622,
      "p2.5": 982.4305153939893,
      "p97.5": 996.5371744510566,
      "rank": 48,
      "rank_p2.5": 44,
      "rank_p97.5": 52,
      "total_output_tokens": 6489661,
      "conso_all_conv": 0.0,
      "n_match": 7739,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48325845294947395,
      "win_rate": 0.47415740501421566,
      "useful": 1366,
      "creative": 272,
      "complete": 1059,
      "clear_formatting": 1016,
      "incorrect": 429,
      "superficial": 589,
      "instructions_not_followed": 139,
      "total_prefs": 4870,
      "positive_prefs_ratio": 0.762422997946612
    },
    {
      "model_name": "gpt-4o-mini-2024-07-18",
      "median": 988.1808684567239,
      "p2.5": 980.2459462636469,
      "p97.5": 996.3020091199957,
      "rank": 49,
      "rank_p2.5": 45,
      "rank_p97.5": 53,
      "total_output_tokens": 5207480,
      "conso_all_conv": 0.0,
      "n_match": 6990,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.48162451888275265,
      "win_rate": 0.49947067238912735,
      "useful": 1491,
      "creative": 319,
      "complete": 856,
      "clear_formatting": 1282,
      "incorrect": 369,
      "superficial": 599,
      "instructions_not_followed": 110,
      "total_prefs": 5026,
      "positive_prefs_ratio": 0.7855153203342619
    },
    {
      "model_name": "gpt-4.1-nano",
      "median": 984.3441886130095,
      "p2.5": 976.2851642605956,
      "p97.5": 992.1527201228135,
      "rank": 50,
      "rank_p2.5": 46,
      "rank_p97.5": 55,
      "total_output_tokens": 3446522,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.476318441567717,
      "win_rate": 0.4663794683776353,
      "useful": 920,
      "creative": 157,
      "complete": 520,
      "clear_formatting": 644,
      "incorrect": 274,
      "superficial": 443,
      "instructions_not_followed": 101,
      "total_prefs": 3059,
      "positive_prefs_ratio": 0.7325923504413206
    },
    {
      "model_name": "aya-expanse-8b",
      "median": 979.6176161664264,
      "p2.5": 963.2755443143753,
      "p97.5": 994.3474146696265,
      "rank": 51,
      "rank_p2.5": 45,
      "rank_p97.5": 60,
      "total_output_tokens": 1057810,
      "conso_all_conv": 0.0,
      "n_match": 1302,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.469790218620674,
      "win_rate": 0.5014285714285714,
      "useful": 243,
      "creative": 55,
      "complete": 133,
      "clear_formatting": 223,
      "incorrect": 149,
      "superficial": 104,
      "instructions_not_followed": 31,
      "total_prefs": 938,
      "positive_prefs_ratio": 0.697228144989339
    },
    {
      "model_name": "jamba-1.5-large",
      "median": 979.3921881086338,
      "p2.5": 935.630940697686,
      "p97.5": 1021.2903552384059,
      "rank": 52,
      "rank_p2.5": 36,
      "rank_p97.5": 68,
      "total_output_tokens": 143976,
      "conso_all_conv": 0.0,
      "n_match": 172,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46947913147386783,
      "win_rate": 0.5449418604651162,
      "useful": 44,
      "creative": 11,
      "complete": 18,
      "clear_formatting": 57,
      "incorrect": 15,
      "superficial": 13,
      "instructions_not_followed": 4,
      "total_prefs": 162,
      "positive_prefs_ratio": 0.8024691358024691
    },
    {
      "model_name": "llama-3.1-70b",
      "median": 978.1205519830942,
      "p2.5": 969.4548645626397,
      "p97.5": 987.5305844711837,
      "rank": 53,
      "rank_p2.5": 49,
      "rank_p97.5": 57,
      "total_output_tokens": 4057254,
      "conso_all_conv": 0.0,
      "n_match": 5583,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4677247902678885,
      "win_rate": 0.530445996775927,
      "useful": 1532,
      "creative": 365,
      "complete": 1047,
      "clear_formatting": 1305,
      "incorrect": 463,
      "superficial": 621,
      "instructions_not_followed": 177,
      "total_prefs": 5510,
      "positive_prefs_ratio": 0.7711433756805808
    },
    {
      "model_name": "claude-3-5-sonnet-v2",
      "median": 977.3412456202674,
      "p2.5": 968.6964714875851,
      "p97.5": 985.8737920372749,
      "rank": 54,
      "rank_p2.5": 50,
      "rank_p97.5": 58,
      "total_output_tokens": 3224219,
      "conso_all_conv": 0.0,
      "n_match": 5683,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46665009281373443,
      "win_rate": 0.49210804152736226,
      "useful": 1301,
      "creative": 291,
      "complete": 721,
      "clear_formatting": 905,
      "incorrect": 250,
      "superficial": 606,
      "instructions_not_followed": 94,
      "total_prefs": 4168,
      "positive_prefs_ratio": 0.7720729366602687
    },
    {
      "model_name": "Apertus-70B-Instruct-2509",
      "median": 975.6322193638907,
      "p2.5": 953.6346304569321,
      "p97.5": 996.0301290673499,
      "rank": 55,
      "rank_p2.5": 45,
      "rank_p97.5": 63,
      "total_output_tokens": 824946,
      "conso_all_conv": 0.0,
      "n_match": 817,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46429446379186734,
      "win_rate": 0.41535539215686273,
      "useful": 52,
      "creative": 5,
      "complete": 38,
      "clear_formatting": 28,
      "incorrect": 28,
      "superficial": 30,
      "instructions_not_followed": 14,
      "total_prefs": 195,
      "positive_prefs_ratio": 0.6307692307692307
    },
    {
      "model_name": "phi-4",
      "median": 972.6554324130522,
      "p2.5": 966.5847202285643,
      "p97.5": 979.3020892585066,
      "rank": 56,
      "rank_p2.5": 52,
      "rank_p97.5": 59,
      "total_output_tokens": 7582246,
      "conso_all_conv": 0.0,
      "n_match": 9122,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.46019558168791524,
      "win_rate": 0.4748459598728209,
      "useful": 1593,
      "creative": 330,
      "complete": 1152,
      "clear_formatting": 1283,
      "incorrect": 685,
      "superficial": 735,
      "instructions_not_followed": 238,
      "total_prefs": 6016,
      "positive_prefs_ratio": 0.7244015957446809
    },
    {
      "model_name": "ministral-8b-instruct-2410",
      "median": 969.4279978705506,
      "p2.5": 962.8091809012394,
      "p97.5": 975.7672736088783,
      "rank": 57,
      "rank_p2.5": 54,
      "rank_p97.5": 61,
      "total_output_tokens": 7488476,
      "conso_all_conv": 0.0,
      "n_match": 9278,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.45575805372614964,
      "win_rate": 0.47576819407008086,
      "useful": 1796,
      "creative": 412,
      "complete": 1332,
      "clear_formatting": 1695,
      "incorrect": 876,
      "superficial": 927,
      "instructions_not_followed": 274,
      "total_prefs": 7312,
      "positive_prefs_ratio": 0.7159463894967177
    },
    {
      "model_name": "gpt-4o-2024-08-06",
      "median": 968.1291691819082,
      "p2.5": 959.3895842308335,
      "p97.5": 976.6017973383351,
      "rank": 58,
      "rank_p2.5": 54,
      "rank_p97.5": 62,
      "total_output_tokens": 3903917,
      "conso_all_conv": 0.0,
      "n_match": 5896,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4539742877484209,
      "win_rate": 0.48863297150610574,
      "useful": 1230,
      "creative": 243,
      "complete": 593,
      "clear_formatting": 1002,
      "incorrect": 275,
      "superficial": 590,
      "instructions_not_followed": 115,
      "total_prefs": 4048,
      "positive_prefs_ratio": 0.7579051383399209
    },
    {
      "model_name": "qwen2.5-32b-instruct",
      "median": 965.7109079705624,
      "p2.5": 909.8261274774112,
      "p97.5": 1022.1273877715993,
      "rank": 59,
      "rank_p2.5": 36,
      "rank_p97.5": 71,
      "total_output_tokens": 75812,
      "conso_all_conv": 0.0,
      "n_match": 142,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4506564589696577,
      "win_rate": 0.5557042253521126,
      "useful": 52,
      "creative": 16,
      "complete": 43,
      "clear_formatting": 57,
      "incorrect": 13,
      "superficial": 21,
      "instructions_not_followed": 15,
      "total_prefs": 217,
      "positive_prefs_ratio": 0.7741935483870968
    },
    {
      "model_name": "Apertus-8B-Instruct-2509",
      "median": 964.5029439349059,
      "p2.5": 913.0065738132566,
      "p97.5": 1016.2630276009618,
      "rank": 60,
      "rank_p2.5": 38,
      "rank_p97.5": 71,
      "total_output_tokens": 52304,
      "conso_all_conv": 0.0,
      "n_match": 86,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.44900083490265236,
      "win_rate": 0.46869047619047627,
      "useful": 2,
      "creative": 0,
      "complete": 3,
      "clear_formatting": 2,
      "incorrect": 3,
      "superficial": 5,
      "instructions_not_followed": 0,
      "total_prefs": 15,
      "positive_prefs_ratio": 0.4666666666666667
    },
    {
      "model_name": "llama-3.1-405b",
      "median": 963.0013475579492,
      "p2.5": 956.838960349078,
      "p97.5": 969.6375963567826,
      "rank": 61,
      "rank_p2.5": 57,
      "rank_p97.5": 63,
      "total_output_tokens": 10383810,
      "conso_all_conv": 0.0,
      "n_match": 9973,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4469443968255052,
      "win_rate": 0.4950747166783673,
      "useful": 2224,
      "creative": 556,
      "complete": 1579,
      "clear_formatting": 1788,
      "incorrect": 947,
      "superficial": 890,
      "instructions_not_followed": 437,
      "total_prefs": 8421,
      "positive_prefs_ratio": 0.7299608122550766
    },
    {
      "model_name": "gemma-2-9b-it",
      "median": 958.6144099086849,
      "p2.5": 949.7670420240064,
      "p97.5": 967.4987481470187,
      "rank": 62,
      "rank_p2.5": 58,
      "rank_p97.5": 65,
      "total_output_tokens": 3108609,
      "conso_all_conv": 0.0,
      "n_match": 5115,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4409474431740518,
      "win_rate": 0.5068660801564027,
      "useful": 1250,
      "creative": 403,
      "complete": 771,
      "clear_formatting": 1197,
      "incorrect": 472,
      "superficial": 595,
      "instructions_not_followed": 177,
      "total_prefs": 4865,
      "positive_prefs_ratio": 0.7442959917780062
    },
    {
      "model_name": "qwq-32b",
      "median": 956.7009783327858,
      "p2.5": 941.0752887182281,
      "p97.5": 971.0456312601877,
      "rank": 63,
      "rank_p2.5": 56,
      "rank_p97.5": 66,
      "total_output_tokens": 1895013,
      "conso_all_conv": 0.0,
      "n_match": 1566,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43833718842668284,
      "win_rate": 0.4485997442455243,
      "useful": 291,
      "creative": 139,
      "complete": 343,
      "clear_formatting": 233,
      "incorrect": 140,
      "superficial": 105,
      "instructions_not_followed": 92,
      "total_prefs": 1343,
      "positive_prefs_ratio": 0.7490692479523455
    },
    {
      "model_name": "deepseek-r1-distill-llama-70b",
      "median": 956.1096449826694,
      "p2.5": 945.0104025067702,
      "p97.5": 967.1171249835753,
      "rank": 64,
      "rank_p2.5": 59,
      "rank_p97.5": 66,
      "total_output_tokens": 2790055,
      "conso_all_conv": 0.0,
      "n_match": 2939,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4375312030173046,
      "win_rate": 0.44376318475672,
      "useful": 488,
      "creative": 129,
      "complete": 355,
      "clear_formatting": 363,
      "incorrect": 188,
      "superficial": 341,
      "instructions_not_followed": 125,
      "total_prefs": 1989,
      "positive_prefs_ratio": 0.6711915535444947
    },
    {
      "model_name": "gpt-5-nano",
      "median": 954.4506093457223,
      "p2.5": 936.7012149722435,
      "p97.5": 972.5885395212063,
      "rank": 65,
      "rank_p2.5": 56,
      "rank_p97.5": 67,
      "total_output_tokens": 1174720,
      "conso_all_conv": 0.0,
      "n_match": 1110,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.43527174667296575,
      "win_rate": 0.3856396396396397,
      "useful": 38,
      "creative": 13,
      "complete": 42,
      "clear_formatting": 24,
      "incorrect": 14,
      "superficial": 21,
      "instructions_not_followed": 17,
      "total_prefs": 169,
      "positive_prefs_ratio": 0.6923076923076923
    },
    {
      "model_name": "hermes-3-llama-3.1-405b",
      "median": 947.4713590243784,
      "p2.5": 940.1158072102758,
      "p97.5": 955.1346422899555,
      "rank": 66,
      "rank_p2.5": 62,
      "rank_p97.5": 67,
      "total_output_tokens": 4092822,
      "conso_all_conv": 0.0,
      "n_match": 6735,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4257974734932149,
      "win_rate": 0.466347438752784,
      "useful": 1450,
      "creative": 285,
      "complete": 830,
      "clear_formatting": 1109,
      "incorrect": 426,
      "superficial": 753,
      "instructions_not_followed": 184,
      "total_prefs": 5037,
      "positive_prefs_ratio": 0.7294024220766329
    },
    {
      "model_name": "c4ai-command-r-08-2024",
      "median": 935.4844167199344,
      "p2.5": 927.8762381149313,
      "p97.5": 943.5031134369464,
      "rank": 67,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 4319206,
      "conso_all_conv": 0.0,
      "n_match": 5959,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4096556955256036,
      "win_rate": 0.42968786709179396,
      "useful": 1075,
      "creative": 255,
      "complete": 843,
      "clear_formatting": 796,
      "incorrect": 471,
      "superficial": 660,
      "instructions_not_followed": 163,
      "total_prefs": 4263,
      "positive_prefs_ratio": 0.6964578935022285
    },
    {
      "model_name": "llama-3.1-8b",
      "median": 934.7331481405843,
      "p2.5": 928.3095038290019,
      "p97.5": 941.4526310965076,
      "rank": 68,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6744680,
      "conso_all_conv": 0.0,
      "n_match": 9526,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.4086500731211125,
      "win_rate": 0.44287664041994745,
      "useful": 1510,
      "creative": 367,
      "complete": 988,
      "clear_formatting": 1137,
      "incorrect": 994,
      "superficial": 951,
      "instructions_not_followed": 395,
      "total_prefs": 6342,
      "positive_prefs_ratio": 0.631031220435194
    },
    {
      "model_name": "qwen2.5-7b-instruct",
      "median": 932.3234835634825,
      "p2.5": 913.0645009984038,
      "p97.5": 948.4147113148955,
      "rank": 69,
      "rank_p2.5": 65,
      "rank_p97.5": 71,
      "total_output_tokens": 1186919,
      "conso_all_conv": 0.0,
      "n_match": 1417,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40542972402580263,
      "win_rate": 0.49383909668313336,
      "useful": 402,
      "creative": 104,
      "complete": 284,
      "clear_formatting": 380,
      "incorrect": 231,
      "superficial": 203,
      "instructions_not_followed": 88,
      "total_prefs": 1692,
      "positive_prefs_ratio": 0.6914893617021277
    },
    {
      "model_name": "qwen2.5-coder-32b-instruct",
      "median": 932.1294617140293,
      "p2.5": 924.0827420574477,
      "p97.5": 940.4247123473243,
      "rank": 70,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 6149486,
      "conso_all_conv": 0.0,
      "n_match": 7299,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.40517077339946284,
      "win_rate": 0.43789423208658723,
      "useful": 1379,
      "creative": 293,
      "complete": 923,
      "clear_formatting": 1142,
      "incorrect": 790,
      "superficial": 742,
      "instructions_not_followed": 234,
      "total_prefs": 5503,
      "positive_prefs_ratio": 0.6790841359258586
    },
    {
      "model_name": "hermes-4-70b",
      "median": 926.9663443362748,
      "p2.5": 908.9795645185386,
      "p97.5": 944.9468156942535,
      "rank": 71,
      "rank_p2.5": 66,
      "rank_p97.5": 71,
      "total_output_tokens": 516144,
      "conso_all_conv": 0.0,
      "n_match": 1130,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3982993892600978,
      "win_rate": 0.3737787610619469,
      "useful": 43,
      "creative": 10,
      "complete": 27,
      "clear_formatting": 30,
      "incorrect": 26,
      "superficial": 44,
      "instructions_not_followed": 23,
      "total_prefs": 203,
      "positive_prefs_ratio": 0.541871921182266
    },
    {
      "model_name": "mixtral-8x7b-instruct-v0.1",
      "median": 890.0503096263355,
      "p2.5": 876.0908000002087,
      "p97.5": 902.6718213338778,
      "rank": 72,
      "rank_p2.5": 72,
      "rank_p97.5": 73,
      "total_output_tokens": 1575791,
      "conso_all_conv": 0.0,
      "n_match": 2560,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.35044228619406137,
      "win_rate": 0.43488671874999996,
      "useful": 727,
      "creative": 164,
      "complete": 375,
      "clear_formatting": 503,
      "incorrect": 256,
      "superficial": 389,
      "instructions_not_followed": 159,
      "total_prefs": 2573,
      "positive_prefs_ratio": 0.687524290711232
    },
    {
      "model_name": "lfm-40b",
      "median": 887.4925588537002,
      "p2.5": 876.4935337163828,
      "p97.5": 898.8438233656093,
      "rank": 73,
      "rank_p2.5": 72,
      "rank_p97.5": 73,
      "total_output_tokens": 2000824,
      "conso_all_conv": 0.0,
      "n_match": 3578,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3472204689778628,
      "win_rate": 0.416296813862493,
      "useful": 841,
      "creative": 166,
      "complete": 419,
      "clear_formatting": 647,
      "incorrect": 357,
      "superficial": 572,
      "instructions_not_followed": 121,
      "total_prefs": 3123,
      "positive_prefs_ratio": 0.6637848222862632
    },
    {
      "model_name": "phi-3.5-mini-instruct",
      "median": 866.5695785077596,
      "p2.5": 853.1651784728258,
      "p97.5": 880.8460897832529,
      "rank": 74,
      "rank_p2.5": 74,
      "rank_p97.5": 75,
      "total_output_tokens": 2149046,
      "conso_all_conv": 0.0,
      "n_match": 2535,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3213890790598592,
      "win_rate": 0.4021104536489152,
      "useful": 636,
      "creative": 226,
      "complete": 477,
      "clear_formatting": 533,
      "incorrect": 428,
      "superficial": 449,
      "instructions_not_followed": 300,
      "total_prefs": 3049,
      "positive_prefs_ratio": 0.6139717940308298
    },
    {
      "model_name": "mistral-nemo-2407",
      "median": 857.1470122783745,
      "p2.5": 849.1672652334728,
      "p97.5": 866.6592582196291,
      "rank": 75,
      "rank_p2.5": 74,
      "rank_p97.5": 76,
      "total_output_tokens": 3305342,
      "conso_all_conv": 0.0,
      "n_match": 6251,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.3100792122526178,
      "win_rate": 0.37482482802751554,
      "useful": 1413,
      "creative": 225,
      "complete": 661,
      "clear_formatting": 900,
      "incorrect": 638,
      "superficial": 1088,
      "instructions_not_followed": 292,
      "total_prefs": 5217,
      "positive_prefs_ratio": 0.6131876557408472
    },
    {
      "model_name": "mixtral-8x22b-instruct-v0.1",
      "median": 846.7621439034135,
      "p2.5": 838.0100953818936,
      "p97.5": 855.7701844901039,
      "rank": 76,
      "rank_p2.5": 75,
      "rank_p97.5": 77,
      "total_output_tokens": 3041056,
      "conso_all_conv": 0.0,
      "n_match": 5455,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2978615270748346,
      "win_rate": 0.3671035747021082,
      "useful": 1117,
      "creative": 203,
      "complete": 516,
      "clear_formatting": 702,
      "incorrect": 535,
      "superficial": 934,
      "instructions_not_followed": 276,
      "total_prefs": 4283,
      "positive_prefs_ratio": 0.5925752976885361
    },
    {
      "model_name": "chocolatine-2-14b-instruct-v2.0.3-q8",
      "median": 824.5134364534072,
      "p2.5": 809.201798646789,
      "p97.5": 839.205755168224,
      "rank": 77,
      "rank_p2.5": 77,
      "rank_p97.5": 78,
      "total_output_tokens": 533384,
      "conso_all_conv": 0.0,
      "n_match": 1796,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.2726040495297969,
      "win_rate": 0.3317316258351893,
      "useful": 280,
      "creative": 37,
      "complete": 100,
      "clear_formatting": 199,
      "incorrect": 161,
      "superficial": 370,
      "instructions_not_followed": 50,
      "total_prefs": 1197,
      "positive_prefs_ratio": 0.5146198830409356
    },
    {
      "model_name": "Yi-1.5-9B-Chat",
      "median": 770.0273427576122,
      "p2.5": 671.4461862140256,
      "p97.5": 856.7158095805287,
      "rank": 78,
      "rank_p2.5": 75,
      "rank_p97.5": 80,
      "total_output_tokens": 47531,
      "conso_all_conv": 0.0,
      "n_match": 65,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.21642185256162488,
      "win_rate": 0.32599999999999996,
      "useful": 16,
      "creative": 5,
      "complete": 14,
      "clear_formatting": 19,
      "incorrect": 12,
      "superficial": 17,
      "instructions_not_followed": 14,
      "total_prefs": 97,
      "positive_prefs_ratio": 0.5567010309278351
    },
    {
      "model_name": "chocolatine-14b-instruct-dpo-v1.2-q4",
      "median": 756.238337934047,
      "p2.5": 709.119254736399,
      "p97.5": 802.5919008442014,
      "rank": 79,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 131187,
      "conso_all_conv": 0.0,
      "n_match": 309,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.20353550209133722,
      "win_rate": 0.2402265372168285,
      "useful": 83,
      "creative": 17,
      "complete": 39,
      "clear_formatting": 43,
      "incorrect": 52,
      "superficial": 107,
      "instructions_not_followed": 50,
      "total_prefs": 391,
      "positive_prefs_ratio": 0.46547314578005117
    },
    {
      "model_name": "qwen2-7b-instruct",
      "median": 731.5817211300639,
      "p2.5": 634.4175847615679,
      "p97.5": 816.1505316893471,
      "rank": 80,
      "rank_p2.5": 78,
      "rank_p97.5": 80,
      "total_output_tokens": 43550,
      "conso_all_conv": 0.0,
      "n_match": 80,
      "mean_conso_per_match": 0.0,
      "mean_conso_per_token": 0.0,
      "mean_win_prob": 0.1818517565954574,
      "win_rate": 0.2525,
      "useful": 19,
      "creative": 4,
      "complete": 12,
      "clear_formatting": 22,
      "incorrect": 11,
      "superficial": 14,
      "instructions_not_followed": 7,
      "total_prefs": 89,
      "positive_prefs_ratio": 0.6404494382022472
    }
  ]
}
