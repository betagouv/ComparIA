[
  {
    "name": "Meta",
    "icon_path": "meta-color",
    "models": [
      {
        "id": "llama-4-scout",
        "simple_name": "Llama 4 Scout",
        "license": "Llama 4",
        "release_date": "04/2025",
        "arch": "moe",
        "params": 109,
        "active_params": 17,
        "url": "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "meta-llama/llama-4-scout"
        },
        "desc": "Grand modèle doté d’une très large fenêtre de contexte, adapté par exemple à la synthèse d'un ensemble de documents.",
        "size_desc": "Avec 109 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il peut être hébergé sur un serveur doté d’une seule carte graphique très puissante. Sa fenêtre de contexte va jusqu’à 10 millions de jetons, ce qui permet de traiter des corpus documentaires extrêmement longs.",
        "fyi": "Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives . L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. Afin d'équilibrer performances multimodales, raisonnement et qualité conversationnelle, l'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données (pour ne garder que les plus complexes et intéressantes), fine-tuning ciblé et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons et peut traiter jusqu’à 8 images simultanément. \n\nLe modèle a été bien reçu à son lancement, notamment pour sa fenêtre de contexte impressionnante, une première dans le domaine, ainsi que pour son rapport qualité-prix sur des tâches comme le résumé, l’appel d’outils et la génération augmentée (RAG). Cela en fait un choix adapté pour les pipelines automatisés."
      },
      {
        "id": "llama-maverick",
        "simple_name": "Llama 4 Maverick",
        "license": "Llama 4",
        "release_date": "04/2025",
        "arch": "moe",
        "params": 400,
        "active_params": 17,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "meta-llama/llama-4-maverick"
        },
        "desc": "Très grand modèle doté d’une très large fenêtre de contexte, adapté par exemple au résumé de plusieurs documents en même temps.",
        "size_desc": "Avec 400 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il nécessite moins de ressources pour fonctionner que les modèles “denses” de cette taille. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.",
        "fyi": "Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Cela permet de transférer ses compétences plus vite et avec moins de calcul.  Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives - il peut traiter jusqu’à 8 images simultanément. L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. L'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données - en gardant seulement les plus complexes et intéressantes, fine-tuning ciblé et apprentissage par renforcement en ligne, pour équilibrer performances multimodales, raisonnement et qualité conversationnelle. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons. \n\nLe modèle Llama 4 Maverick a été présenté comme la réponse directe de Meta aux modèles DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estimé qu’il ne répondait pas aux attentes, en particulier sur les tâches de programmation et les travaux créatifs."
      },
      {
        "id": "llama-3.3-70b",
        "simple_name": "Llama 3.3 70B",
        "license": "Llama 3.3",
        "release_date": "12/2024",
        "arch": "dense",
        "params": 70,
        "url": "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "meta-llama/llama-3.3-70b-instruct"
        },
        "desc": "Grand modèle destiné à un large éventail de tâches et pouvant rivaliser avec des modèles plus volumineux.",
        "size_desc": "Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation significatifs. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.",
        "fyi": "Ce modèle est une version distillée issue du modèle 405B, auquel il doit une partie de ses connaissances transférées. Il a aussi bénéficié de techniques récentes d’alignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome.  Son entraînement s’appuie sur 15 billions de jetons."
      },
      {
        "status": "archived",
        "id": "llama-3.1-405b",
        "simple_name": "Llama 3.1 405B",
        "license": "Llama 3.1",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 405,
        "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct",
        "endpoint": {
          "api_type": "vertex_ai",
          "api_model_id": "meta/llama3-405b-instruct-maas"
        },
        "desc": "Très grand modèle conçu pour des tâches complexes ou spécialisées. Souvent utilisé en tant que “modèle professeur” pour l’entraînement de modèles plus spécialisés.",
        "size_desc": "Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important. Le modèle est doté d’une fenêtre de contexte jusqu’à 128 000 jetons, ce qui le rend intéressant pour des tâches d’analyse de longs documents.",
        "fyi": "Le modèle a été entraîné sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le marché en 2025). L'entraînement a combiné génération de données synthétiques et optimisation par préférences directes (DPO). Ce modèle est lui-même souvent utilisé pour générer des données synthétiques pour entraîner de plus petits modèles. Le modèle utilise par défaut une compression 8-bit pour réduire les besoins en mémoire et permettre l'exécution sur un seul serveur très puissant."
      },
      {
        "status": "archived",
        "id": "llama-3.1-8b",
        "simple_name": "Llama 3.1 8B",
        "license": "Llama 3.1",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 8,
        "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct",
        "endpoint": {
          "api_base": "https://albert.api.etalab.gouv.fr/v1/",
          "api_model_id": "meta-llama/Llama-3.1-8B-Instruct"
        },
        "desc": "Petit modèle conçu pour un usage local sur ordinateur portable, tout en offrant de bonnes capacités pour la synthèse de texte et les réponses simples.",
        "size_desc": "Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.",
        "fyi": "Ce modèle est une version distillée issue des modèles Llama 3 de plus grande tailles : il a été entraîné grâce à un transfert d’une partie des connaissances des plus grands modèles."
      },
      {
        "status": "archived",
        "id": "llama-3.1-70b",
        "simple_name": "Llama 3.1 70B",
        "license": "Llama 3.1",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 70,
        "url": "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct",
        "desc": "Doté de 70 milliards de paramètres et sorti en avril 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues.",
        "size_desc": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "fyi": "Comme les autres modèles de la famille Llama 3.1, ce modèle sorti en avril 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
      },
      {
        "status": "archived",
        "id": "Meta-Llama-3-70B-Instruct",
        "simple_name": "Llama 3 70B",
        "license": "Llama 3 Community",
        "release_date": "04/2024",
        "arch": "dense",
        "params": 70,
        "desc": "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens mais supporte un contexte relativement restreint de 8000 tokens.",
        "size_desc": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "fyi": "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens."
      },
      {
        "status": "archived",
        "id": "Meta-Llama-3-8B-Instruct",
        "simple_name": "Llama 3 8B",
        "license": "Llama 3 Community",
        "release_date": "04/2024",
        "arch": "dense",
        "params": 8,
        "desc": "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité."
      }
    ]
  },
  {
    "name": "DeepSeek",
    "icon_path": "deepseek-color",
    "models": [
      {
        "new": true,
        "id": "DeepSeek-V3.2",
        "simple_name": "DeepSeek V3.2",
        "license": "Apache 2.0",
        "release_date": "12/2025",
        "arch": "moe",
        "params": 685,
        "active_params": 37,
        "url": "https://huggingface.co/deepseek/deepseek-v3.2",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/deepseek-v3.2"
        },
        "desc": "Très grand modèle conçu pour des tâches complexes : orchestration agentique, génération de code, analyse de documents longs. Cette version est particulièrement forte en utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.",
        "size_desc": "DeepSeek V3.2 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille. La fenêtre de contexte atteint 163 000 jetons, ce qui est utile pour l’analyse de très longs documents ou des bases de code.",
        "fyi": "Pour cette version, le coût de l’API et les besoins en calcul sont réduits d’environ 50 % ou plus pour les contextes longs. Cette amélioration repose sur le \"DeepSeek Sparse Attention (DSA)\", un mécanisme d’attention clairsemée à granularité fine qui calcule l’attention de façon sélective afin de diminuer la complexité sur les longues séquences tout en préservant l’essentiel du contexte.\n\nLe modèle a été entraîné en donnant la priorité aux capacités de raisonnement et aux usages agentiques."
      },
      {
        "id": "deepseek-chat-v3.1",
        "simple_name": "DeepSeek v3.1",
        "license": "MIT",
        "release_date": "08/2025",
        "arch": "moe",
        "params": 685,
        "active_params": 37,
        "reasoning": true,
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3.1",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/deepseek-chat-v3.1"
        },
        "desc": "Très grand modèle conçu pour des tâches complexes : génération de code, analyse de documents longs. Cette version est particulièrement forte en  utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.",
        "size_desc": "DeepSeek V3.1 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.\n\nLa fenêtre de contexte atteint désormais 163 000 jetons, contre 128 000 dans la version précédente, ce qui améliore l’analyse de très longs documents.",
        "fyi": "Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code. L’entraînement a recours au FP8 microscaling, ce qui réduit les coûts de calcul et de mémoire tout en maintenant la précision. Le modèle a été formé en deux phases : d’abord sur des séquences de 32 000 jetons, puis étendu à 163 000 jetons, permettant une meilleure stabilité et une performance accrue sur les contextes très longs."
      },
      {
        "id": "deepseek-r1-0528",
        "simple_name": "DeepSeek R1 0528",
        "license": "MIT",
        "release_date": "05/2025",
        "arch": "moe",
        "params": 685,
        "active_params": 37,
        "reasoning": true,
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1-0528",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/DeepSeek-R1-0528"
        },
        "desc": "Modèle de très grande taille, spécialisé dans les tâches mathématiques, scientifiques et de programmation. Il simule une étape de raisonnement avant de générer sa réponse et avec la mise à jour de mai 2025 a gagné en profondeur d’analyse et en précision grâce à une optimisation du post-entraînement.",
        "size_desc": "Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 163 840 jetons, ce qui est adapté à l’analyse de longs documents.",
        "fyi": "Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (*supervised fine-tuning* : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage. Sa dernière version (DeepSeek-R1-0528) améliore sensiblement ses capacités de raisonnement, réduisant le taux d’hallucination et renforçant l’efficacité en programmation, logique et appel de fonctions. Sur le test AIME 2025, son score est passé de 70 % à 87,5 %, se rapprochant ainsi des modèles comme o3 et Gemini 2.5 Pro."
      },
      {
        "status": "archived",
        "id": "deepseek-v3-0324",
        "simple_name": "DeepSeek V3",
        "license": "MIT",
        "release_date": "03/2025",
        "arch": "moe",
        "params": 685,
        "active_params": 37,
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/deepseek-chat-v3-0324"
        },
        "desc": "Très grand modèle conçu pour des tâches complexes : génération de code, utilisation d’outils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particulièrement adapté à l’anglais et au chinois.",
        "size_desc": "DeepSeek V3 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.\n\nLa fenêtre de contexte atteint 128 000 jetons, ce qui est utile pour l’analyse de longs documents.",
        "fyi": "Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code."
      },
      {
        "status": "archived",
        "id": "deepseek-r1",
        "simple_name": "DeepSeek R1",
        "license": "MIT",
        "release_date": "01/2025",
        "arch": "moe",
        "params": 671,
        "active_params": 37,
        "reasoning": true,
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-R1",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/deepseek-r1"
        },
        "desc": "Très grand modèle très performant sur les tâches mathématiques, scientifiques et de programmation, qui simule une étape de raisonnement avant de générer sa réponse.",
        "size_desc": "Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 163 840 jetons, ce qui est adapté à l’analyse de longs documents.",
        "fyi": "Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (*supervised fine-tuning* : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage."
      },
      {
        "status": "archived",
        "id": "deepseek-r1-distill-llama-70b",
        "simple_name": "DeepSeek R1 Llama 70B",
        "license": "Llama 3.3",
        "release_date": "01/2025",
        "arch": "dense",
        "params": 70,
        "reasoning": true,
        "url": "https://huggingface.co/deepseek-ai/deepseek-r1-distill-llama-70b",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "deepseek/deepseek-r1-distill-llama-70b"
        },
        "desc": "Grand modèle basé sur Meta Llama 3.3 70B, ré-entraîné avec des exemples de raisonnement issus du modèle DeepSeek R1. Il offre de bonnes capacités en mathématiques et code.",
        "size_desc": "Avec 70 milliards de paramètres, ce modèle est classé parmi les modèles de grande taille. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne un coût de fonctionnement élevé. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.\n\nLa fenêtre de contexte est de 16 000 jetons, ce qui peut être limitant pour l’analyse de très grands documents.",
        "fyi": "Le modèle n’a pas été entraîné depuis zéro. Il s’appuie sur Llama 3.3 70B, ré-entraîné en utilisant des résultats générés par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B d’une capacité à simuler le raisonnement, sans possibilité pour l’utilisateur de choisir d’activer ou non cette fonction.\n\nConformément aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du modèle source dans le nom du modèle, soumis au même régime de licence."
      },
      {
        "status": "archived",
        "id": "deepseek-v3-chat",
        "simple_name": "DeepSeek v3",
        "license": "MIT",
        "release_date": "12/2024",
        "arch": "moe",
        "params": 671,
        "active_params": 37,
        "url": "https://huggingface.co/deepseek-ai/DeepSeek-V3",
        "desc": "Sorti en décembre 2024, le modèle DeepSeek V3 possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Sorti en décembre 2024, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
      }
    ]
  },
  {
    "name": "Cohere",
    "icon_path": "cohere-color",
    "models": [
      {
        "id": "command-a",
        "simple_name": "Command A",
        "license": "CC-BY-NC-4.0",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 111,
        "url": "https://huggingface.co/CohereForAI/c4ai-command-a-03-2025",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "cohere/command-a"
        },
        "desc": "Grand modèle, performant pour la programmation, l’utilisation d’outils externes, la “génération augmentée de récupération” (RAG, retrieval augmented generation).",
        "size_desc": "Avec 111 milliards de paramètres, ce modèle fait partie des grands modèles. Il nécessite au moins deux cartes graphiques puissantes pour l’hébergement, ce qui entraîne un coût de fonctionnement significatif.\n\nSa fenêtre de contexte atteint 256 000 jetons, adaptée à l’analyse de vastes ensembles de documents ou de bases de code.",
        "fyi": "Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier [« Attention Is All You Need »](https://arxiv.org/abs/1706.03762) paru en 2017 et qui a révolutionné l'IA. L'entreprise se démarque par sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.\n\nCe modèle est conçu pour fonctionner dans plus de 23 langues et pour s’intégrer facilement dans les systèmes d’entreprise.  Il fait partie des rares modèles distribués sous licence **CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.** Ce choix de licence reflète la volonté de Cohere de contribuer à la recherche et la communauté open source, tout en gardant le contrôle sur les usages commerciaux pour protéger son modèle économique... Cela exclut par exemple l’intégration du modèle dans des produits ou services vendus par une entreprise à des clients mais autorise un usage académique, des tests ou des projets internes, restreints à un cadre non-commercial."
      },
      {
        "status": "archived",
        "id": "aya-expanse-32b",
        "simple_name": "Aya Expanse 32B",
        "license": "CC-BY-NC-4.0",
        "release_date": "12/2024",
        "arch": "dense",
        "params": 32,
        "url": "https://huggingface.co/CohereForAI/aya-expanse-32b",
        "endpoint": {
          "api_base": "https://router.huggingface.co/cohere/compatibility/v1/",
          "api_model_id": "c4ai-aya-expanse-32b"
        },
        "desc": "Modèle de taille moyenne multilingue, capable de traiter 23 langues.",
        "size_desc": "Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.\n\nIl dispose d’une fenêtre de contexte allant jusqu’à 130 000 jetons, utile pour l’analyse de documents longs.",
        "fyi": "Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.\n\nCe modèle a été conçu pour offrir de bonnes capacités dans chacune des 23 langues de son corpus d’entraînement."
      },
      {
        "status": "archived",
        "id": "aya-expanse-8b",
        "simple_name": "Aya Expanse 8B",
        "license": "CC-BY-NC-4.0",
        "release_date": "10/2024",
        "arch": "dense",
        "params": 8,
        "desc": "Petit modèle multilingue, sencode itération de la série Aya, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Aya Expanse 8B de Cohere, entreprise canadienne, est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue."
      },
      {
        "status": "disabled",
        "id": "c4ai-command-r-08-2024",
        "simple_name": "Command R",
        "license": "CC-BY-NC-4.0",
        "release_date": "08/2024",
        "arch": "dense",
        "params": 35,
        "desc": "Modèle de taille moyenne optimisé pour la synthèse, les questions générales, l’utilisation d’outils et efficace dans les systèmes de génération augmentée de récupération (RAG, retrieval augmented generation).",
        "size_desc": "Avec 35 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.",
        "fyi": "Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.\n\nCe modèle a été évalué dans plus de 10 langues. Sa fenêtre de contexte atteint 128 000 jetons, ce qui facilite l’analyse de documents longs. Cette fenêtre a été doublée sur la version suivante du modèle (Command A)."
      },
      {
        "status": "archived",
        "id": "Aya23-35B",
        "simple_name": "Aya23-35B",
        "license": "CC-BY-NC-4.0",
        "release_date": "05/2024",
        "arch": "dense",
        "params": 35,
        "desc": "Modèle de taille moyenne multilingue, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Aya 23 35B de Cohere est un modèle de taille moyenne de la famille Command R qui a spécialement été entraîné sur un corpus multilingue."
      },
      {
        "status": "archived",
        "id": "aya-23-8b",
        "simple_name": "Aya 23 8B",
        "license": "CC-BY-NC-4.0",
        "release_date": "05/2024",
        "arch": "dense",
        "params": 8,
        "desc": "Petit modèle multilingue, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Aya 23 8B de Cohere est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue."
      },
      {
        "status": "archived",
        "id": "command-r-plus",
        "simple_name": "Command R+",
        "license": "CC-BY-NC-4.0",
        "release_date": "04/2024",
        "arch": "dense",
        "params": 104,
        "desc": "Modèle multilingue spécialisé sur 10 langues, spécialisé pour des cas d'usage business.",
        "size_desc": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "fyi": "Grand frère de la famille Command R de Cohere, ce modèle de langage est orienté pour l'usage professionnel et conçu spécifiquement pour les tâches de recherche et d'extraction d'informations."
      }
    ]
  },
  {
    "name": "xAI",
    "icon_path": "xai",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via X et xAI, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "models": [
      {
        "id": "grok-4.1-fast",
        "simple_name": "Grok 4.1 Fast",
        "license": "proprietary",
        "release_date": "11/2025",
        "arch": "maybe-moe",
        "params": 1700,
        "active_params": 121,
        "url": "https://x.ai/news/grok-4-1-fast",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "x-ai/grok-4.1-fast"
        },
        "desc": "Grok 4.1 Fast est une nouvelle itération de la quatrième génération de Grok. Basé sur le même modèle de base, selon l’éditeur il bénéficie d'un processus de post-entraînement amélioré qui lui confère un meilleur style, un comportement agentique supérieur et une meilleure cohérence des réponses, en particulier lors de discussions prolongées.",
        "size_desc": "Grok 4 Fast intègre un mode de raisonnement et un mode de réponse directe. L'utilisateur ou le développeur peut contrôler le choix d'activer ou non le raisonnement. Le modèle est doté d'une fenêtre de contexte de 2 millions de jetons, ce qui lui permet d'analyser des corpus de documents très volumineux ou des bases de code massives.",
        "fyi": "Les évaluations disponibles tendent à indiquer un taux d'hallucinations plus faible pour cette version du modèle. L'entraînement du Grok 4.1 aurait intégré une modélisation des récompenses (reward modeling) élargie, incluant non seulement des évaluations humaines, mais aussi des retours des modèles agissant comme des évaluateurs IA."
      },
      {
        "status": "archived",
        "id": "grok-4-fast",
        "simple_name": "Grok 4 Fast",
        "license": "proprietary",
        "release_date": "09/2025",
        "arch": "maybe-moe",
        "params": 1700,
        "active_params": 121,
        "url": "https://x.ai/news/grok-4-fast",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "x-ai/grok-4-fast"
        },
        "desc": "Grok 4 Fast est un modèle axé sur l'équilibre entre performance, vitesse et coût, notamment pour des tâches de recherche d'information et d'autres actions \"agentiques\".",
        "size_desc": "Doté d’une fenêtre de contexte de 2 millions de jetons, Grok 4 Fast combine un mode raisonnement et un mode réponse directe dans un même modèle. Il utilise environ 40 % de jetons de raisonnement en moins que Grok 4, permettant une réduction sensible du coût d’exécution et de latence.",
        "fyi": "La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 4 Fast est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.\n\nEntraîné par renforcement, Grok 4 Fast atteint des scores proches du très grand modèle - Grok 4 tout en restant plus économe. Il a été entraîné pour être performant sur la navigation web et spécifiquement de la plateforme X, ainsi que des capacités d’appel d’outils et de l’exécution de code."
      },
      {
        "status": "archived",
        "id": "grok-3-mini-beta",
        "simple_name": "Grok 3 Mini",
        "license": "proprietary",
        "release_date": "04/2025",
        "arch": "maybe-moe",
        "params": 180,
        "active_params": 22,
        "reasoning": true,
        "url": "https://x.ai/news/grok-3",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "x-ai/grok-3-mini-beta"
        },
        "desc": "Version plus légère du modèle Grok 3, permettant de réduire les coûts tout en conservant de bonnes performances pour de nombreuses tâches. Il peut simuler une phase de raisonnement avant de fournir une réponse finale.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 3 Mini est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.",
        "fyi": "Grok 3 Mini est une version distillée de Grok 3: il s’en approche en termes de capacités, tout en étant plus rapide et moins coûteux.\nLe modèle propose deux modes : un mode réflexion avec raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses immédiates.\nSa fenêtre de contexte atteint 131 000 jetons, ce qui le rend adapté à l’analyse de longs documents."
      }
    ]
  },
  {
    "name": "Microsoft",
    "icon_path": "microsoft-color",
    "models": [
      {
        "status": "archived",
        "id": "phi-4",
        "simple_name": "Phi-4",
        "license": "MIT",
        "release_date": "12/2024",
        "arch": "dense",
        "params": 14,
        "url": "https://huggingface.co/microsoft/Phi-4",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "microsoft/phi-4"
        },
        "desc": "Petit modèle multilingue, capable d’utiliser des outils et performant sur des tâches complexes comme la logique, les mathématiques et le code, tout en restant compact.",
        "size_desc": "Avec 14 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles. Il peut être déployé localement sur un ordinateur suffisamment puissant, ou hébergé sur un serveur avec une seule carte graphique, ce qui réduit les coûts d’infrastructure. La fenêtre de contexte, de 16 000 jetons, peut être limitante pour l’analyse de documents très longs.",
        "fyi": "Ce modèle utilise tiktoken pour la tokenisation, ce qui améliore ses capacités en contexte multilingue. Il a été entraîné sur un total de 9,8 **billions** de jetons, dont 400 milliards proviennent spécifiquement de données synthétiques de haute qualité, le reste étant constitué de données organiques filtrées. L'entraînement s'est déroulé sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-évaluation – pendant laquelle le modèle critique et réécrit ses réponses – ainsi que l'inversion des instructions ont été utilisées pour renforcer sa compréhension des consignes et ses capacités de raisonnement."
      },
      {
        "status": "archived",
        "id": "phi-3.5-mini-instruct",
        "simple_name": "Phi-3.5-mini",
        "license": "MIT",
        "release_date": "08/2024",
        "arch": "dense",
        "params": 3.8,
        "url": "https://huggingface.co/microsoft/Phi-3.5-mini-instruct",
        "desc": "Performant pour des tâches de génération de code et de résumé, ce modèle gère un grand contexte de 128k tokens.",
        "size_desc": "Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.",
        "fyi": "Petit modèle de la famille Phi, remplaçant Phi-3-mini, ce modèle supporte un grand contexte de 128 000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
      },
      {
        "status": "archived",
        "id": "phi-3-mini-4k",
        "simple_name": "Phi-3-Mini",
        "license": "MIT",
        "release_date": "05/2024",
        "arch": "dense",
        "params": 3.8,
        "url": "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct",
        "desc": "Performant pour des tâches de génération de code, et de résumé, ce modèle compact supporte un contexte restreint de 4000 tokens.",
        "size_desc": "Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.",
        "fyi": "Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
      },
      {
        "status": "archived",
        "id": "Phi-3-small-8k-Instruct",
        "simple_name": "Phi-3-small-8k-Instruct",
        "license": "MIT",
        "release_date": "05/2024",
        "arch": "dense",
        "params": 7,
        "desc": "Optimisé pour le raisonnement logique, ce petit modèle supporte un contexte de 8000 tokens, adapté pour la génération de code et tâches complexes.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
      }
    ]
  },
  {
    "name": "Google",
    "icon_path": "google-color",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Google, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée de Google.",
    "proprietary_reuse_specificities": "sauf pour entraîner d’autres modèles sur Vertex AI",
    "models": [
        {
        "new": true,
        "id": "gemini-3-flash-preview",
        "simple_name": "Gemini 3 Flash",
        "license": "proprietary",
        "release_date": "12/2025",
        "arch": "moe",
        "params": 440,
        "active_params": 137,
        "url": "https://blog.google/products/gemini/gemini-3-flash/",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemini-3-flash-preview"
        },
        "desc": "Grand modèle multimodal natif et multilingue, distillé depuis Gemini 3 Pro. Il intègre une capacité de raisonnement avancée (\"Deep Think\") activable à la demande, avec un niveau de raisonnement paramétrable par l'utilisateur ou le développeur. Le modèle prend en charge nativement le texte, le code, l'audio, l'image, la vidéo et les PDF.",
        "size_desc": "La taille exacte du modèle n'est pas communiquée, mais il repose sur une architecture de type mélange d'experts (MoE). Cette architecture active uniquement un sous-ensemble de paramètres pour chaque jeton, ce qui réduit la puissance de calcul nécessaire. Sa fenêtre de contexte s'étend jusqu'à 1 million de jetons. Le modèle est environ 9 fois moins coûteux que Gemini 3 Pro. L'impact environnemental varie selon le niveau de raisonnement choisi : un raisonnement approfondi génère davantage de jetons et consomme donc plus de ressources.",
        "fyi": "Le modèle a été entraîné sur une infrastructure uniquement composée de TPU (Tensor Processing Units). Gemini 3 Flash peut traiter des volumes importants de contenu multimodal par prompt : jusqu'à 900 images, 900 PDFs de 900 pages chacun, des vidéos de 45 minutes à 1 heure, et jusqu'à 8,4 heures d'audio. Au moment de sa sortie, Gemini 3 Flash se montre très performant tout en étant environ neuf fois moins coûteux que Gemini 3 Pro. "
      },
      {
        "new": true,
        "id": "gemini-3-pro-preview",
        "simple_name": "Gemini 3 Pro",
        "license": "proprietary",
        "release_date": "11/2025",
        "arch": "moe",
        "params": 750,
        "active_params": 45,
        "url": "https://blog.google/products/gemini/gemini-3/#note-from-ceo",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemini-3-pro-preview"
        },
        "desc": "Grand modèle multimodal natif et multilingue. Il intègre une capacité de raisonnement avancée (\"Deep Think\") activable à la demande pour les tâches complexes (mathématiques, logique, codage), distincte de sa capacité de génération standard plus rapide. Le modèle prend en charge nativement le texte, le code, l'audio, l'image, la vidéo et la 3D.",
        "size_desc": "La taille exacte du modèle n'est pas communiquée, mais il repose sur une architecture de type mélange d'experts (MoE). Cette architecture active uniquement un sous-ensemble de paramètres pour chaque jeton d'entrée, ce qui nécessite moins de puissance de calcul pour générer. Sa fenêtre de contexte s'étend jusqu'à 1 million de jetons, adaptée à l'analyse de vastes ensembles de documents, y compris des dépôts de code entiers et des fichiers vidéo.",
        "fyi": "Le modèle a été entraîné sur une infrastructure uniquement composée de TPU (Tensor Processing Units). Au moment de sa sortie, le modèle marque un très grand pas en avant au niveau des évaluations - par exemple sur Humanity's Last Exam, ARC-AGI-2 et MathArena Apex. Le modèle est optimisé pour l'usage \"agentique\" : il a été entraîné pour simuler des étapes de planification, utilisation d'outils, exécution du code et auto-correction au sein d'environnements de code. Sa compréhension multimodale a été affinée pour réduire significativement le nombre de jetons nécessaires à l'encodage vidéo et audio."
      },
      {
        "id": "gemini-2.5-flash",
        "simple_name": "Gemini 2.5 Flash",
        "license": "proprietary",
        "release_date": "06/2025",
        "arch": "moe",
        "params": 440,
        "active_params": 137,
        "endpoint": {
          "api_type": "vertex_ai",
          "api_model_id": "gemini-2.5-flash"
        },
        "desc": "Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant plusieurs cartes graphiques puissantes pour le fonctionnement. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.",
        "fyi": "Ce modèle repose sur une architecture de mélange d’experts (MoE, Mixture of Experts) et a été distillé en ne conservant qu'une approximation des prédictions du modèle enseignant - Gemini 2.5 Pro. Il a été entraîné sur une architecture TPUv5p intégrant des avancées comme la possibilité de poursuivre l'entraînement automatiquement même en cas d’erreurs d’entraînement, de corruption de données ou de problèmes de mémoire.\n\nGemini 2.5 Flash gère des contextes allant jusqu'à 1 million de jetons, et trois heures de contenu vidéo. L'optimisation du traitement de la vision permet de traiter des vidéos environ trois fois plus longues dans la même fenêtre de contexte: seuls 66 jetons visuels sont nécessaires pour générer une image contre 258 auparavant. Ce modèle permet  également la génération audio native pour les dialogues et la synthèse vocale."
      },
      {
        "id": "gemma-3n-e4b-it",
        "simple_name": "Gemma 3n 4B",
        "license": "Gemma",
        "release_date": "05/2025",
        "arch": "matformer",
        "params": 8,
        "active_params": 4,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemma-3n-e4b-it"
        },
        "desc": "Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin.",
        "size_desc": "Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement sur un ordinateur ou un smartphone pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand.\n\nSa fenêtre de contexte va jusqu’à 32 000 jetons.",
        "fyi": "Ce modèle peut traiter du texte, des images et de l’audio. Il repose sur l’architecture MatFormer et un système de cache PLE (per-layer embeddings), qui active uniquement les paramètres utiles selon la tâche, s'adaptant à la capacité des machines sur lesquelles fonctionne le modèle."
      },
      {
        "id": "gemma-3-4b",
        "simple_name": "Gemma 3 4B",
        "license": "Gemma",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 4,
        "url": "https://huggingface.co/google/gemma-3-4b-it",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemma-3-4b-it"
        },
        "desc": "Très petit modèle multimodal et compact adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.",
        "size_desc": "Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand. \n\nSa fenêtre de contexte peut atteindre 128 000 jetons, ce qui permet d’analyser de longs documents.",
        "fyi": "Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques."
      },
      {
        "id": "gemma-3-12b",
        "simple_name": "Gemma 3 12B",
        "license": "Gemma",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 12,
        "url": "https://huggingface.co/google/gemma-3-12b-it",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemma-3-12b-it"
        },
        "desc": "Petit modèle multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.",
        "size_desc": "Avec 12 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. \n\nSa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.",
        "fyi": "Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques."
      },
      {
        "id": "gemma-3-27b",
        "simple_name": "Gemma 3 27B",
        "license": "Gemma",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 27,
        "url": "https://huggingface.co/google/gemma-3-27b-it",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "google/gemma-3-27b-it"
        },
        "desc": "Modèle de taille moyenne multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.",
        "size_desc": "Avec 27 milliards de paramètres, il appartient à la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur avec une seule carte graphique (GPU). \n\nIl accepte des contextes jusqu’à 128 000 jetons, ce qui convient pour l’analyse de documents longs.",
        "fyi": "Il peut traiter du texte et des images sur un serveur équipé d’une seule carte graphique puissante. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques."
      },
      {
        "status": "archived",
        "id": "gemini-2.0-flash",
        "simple_name": "Gemini 2.0 Flash",
        "license": "proprietary",
        "release_date": "12/2024",
        "arch": "moe",
        "params": 440,
        "active_params": 137,
        "url": "https://ai.google.dev/gemini-api",
        "desc": "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés que les modèles Gemini Pro."
      },
      {
        "status": "archived",
        "id": "gemini-1.5-pro",
        "simple_name": "Gemini 1.5 Pro",
        "license": "proprietary",
        "release_date": "09/2024",
        "arch": "moe",
        "params": 600,
        "active_params": 220,
        "url": "https://ai.google.dev/gemini-api",
        "desc": "Sorti en septembre 2024, ce modèle multimodal s'applique à la génération de textes et d'images, l'analyse de vidéos et la transcription d'audio.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Sorti en février 2024 et amélioré en septembre 2024, ce modèle multilingue et multimodal est capable de traiter un très grand volume de données d’entrées qu’il s’agisse de données textuelles, d’image, de son (jusqu’à 11h d’audio) ou de vidéo (jusqu’à une heure). C’est le modèle LLM qui alimente le chatbot Gemini de Google."
      },
      {
        "status": "archived",
        "id": "gemma-2-2b-it",
        "simple_name": "Gemma 2 2B",
        "license": "Gemma",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 2,
        "url": "https://huggingface.co/google/gemma-2-2b-it",
        "desc": "Très petit modèle qui offrait des performances très compétitives pour sa taille et la plupart des tâches.",
        "size_desc": "Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.",
        "fyi": "Petit frère de la famille Gemma 2, ce très petit modèle sorti en juillet 2024 arrive à rivaliser avec des modèles bien plus gros."
      },
      {
        "status": "archived",
        "id": "gemma-2-27b-it-q8",
        "simple_name": "Gemma 2 27B",
        "license": "Gemma",
        "release_date": "06/2024",
        "arch": "dense",
        "params": 27,
        "url": "https://huggingface.co/google/gemma-2-27b-it",
        "desc": "Modèle performant avec une taille correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions. Le modèle sollicité ici est la version quantisée (q8)."
      },
      {
        "status": "archived",
        "id": "gemma-2-27b-it",
        "simple_name": "Gemma 2 27B",
        "license": "Gemma",
        "release_date": "06/2024",
        "arch": "dense",
        "params": 27,
        "url": "https://huggingface.co/google/gemma-2-27b-it",
        "desc": "Modèle performant avec une taille correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions. Le modèle sollicité ici est la version quantisée (q8)."
      },
      {
        "status": "archived",
        "id": "gemma-2-9b-it",
        "simple_name": "Gemma 2 9B",
        "license": "Gemma",
        "release_date": "06/2024",
        "arch": "dense",
        "params": 9,
        "url": "https://huggingface.co/google/gemma-2-9b-it",
        "desc": "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
      }
    ]
  },
  {
    "name": "Alibaba",
    "icon_path": "qwen-color",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Alibaba, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "models": [
      {
        "id": "qwen3-max-2025-09-23",
        "simple_name": "Qwen 3 Max",
        "license": "proprietary",
        "release_date": "09/2025",
        "arch": "moe",
        "params": 1000,
        "active_params": 32,
        "url": "https://qwen.ai/blog?id=241398b9cd6353de490b0f82806c7848c5d2777d&from=research.latest-advancements-list",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-max"
        },
        "desc": "Parmi les rares modèles propriétaires de Qwen, celui-ci est le plus grand et le plus puissant de la troisième génération. Il a été entraîné avec une attention particulière à l’usage en entreprise et aux cas d’utilisation agentiques.",
        "size_desc": "La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 256 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.",
        "fyi": "Ce modèle a été entraîné sur 36 billions de jetons, soit presque le double de Qwen 2.5, et il est officiellement capable de répondre dans 100 langues."
      },
      {
        "new": true,
        "id": "qwen3-coder-next",
        "simple_name": "Qwen3 Coder Next",
        "license": "Apache 2.0",
        "release_date": "02/2026",
        "arch": "moe",
        "params": 80,
        "active_params": 3,
        "url": "https://huggingface.co/Qwen/Qwen3-Coder-Next",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-coder-next"
        },
        "desc": "Modèle conçu spécifiquement pour le code (notamment pour du développement local). Grâce à une architecture hybride combinant Gated DeltaNet, Gated Attention et MoE, il atteint des performances comparables à des modèles bien plus grands tout en n'activant que 3 milliards de paramètres sur 80 milliards au total.",
        "size_desc": "Qwen3-Coder-Next utilise une architecture de mélange d'experts (MoE) ultra-sparse avec 512 experts, dont seulement 10 sont activés par jeton. Cette conception permet de faire fonctionner le modèle sur du matériel haut de gamme mais grand public (MacBook 64 Go, RTX 5090 ou AMD Radeon 7900 XTX) tout en conservant une fenêtre de contexte native de 256 000 jetons.",
        "fyi": "Ce modèle a été entraîné sur plus de 800 000 tâches de code vérifiables via un processus d'apprentissage par renforcement. Il excelle dans le raisonnement sur des horizons longs, l'utilisation d'outils complexes et la récupération après des erreurs d'exécution."
      },
      {
        "status": "archived",
        "id": "Qwen3-Coder-480B-A35B-Instruct",
        "simple_name": "Qwen3 Coder 480B A35B",
        "license": "Apache 2.0",
        "release_date": "07/2025",
        "arch": "moe",
        "params": 480,
        "active_params": 35,
        "reasoning": true,
        "url": "https://huggingface.co/Qwen/Qwen3-Coder-480B-A35B-Instruct",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-coder"
        },
        "desc": "Très grand modèle spécialisé dans la génération de code, analyse de dépôts entiers et résolution de problèmes multi-étapes. Cette version est particulièrement forte en utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.",
        "size_desc": "Qwen3-Coder-480B-A35B-Instruct est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une fraction des paramètres (35 B sur 480 B), ce qui réduit considérablement l'impact environnemental et le coût par rapport à un modèle dense équivalent.\nLa fenêtre de contexte atteint nativement 256 000 jetons et peut être étendue jusqu’à 1 million grâce à des techniques d’extrapolation (YaRN), ce qui est idéal pour l’analyse de bases de code volumineuses.",
        "fyi": "Ce modèle a été pré-entraîné sur 7,5 trillions de jetons (dont 70 % de code), et utilise un processus de post-entraînement avancée - Code RL (Hard to Solve, Easy to Verify) pour renforcer l’exécution correcte du code et Agent RL (long-horizon reinforcement learning) pour optimiser la résolution de tâches logicielles multi-tours, avec un environnement massivement parallèle (20 000 simulations en parallèle sur Alibaba Cloud)."
      },
      {
        "status": "archived",
        "id": "qwen-3-8b",
        "simple_name": "Qwen 3 8B",
        "license": "Apache 2.0",
        "release_date": "07/2025",
        "arch": "dense",
        "params": 8,
        "url": "https://huggingface.co/Qwen/Qwen3-8B-AWQ",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-8b"
        },
        "desc": "Petit modèle multilingue dense de la famille Qwen 3, offrant un mode “raisonnement” pour des tâches complexes (mathématiques, code) et un mode “réponse directe” pour des réponses plus rapides.",
        "size_desc": "Avec 8 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. \n\nSa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.",
        "fyi": "Qwen 3 8B a été entraîné sur le même corpus que les modèles plus grands de la famille Qwen : 36 billions de jetons couvrant 119 langues. Son apprentissage suit trois étapes : pré-entraînement sur 30 billions de jetons avec une fenêtre de 4 000, enrichissement factuel avec 5 billions de jetons, puis une phase spécialisée pour les contextes longs."
      },
      {
        "id": "qwen3-30b-a3b",
        "simple_name": "Qwen 3 30B A3B",
        "license": "Apache 2.0",
        "release_date": "05/2025",
        "arch": "moe",
        "params": 30,
        "active_params": 3.3,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-30b-a3b"
        },
        "desc": "Modèle de taille moyenne multilingue.",
        "size_desc": "Avec 30 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. De plus l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.",
        "fyi": "Ce modèle MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activés par jeton, ce qui permet une inférence plus rapide et plus efficace. Il utilise un système appelé *global-batch* pour optimiser la répartition du travail entre les experts, afin qu'ils soient tous utilisés de manière équilibrée.\n\nContrairement à d'autres modèles comme Qwen 2.5-MoE qui recyclent les mêmes experts à travers plusieurs couches du réseau, Qwen 3 30B A2B attribue des experts uniques à chaque couche. Concrètement, cela signifie que les experts de la première couche ne sont jamais réutilisés dans les couches suivantes - chaque niveau du modèle dispose de son propre ensemble d'experts spécialisés. Cette architecture permet à chaque expert de se concentrer exclusivement sur les tâches spécifiques à sa position dans le réseau neuronal, résultant en une spécialisation plus fine et des performances optimisées pour chaque étape du traitement de l'information."
      },
      {
        "status": "archived",
        "id": "qwq-32b",
        "simple_name": "qwq 32B",
        "license": "Apache 2.0",
        "release_date": "04/2025",
        "arch": "dense",
        "params": 32,
        "reasoning": true,
        "url": "https://huggingface.co/Qwen/QwQ-32B",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwq-32b"
        },
        "desc": "Modèle de raisonnement de taille moyenne spécialisé et très performant en mathématiques, génération de code, et résolution de problèmes logiques.",
        "size_desc": "Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Néanmoins, les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse car une phase de raisonnement précède la génération du résultat final, ce qui augmente la consommation énergétique.",
        "fyi": "Ce modèle a été entraîné avec une méthode d’apprentissage par renforcement (RL) pour optimiser la gestion des problèmes de mathématiques et des tâches de programmation. Il utilise plusieurs techniques récentes pour améliorer la qualité des réponses. Par exemple, la méthode RoPE (Rotary Position Embedding) lui permet de mieux comprendre l’ordre des mots dans un texte. La fonction d'activation SwiGLU est une manière plus efficace de gérer les calculs au sein du réseau de neurones qui aide le modèle à produire des réponses plus fiables. La méthode d'ajustement QKV (Query Key Value-biais) améliore la manière dont le modèle repère et sélectionne les informations importantes. Enfin, grâce à la méthode YaRN (Yet another RoPE extensioN method), il peut traiter de très longs textes allant jusqu’à 130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou très détaillés."
      },
      {
        "status": "disabled",
        "id": "qwen2.5-coder-32b-instruct",
        "simple_name": "Qwen 2.5 Coder 32B",
        "license": "Apache 2.0",
        "release_date": "04/2025",
        "arch": "dense",
        "params": 32,
        "url": "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct",
        "desc": "Modèle de taille moyenne spécialisé en programmation et dans l’usage d’outils externes (recherches web, interactions avec des logiciels…).",
        "size_desc": "Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.\n\nSa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.",
        "fyi": "Ce modèle a été entraîné sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code spécialisés comme Haskell ou Racket. \n\nGrâce à ses performances en code, il est  capable de bien gérer les appels à des outils externes, ce qui est utile pour des usages agentiques."
      },
      {
        "status": "archived",
        "id": "qwen3-32b",
        "simple_name": "Qwen 3 32B",
        "license": "Apache 2.0",
        "release_date": "04/2025",
        "arch": "dense",
        "params": 32,
        "reasoning": true,
        "url": "https://huggingface.co/Qwen/Qwen3-32B",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "qwen/qwen3-32b"
        },
        "desc": "Modèle de taille moyenne multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.",
        "size_desc": "Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.\n\nSa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.",
        "fyi": "Ce modèle a été entraîné sur un très grand volume de données : 36 billions de jetons, en 119 langues. L'entraînement s’est fait en trois étapes. Le modèle a d'abord appris à partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont été ajoutés pour renforcer ses connaissances factuelles. Enfin, il a été exposé à un corpus spécifique pour l’aider à mieux gérer les très longs textes. Résultat : il dispose en fin d'entrainement d'une fenêtre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents."
      },
      {
        "status": "archived",
        "id": "qwen2.5-7b-instruct",
        "simple_name": "Qwen2.5-7B",
        "license": "Apache 2.0",
        "release_date": "09/2024",
        "arch": "dense",
        "params": 7,
        "url": "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct",
        "desc": "Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Petit modèle de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
      },
      {
        "status": "archived",
        "id": "qwen2.5-32b-instruct",
        "simple_name": "Qwen2.5-32B",
        "license": "Apache 2.0",
        "release_date": "09/2024",
        "arch": "dense",
        "params": 32,
        "url": "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct",
        "desc": "Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Modèle intermédiaire de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
      },
      {
        "status": "archived",
        "id": "Qwen2-57B-A14B-Instruct",
        "simple_name": "Qwen2-57B-A14B-Instruct",
        "license": "Apache 2.0",
        "release_date": "07/2024",
        "arch": "moe",
        "params": 57,
        "active_params": 14,
        "desc": "Modèle de taille moyenne avec une architecture de mix d'experts, performant en code, mathématiques et tâches multilingues.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Cette itération des modèles Qwen a des fenêtres de contexte plus longues."
      },
      {
        "status": "archived",
        "id": "Qwen2-72b-instruct",
        "simple_name": "Qwen2-72b-instruct",
        "license": "Apache 2.0",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 72,
        "url": "https://huggingface.co/Qwen/Qwen2-72B-Instruct",
        "desc": "Grand modèle performant en code, mathématiques et tâches multilingues.",
        "size_desc": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "fyi": "Cette itération des modèles Qwen a des fenêtres de contexte plus longues."
      },
      {
        "status": "archived",
        "id": "qwen2-7b-instruct",
        "simple_name": "Qwen2-7B",
        "license": "Apache 2.0",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 7,
        "url": "https://huggingface.co/Qwen/Qwen2-7B-Instruct",
        "desc": "Supportant un contexte de 130k tokens, ce petit modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Petit frère de la famille Qwen2 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
      },
      {
        "status": "archived",
        "id": "Qwen1.5-32B-Chat",
        "simple_name": "Qwen1.5-32B",
        "license": "Apache 2.0",
        "release_date": "02/2024",
        "arch": "dense",
        "params": 32,
        "desc": "Modèle de taille moyenne avec un processus d'entraînement très ciblé sur l'alignement aux préférences des utilisateurs.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Le modèle a traversé une phase d'alignement avec les préférences des utilisateurs via des techniques telles que le DPO (Direct Preference Optimization) et PPO (Proximal policy optimization). En plus de ces techniques, très innovantes au moment de la conception du modèle, les équipes d'Alibaba ont aussi optimisé les données d'entraînement pour les rendre très multilingues, notamment sur des langues européennes, d'Asie de l'Est et Sud-Est."
      },
      {
        "simple_name": "Qwen 2.5 max 0125",
        "license": "proprietary",
        "status": "missing_data",
        "id": "qwen-2.5-max-0125",
        "release_date": "01/2025",
        "params": 325,
        "active_params": 2.7,
        "arch": "moe",
        "desc": "Très grand modèle de raisonnement spécialisé et très performant en mathématiques, code et résolution de problèmes logiques.",
        "size_desc": "Ce modèle propriétaire basé sur une **architecture MoE à grande échelle a été**entraîné sur **plus de 20 billions de jetons**. Il est conçu pour des tâches nécessitant plusieurs étapes de réflexion. \n\nLa fenêtre de contexte va jusqu’à 32 000 jetons.",
        "fyi": "La taille exacte du modèle n’est pas connue, mais c’est très probablement un très grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse."
      }
    ]
  },
  {
    "name": "Moonshot AI",
    "icon_path": "moonshot",
    "models": [
        {
        "id": "kimi-k2.5",
        "simple_name": "Kimi K2.5",
        "license": "MIT",
        "release_date": "01/2026",
        "arch": "moe",
        "params": 1000,
        "active_params": 32,
        "reasoning": true,
        "url": "https://moonshotai.github.io/Kimi-K2/thinking.html",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "moonshotai/kimi-k2.5"
        },
        "desc": "Construit à partir de Kimi K2, l'éditeur à continué le pré-entraînement sur environ 15 billions de jetons supplémentaires, mêlant texte et données visuelles. L’éditeur met en avant sa multimodalité native ainsi que ses capacités agentiques, particulièrement adaptées à des tâches comme le développement logiciel.",
        "size_desc": "Avec 1 billion de paramètres au total, dont environ 32 milliards activés pour chaque jeton généré, ce modèle fait partie des modèles de très grande taille. Son architecture de type mélange d’experts (MoE) permet toutefois une utilisation plus efficace des ressources que des modèles denses de taille comparable. Le modèle est par ailleurs quantifié nativement en INT4, une technique qui consiste à réduire la précision des poids afin de simplifier les calculs et de limiter le temps de calcul à l’inférence. Sa fenêtre de contexte peut atteindre 256 000 jetons, ce qui permet le traitement de documents très volumineux ou de bases de code étendues.",
        "fyi": "Le modèle a été entraîné avec un mode spécifique appelé « Agent Swarm » (essaim d’agents). Dans ce mode d’orchestration, le problème n’est pas résolu de manière strictement séquentielle, mais découpé en plusieurs sous-tâches exécutées en parallèle par des sous-agents. Le modèle a appris au cours de son entraînement à agir comme un agent principal : il analyse la demande, décide combien de sous-agents lancer, leur assigne des objectifs, puis agrège leurs résultats. Cette capacité a été atteinte grâce à des méthodes d’apprentissage par renforcement multi-agent, où l’orchestrateur était récompensé lorsqu’il choisissait une bonne décomposition du problème et une coordination efficace entre agents, et pénalisé lorsqu’il adoptait un raisonnement trop linéaire ou redondant."
      },
      {
        "id": "kimi-k2-thinking",
        "simple_name": "Kimi K2 Thinking",
        "license": "MIT",
        "release_date": "11/2025",
        "arch": "moe",
        "params": 1000,
        "active_params": 32,
        "reasoning": true,
        "url": "https://moonshotai.github.io/Kimi-K2/thinking.html",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "moonshotai/kimi-k2-thinking"
        },
        "desc": "Cette version de Kimi K2 intègre une phase de raisonnement plus avancée, améliorant ses performances par rapport à l'itération originale. Il a été développé par Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), une société basée à Pékin.",
        "size_desc": "Avec 1 billion de paramètres (dont 32 milliards actifs par jeton), ce modèle figure parmi les modèles de très grande taille. Son architecture de Mélange d'Experts (MoE) permet néanmoins une meilleure efficacité par rapport à certains modèles de taille comparable. De plus, il a été quantifié nativement au niveau INT4. Cette quantification, qui consiste à « arrondir » les poids du modèle à moins de chiffres pour simplifier les calculs, est présentée par Moonshot AI comme permettant de réduire le temps d'inférence d'un facteur 2. La fenêtre de contexte atteint 256 000 jetons, offrant la capacité de traiter des documents très longs ou des bases de code importantes.",
        "fyi": "Le modèle semble démontrer des performances accrues sur les raisonnements longs et les tâches nécessitant des étapes d'utilisation d'outils. Ces caractéristiques sont supposées favoriser une « autonomie » prolongée dans l'exécution des tâches."
      },
      {
        "id": "kimi-k2",
        "simple_name": "Kimi K2",
        "license": "MIT",
        "release_date": "09/2025",
        "arch": "moe",
        "params": 1000,
        "active_params": 32,
        "reasoning": true,
        "url": "https://huggingface.co/moonshotai/Kimi-K2-Instruct",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "moonshotai/kimi-k2-0905"
        },
        "desc": "Développé par Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), une société basée à Pékin, Kimi K2 est un très grand modèle orienté code et usages agentiques. Il est reconnu pour les tâches de développement dans des contextes agentiques (par ex. dans Cursor ou Windsurf) notamment pour son rôle en tant qu’orchestrateur. Il n’expose pas de “mode raisonnement” explicite, mais pour les grandes tâches il sous-divise sa réponse en étapes et alterne entre actions (appels d’outils) et rédaction de texte.",
        "size_desc": "Avec 1 billion de paramètres, ce modèle est un des plus grands modèles qui existe. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.",
        "fyi": "Pour stabiliser l’entraînement à très grande échelle, Moonshot AI a introduit MuonClip, un “limiteur de vitesse” pour l’entraînement qui permet d’entraîner un modèle de cette taille et sur un corpus de 15,5 trillions de jetons sans dérailler dans l’apprentissage.\n\nCôté données, K2 a beaucoup pratiqué en “simulateur” avec de vrais outils (navigateur, terminal, exécuteurs de code, API…). Comme un pilote sur simulateur, il apprend à planifier, essayer, rater puis réessayer, et à enchaîner plusieurs actions pour atteindre un objectif. Résultat: il est particulièrement bon pour orchestrer des outils et réussir des tâches en plusieurs étapes."
      }
    ]
  },
  {
    "name": "Nvidia",
    "icon_path": "nvidia-color",
    "models": [
      {
        "status": "archived",
        "id": "llama-3.1-nemotron-70b-instruct",
        "simple_name": "Nemotron Llama 3.1 70B",
        "license": "Llama 3.1",
        "release_date": "10/2024",
        "arch": "dense",
        "params": 70,
        "url": "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "nvidia/llama-3.1-nemotron-70b-instruct"
        },
        "desc": "Grand modèle entraîné à partir de Llama 3.1 70B. Cette version réentraînée (fine-tune) a tendance à détailler davantage et fournir des réponses plus structurées.",
        "size_desc": "Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation notables.",
        "fyi": "Ce modèle est issu d’un réentraînement du Llama 3.1 70B, d'où la présence de son modèle-source dans son nom ! Il introduit des améliorations grâce à l’apprentissage par renforcement avec retour humain (RLHF) et à l’algorithme REINFORCE : le modèle explore différentes réponses, reçoit des retours sous forme de récompenses, puis ajuste ses choix progressivement pour mieux répondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilisé quand on veut que le modèle s’adapte à des préférences humaines ou qu’il optimise ses réponses selon des critères spécifiques."
      }
    ]
  },
  {
    "name": "OpenAI",
    "icon_path": "openai",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI ou via les services Microsoft Azure, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "models": [
      {
        "new": true,
        "id": "gpt-5.2",
        "simple_name": "GPT 5.2",
        "license": "proprietary",
        "release_date": "12/2025",
        "arch": "maybe-moe",
        "params": 300,
        "active_params": 94,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-5.2"
        },
        "desc": "Troisième itération de GPT 5, avec une attention spéciale portée à son utilité sur des tâches professionelles.",
        "size_desc": "Les tailles des modèles derrière le système GPT-5.2 ne sont pas connues. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse. GPT 5.2 prend en charge une fenêtre de contexte allant jusqu’à 400 000 jetons, ce qui le rend adapté à l’analyse de corpus volumineux ou de bases de code étendues.",
        "fyi": "Le modèle a été présenté comme atteignant, voire dépassant, le niveau de performance d’experts humains sur le benchmark GDPval, qui évalue des tâches professionnelles numériques bien définies. Il propose différents niveaux d’effort de raisonnement configurables par l’utilisateur ou le développeur, permettant d’arbitrer entre coût, latence et qualité des réponses selon la complexité de la tâche. GPT 5.2 se distingue également par sa capacité à retrouver des informations précises au sein de contextes très volumineux, y compris lorsque celles-ci sont rares et dispersées dans de longs corpus (needle in the haystack)."
      },
      {
        "id": "gpt-5.1",
        "simple_name": "GPT 5.1",
        "license": "proprietary",
        "release_date": "11/2025",
        "arch": "maybe-moe",
        "params": 300,
        "active_params": 94,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-5.1"
        },
        "desc": "Deuxième itération de GPT 5, avec un style jugé (par l'éditeur) plus naturel, avec de meilleurs résultats en code et en tâches d’agents. Le modèle a la spécificité d'ajuster son temps de raisonnement selon la difficulté.",
        "size_desc": "Les tailles des modèles derrière le système GPT-5.1 ne sont pas connues. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.",
        "fyi": "L’arrivée de cette nouvelle itération apporte un routage automatique entre deux modes. Pour les demandes simples, le modèle choisit une réponse rapide. Pour les tâches plus complexes, il bascule vers un mode avec raisonnement avant de répondre. Cette logique évite de perdre du temps sur les requêtes faciles tout en gardant une vraie profondeur quand il le faut.\n\nLa première sortie de la famille GPT 5 avait provoqué beaucoup de réactions négatives vis-à-vis de son style d'écriture. Beaucoup d’utilisateurs trouvaient le modèle moins chaleureux que GPT 4o. Cette itération vise à revenir vers un style plus chaleureux.\n\nCette nouvelle version marque aussi des progrès en code et en mathématiques qui sont confirmés par des benchmarks comme SWE Bench pour le code et AIME pour les compétences mathématiques."
      },
      {
        "id": "gpt-oss-120b",
        "simple_name": "GPT OSS-120B",
        "license": "Apache 2.0",
        "release_date": "08/2025",
        "arch": "moe",
        "params": 117,
        "active_params": 5.1,
        "reasoning": true,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-oss-120b"
        },
        "desc": "Le plus grand des deux premiers modèles semi-ouverts d'OpenAI depuis GPT-2. Conçu en réponse à la montée en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modèle de raisonnement performant, notamment sur des tâches complexes et dans des environnements « agentiques ».",
        "size_desc": "L'architecture est basée sur le principe du « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (5,1 milliards par jeton) pour chaque requête. C’est un modèle de raisonnement, donc sa consommation d’énergie est plus élevée car ils génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.",
        "fyi": "Ce modèle peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux. \n\nDans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (*low*, *medium*, et *high*) qui déterminent la verbosité du modèle."
      },
      {
        "id": "gpt-oss-20b",
        "simple_name": "GPT OSS-20B",
        "license": "Apache 2.0",
        "release_date": "08/2025",
        "arch": "moe",
        "params": 21,
        "active_params": 3.6,
        "reasoning": true,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-oss-20b"
        },
        "desc": "Le plus petit des deux modèles semi-ouverts d'OpenAI. Il a été conçu en réponse à la concurrence de l'open source et est destiné aux cas d'utilisation nécessitant une faible latence ainsi qu'aux déploiements locaux ou spécialisés.",
        "size_desc": "Avec 20 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. L'architecture est basée sur le « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (3,6 milliards par jeton) pour chaque requête. Il s'agit d'un modèle de raisonnement, ce qui se traduit par une consommation d'énergie plus élevée car il génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.",
        "fyi": "Ce modèle peut être exécuté localement sur un ordinateur portable haut de gamme équipé de seulement 16 Go de VRAM (ou de RAM système). Cela en fait une option très accessible pour les développeurs. \n\nDans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (*low*, *medium*, et *high*) qui déterminent la verbosité du modèle."
      },
      {
        "status": "archived",
        "id": "gpt-5",
        "simple_name": "GPT 5",
        "license": "proprietary",
        "release_date": "08/2025",
        "arch": "maybe-moe",
        "params": 300,
        "active_params": 94,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-5"
        },
        "desc": "Le GPT-5 n'est pas un modèle unique, mais un système unifié composé de deux modèles distincts : un modèle rapide (`gpt-5-main`) pour les requêtes courantes et un modèle de raisonnement (`gpt-5-thinking`) pour les problèmes complexes. Comparé à ses prédécesseurs, OpenAI affirme qu'il est plus utile dans les requêtes du monde réel, avec des améliorations notables dans les domaines de l'écriture, du codage et de la santé. Il réduit également le phénomène des hallucinations. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.",
        "size_desc": "Le système GPT-5 est composé de modèles de différentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est conçue pour inclure plusieurs modèles, orchestrés par un système de routage interne, qui sélectionne le plus petit modèle adapté à la tâche pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.",
        "fyi": "Les développeurs qui utilisent ce modèle peuvent configurer un paramètre de verbosité pour ajuster la longueur de la phase de raisonnement.\n\nEn matière de sécurité, le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête. Les créateurs du modèle ont aussi utilisé la phase d’entraînement au “raisonnement” pour le rendre plus “résistant” aux tentatives de contournement de leurs règles de sécurité (*jailbreaking*)."
      },
      {
        "id": "gpt-5-mini",
        "simple_name": "GPT 5 Mini",
        "license": "proprietary",
        "release_date": "08/2025",
        "arch": "maybe-dense",
        "params": 47,
        "url": "https://platform.openai.com/docs/models/gpt-5-mini",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-5-mini"
        },
        "desc": "Le GPT-5 Mini est une version allégée du modèle GPT-5 principal. Il est conçu pour être utilisé dans des environnements où il est nécessaire de limiter les coûts, par exemple à grande échelle. Son modèle de raisonnement est presque aussi performant que celui du modèle principal (`gpt-5-thinking`) malgré sa taille plus petite. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.",
        "size_desc": "Le modèle Mini est une déclinaison plus compacte (taille moyenne supposée) du système GPT-5. Il est conçu pour fonctionner de manière optimale pour un bon équilibre entre performance et coût, grâce à un système de routage qui le sélectionne pour des tâches spécifiques. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Néanmoins, les modèles sont probablement très grands, nécessitant plusieurs cartes graphiques puissantes pour l’inférence.",
        "fyi": "Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.\n\nBien qu'il soit une version plus petite, il se montre très compétitif face au modèle GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine médical."
      },
      {
        "id": "gpt-5-nano",
        "simple_name": "GPT 5 Nano",
        "license": "proprietary",
        "release_date": "04/2025",
        "arch": "maybe-dense",
        "params": 11,
        "url": "https://platform.openai.com/docs/models/gpt-5-nano",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-5-nano"
        },
        "desc": "Le GPT-5 Nano est la plus petite et la plus rapide version du modèle de raisonnement GPT-5. Il est conçu pour des contextes où une latence ou un coût ultra-faible est nécessaire. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.",
        "size_desc": "Le modèle Nano est le plus compact de la famille GPT-5 (taille petite supposée). Il est sélectionné par le système de routage pour les requêtes nécessitant une latence ultra-faible et des réponses instantanées. Son architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui permet une meilleure efficacité énergétique et des performances élevées, même sur des requêtes nécessitant une réponse rapide.",
        "fyi": "Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête."
      },
      {
        "status": "archived",
        "id": "gpt-4.1-nano",
        "simple_name": "GPT 4.1 Nano",
        "license": "proprietary",
        "release_date": "04/2025",
        "arch": "maybe-dense",
        "params": 23,
        "url": "https://openai.com/index/gpt-4-1/",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-4.1-nano"
        },
        "desc": "Plus petite version allégée du modèle GPT 4.1 , conçue pour limiter les coûts tout en restant compétitive sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.  Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.",
        "fyi": "Il s'agit d'une version distillée d’un modèle de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio. Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus de textes ou de dépôts de code très longs."
      },
      {
        "status": "archived",
        "id": "gpt-4.1-mini",
        "simple_name": "GPT-4.1 Mini",
        "license": "proprietary",
        "release_date": "04/2025",
        "arch": "maybe-dense",
        "params": 75,
        "url": "https://platform.openai.com/docs/models/gpt-4.1-mini",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/gpt-4.1-mini"
        },
        "desc": "Version allégée de GPT 4.1 mais qui reste tout de même de grande taille, conçue pour limiter les coûts tout en restant compétitif sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.",
        "fyi": "Il s'agit d'une version distillée d’un modèle plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio.  Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus très longs ou de dépôts de code."
      },
      {
        "status": "archived",
        "id": "o4-mini",
        "simple_name": "o4 mini",
        "license": "proprietary",
        "release_date": "04/2025",
        "arch": "maybe-dense",
        "params": 18,
        "reasoning": true,
        "url": "https://platform.openai.com/docs/models/o4-mini",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "openai/o4-mini"
        },
        "desc": "Très grand modèle de raisonnement, adapté pour des tâches et questions scientifiques et technologiques complexes.",
        "size_desc": "Malgré son nom et le fait que la taille exacte n’est pas connue, o4 mini est très probablement un grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Les modèles de raisonnement comme o4 mini nécessitent plus de temps pour répondre, car une phase de raisonnement précède la génération du résultat final, ce qui accroit leur consommation énergétique. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres pour générer chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.",
        "fyi": "Ce modèle est très performant pour l’analyse d’images et de graphiques. Il a aussi été entraîné pour interagir avec d’autres systèmes via des appels de fonctions, ce qui rend possible son utilisation pour des cas d’usage agentiques. En tant que modèle très puissant de raisonnement, il peut notamment être utilisé pour répartir des tâches entre plusieurs modèles plus petits et/ou plus spécialisés.  Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, ce qui facilite l’analyse de longs documents."
      },
      {
        "status": "archived",
        "id": "o3-mini",
        "simple_name": "o3-mini",
        "license": "proprietary",
        "release_date": "11/2024",
        "arch": "maybe-dense",
        "params": 18,
        "reasoning": true,
        "url": "https://platform.openai.com/docs/models/o3-mini",
        "desc": "o3-mini est fait pour le raisonnement et le code. Il offre un bon équilibre entre performance, coût et latence, tout en étant plus petit que d'autres modèles de chez OpenAI.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Modèle optimisé pour les tâches de raisonnement STEM (science, technologie, ingénierie, mathématiques) et l'écriture de code. Il se démarque dans les domaines scientifiques, mathématiques et de la programmation."
      },
      {
        "status": "archived",
        "id": "gpt-4o-2024-08-06",
        "simple_name": "GPT-4o",
        "license": "proprietary",
        "release_date": "08/2024",
        "arch": "maybe-moe",
        "params": 440,
        "active_params": 137.5,
        "url": "https://platform.openai.com/docs/models/gpt-4o",
        "desc": "Le plus grand des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en août 2024.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Modèle lancé en août 2024 et successeur de GPT-4, GPT-4o est une version améliorée de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA."
      },
      {
        "status": "archived",
        "id": "gpt-4o-mini-2024-07-18",
        "simple_name": "GPT-4o mini",
        "license": "proprietary",
        "release_date": "07/2024",
        "arch": "maybe-dense",
        "params": 18,
        "url": "https://platform.openai.com/docs/models/gp#gpt-4o-mini",
        "desc": "Le plus petit des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en juillet 2024.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Modèle lancé en juillet 2024 et remplaçant GPT-3.5, GPT-4o mini est une version plus petite de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA."
      },
      {
        "status": "archived",
        "id": "gpt-3.5-turbo-0301",
        "simple_name": "GPT-3.5",
        "license": "proprietary",
        "release_date": "03/2023",
        "arch": "maybe-dense",
        "params": 55,
        "url": "https://platform.openai.com/docs/models/gp#gpt-3-5-turbo",
        "desc": "Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel.",
        "size_desc": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "fyi": "Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel."
      }
    ]
  },
  {
    "name": "Mistral AI",
    "icon_path": "mistral-color",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres hébergeurs, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "models": [
      {
        "new": true,
        "id": "mistral-large-2512",
        "simple_name": "Mistral 3 Large",
        "license": "Apache 2.0",
        "release_date": "12/2025",
        "arch": "moe",
        "params": 675,
        "active_params": 41,
        "url": "https://mistral.ai/news/mistral-3",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/mistral-large-2512"
        },
        "desc": "Très grand modèle multimodal semi-ouvert performant en code et contextes multilingues.",
        "size_desc": "Avec 675 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que des modèles denses de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Ce modèle active 41 milliards de paramètres par génération de jeton. Sa fenêtre de contexte va jusqu’à 256 000 jetons, ce qui permet de traiter de très longs documents.",
        "fyi": "Ce modèle a été entraîné sur 3 000 GPU Nvidia H200. Il se positionne en concurrence directe avec les modèles semi-ouverts chinois. À sa sortie, il est compétitif avec DeepSeek V3.1, Kimi K2, GLM 4.6 et d’autres références en termes de code et cas d'usages généralistes."
      },
      {
        "id": "mistral-medium-2508",
        "simple_name": "Mistral Medium 3.1",
        "license": "proprietary",
        "release_date": "08/2025",
        "arch": "maybe-dense",
        "params": 123,
        "url": "https://mistral.ai/news/mistral-medium-3.1",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/mistral-medium-3.1"
        },
        "desc": "Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est devenu particulièrement intéressant après une mise à jour en août 2025 avec des améliorations importantes de performance générale, un ton \"amélioré\" et une meilleure capacité de chercher des informations sur Internet.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.\n\nIl dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.",
        "fyi": "Ce modèle a été conçu pour offrir des performances solides à un coût inférieur à celui des modèles propriétaires ou semi-ouverts. Une attention particulière a été portée aux données d’usage professionnel pendant son entraînement. Il est particulièrement bon en comparaison à d’autres modèles de taille similaire à générer du code et réaliser des tâches mathématiques.\n\nCe modèle a servi de base pour entraîner Magistral Medium - un modèle de raisonnement."
      },
      {
        "id": "mistral-small-2506",
        "simple_name": "Mistral Small 3.2",
        "license": "Apache 2.0",
        "release_date": "06/2025",
        "arch": "dense",
        "params": 24,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/mistral-small-3.2-24b-instruct"
        },
        "desc": "Malgré son nom, c’est un modèle de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se démarque par un respect précis des requêtes et sa capacité à utiliser des outils avancées.",
        "size_desc": "Avec 32 milliards de paramètres, ce modèle est considéré comme un modèle de taille moyenne. Il peut être hébergé sur un serveur disposant d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.",
        "fyi": "La version 3.2 de ce modèle est optimisée pour générer des sorties structurées, notamment en JSON, tout en limitant la répétitivité et les comportements indésirables lors de longues générations. Multimodal, il traite à la fois des entrées textuelles et des images, permettant une analyse conjointe."
      },
      {
        "status": "archived",
        "id": "magistral-medium",
        "simple_name": "Magistral Medium",
        "license": "proprietary",
        "release_date": "06/2025",
        "arch": "maybe-dense",
        "params": 123,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/magistral-medium-2506"
        },
        "desc": "Modèle de raisonnement de taille moyenne multimodal et multilingue. Adapté à des tâches de programmation ou autres tâches nécessitant analyse approfondie compréhension de systèmes logiques complexes ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement requièrent plus de capacité de calcul pour produire une réponse, ce qui augmente leur consommation énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.\n\nIl dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.",
        "fyi": "Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. Il a été entraîné avec de l’apprentissage par renforcement sur Mistral Medium 3 et n'a pas été distillé à partir de modèles de raisonnement existants. Ce modèle hérite des capacités multimodales de Mistral Medium 3 même si l'apprentissage par renforcement n'a été réalisé que sur du texte."
      },
      {
        "status": "archived",
        "id": "magistral-small-2506",
        "simple_name": "Magistral Small",
        "license": "Apache 2.0",
        "release_date": "06/2025",
        "arch": "dense",
        "params": 24,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/magistral-small-2506"
        },
        "desc": "Modèle de raisonnement de taille moyenne, multimodal et multilingue. Adapté à des tâches nécessitant une analyse approfondie, compréhension de systèmes logiques ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.",
        "size_desc": "Avec 24 milliards de paramètres, ce modèle est classé parmi les modèles de taille moyenne. Il nécessite une seule carte graphiques puissante pour fonctionner. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.\n\nIl dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.",
        "fyi": "Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. \n\nL'entraînement s'est fait en deux phases. La première, dite de raisonnement *cold-start* par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au modèle d'acquérir des capacités de base en raisonnement à partir de données d'instruction générale (10%). La seconde est une phase d'apprentissage par renforcement (RL, *renforcement learning*) à haute entropie, où le modèle est encouragé à explorer des solutions diverses et variées plutôt que de converger vers une seule réponse, et à générer des complétions longues (jusqu'à 32 000 jetons), ce qui permet de développer des capacités de raisonnement qui dépassent celles du modèle enseignant."
      },
      {
        "status": "archived",
        "id": "mistral-small-3.1-24b",
        "simple_name": "Mistral Small 3.1 24B",
        "license": "Apache 2.0",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 24,
        "url": "https://huggingface.co/mistralai/mistral-small-3.1-24b-instruct-2503",
        "desc": "Mistral Small 3.1 24B Instruct est une variante améliorée du Mistral Small 3 (janvier 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Mistral Small 3.1 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues."
      },
      {
        "status": "archived",
        "id": "mistral-small-3.2-24b",
        "simple_name": "Mistral Small 3.2 24B",
        "license": "Apache 2.0",
        "release_date": "03/2025",
        "arch": "dense",
        "params": 24,
        "url": "https://huggingface.co/mistralai/mistral-small-3.2-24b-instruct-2506",
        "desc": "Mistral Small 3.2 24B Instruct est une variante améliorée du Mistral Small 3.1 (mars 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Mistral Small 3.2 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues."
      },
      {
        "status": "archived",
        "id": "mistral-saba",
        "simple_name": "Mistral Saba",
        "license": "proprietary",
        "release_date": "02/2025",
        "arch": "maybe-dense",
        "params": 24,
        "url": "https://mistral.ai/news/mistral-saba",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "mistralai/mistral-saba"
        },
        "desc": "Modèle de taille moyenne conçu pour une compréhension linguistique et culturelle fine des langues du Moyen-Orient et d’Asie du Sud, notamment l’arabe, le tamoul et le malayalam.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. \n\nLe modèle propose une fenêtre de contexte allant jusqu’à 128 000 jetons, adaptée à l’analyse de longs documents.",
        "fyi": "L’entraînement a porté principalement sur des textes en arabe, tamoul et malayalam. Les corpus régionaux ont été sélectionnés pour refléter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (découpage du texte en unités de base que le modèle peut traiter), une stratégie spécialisée adaptée aux langues à morphologie complexe comme l'arabe a été employée. Des optimisations visent à éviter la fragmentation excessive des mots et à maximiser la couverture du vocabulaire."
      },
      {
        "status": "archived",
        "id": "mistral-small-24b-instruct-2501",
        "simple_name": "Mistral Small 3",
        "license": "Apache 2.0",
        "release_date": "01/2025",
        "arch": "dense",
        "params": 24,
        "url": "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501",
        "desc": "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme et possède des capacités de raisonnement avancées.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme, possède un mode d'appel de fonction et un contexte de 32 000 tokens."
      },
      {
        "status": "disabled",
        "id": "mistral-large-2411",
        "simple_name": "Mistral Large 2",
        "license": "Mistral AI Research License",
        "release_date": "11/2024",
        "arch": "dense",
        "params": 123,
        "url": "https://huggingface.co/mistralai/Mistral-Large-Instruct-2411",
        "desc": "Grand modèle prévu pour traiter des questions et tâches complexes : par exemple génération de code, utilisation d’outils, analyse de documents longs ou compréhension précise du langage.",
        "size_desc": "Avec 123 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite un serveur équipé d’au moins une carte graphique puissante, ce qui implique un coût de fonctionnement important. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.",
        "fyi": "Ce modèle a été entraîné avec une forte proportion de données en code (plus de 80 langages de programmation) et de mathématiques, ce qui améliore sa capacité à résoudre des problèmes complexes et à utiliser des outils externes."
      },
      {
        "status": "disabled",
        "id": "ministral-8b-instruct-2410",
        "simple_name": "Ministral",
        "license": "Mistral AI Research License",
        "release_date": "10/2024",
        "arch": "dense",
        "params": 8,
        "url": "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410",
        "desc": "Petit modèle multilingue conçu pour fonctionner sur un ordinateur portable sans connexion à un serveur, tout en offrant de bonnes capacités en synthèse de texte, réponses à des questions simples et utilisation d’outils.",
        "size_desc": "Avec ses 8 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles (entre 7 et 20 milliards de paramètres). Il peut être déployé localement sur un ordinateur assez puissant, garantissant la confidentialité des données ou hébergé sur un serveur avec une seule carte graphique pour limiter les coûts d’infrastructure.",
        "fyi": "Ce modèle utilise une méthode d'attention de requête groupée (GQA, grouped query attention) pour limiter le texte analysé à chaque étape de génération et gagner en vitesse et en mémoire: les temps de calculs sont réduits sans incidence sur la qualité. Le mécanisme d'attention est amélioré en appliquant des fenêtres de tailles différentes, ce qui permet de gérer de longs contextes (jusqu’à 128 000 jetons) tout en restant léger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui améliore ses performances sur des tâches multilingues."
      },
      {
        "status": "archived",
        "id": "mistral-nemo-2407",
        "simple_name": "Mistral Nemo",
        "license": "Apache 2.0",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 12,
        "url": "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407",
        "desc": "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter un contexte de 128k tokens en plus de 100 langues. Sorti en juillet 2024.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Sorti en juillet 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes allant jusqu’à 128 000 tokens en plus de 100 langues."
      },
      {
        "status": "archived",
        "id": "mixtral-8x22b-instruct-v0.1",
        "simple_name": "Mixtral 8x22B",
        "license": "Apache 2.0",
        "release_date": "04/2024",
        "arch": "moe",
        "params": 176,
        "active_params": 44,
        "url": "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1",
        "desc": "Ce modèle multilingue sorti en avril 2024 a particulièrement été entraîné en anglais, français, allemand, italien et espagnol ainsi que sur des tâches de mathématiques, programmation et raisonnement.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents. Sorti en avril 2024."
      },
      {
        "status": "archived",
        "id": "mixtral-8x7b-instruct-v0.1",
        "simple_name": "Mixtral-8x7B",
        "license": "Apache 2.0",
        "release_date": "12/2023",
        "arch": "moe",
        "params": 56,
        "active_params": 14,
        "url": "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1",
        "desc": "Ce modèle entraîné sur un corpus multilingue est efficace pour des tâches variées et peu complexes.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence."
      }
    ]
  },
  {
    "name": "Amazon",
    "icon_path": "aws-color",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via Amazon Bedrock, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "proprietary_reuse_specificities": "sauf pour distiller ou entraîner d’autres modèles sur les plateformes d’Amazon.",
    "models": [
      {
        "id": "nova-lite",
        "status": "missing_data",
        "simple_name": "Nova Lite",
        "license": "proprietary",
        "release_date": "12/2024",
        "desc": "Modèle multimodal et multilingue.",
        "fyi": "Ce modèle est capable de traiter plus de 200 langues et est multimodal, acceptant des entrées textuelles, des images et des vidéos. \n\nLa fenêtre de contexte atteint 300 000 jetons, ce qui est adapté à l’analyse de très longs documents."
      }
    ]
  },
  {
    "name": "Anthropic",
    "icon_path": "anthropic",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic ou des sociétés partenaires, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
    "models": [
      {
        "id": "claude-4-5-sonnet",
        "simple_name": "Claude 4.5 Sonnet",
        "license": "proprietary",
        "release_date": "09/2025",
        "arch": "maybe-moe",
        "params": 440,
        "active_params": 137.5,
        "url": "https://www.anthropic.com/news/claude-sonnet-4-5",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "anthropic/claude-sonnet-4.5"
        },
        "desc": "Très grand modèle multimodal et multilingue, extrêmement performant en code, en raisonnement et en mathématiques. L'utilisateur ou le développeur qui utilise ce modèle peut choisir entre plusieurs niveaux de raisonnement.",
        "size_desc": "La taille exacte n’est pas connue. Tout indique qu’il s’agit d’un très grand modèle, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour le faire fonctionner. Les estimations de taille et de consommation énergétique reposent sur des indices indirects comme les coûts d’inférence et la latence observée. Claude Sonnet 4.5 dispose d’une fenêtre de contexte allant jusqu’à 1 000 000 jetons, adaptée à l’analyse de dépôts de code entiers ou de documents très volumineux.",
        "fyi": "Claude Sonnet 4.5 est une évolution directe de Sonnet 4. Le « .5 » désigne les changements majeurs introduits lors du post-entraînement, qui se traduisent par des gains significatifs en raisonnement, en mathématiques et surtout dans l’utilisation concrète des ordinateurs. Au moment de sa sortie, il est considéré comme le meilleur modèle au monde pour le code et excelle dans la résolution de tâches multi-étapes longues et complexes. Ses performances sur des benchmarks comme SWE-bench Verified et OSWorld marquent un net progrès par rapport aux versions précédentes, avec une capacité à maintenir sa \"concentration\" pendant plus de trente heures sur un même problème."
      },
      {
        "status": "archived",
        "id": "claude-4-sonnet",
        "simple_name": "Claude 4 Sonnet",
        "license": "proprietary",
        "release_date": "05/2025",
        "arch": "maybe-moe",
        "params": 440,
        "active_params": 137.5,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "anthropic/claude-sonnet-4"
        },
        "desc": "Très grand modèle multimodal et multilingue très puissant en code. L'utilisateur ou le développeur qui utilise ce modèle peut choisir entre plusieurs niveaux de raisonnement.",
        "size_desc": "La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 1 000 000 jetons, adaptée à l’analyse de très longs documents ou de dépôts de code.",
        "fyi": "Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisée pour la vitesse, l’efficacité et l’accessibilité. Il est un peu moins bon sur les tâches qui demandent un raisonnement complexe en plusieurs étapes. Néanmoins, il est nettement moins coûteux, plus rapide et consomme moins d’énergie que Opus 4.\n\nLe modèle offre la possibilité de choisir l'intensité de \"raisonnement\". À la différence d’autres modèles, le mode raisonnement n’a pas été surtout entraîné sur des données mathématiques, mais surtout sur des cas d’usage réels."
      },
      {
        "status": "archived",
        "id": "claude-3-7-sonnet",
        "simple_name": "Claude 3.7 Sonnet",
        "license": "proprietary",
        "release_date": "02/2025",
        "arch": "maybe-moe",
        "params": 440,
        "active_params": 137.5,
        "url": "https://www.anthropic.com/news/claude-3-7-sonnet",
        "desc": "Très grand modèle multimodal et multilingue, performant pour la génération de code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.",
        "size_desc": "La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.",
        "fyi": "Claude 4 Opus est la version la plus avancée de la famille Claude 4. Il est optimisé pour la puissance brute et les tâches complexes nécessitant un raisonnement soutenu sur de longues périodes : il peut par exemple travailler sur des tâches à long terme (Anthropic déclarent qu'il peut travailler jusqu'à sept heures de manière indépendante). En contrepartie, Opus est plus coûteux à utiliser, plus lent à répondre et nécessite davantage de ressources pour fonctionner.\n\nLe modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été majoritairement entraîné sur des données mathématiques, mais adapté à des cas d’usage réels."
      },
      {
        "status": "archived",
        "id": "claude-3-5-sonnet-v2",
        "simple_name": "Claude 3.5 Sonnet v2",
        "license": "proprietary",
        "release_date": "10/2024",
        "arch": "maybe-moe",
        "params": 440,
        "active_params": 137.5,
        "url": "https://www.anthropic.com/news/3-5-models-and-computer-use",
        "desc": "Modele tres performant en code, fait apres une amelioration de post-training par rapport au claude 3",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Meilleur modèle de la famille Claude 3.5, ce modèle est spécialisé dans la génération de textes littéraires et un ton plus naturel. La version v2 est sortie en octobre 2024."
      }
    ]
  },
  {
    "name": "Zhipu",
    "icon_path": "zhipu-color",
    "models": [
        {
        "id": "glm-4.7",
        "simple_name": "GLM 4.7",
        "license": "MIT",
        "release_date": "12/2025",
        "arch": "moe",
        "params": 357,
        "active_params": 32,
        "url": "https://z.ai/blog/glm-4.7",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "z-ai/glm-4.7"
        },
        "desc": "Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent. Cette mise à jour améliore sa performance en code (notamment pour les interfaces web), interragi mieux avec des environnements de code assisté par IA et plus généralement performe mieux dans des contextes agentiques.",
        "size_desc": "Avec 357 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 200 000 jetons, ce qui permet de traiter de très longs documents.",
        "fyi": "Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications et des interfaces. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin."
      },
      {
        "id": "glm-4.6",
        "simple_name": "GLM 4.6",
        "license": "MIT",
        "release_date": "09/2025",
        "arch": "moe",
        "params": 357,
        "active_params": 32,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "z-ai/glm-4.6"
        },
        "desc": "Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent. Cette mise à jour augmente la taille de la fenêtre de contexte, améliore sa performance en code, s'aligne plus avec les préférences humaines et est plus capable en cas d'usages agentiques/utilisation d'outils.",
        "size_desc": "Avec 357 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 200 000 jetons, ce qui permet de traiter de très longs documents.",
        "fyi": "Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications web complètes et de générer des artefacts, qui sont des programmes d’un seul fichier utilisable à l’intérieur même des interfaces des agents conversationnels. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin."
      },
      {
        "status": "archived",
        "id": "glm-4.5",
        "simple_name": "GLM 4.5",
        "license": "MIT",
        "release_date": "07/2025",
        "arch": "moe",
        "params": 355,
        "active_params": 32,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "z-ai/glm-4.5"
        },
        "desc": "Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent.  Le modèle a deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.",
        "size_desc": "Avec 355 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.",
        "fyi": "Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications web complètes et de générer des artefacts, qui sont des programmes d’un seul fichier utilisable à l’intérieur même des interfaces des agents conversationnels. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin."
      }
    ]
  },
  {
    "name": "Nous",
    "icon_path": "nousresearch",
    "models": [
      {
        "id": "hermes-4-70b",
        "simple_name": "Hermes 4 70B",
        "license": "Llama 3.1",
        "release_date": "08/2025",
        "arch": "dense",
        "params": 70,
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "nousresearch/hermes-4-70b"
        },
        "desc": "Grand modèle réentraîné à partir du Llama 3.1 70B, ajusté pour mieux répondre aux demandes et instructions stylistiques des utilisateurs.",
        "size_desc": "Hermes 4-70B est un modèle de très grande taille, nécessitant au moins une carte graphique puissante.\n\nLa fenêtre de contexte atteint 40 960 jetons en raisonnement et 32 768 jetons pour d’autres tâches, avec des mécanismes de fine-tuning qui ont été utilisés pour lui apprendre à “fermer” la séquence de réflexion vers 30k jetons.",
        "fyi": "Hermes 4-70B a été entraîné sur 56 milliards de tokens en combinant Fully Sharded Data Parallel (FSDP) et Tensor Parallelism pour gérer sa taille. Le modèle repose sur la base de Llama 3.1 70B, adaptée avec TorchTitan et enrichie par environ 19 milliards de tokens synthétiques centrés sur le raisonnement. Son entraînement suit une approche multi-phase, avec un supervised fine-tuning sur des chaînes de raisonnement pouvant dépasser les 30 000 jetons. Il exploite également l’environnement Atropos, utilisé pour générer et vérifier des trajectoires complexes (code, JSON, tâches agentiques), grâce à un rejection sampling massif qui garantit la qualité des données)."
      },
      {
        "status": "disabled",
        "id": "hermes-3-llama-3.1-405b",
        "simple_name": "Hermes 3 405B",
        "license": "Llama 3.1",
        "release_date": "07/2024",
        "arch": "dense",
        "params": 405,
        "url": "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B",
        "desc": "Très grand modèle réentraîné à partir du Llama 3.1 405B, ajusté pour mieux répondre aux demandes des utilisateurs et faciliter l’utilisation d’outils externes.",
        "size_desc": "Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important.",
        "fyi": "Ce modèle est le résultat d’un réentraînement de l’ensemble des paramètres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et système - l’utilisateur dispose ainsi d’un plus grand contrôle sur la “personnalité” et comportement du modèle. Des fonctions de raisonnement spécifiques telles que **`<SCRATCHPAD>`**, **`<REASONING>`**, **`<THINKING>`** ont été ajoutées pour simuler un raisonnement sur les tâches complexes. L'entraînement a utilisé un outil appelé AdamW (vitesse d'apprentissage de 3.5×10⁻⁶), qui aide le modèle à apprendre de manière efficace en ajustant progressivement ses paramètres. Ensuite, il a été affiné avec une méthode appelée DPO (direct preference optimisation), qui permet d'améliorer ses réponses en se basant sur des préférences spécifiques. Pour rendre cet entraînement plus léger et rapide, des adaptateurs LoRA ont été utilisés ; ce sont des modules plus petits qui modifient seulement une partie du modèle, ce qui évite de devoir retravailler tous les paramètres en même temps."
      }
    ]
  },
  {
    "name": "jpacifico",
    "icon_path": "huggingface-color",
    "models": [
      {
        "status": "archived",
        "id": "chocolatine-2-14b-instruct-v2.0.3-q8",
        "simple_name": "Chocolatine 2 14B",
        "license": "MIT",
        "release_date": "02/2025",
        "arch": "dense",
        "params": 14,
        "quantization": "q8",
        "url": "https://huggingface.co/jpacifico/jpacifico/Chocolatine-2-14B-Instruct-v2.0.3",
        "desc": "Sorti en février 2025, et reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Sorti en février 2025, et reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français. Il est proposé ici en version quantisée q8 (compressée de moitié)."
      },
      {
        "status": "archived",
        "id": "chocolatine-14b-instruct-dpo-v1.2",
        "simple_name": "Chocolatine 14B",
        "license": "MIT",
        "release_date": "09/2024",
        "arch": "dense",
        "params": 14,
        "url": "https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-DPO-v1.2",
        "desc": "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."
      },
      {
        "status": "archived",
        "id": "chocolatine-14b-instruct-dpo-v1.2-q4",
        "simple_name": "Chocolatine 14B",
        "license": "MIT",
        "release_date": "09/2024",
        "arch": "dense",
        "params": 14,
        "quantization": "q4",
        "url": "https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-DPO-v1.2-Q4_K_M-GGUF",
        "desc": "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."
      },
      {
        "status": "archived",
        "id": "chocolatine-2-14b-instruct-v2.0.3",
        "simple_name": "Chocolatine 2 14B",
        "license": "MIT",
        "release_date": "09/2024",
        "arch": "dense",
        "params": 14,
        "url": "https://huggingface.co/jpacifico/jpacifico/Chocolatine-2-14B-Instruct-v2.0.3",
        "desc": "Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."
      }
    ]
  },
  {
    "name": "Ai2",
    "icon_path": "ai2.svg",
    "models": [
      {
        "id": "olmo-3-32b-think",
        "simple_name": "Olmo 3 32B Think",
        "license": "Apache 2.0",
        "fully_open_source": true,
        "release_date": "11/2025",
        "arch": "dense",
        "params": 32,
        "reasoning": true,
        "url": "https://allenai.org/blog/olmo2-32b",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "allenai/olmo-3-32b-think"
        },
        "desc": "Modèle de raisonnement dont le code et les données sont complètement ouverts. Il a été entraîné par AI2, un institut de recherche à but non lucratif.",
        "size_desc": "Avec 32 milliards de paramètres, il est classé dans la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur équipé d'une seule carte graphique (GPU). Sa fenêtre de contexte atteint 65 000 jetons, ce qui le rend adapté à l'analyse d'assez longs documents.",
        "fyi": "Ce modèle est un exemple rare de transparence totale. Le code, les données (6 billions de jetons), les “checkpoints” d'entraînement et les recettes d'évaluation sont tous publics. Bien que la reproduction complète de l'entraînement soit coûteuse en infrastructure et probablement rare, cette transparence permet aux développeurs de mieux réentrainer (fine-tune) le modèle pour des tâches spécifiques, notamment en mélangeant les données de comportement souhaitées avec le jeu de données existant, garantissant ainsi un entraînement plus stable."
      },
      {
        "status": "archived",
        "id": "olmo-2-0325-32b",
        "simple_name": "OLMo-2 32B",
        "license": "Apache 2.0",
        "fully_open_source": true,
        "release_date": "03/2025",
        "arch": "dense",
        "params": 32,
        "url": "https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct",
        "desc": "OLMo 2 32B est un modèle entièrement open source (corpus et code d'entraînement inclus) créé par l'Allen AI Institute (Ai2), publié en mars 2025. ",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "OLMo 2 32B est un modèle entièrement open source : le corpus et le code d'entraînement sont entièrement accessibles. Cette famille de modèle OLMo a été conçue par l'Allen Institute for AI (Ai2)."
      }
    ]
  },
  {
    "name": "AI21",
    "icon_path": "ai21",
    "models": [
      {
        "status": "archived",
        "id": "jamba-1.5-large",
        "simple_name": "Jamba 1.5 Large",
        "license": "Jamba Open Model",
        "release_date": "08/2024",
        "arch": "moe",
        "params": 400,
        "active_params": 94,
        "url": "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large",
        "desc": "Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.",
        "size_desc": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
        "fyi": "Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' (State Space Models) et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
      }
    ]
  },
  {
    "name": "Liquid",
    "icon_path": "liquid",
    "proprietary_license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Liquid AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
    "models": [
      {
        "id": "lfm2-8b-a1b",
        "simple_name": "LFM 2 8B A1B",
        "license": "LFM 1.0",
        "release_date": "10/2025",
        "arch": "moe",
        "params": 8,
        "active_params": 1.5,
        "url": "https://www.liquid.ai/blog/lfm2-8b-a1b-an-efficient-on-device-mixture-of-experts",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "liquid/lfm2-8b-a1b"
        },
        "desc": "Un modèle spécifiquement conçu pour l'inférence efficace sur appareils locaux (on-device deployment). Son architecture vise à offrir une qualité de sortie compétitive avec celle de modèles denses de plus grande taille, tout en minimisant la latence et les exigences en ressources de calcul.",
        "size_desc": "Le modèle possède un total de 8,3 milliards de paramètres. Grâce à une architecture de mélange d'experts (Mixture of Experts, MoE), l'activation est limitée à environ 1,5 milliard de paramètres par jeton lors de l'inférence. Cette configuration lui permet d'atteindre une performance généralement associée aux modèles denses de la classe des 3 à 4 milliards de paramètres, avec une vitesse d'exécution comparable à celle des modèles de 1,5 milliard de paramètres. Il est proposé dans des variantes quantifiées optimisées pour les environnements locaux (comme par exemple pour un smartphone). La fenêtre de contexte est limitée à 32 000 jetons.",
        "fyi": "LFM2-8B-A1B est basé sur une architecture hybride intégrant des blocs de convolution ainsi que des mécanismes d'Attention à Requêtes Groupées (GQA). L'implémentation de mélange d'experts (MoE) est composée de 32 experts par bloc de transformation. Le \"routeur\" sélectionne et active les 4 meilleurs experts (top-4 gating) pour le traitement de chaque jeton. Cette conception vise à établir une référence pour les modèles de mélange d'experts optimisés pour l'inférence locale."
      },
      {
        "status": "archived",
        "id": "lfm-40b",
        "simple_name": "LFM 40B",
        "license": "proprietary",
        "release_date": "09/2024",
        "arch": "moe",
        "params": 40,
        "active_params": 12,
        "url": "https://www.liquid.ai/liquid-foundation-models",
        "desc": "Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.",
        "size_desc": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "fyi": "Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts (MoE), dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
      }
    ]
  },
  {
    "name": "01-ai",
    "icon_path": "yi-color",
    "models": [
      {
        "status": "archived",
        "id": "Yi-1.5-9B-Chat",
        "simple_name": "Yi-1.5 9B",
        "license": "Apache 2.0",
        "release_date": "05/2024",
        "arch": "dense",
        "params": 9,
        "url": "https://huggingface.co/01-ai/Yi-1.5-9B-Chat",
        "desc": "Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage.",
        "size_desc": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "fyi": "Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage."
      }
    ]
  },
  {
    "name": "Swiss AI",
    "icon_path": "swissai.webp",
    "models": [
      {
        "status": "archived",
        "id": "Apertus-8B-Instruct-2509",
        "simple_name": "Apertus 8B Instruct",
        "license": "Apache 2.0",
        "fully_open_source": true,
        "release_date": "09/2025",
        "arch": "dense",
        "params": 8,
        "url": "https://huggingface.co/swiss-ai/Apertus-8B-Instruct-2509",
        "desc": "Petit modèle semi-ouvert et reproductible, développé par un consortium d’institutions suisses. Ses poids et son code d’entraînement sont publiés sous licence permissive. Il a été entraîné sur plus de 1 800 langues sur plus de 15 000 milliards de jetons. L’entraînement a eu lieu sur le supercalculateur Alps du CSCS à Lugano, alimenté par une énergie hydroélectrique neutre en carbone.",
        "size_desc": "Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 65 536 jetons ce qui permet de traiter d'assez longs documents.",
        "fyi": "Le modèle a été entraîné sur le supercalculateur Alps à Lugano, utilisant plus de 10 millions d’heures GPU alimentées par une énergie hydroélectrique neutre en carbone. Le modèle a été préentraîné sur 15 000 milliards jetons couvrant plus de 1 800 langues, dont une part importante de langues peu représentées.\n\nApertus s’appuie sur un tokenizer byte-level BPE de 131 000 entrées, dérivé du tokenizer « tekken » de Mistral AI, optimisé pour le multilinguisme, le code et les expressions mathématiques. L’architecture combine plusieurs innovations : Rotary Positional Embeddings (RoPE) avec base étendue et ajustement NTK-aware pour les longs contextes, Grouped Query Attention (GQA) pour une meilleure efficacité mémoire, normalisation QK-Norm pour stabiliser l’entraînement, et une fonction d’activation xIELU (extended Integrated ELU) améliorant la performance des MLP.\n\nL’affinage final du modèle repose sur un algorithme d’alignement appelé QRPO (Quantile Reward-Preferring Optimization), une alternative au RLHF classique, qui utilise des signaux de récompense absolus pour un apprentissage plus stable et mieux aligné sur les préférences humaines. Bien qu’il ne rivalise pas directement avec les modèles propriétaires les plus avancés, Apertus se distingue par son niveau de transparence."
      },
      {
        "id": "Apertus-70B-Instruct-2509",
        "simple_name": "Apertus 70B Instruct",
        "license": "Apache 2.0",
        "fully_open_source": true,
        "release_date": "09/2025",
        "arch": "dense",
        "params": 70,
        "url": "https://huggingface.co/swiss-ai/Apertus-70B-Instruct-2509",
        "endpoint": {
          "api_type": "huggingface",
          "api_base": "https://router.huggingface.co/v1",
          "api_model_id": "swiss-ai/Apertus-70B-Instruct-2509:publicai"
        },
        "desc": "Modèle semi-ouvert et reproductible de taille moyenne, développé par un consortium d’institutions suisses. Ses poids et son code d’entraînement sont publiés sous licence permissive. Il a été entraîné sur plus de 1 800 langues sur plus de 15 000 milliards de jetons. L’entraînement a eu lieu sur le supercalculateur Alps du CSCS à Lugano, alimenté par une énergie hydroélectrique neutre en carbone.",
        "size_desc": "Avec 70 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 65 536 jetons ce qui permet de traiter d'assez longs documents.",
        "fyi": "Le modèle a été entraîné sur le supercalculateur Alps à Lugano, utilisant plus de 10 millions d’heures GPU alimentées par une énergie hydroélectrique neutre en carbone. Le modèle a été préentraîné sur 15 000 milliards jetons couvrant plus de 1 800 langues, dont une part importante de langues peu représentées.\n\nApertus s’appuie sur un tokenizer byte-level BPE de 131 000 entrées, dérivé du tokenizer « tekken » de Mistral AI, optimisé pour le multilinguisme, le code et les expressions mathématiques. L’architecture combine plusieurs innovations : Rotary Positional Embeddings (RoPE) avec base étendue et ajustement NTK-aware pour les longs contextes, Grouped Query Attention (GQA) pour une meilleure efficacité mémoire, normalisation QK-Norm pour stabiliser l’entraînement, et une fonction d’activation xIELU (extended Integrated ELU) améliorant la performance des MLP.\n\nL’affinage final du modèle repose sur un algorithme d’alignement appelé QRPO (Quantile Reward-Preferring Optimization), une alternative au RLHF classique, qui utilise des signaux de récompense absolus pour un apprentissage plus stable et mieux aligné sur les préférences humaines. Bien qu’il ne rivalise pas directement avec les modèles propriétaires les plus avancés, Apertus se distingue par son niveau de transparence."
      }
    ]
  },
  {
    "name": "MiniMax",
    "icon_path": "minimax-color",
    "models": [
      {
        "id": "minimax-m2",
        "simple_name": "MiniMax M2",
        "license": "MIT",
        "release_date": "10/2025",
        "arch": "moe",
        "params": 230,
        "active_params": 10,
        "url": "https://www.minimax.io/news/minimax-m2",
        "endpoint": {
          "api_type": "openrouter",
          "api_model_id": "minimax/minimax-m2"
        },
        "desc": "Modèle spécialisé dans le code avec un rapport qualité/rapidité/prix très compétitif. Il a été conçu par MiniMax, entreprise basée à Shanghai en Chine.",
        "size_desc": "Modèle d'architecture mélange d'experts avec 230 milliards de paramètres (dont 10 milliards actifs par génération de jeton). La fenêtre de contexte s'élève à 200 000 jetons, permettant de traîter des bases de code entières et des documents longs.",
        "fyi": "Modèle conçu spécifiquement pour des tâches agentiques (notamment du code), avec un entraînement pour respecter des protocoles stricts de contrôle d'agent (planification, appels d'outils, vérification)."
      }
    ]
  }, 
  {
    "name": "EuroLLM",
    "icon_path": "euroLLM.webp",  
    "models": [
      {
        "new": true,
        "id": "EuroLLM-22B-Instruct-2512",
        "simple_name": "EuroLLM 22B Instruct",
        "license": "Apache 2.0",
        "release_date": "12/2025",
        "arch": "dense",
        "params": 22,
        "url": "https://huggingface.co/utter-project/EuroLLM-22B-Instruct-2512",
        "endpoint": {
          "api_type": "huggingface",
          "api_base": "https://router.huggingface.co/v1",
          "api_model_id": "utter-project/EuroLLM-22B-Instruct-2512"
        },
        "desc": "Modèle multilingue conçu spécifiquement pour la diversité linguistique européenne, avec de fortes capacités en traduction et compréhension multilingue.",
        "size_desc": "Avec 22 milliards de paramètres, ce modèle se situe dans la catégorie des modèles de taille intermédiaire. Son utilisation nécessite un ordinateur personnel très puissant ou, plus généralement, un serveur disposant d’au moins une carte graphique performante. La fenêtre de contexte atteint 32 000 jetons.",
        "fyi": "EuroLLM-22B est un modèle créé par un consortium composé de Sorbonne Université, Université Paris-Saclay, Artefact Research Center, Instituto Superior Técnico – University of Lisbon, Instituto de Telecomunicações, University of Edinburgh, Aveni, Unbabel, University of Amsterdam et Naver Labs. Il a été entraîné avec pour objectif principal la couverture équilibrée des langues européennes. Il couvre les 24 langues officielles de l’Union européenne ainsi que 11 langues supplémentaires. L’entraînement a été réalisé sur environ 4 000 milliards de jetons, en utilisant 400 GPU Nvidia H100 sur le supercalculateur MareNostrum 5, opéré par le Barcelona Supercomputing Center. Le projet a bénéficié d’un soutien institutionnel européen via Horizon Europe et EuroHPC, dans le cadre d’une allocation de calcul « extreme-scale ».\n\nLes données d’entraînement combinent des données issues du web, des données parallèles multilingues (en–xx et xx–en) et des jeux de données de haute qualité soigneusement sélectionnés, avec un accent fort mis sur l’équilibre entre les langues européennes. Le modèle est particulièrement performant pour les tâches de traduction multilingue."
      }
    ]
  }
]
