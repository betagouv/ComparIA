{
  "Qwen2-7B-Instruct-GPTQ-Int8": {
    "simple_name": "Qwen2-7B",
    "organisation": "Alibaba",
    "country": "Chine",
    "icon_path": "qwen.jpg",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "description": "Petit frère de la famille Qwen2, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes",
    "params": 7,
    "license": "Apache 2.0"
  },
  "Mistral-7B-Instruct-v0.3": {
    "simple_name": "Mistral-7B",
    "params": 7,
    "organisation": "Mistral",
    "country": "France",
    "icon_path": "mistral.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "license": "Apache 2.0"
  },
  "Phi-3-Mini-4k-Instruct": {
    "simple_name": "Phi-3-Mini",
    "organisation": "Microsoft",
    "country": "États-Unis",
    "params": 3.8,
    "icon_path": "microsoft.png",
    "friendly_size": "XS",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "description": "Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
  },
  "Mixtral-8x7B-Instruct-v0.1": {
    "simple_name": "Mixtral-8x7B",
    "organisation": "Mistral",
    "active_params": 14,
    "total_params": 56,
    "country": "France",
    "icon_path": "mistral.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "description": "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence.",
    "excerpt": "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol.",
    "license": "Apache 2.0"
  },
  "Mixtral-8x22B-Instruct-v0.1": {
    "simple_name": "Mixtral-8x22B",
    "active_params": 44,
    "total_params": 176,
    "organisation": "Mistral",
    "country": "France",
    "icon_path": "mistral.png",
    "friendly_size": "L",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "excerpt": "L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût.",
    "description": "L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents.",
    "license": "Apache 2.0"
  },
  "mistral-nemo-2407": {
    "simple_name": "Mistral Nemo",
    "params": 12,
    "organisation": "Mistral",
    "country": "France",
    "icon_path": "mistral.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "free",
    "description": "",
    "license": "Apache 2.0"
  },
  "Meta-Llama-3-70B-Instruct": {
    "params": 70,
    "simple_name": "Llama-3-70b",
    "organisation": "Meta",
    "country": "États-Unis",
    "icon_path": "meta.svg",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "open-weights",
    "conditions": "restricted",
    "excerpt": "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens",
    "description": "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens.",
    "license": "Llama 3 Community"
  },
  "Meta-Llama-3-8B-Instruct": {
    "params": 8,
    "simple_name": "Llama-3-8b",
    "organisation": "Meta",
    "country": "États-Unis",
    "icon_path": "meta.svg",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted",
    "description": "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité.",
    "license": "Llama 3 Community"
  },
  "Aya23-35B": {
    "simple_name": "Aya23-35B",
    "organisation": "Cohere",
    "country": "Canada",
    "icon_path": "cohere.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Gemma-2-9B-it": {
    "simple_name": "Gemma-2-9B-it",
    "organisation": "Google",
    "country": "États-Unis",
    "icon_path": "google.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "license": "Gemma",
    "conditions": "restricted",
    "description": "Petit frère de la famille Gemma 2, ce modèle est particulièrement doué pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
  },
  "Gemma 2-27b-it": {
    "simple_name": "Gemma 2-27b-it",
    "organisation": "Google",
    "country": "États-Unis",
    "icon_path": "google.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted",
    "description": "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions."
  },
  "gemini-1.5-pro-001": {
    "active_params": "220",
    "total_params": "440",
    "simple_name": "Gemini 1.5 Pro",
    "organisation": "Google",
    "country": "États-Unis",
    "icon_path": "google.png",
    "friendly_size": "XL",
    "dataset": "private",
    "distribution": "api-only",
    "license": "propriétaire Google",
    "conditions": "restricted",
    "description": "Ce modèle est capable de traiter du texte mais aussi des images, des vidéos et de l'audio. Idéal pour des applications variées comme la génération de textes, la compréhension d'images, l'analyse de vidéos et la transcription d'audio."
  },
  "llama-3.1-405b": {
    "simple_name": "Llama 3.1 405B",
    "organisation": "Meta",
    "country": "États-Unis",
    "icon_path": "meta.svg",
    "friendly_size": "XL",
    "dataset": "private",
    "distribution": "open-weights",
    "conditions": "free",
    "description": "Modèle le plus puissant de la famille Llama 3.1 avec 405 milliards de paramètres, il est doté de capacités accrues pour des tâches de programmation, de mathématique et de raisonnement.",
    "params": 405,
    "license": "Llama 3.1 Community"
  },
  "llama-3.1-8b": {
    "simple_name": "Llama 3.1 8B",
    "organisation": "Meta",
    "country": "États-Unis",
    "icon_path": "meta.svg",
    "friendly_size": "M",
    "dataset": "private",
    "distribution": "open-weights",
    "conditions": "free",
    "description": "Cadet de la famille Llama 3.1, ce petit modèle est adapté aux tâches multilingues et au traitement de longs textes pouvant aller jusqu’à 128 000 tokens.",
    "params": 405,
    "license": "Llama 3.1 Community"
  },
  "GPT-4-Turbo-2024-04-09": {
    "simple_name": "GPT-4-Turbo-2024-04-09",
    "organisation": "OpenAI",
    "country": "États-Unis",
    "icon_path": "openai.png",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "api-only",
    "conditions": "restricted"
  },
  "Zephyr-ORPO-141b-A35b-v0.1": {
    "simple_name": "Zephyr-ORPO-141b-A35b-v0.1",
    "organisation": "Hugging Face",
    "country": "France",
    "friendly_size": "L",
    "active_params": 35,
    "total_params": 141,
    "icon_path": "huggingface.svg",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Mistral-Large-2402": {
    "simple_name": "Mistral-Large-2402",
    "organisation": "Mistral",
    "country": "France",
    "icon_path": "mistral.png",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "api-only",
    "conditions": "restricted"
  },
  "GPT-3.5-Turbo-0613": {
    "simple_name": "GPT-3.5-Turbo-0613",
    "organisation": "OpenAI",
    "country": "États-Unis",
    "icon_path": "openai.png",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "api-only",
    "conditions": "restricted"
  },
  "CroissantLLMChat-v0.1": {
    "simple_name": "CroissantLLMChat-v0.1",
    "organisation": "ILLUIN",
    "country": "France",
    "icon_path": "illuin.png",
    "friendly_size": "XS",
    "distribution": "open-weights",
    "dataset": "public",
    "conditions": "free"
  },
  "Vicuna-13B": {
    "simple_name": "Vicuna-13B",
    "organisation": "Meta",
    "country": "États-Unis",
    "icon_path": "meta.svg",
    "friendly_size": "S",
    "dataset": "private",
    "distribution": "open-weights",
    "conditions": "restricted"
  },
  "Qwen1.5-32B-Chat": {
    "simple_name": "Qwen1.5-32B",
    "organisation": "Alibaba",
    "country": "Chine",
    "icon_path": "qwen.jpg",
    "friendly_size": "M",
    "dataset": "private",
    "distribution": "open-weights",
    "conditions": "restricted"
  },
  "Vicuna-33B": {
    "simple_name": "Vicuna-33B",
    "organisation": "LMSYS",
    "country": "États-Unis",
    "icon_path": "lmsys.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Alpaca-13B": {
    "simple_name": "Alpaca-13B",
    "organisation": "Stanford",
    "country": "États-Unis",
    "icon_path": "stanford.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Command R+": {
    "simple_name": "Command R+",
    "organisation": "Cohere",
    "country": "Canada",
    "icon_path": "cohere.png",
    "friendly_size": "L",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Claude 3 Opus": {
    "simple_name": "Claude 3 Opus",
    "organisation": "Anthropic",
    "country": "États-Unis",
    "icon_path": "anthropic.png",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "api-only",
    "conditions": "restricted"
  },
  "Aya23-8B": {
    "simple_name": "Aya23-8B",
    "organisation": "Cohere",
    "country": "Canada",
    "icon_path": "cohere.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Command R": {
    "simple_name": "Command R",
    "organisation": "Cohere",
    "country": "Canada",
    "icon_path": "cohere.png",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Albert MFS": {
    "simple_name": "Albert MFS",
    "organisation": "Albert",
    "country": "France",
    "icon_path": "marianne.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "communiqué",
    "conditions": "free"
  },
  "Qwen2-72b-instruct": {
    "simple_name": "Qwen2-72b-instruct",
    "organisation": "Alibaba",
    "country": "Chine",
    "icon_path": "qwen.jpg",
    "friendly_size": "L",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Qwen2-57B-A14B-Instruct": {
    "simple_name": "Qwen2-57B-A14B-Instruct",
    "organisation": "Alibaba",
    "country": "Chine",
    "icon_path": "qwen.jpg",
    "friendly_size": "M",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Phi-3-small-8k-Instruct": {
    "simple_name": "Phi-3-small-8k-Instruct",
    "description": "Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.",
    "organisation": "Microsoft",
    "country": "États-Unis",
    "icon_path": "microsoft.png",
    "friendly_size": "S",
    "distribution": "open-weights",
    "dataset": "private",
    "conditions": "restricted"
  },
  "Claude 3.5 Sonnet": {
    "simple_name": "Claude 3.5 Sonnet",
    "organisation": "Anthropic",
    "country": "États-Unis",
    "icon_path": "anthropic.png",
    "friendly_size": "L",
    "dataset": "private",
    "distribution": "api-only",
    "conditions": "restricted"
  }
}