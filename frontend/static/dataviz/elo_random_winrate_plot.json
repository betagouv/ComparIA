{"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}}, "layer": [{"mark": {"type": "bar"}, "encoding": {"color": {"field": "win_rate", "legend": null, "scale": {"scheme": "redblue"}, "type": "quantitative"}, "tooltip": [{"field": "model", "type": "nominal"}, {"field": "win_rate", "format": ".2f", "type": "quantitative"}], "x": {"field": "model", "sort": "-y", "title": "Mod\u00e8le", "type": "nominal"}, "y": {"field": "win_rate", "title": "Taux de victoire moyen (pond\u00e9r\u00e9)", "type": "quantitative"}}}, {"mark": {"type": "text", "align": "center", "baseline": "bottom", "dy": -5, "fontSize": 10}, "encoding": {"color": {"field": "win_rate", "legend": null, "scale": {"scheme": "redblue"}, "type": "quantitative"}, "text": {"field": "win_rate", "format": ".2f", "type": "quantitative"}, "tooltip": [{"field": "model", "type": "nominal"}, {"field": "win_rate", "format": ".2f", "type": "quantitative"}], "x": {"field": "model", "sort": "-y", "title": "Mod\u00e8le", "type": "nominal"}, "y": {"field": "win_rate", "title": "Taux de victoire moyen (pond\u00e9r\u00e9)", "type": "quantitative"}}}], "data": {"name": "data-6f57d36ad53c5005b62c7fb245772fbb"}, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-6f57d36ad53c5005b62c7fb245772fbb": [{"model": "qwen3-32b", "final_weighted_sum": null, "final_total_count": 972, "win_rate": null}, {"model": "llama-4-scout", "final_weighted_sum": null, "final_total_count": 2992, "win_rate": null}, {"model": "gemini-2.0-flash-001", "final_weighted_sum": 3440.06, "final_total_count": 5840, "win_rate": 0.5890513698630137}, {"model": "gemini-2.0-flash-exp", "final_weighted_sum": 800.3199999999999, "final_total_count": 1372, "win_rate": 0.5833236151603498}, {"model": "deepseek-v3-0324", "final_weighted_sum": 2765.8399999999997, "final_total_count": 5054, "win_rate": 0.5472576177285318}, {"model": "command-a", "final_weighted_sum": 2025.8200000000002, "final_total_count": 3702, "win_rate": 0.5472231226364128}, {"model": "gemma-3-12b", "final_weighted_sum": 2039.08, "final_total_count": 3790, "win_rate": 0.5380158311345646}, {"model": "deepseek-v3-chat", "final_weighted_sum": 1575.02, "final_total_count": 2938, "win_rate": 0.5360857726344452}, {"model": "grok-3-mini-beta", "final_weighted_sum": 1087.9, "final_total_count": 2050, "win_rate": 0.5306829268292683}, {"model": "gemma-3-27b", "final_weighted_sum": 2291.4399999999996, "final_total_count": 4348, "win_rate": 0.5270101195952162}, {"model": "claude-3-7-sonnet", "final_weighted_sum": 2547.04, "final_total_count": 4966, "win_rate": 0.5128956906967378}, {"model": "gemma-3-4b", "final_weighted_sum": 2820.2599999999998, "final_total_count": 5532, "win_rate": 0.5098083875632682}, {"model": "deepseek-r1", "final_weighted_sum": 1756.7000000000003, "final_total_count": 3458, "win_rate": 0.5080104106419897}, {"model": "gemma-2-27b-it-q8", "final_weighted_sum": 77.03999999999999, "final_total_count": 154, "win_rate": 0.5002597402597402}, {"model": "llama-3.1-nemotron-70b-instruct", "final_weighted_sum": 2478.72, "final_total_count": 5018, "win_rate": 0.49396572339577516}, {"model": "gpt-4.1-mini", "final_weighted_sum": 2970.38, "final_total_count": 6030, "win_rate": 0.49260033167495854}, {"model": "gemini-1.5-pro-001", "final_weighted_sum": 68.4, "final_total_count": 140, "win_rate": 0.4885714285714286}, {"model": "mistral-large-2411", "final_weighted_sum": 3322.8, "final_total_count": 6890, "win_rate": 0.48226415094339625}, {"model": "gemini-1.5-pro-002", "final_weighted_sum": 1316.54, "final_total_count": 2748, "win_rate": 0.47909024745269285}, {"model": "mistral-small-3.1-24b", "final_weighted_sum": 2359.1600000000008, "final_total_count": 5282, "win_rate": 0.4466414237031429}, {"model": "o3-mini", "final_weighted_sum": 602.9200000000001, "final_total_count": 1352, "win_rate": 0.4459467455621302}, {"model": "o4-mini", "final_weighted_sum": 1432.4199999999998, "final_total_count": 3224, "win_rate": 0.44429900744416867}, {"model": "mistral-saba", "final_weighted_sum": 1411.2800000000002, "final_total_count": 3304, "win_rate": 0.4271428571428572}, {"model": "aya-expanse-32b", "final_weighted_sum": 1639.36, "final_total_count": 3906, "win_rate": 0.41970302099334356}, {"model": "mistral-small-24b-instruct-2501", "final_weighted_sum": 781.1800000000001, "final_total_count": 1958, "win_rate": 0.3989683350357508}]}}