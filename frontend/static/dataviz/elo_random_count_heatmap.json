{"config": {"view": {"continuousWidth": 300, "continuousHeight": 300}, "axisX": {"labelAngle": 45}}, "layer": [{"mark": {"type": "rect", "opacity": 0.7}, "encoding": {"color": {"field": "count", "legend": null, "scale": {"scheme": "viridis"}, "type": "quantitative"}, "x": {"field": "model_b", "sort": {"field": "model_b", "order": "descending"}, "title": "Mod\u00e8le B", "type": "nominal"}, "y": {"field": "model_a", "sort": {"field": "model_a"}, "title": "Mod\u00e8le A", "type": "nominal"}}}, {"mark": {"type": "text", "color": "black", "fontSize": 7}, "encoding": {"text": {"field": "count", "type": "quantitative"}, "x": {"field": "model_b", "sort": {"field": "model_b", "order": "descending"}, "title": "Mod\u00e8le B", "type": "nominal"}, "y": {"field": "model_a", "sort": {"field": "model_a"}, "title": "Mod\u00e8le A", "type": "nominal"}}}], "data": {"name": "data-20b98c7c3f7bb8d371dd5f150a7e69af"}, "height": 1000, "width": 1000, "$schema": "https://vega.github.io/schema/vega-lite/v5.20.1.json", "datasets": {"data-20b98c7c3f7bb8d371dd5f150a7e69af": [{"model_a": "mistral-saba", "model_b": "gpt-4.1-nano", "a_wins": 49, "b_wins": 60, "draws": 44, "count": 153, "a_win_ratio": 0.45}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "c4ai-command-r-08-2024", "a_wins": 11, "b_wins": 14, "draws": 14, "count": 39, "a_win_ratio": 0.44}, {"model_a": "qwq-32b", "model_b": "llama-4-scout", "a_wins": 1, "b_wins": 0, "draws": 0, "count": 1, "a_win_ratio": 1.0}, {"model_a": "mistral-large-2411", "model_b": "c4ai-command-r-08-2024", "a_wins": 85, "b_wins": 60, "draws": 75, "count": 220, "a_win_ratio": 0.59}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemini-1.5-pro-002", "a_wins": 55, "b_wins": 89, "draws": 88, "count": 232, "a_win_ratio": 0.38}, {"model_a": "phi-4", "model_b": "phi-4", "a_wins": 12, "b_wins": 12, "draws": 2, "count": 26, "a_win_ratio": 0.5}, {"model_a": "jamba-1.5-large", "model_b": "mistral-nemo-2407", "a_wins": 3, "b_wins": 1, "draws": 2, "count": 6, "a_win_ratio": 0.75}, {"model_a": "gemma-3-27b", "model_b": "c4ai-command-r-08-2024", "a_wins": 68, "b_wins": 24, "draws": 61, "count": 153, "a_win_ratio": 0.74}, {"model_a": "gemma-3-12b", "model_b": "deepseek-v3-chat", "a_wins": 11, "b_wins": 12, "draws": 9, "count": 32, "a_win_ratio": 0.48}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemma-2-9b-it", "a_wins": 77, "b_wins": 88, "draws": 95, "count": 260, "a_win_ratio": 0.47}, {"model_a": "deepseek-v3-chat", "model_b": "deepseek-r1", "a_wins": 15, "b_wins": 16, "draws": 13, "count": 44, "a_win_ratio": 0.48}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-3.1-8b", "a_wins": 83, "b_wins": 92, "draws": 83, "count": 258, "a_win_ratio": 0.47}, {"model_a": "gemini-2.0-flash-exp", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 49, "b_wins": 16, "draws": 38, "count": 103, "a_win_ratio": 0.75}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "phi-3.5-mini-instruct", "a_wins": 4, "b_wins": 5, "draws": 6, "count": 15, "a_win_ratio": 0.44}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemini-1.5-pro-001", "a_wins": 5, "b_wins": 5, "draws": 9, "count": 19, "a_win_ratio": 0.5}, {"model_a": "mistral-small-3.1-24b", "model_b": "aya-expanse-32b", "a_wins": 22, "b_wins": 27, "draws": 17, "count": 66, "a_win_ratio": 0.45}, {"model_a": "gemma-3-27b", "model_b": "mistral-nemo-2407", "a_wins": 18, "b_wins": 12, "draws": 13, "count": 43, "a_win_ratio": 0.6}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-3.1-8b", "a_wins": 75, "b_wins": 68, "draws": 98, "count": 241, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-8b", "model_b": "deepseek-r1", "a_wins": 15, "b_wins": 14, "draws": 11, "count": 40, "a_win_ratio": 0.52}, {"model_a": "qwen2-7b-instruct", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 1, "b_wins": 2, "draws": 3, "count": 6, "a_win_ratio": 0.33}, {"model_a": "gemma-3-12b", "model_b": "gemini-1.5-pro-002", "a_wins": 8, "b_wins": 13, "draws": 14, "count": 35, "a_win_ratio": 0.38}, {"model_a": "llama-3.1-405b", "model_b": "Yi-1.5-9B-Chat", "a_wins": 4, "b_wins": 1, "draws": 3, "count": 8, "a_win_ratio": 0.8}, {"model_a": "qwq-32b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 69, "b_wins": 68, "draws": 62, "count": 199, "a_win_ratio": 0.5}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 4, "draws": 3, "count": 9, "a_win_ratio": 0.33}, {"model_a": "qwen3-32b", "model_b": "aya-expanse-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-405b", "model_b": "gemini-2.0-flash-exp", "a_wins": 27, "b_wins": 49, "draws": 50, "count": 126, "a_win_ratio": 0.36}, {"model_a": "gemini-1.5-pro-002", "model_b": "mistral-nemo-2407", "a_wins": 92, "b_wins": 41, "draws": 74, "count": 207, "a_win_ratio": 0.69}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemini-2.0-flash-001", "a_wins": 66, "b_wins": 99, "draws": 80, "count": 245, "a_win_ratio": 0.4}, {"model_a": "gemma-3-4b", "model_b": "gpt-4.1-nano", "a_wins": 55, "b_wins": 44, "draws": 49, "count": 148, "a_win_ratio": 0.56}, {"model_a": "llama-3.1-8b", "model_b": "gpt-4.1-nano", "a_wins": 29, "b_wins": 26, "draws": 37, "count": 92, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-8b", "model_b": "aya-expanse-32b", "a_wins": 21, "b_wins": 21, "draws": 28, "count": 70, "a_win_ratio": 0.5}, {"model_a": "llama-3.3-70b", "model_b": "gpt-4.1-nano", "a_wins": 40, "b_wins": 41, "draws": 40, "count": 121, "a_win_ratio": 0.49}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "o3-mini", "a_wins": 5, "b_wins": 8, "draws": 13, "count": 26, "a_win_ratio": 0.38}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "deepseek-v3-chat", "a_wins": 136, "b_wins": 227, "draws": 194, "count": 557, "a_win_ratio": 0.37}, {"model_a": "command-a", "model_b": "gemini-2.0-flash-001", "a_wins": 41, "b_wins": 64, "draws": 55, "count": 160, "a_win_ratio": 0.39}, {"model_a": "aya-expanse-32b", "model_b": "o4-mini", "a_wins": 0, "b_wins": 2, "draws": 4, "count": 6, "a_win_ratio": 0.0}, {"model_a": "qwq-32b", "model_b": "o3-mini", "a_wins": 40, "b_wins": 47, "draws": 51, "count": 138, "a_win_ratio": 0.46}, {"model_a": "gpt-4o-2024-08-06", "model_b": "c4ai-command-r-08-2024", "a_wins": 25, "b_wins": 14, "draws": 18, "count": 57, "a_win_ratio": 0.64}, {"model_a": "gpt-4.1-mini", "model_b": "mistral-small-3.1-24b", "a_wins": 112, "b_wins": 86, "draws": 99, "count": 297, "a_win_ratio": 0.57}, {"model_a": "ministral-8b-instruct-2410", "model_b": "o3-mini", "a_wins": 17, "b_wins": 17, "draws": 19, "count": 53, "a_win_ratio": 0.5}, {"model_a": "mistral-small-3.1-24b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 7, "b_wins": 3, "draws": 13, "count": 23, "a_win_ratio": 0.7}, {"model_a": "gemma-2-9b-it", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 108, "b_wins": 53, "draws": 138, "count": 299, "a_win_ratio": 0.67}, {"model_a": "c4ai-command-r-08-2024", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 25, "b_wins": 19, "draws": 26, "count": 70, "a_win_ratio": 0.57}, {"model_a": "mistral-nemo-2407", "model_b": "Yi-1.5-9B-Chat", "a_wins": 1, "b_wins": 1, "draws": 2, "count": 4, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-70b", "model_b": "qwq-32b", "a_wins": 9, "b_wins": 3, "draws": 15, "count": 27, "a_win_ratio": 0.75}, {"model_a": "llama-3.1-70b", "model_b": "phi-3.5-mini-instruct", "a_wins": 86, "b_wins": 45, "draws": 131, "count": 262, "a_win_ratio": 0.66}, {"model_a": "gemma-2-9b-it", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 65, "b_wins": 42, "draws": 94, "count": 201, "a_win_ratio": 0.61}, {"model_a": "deepseek-v3-0324", "model_b": "llama-3.1-8b", "a_wins": 40, "b_wins": 20, "draws": 54, "count": 114, "a_win_ratio": 0.67}, {"model_a": "qwen3-32b", "model_b": "gpt-4.1-mini", "a_wins": 2, "b_wins": 2, "draws": 2, "count": 6, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "claude-3-7-sonnet", "a_wins": 27, "b_wins": 59, "draws": 37, "count": 123, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-405b", "model_b": "llama-3.1-8b", "a_wins": 156, "b_wins": 108, "draws": 171, "count": 435, "a_win_ratio": 0.59}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "mistral-nemo-2407", "a_wins": 2, "b_wins": 9, "draws": 14, "count": 25, "a_win_ratio": 0.18}, {"model_a": "deepseek-r1", "model_b": "mistral-large-2411", "a_wins": 37, "b_wins": 40, "draws": 44, "count": 121, "a_win_ratio": 0.48}, {"model_a": "deepseek-v3-0324", "model_b": "qwen3-32b", "a_wins": 4, "b_wins": 3, "draws": 0, "count": 7, "a_win_ratio": 0.57}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemma-3-27b", "a_wins": 13, "b_wins": 21, "draws": 27, "count": 61, "a_win_ratio": 0.38}, {"model_a": "deepseek-r1", "model_b": "grok-3-mini-beta", "a_wins": 100, "b_wins": 93, "draws": 74, "count": 267, "a_win_ratio": 0.52}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-3.1-405b", "a_wins": 100, "b_wins": 87, "draws": 123, "count": 310, "a_win_ratio": 0.53}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "qwen2.5-7b-instruct", "a_wins": 14, "b_wins": 8, "draws": 23, "count": 45, "a_win_ratio": 0.64}, {"model_a": "phi-4", "model_b": "o3-mini", "a_wins": 9, "b_wins": 17, "draws": 12, "count": 38, "a_win_ratio": 0.35}, {"model_a": "deepseek-v3-chat", "model_b": "mistral-nemo-2407", "a_wins": 60, "b_wins": 20, "draws": 54, "count": 134, "a_win_ratio": 0.75}, {"model_a": "gemini-2.0-flash-001", "model_b": "lfm-40b", "a_wins": 27, "b_wins": 11, "draws": 25, "count": 63, "a_win_ratio": 0.71}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "phi-3.5-mini-instruct", "a_wins": 2, "b_wins": 5, "draws": 6, "count": 13, "a_win_ratio": 0.29}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 2, "b_wins": 1, "draws": 1, "count": 4, "a_win_ratio": 0.67}, {"model_a": "mistral-small-3.1-24b", "model_b": "mistral-saba", "a_wins": 44, "b_wins": 40, "draws": 28, "count": 112, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-70b", "model_b": "gemma-2-9b-it", "a_wins": 77, "b_wins": 90, "draws": 143, "count": 310, "a_win_ratio": 0.46}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemini-2.0-flash-001", "a_wins": 37, "b_wins": 100, "draws": 73, "count": 210, "a_win_ratio": 0.27}, {"model_a": "gemma-3-12b", "model_b": "claude-3-7-sonnet", "a_wins": 33, "b_wins": 28, "draws": 61, "count": 122, "a_win_ratio": 0.54}, {"model_a": "mistral-large-2411", "model_b": "phi-3.5-mini-instruct", "a_wins": 1, "b_wins": 1, "draws": 2, "count": 4, "a_win_ratio": 0.5}, {"model_a": "jamba-1.5-large", "model_b": "mistral-large-2411", "a_wins": 3, "b_wins": 3, "draws": 0, "count": 6, "a_win_ratio": 0.5}, {"model_a": "o3-mini", "model_b": "c4ai-command-r-08-2024", "a_wins": 19, "b_wins": 7, "draws": 11, "count": 37, "a_win_ratio": 0.73}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemini-2.0-flash-exp", "a_wins": 2, "b_wins": 10, "draws": 11, "count": 23, "a_win_ratio": 0.17}, {"model_a": "mistral-saba", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 52, "b_wins": 47, "draws": 33, "count": 132, "a_win_ratio": 0.53}, {"model_a": "aya-expanse-8b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 19, "b_wins": 25, "draws": 29, "count": 73, "a_win_ratio": 0.43}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-3.1-8b", "a_wins": 48, "b_wins": 43, "draws": 44, "count": 135, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-8b", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 4, "b_wins": 17, "draws": 15, "count": 36, "a_win_ratio": 0.19}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "deepseek-r1", "a_wins": 12, "b_wins": 8, "draws": 16, "count": 36, "a_win_ratio": 0.6}, {"model_a": "mistral-large-2411", "model_b": "gemma-3-12b", "a_wins": 52, "b_wins": 56, "draws": 46, "count": 154, "a_win_ratio": 0.48}, {"model_a": "qwen2.5-32b-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 4, "b_wins": 3, "draws": 4, "count": 11, "a_win_ratio": 0.57}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "deepseek-v3-chat", "a_wins": 13, "b_wins": 33, "draws": 37, "count": 83, "a_win_ratio": 0.28}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 12, "b_wins": 27, "draws": 22, "count": 61, "a_win_ratio": 0.31}, {"model_a": "c4ai-command-r-08-2024", "model_b": "deepseek-v3-0324", "a_wins": 27, "b_wins": 63, "draws": 47, "count": 137, "a_win_ratio": 0.3}, {"model_a": "mistral-nemo-2407", "model_b": "qwen2.5-7b-instruct", "a_wins": 26, "b_wins": 48, "draws": 52, "count": 126, "a_win_ratio": 0.35}, {"model_a": "claude-3-7-sonnet", "model_b": "gemma-3-27b", "a_wins": 34, "b_wins": 42, "draws": 67, "count": 143, "a_win_ratio": 0.45}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "deepseek-v3-chat", "a_wins": 46, "b_wins": 94, "draws": 105, "count": 245, "a_win_ratio": 0.33}, {"model_a": "gemma-3-12b", "model_b": "mistral-large-2411", "a_wins": 56, "b_wins": 52, "draws": 46, "count": 154, "a_win_ratio": 0.52}, {"model_a": "claude-3-7-sonnet", "model_b": "mistral-small-3.1-24b", "a_wins": 75, "b_wins": 50, "draws": 63, "count": 188, "a_win_ratio": 0.6}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gpt-4.1-nano", "a_wins": 51, "b_wins": 57, "draws": 48, "count": 156, "a_win_ratio": 0.47}, {"model_a": "command-a", "model_b": "gemma-2-9b-it", "a_wins": 7, "b_wins": 4, "draws": 11, "count": 22, "a_win_ratio": 0.64}, {"model_a": "deepseek-v3-0324", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 66, "b_wins": 42, "draws": 56, "count": 164, "a_win_ratio": 0.61}, {"model_a": "mistral-nemo-2407", "model_b": "deepseek-r1", "a_wins": 8, "b_wins": 20, "draws": 18, "count": 46, "a_win_ratio": 0.29}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 40, "b_wins": 37, "draws": 70, "count": 147, "a_win_ratio": 0.52}, {"model_a": "aya-expanse-8b", "model_b": "llama-3.1-405b", "a_wins": 27, "b_wins": 25, "draws": 24, "count": 76, "a_win_ratio": 0.52}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-3.1-405b", "a_wins": 80, "b_wins": 33, "draws": 60, "count": 173, "a_win_ratio": 0.71}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 10, "b_wins": 12, "draws": 10, "count": 32, "a_win_ratio": 0.45}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mistral-saba", "a_wins": 29, "b_wins": 40, "draws": 35, "count": 104, "a_win_ratio": 0.42}, {"model_a": "deepseek-r1", "model_b": "gemma-2-9b-it", "a_wins": 12, "b_wins": 7, "draws": 9, "count": 28, "a_win_ratio": 0.63}, {"model_a": "llama-3.1-70b", "model_b": "phi-4", "a_wins": 54, "b_wins": 53, "draws": 51, "count": 158, "a_win_ratio": 0.5}, {"model_a": "grok-3-mini-beta", "model_b": "gemma-3-27b", "a_wins": 7, "b_wins": 4, "draws": 7, "count": 18, "a_win_ratio": 0.64}, {"model_a": "gpt-4.1-mini", "model_b": "grok-3-mini-beta", "a_wins": 13, "b_wins": 20, "draws": 13, "count": 46, "a_win_ratio": 0.39}, {"model_a": "qwen2-7b-instruct", "model_b": "mistral-nemo-2407", "a_wins": 1, "b_wins": 0, "draws": 1, "count": 2, "a_win_ratio": 1.0}, {"model_a": "gemma-3-12b", "model_b": "gemma-3-4b", "a_wins": 59, "b_wins": 32, "draws": 35, "count": 126, "a_win_ratio": 0.65}, {"model_a": "command-a", "model_b": "llama-3.3-70b", "a_wins": 49, "b_wins": 31, "draws": 46, "count": 126, "a_win_ratio": 0.61}, {"model_a": "command-a", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 66, "b_wins": 35, "draws": 52, "count": 153, "a_win_ratio": 0.65}, {"model_a": "gpt-4.1-nano", "model_b": "o4-mini", "a_wins": 5, "b_wins": 16, "draws": 5, "count": 26, "a_win_ratio": 0.24}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-3.1-405b", "a_wins": 103, "b_wins": 65, "draws": 78, "count": 246, "a_win_ratio": 0.61}, {"model_a": "ministral-8b-instruct-2410", "model_b": "c4ai-command-r-08-2024", "a_wins": 75, "b_wins": 65, "draws": 83, "count": 223, "a_win_ratio": 0.54}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "o3-mini", "a_wins": 22, "b_wins": 22, "draws": 16, "count": 60, "a_win_ratio": 0.5}, {"model_a": "llama-3.3-70b", "model_b": "ministral-8b-instruct-2410", "a_wins": 83, "b_wins": 67, "draws": 74, "count": 224, "a_win_ratio": 0.55}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "qwq-32b", "a_wins": 11, "b_wins": 9, "draws": 19, "count": 39, "a_win_ratio": 0.55}, {"model_a": "gemini-2.0-flash-exp", "model_b": "phi-3.5-mini-instruct", "a_wins": 8, "b_wins": 0, "draws": 6, "count": 14, "a_win_ratio": 1.0}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-3.1-70b", "a_wins": 19, "b_wins": 16, "draws": 10, "count": 45, "a_win_ratio": 0.54}, {"model_a": "gemma-3-27b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 10, "b_wins": 1, "draws": 7, "count": 18, "a_win_ratio": 0.91}, {"model_a": "aya-expanse-32b", "model_b": "claude-3-7-sonnet", "a_wins": 33, "b_wins": 40, "draws": 31, "count": 104, "a_win_ratio": 0.45}, {"model_a": "mistral-nemo-2407", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 25, "b_wins": 22, "draws": 28, "count": 75, "a_win_ratio": 0.53}, {"model_a": "c4ai-command-r-08-2024", "model_b": "grok-3-mini-beta", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "mistral-nemo-2407", "model_b": "o3-mini", "a_wins": 8, "b_wins": 24, "draws": 9, "count": 41, "a_win_ratio": 0.25}, {"model_a": "llama-4-scout", "model_b": "o4-mini", "a_wins": 8, "b_wins": 8, "draws": 5, "count": 21, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-405b", "model_b": "gemma-3-12b", "a_wins": 23, "b_wins": 64, "draws": 59, "count": 146, "a_win_ratio": 0.26}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 11, "b_wins": 17, "draws": 11, "count": 39, "a_win_ratio": 0.39}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "qwen2.5-7b-instruct", "a_wins": 19, "b_wins": 26, "draws": 26, "count": 71, "a_win_ratio": 0.42}, {"model_a": "llama-3.3-70b", "model_b": "o3-mini", "a_wins": 11, "b_wins": 18, "draws": 18, "count": 47, "a_win_ratio": 0.38}, {"model_a": "grok-3-mini-beta", "model_b": "mistral-saba", "a_wins": 2, "b_wins": 1, "draws": 3, "count": 6, "a_win_ratio": 0.67}, {"model_a": "mistral-nemo-2407", "model_b": "gpt-4o-2024-08-06", "a_wins": 49, "b_wins": 72, "draws": 73, "count": 194, "a_win_ratio": 0.4}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemma-2-9b-it", "a_wins": 32, "b_wins": 10, "draws": 18, "count": 60, "a_win_ratio": 0.76}, {"model_a": "c4ai-command-r-08-2024", "model_b": "o3-mini", "a_wins": 7, "b_wins": 19, "draws": 11, "count": 37, "a_win_ratio": 0.27}, {"model_a": "gemini-1.5-pro-001", "model_b": "qwen2.5-7b-instruct", "a_wins": 48, "b_wins": 23, "draws": 57, "count": 128, "a_win_ratio": 0.68}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 37, "b_wins": 37, "draws": 42, "count": 116, "a_win_ratio": 0.5}, {"model_a": "mistral-small-3.1-24b", "model_b": "grok-3-mini-beta", "a_wins": 6, "b_wins": 4, "draws": 3, "count": 13, "a_win_ratio": 0.6}, {"model_a": "ministral-8b-instruct-2410", "model_b": "ministral-8b-instruct-2410", "a_wins": 21, "b_wins": 21, "draws": 0, "count": 42, "a_win_ratio": 0.5}, {"model_a": "gpt-4.1-nano", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 37, "b_wins": 42, "draws": 47, "count": 126, "a_win_ratio": 0.47}, {"model_a": "aya-expanse-8b", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 1, "draws": 3, "count": 6, "a_win_ratio": 0.67}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 14, "b_wins": 14, "draws": 11, "count": 39, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "llama-3.1-405b", "a_wins": 39, "b_wins": 89, "draws": 93, "count": 221, "a_win_ratio": 0.3}, {"model_a": "deepseek-v3-chat", "model_b": "gemma-3-27b", "a_wins": 21, "b_wins": 20, "draws": 21, "count": 62, "a_win_ratio": 0.51}, {"model_a": "gemma-3-4b", "model_b": "llama-3.1-405b", "a_wins": 50, "b_wins": 25, "draws": 52, "count": 127, "a_win_ratio": 0.67}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mistral-nemo-2407", "a_wins": 102, "b_wins": 66, "draws": 103, "count": 271, "a_win_ratio": 0.61}, {"model_a": "gemma-3-27b", "model_b": "mistral-small-3.1-24b", "a_wins": 58, "b_wins": 53, "draws": 57, "count": 168, "a_win_ratio": 0.52}, {"model_a": "gemma-3-27b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 15, "b_wins": 10, "draws": 10, "count": 35, "a_win_ratio": 0.6}, {"model_a": "llama-3.1-8b", "model_b": "gemini-2.0-flash-001", "a_wins": 33, "b_wins": 56, "draws": 68, "count": 157, "a_win_ratio": 0.37}, {"model_a": "lfm-40b", "model_b": "mistral-large-2411", "a_wins": 39, "b_wins": 51, "draws": 52, "count": 142, "a_win_ratio": 0.43}, {"model_a": "gemini-1.5-pro-002", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 15, "b_wins": 9, "draws": 23, "count": 47, "a_win_ratio": 0.62}, {"model_a": "o4-mini", "model_b": "ministral-8b-instruct-2410", "a_wins": 21, "b_wins": 17, "draws": 23, "count": 61, "a_win_ratio": 0.55}, {"model_a": "mistral-large-2411", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 120, "b_wins": 76, "draws": 110, "count": 306, "a_win_ratio": 0.61}, {"model_a": "llama-3.1-405b", "model_b": "deepseek-r1", "a_wins": 25, "b_wins": 26, "draws": 24, "count": 75, "a_win_ratio": 0.49}, {"model_a": "gemini-1.5-pro-002", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 28, "b_wins": 12, "draws": 22, "count": 62, "a_win_ratio": 0.7}, {"model_a": "llama-3.1-405b", "model_b": "command-a", "a_wins": 20, "b_wins": 56, "draws": 42, "count": 118, "a_win_ratio": 0.26}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 58, "b_wins": 46, "draws": 45, "count": 149, "a_win_ratio": 0.56}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 11, "b_wins": 2, "draws": 13, "count": 26, "a_win_ratio": 0.85}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gemma-2-9b-it", "a_wins": 5, "b_wins": 16, "draws": 18, "count": 39, "a_win_ratio": 0.24}, {"model_a": "aya-expanse-32b", "model_b": "llama-4-scout", "a_wins": 23, "b_wins": 30, "draws": 20, "count": 73, "a_win_ratio": 0.43}, {"model_a": "lfm-40b", "model_b": "o3-mini", "a_wins": 6, "b_wins": 14, "draws": 19, "count": 39, "a_win_ratio": 0.3}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 97, "b_wins": 71, "draws": 89, "count": 257, "a_win_ratio": 0.58}, {"model_a": "mistral-small-3.1-24b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 48, "b_wins": 49, "draws": 52, "count": 149, "a_win_ratio": 0.49}, {"model_a": "llama-3.1-8b", "model_b": "qwen2-7b-instruct", "a_wins": 2, "b_wins": 0, "draws": 4, "count": 6, "a_win_ratio": 1.0}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemma-3-12b", "a_wins": 45, "b_wins": 45, "draws": 52, "count": 142, "a_win_ratio": 0.5}, {"model_a": "gemini-2.0-flash-001", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 16, "b_wins": 10, "draws": 20, "count": 46, "a_win_ratio": 0.62}, {"model_a": "command-a", "model_b": "llama-3.1-70b", "a_wins": 13, "b_wins": 8, "draws": 8, "count": 29, "a_win_ratio": 0.62}, {"model_a": "aya-expanse-8b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 16, "b_wins": 25, "draws": 13, "count": 54, "a_win_ratio": 0.39}, {"model_a": "gemini-2.0-flash-001", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 93, "b_wins": 44, "draws": 73, "count": 210, "a_win_ratio": 0.68}, {"model_a": "gemini-2.0-flash-exp", "model_b": "llama-3.1-70b", "a_wins": 40, "b_wins": 24, "draws": 41, "count": 105, "a_win_ratio": 0.62}, {"model_a": "gpt-4.1-nano", "model_b": "aya-expanse-32b", "a_wins": 33, "b_wins": 31, "draws": 26, "count": 90, "a_win_ratio": 0.52}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gpt-4.1-mini", "a_wins": 26, "b_wins": 43, "draws": 48, "count": 117, "a_win_ratio": 0.38}, {"model_a": "ministral-8b-instruct-2410", "model_b": "qwen3-32b", "a_wins": 0, "b_wins": 0, "draws": 2, "count": 2, "a_win_ratio": null}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mistral-large-2411", "a_wins": 137, "b_wins": 166, "draws": 166, "count": 469, "a_win_ratio": 0.45}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "llama-3.1-8b", "a_wins": 46, "b_wins": 32, "draws": 34, "count": 112, "a_win_ratio": 0.59}, {"model_a": "deepseek-r1", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 10, "b_wins": 4, "draws": 21, "count": 35, "a_win_ratio": 0.71}, {"model_a": "lfm-40b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 29, "b_wins": 15, "draws": 17, "count": 61, "a_win_ratio": 0.66}, {"model_a": "gemma-2-9b-it", "model_b": "qwq-32b", "a_wins": 10, "b_wins": 7, "draws": 10, "count": 27, "a_win_ratio": 0.59}, {"model_a": "llama-3.3-70b", "model_b": "llama-4-scout", "a_wins": 32, "b_wins": 43, "draws": 28, "count": 103, "a_win_ratio": 0.43}, {"model_a": "grok-3-mini-beta", "model_b": "phi-4", "a_wins": 6, "b_wins": 2, "draws": 0, "count": 8, "a_win_ratio": 0.75}, {"model_a": "gemma-3-12b", "model_b": "mistral-nemo-2407", "a_wins": 17, "b_wins": 5, "draws": 12, "count": 34, "a_win_ratio": 0.77}, {"model_a": "jamba-1.5-large", "model_b": "claude-3-5-sonnet-v2", "a_wins": 4, "b_wins": 2, "draws": 3, "count": 9, "a_win_ratio": 0.67}, {"model_a": "aya-expanse-32b", "model_b": "gpt-4.1-nano", "a_wins": 31, "b_wins": 33, "draws": 26, "count": 90, "a_win_ratio": 0.48}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "c4ai-command-r-08-2024", "a_wins": 0, "b_wins": 1, "draws": 2, "count": 3, "a_win_ratio": 0.0}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "phi-3.5-mini-instruct", "a_wins": 53, "b_wins": 53, "draws": 98, "count": 204, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 93, "b_wins": 77, "draws": 84, "count": 254, "a_win_ratio": 0.55}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "llama-3.3-70b", "a_wins": 28, "b_wins": 41, "draws": 43, "count": 112, "a_win_ratio": 0.41}, {"model_a": "gemma-3-12b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 9, "b_wins": 4, "draws": 12, "count": 25, "a_win_ratio": 0.69}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gpt-4.1-nano", "a_wins": 45, "b_wins": 42, "draws": 31, "count": 118, "a_win_ratio": 0.52}, {"model_a": "gpt-4.1-mini", "model_b": "command-a", "a_wins": 36, "b_wins": 42, "draws": 34, "count": 112, "a_win_ratio": 0.46}, {"model_a": "gemini-2.0-flash-exp", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 32, "b_wins": 32, "draws": 23, "count": 87, "a_win_ratio": 0.5}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemma-3-27b", "a_wins": 38, "b_wins": 83, "draws": 65, "count": 186, "a_win_ratio": 0.31}, {"model_a": "claude-3-7-sonnet", "model_b": "deepseek-r1", "a_wins": 28, "b_wins": 27, "draws": 38, "count": 93, "a_win_ratio": 0.51}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "deepseek-r1", "a_wins": 10, "b_wins": 10, "draws": 20, "count": 40, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemma-3-4b", "a_wins": 64, "b_wins": 72, "draws": 69, "count": 205, "a_win_ratio": 0.47}, {"model_a": "grok-3-mini-beta", "model_b": "gemini-2.0-flash-001", "a_wins": 12, "b_wins": 10, "draws": 20, "count": 42, "a_win_ratio": 0.55}, {"model_a": "mistral-small-3.1-24b", "model_b": "gpt-4.1-nano", "a_wins": 63, "b_wins": 66, "draws": 71, "count": 200, "a_win_ratio": 0.49}, {"model_a": "deepseek-v3-0324", "model_b": "command-a", "a_wins": 44, "b_wins": 40, "draws": 20, "count": 104, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-8b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 49, "b_wins": 82, "draws": 85, "count": 216, "a_win_ratio": 0.37}, {"model_a": "gemma-2-9b-it", "model_b": "llama-3.1-70b", "a_wins": 90, "b_wins": 77, "draws": 143, "count": 310, "a_win_ratio": 0.54}, {"model_a": "ministral-8b-instruct-2410", "model_b": "lfm-40b", "a_wins": 54, "b_wins": 30, "draws": 51, "count": 135, "a_win_ratio": 0.64}, {"model_a": "qwq-32b", "model_b": "phi-4", "a_wins": 6, "b_wins": 8, "draws": 13, "count": 27, "a_win_ratio": 0.43}, {"model_a": "gpt-4.1-nano", "model_b": "gemma-3-12b", "a_wins": 42, "b_wins": 56, "draws": 42, "count": 140, "a_win_ratio": 0.43}, {"model_a": "gemma-2-9b-it", "model_b": "deepseek-v3-chat", "a_wins": 36, "b_wins": 69, "draws": 46, "count": 151, "a_win_ratio": 0.34}, {"model_a": "qwen2.5-7b-instruct", "model_b": "llama-3.1-405b", "a_wins": 31, "b_wins": 42, "draws": 72, "count": 145, "a_win_ratio": 0.42}, {"model_a": "mistral-large-2411", "model_b": "gemini-2.0-flash-001", "a_wins": 77, "b_wins": 118, "draws": 86, "count": 281, "a_win_ratio": 0.39}, {"model_a": "mistral-small-3.1-24b", "model_b": "c4ai-command-r-08-2024", "a_wins": 53, "b_wins": 43, "draws": 52, "count": 148, "a_win_ratio": 0.55}, {"model_a": "lfm-40b", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 9, "b_wins": 1, "draws": 12, "count": 22, "a_win_ratio": 0.9}, {"model_a": "gemma-2-27b-it-q8", "model_b": "claude-3-5-sonnet-v2", "a_wins": 0, "b_wins": 1, "draws": 2, "count": 3, "a_win_ratio": 0.0}, {"model_a": "llama-3.1-70b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 31, "b_wins": 41, "draws": 44, "count": 116, "a_win_ratio": 0.43}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gemini-1.5-pro-002", "a_wins": 9, "b_wins": 15, "draws": 23, "count": 47, "a_win_ratio": 0.38}, {"model_a": "mistral-saba", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 35, "b_wins": 43, "draws": 45, "count": 123, "a_win_ratio": 0.45}, {"model_a": "gemini-2.0-flash-001", "model_b": "mistral-saba", "a_wins": 54, "b_wins": 23, "draws": 40, "count": 117, "a_win_ratio": 0.7}, {"model_a": "gemini-1.5-pro-002", "model_b": "aya-expanse-8b", "a_wins": 22, "b_wins": 21, "draws": 30, "count": 73, "a_win_ratio": 0.51}, {"model_a": "deepseek-v3-chat", "model_b": "mistral-large-2411", "a_wins": 96, "b_wins": 79, "draws": 94, "count": 269, "a_win_ratio": 0.55}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 13, "b_wins": 13, "draws": 0, "count": 26, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "lfm-40b", "a_wins": 52, "b_wins": 31, "draws": 43, "count": 126, "a_win_ratio": 0.63}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "gemini-1.5-pro-001", "a_wins": 1, "b_wins": 13, "draws": 21, "count": 35, "a_win_ratio": 0.07}, {"model_a": "llama-3.1-405b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 57, "b_wins": 46, "draws": 57, "count": 160, "a_win_ratio": 0.55}, {"model_a": "qwq-32b", "model_b": "llama-3.3-70b", "a_wins": 8, "b_wins": 17, "draws": 15, "count": 40, "a_win_ratio": 0.32}, {"model_a": "c4ai-command-r-08-2024", "model_b": "claude-3-5-sonnet-v2", "a_wins": 46, "b_wins": 31, "draws": 43, "count": 120, "a_win_ratio": 0.6}, {"model_a": "gemma-3-4b", "model_b": "gemma-2-9b-it", "a_wins": 10, "b_wins": 3, "draws": 15, "count": 28, "a_win_ratio": 0.77}, {"model_a": "deepseek-r1", "model_b": "gemini-1.5-pro-002", "a_wins": 16, "b_wins": 18, "draws": 15, "count": 49, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 113, "b_wins": 68, "draws": 93, "count": 274, "a_win_ratio": 0.62}, {"model_a": "grok-3-mini-beta", "model_b": "o4-mini", "a_wins": 104, "b_wins": 81, "draws": 96, "count": 281, "a_win_ratio": 0.56}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 108, "b_wins": 101, "draws": 100, "count": 309, "a_win_ratio": 0.52}, {"model_a": "gemma-2-9b-it", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 1, "draws": 5, "count": 9, "a_win_ratio": 0.75}, {"model_a": "deepseek-v3-chat", "model_b": "gemma-2-9b-it", "a_wins": 69, "b_wins": 36, "draws": 46, "count": 151, "a_win_ratio": 0.66}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-3.1-405b", "a_wins": 131, "b_wins": 144, "draws": 180, "count": 455, "a_win_ratio": 0.48}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemini-2.0-flash-exp", "a_wins": 32, "b_wins": 59, "draws": 68, "count": 159, "a_win_ratio": 0.35}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemini-1.5-pro-002", "a_wins": 50, "b_wins": 61, "draws": 64, "count": 175, "a_win_ratio": 0.45}, {"model_a": "mistral-large-2411", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 51, "b_wins": 47, "draws": 54, "count": 152, "a_win_ratio": 0.52}, {"model_a": "llama-4-scout", "model_b": "claude-3-7-sonnet", "a_wins": 27, "b_wins": 36, "draws": 46, "count": 109, "a_win_ratio": 0.43}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemini-2.0-flash-001", "a_wins": 28, "b_wins": 26, "draws": 29, "count": 83, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-8b", "model_b": "mistral-nemo-2407", "a_wins": 92, "b_wins": 84, "draws": 128, "count": 304, "a_win_ratio": 0.52}, {"model_a": "grok-3-mini-beta", "model_b": "gpt-4.1-nano", "a_wins": 6, "b_wins": 1, "draws": 5, "count": 12, "a_win_ratio": 0.86}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 16, "b_wins": 9, "draws": 18, "count": 43, "a_win_ratio": 0.64}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "phi-4", "a_wins": 88, "b_wins": 118, "draws": 96, "count": 302, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-70b", "model_b": "gpt-4o-2024-08-06", "a_wins": 42, "b_wins": 51, "draws": 66, "count": 159, "a_win_ratio": 0.45}, {"model_a": "qwq-32b", "model_b": "gemini-1.5-pro-002", "a_wins": 5, "b_wins": 7, "draws": 14, "count": 26, "a_win_ratio": 0.42}, {"model_a": "mistral-large-2411", "model_b": "gpt-4.1-nano", "a_wins": 69, "b_wins": 58, "draws": 53, "count": 180, "a_win_ratio": 0.54}, {"model_a": "mistral-saba", "model_b": "llama-3.3-70b", "a_wins": 34, "b_wins": 44, "draws": 33, "count": 111, "a_win_ratio": 0.44}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "llama-3.1-8b", "a_wins": 0, "b_wins": 1, "draws": 2, "count": 3, "a_win_ratio": 0.0}, {"model_a": "gemma-3-4b", "model_b": "c4ai-command-r-08-2024", "a_wins": 52, "b_wins": 46, "draws": 35, "count": 133, "a_win_ratio": 0.53}, {"model_a": "command-a", "model_b": "gpt-4.1-mini", "a_wins": 42, "b_wins": 36, "draws": 34, "count": 112, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-70b", "model_b": "mistral-nemo-2407", "a_wins": 155, "b_wins": 65, "draws": 151, "count": 371, "a_win_ratio": 0.7}, {"model_a": "gpt-4.1-nano", "model_b": "deepseek-v3-0324", "a_wins": 42, "b_wins": 78, "draws": 72, "count": 192, "a_win_ratio": 0.35}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 40, "b_wins": 48, "draws": 55, "count": 143, "a_win_ratio": 0.45}, {"model_a": "aya-expanse-32b", "model_b": "ministral-8b-instruct-2410", "a_wins": 21, "b_wins": 11, "draws": 14, "count": 46, "a_win_ratio": 0.66}, {"model_a": "command-a", "model_b": "phi-4", "a_wins": 57, "b_wins": 49, "draws": 43, "count": 149, "a_win_ratio": 0.54}, {"model_a": "deepseek-v3-chat", "model_b": "gemini-2.0-flash-001", "a_wins": 37, "b_wins": 36, "draws": 25, "count": 98, "a_win_ratio": 0.51}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 5, "draws": 5, "count": 12, "a_win_ratio": 0.29}, {"model_a": "c4ai-command-r-08-2024", "model_b": "o4-mini", "a_wins": 4, "b_wins": 0, "draws": 1, "count": 5, "a_win_ratio": 1.0}, {"model_a": "llama-3.1-8b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 70, "b_wins": 54, "draws": 65, "count": 189, "a_win_ratio": 0.56}, {"model_a": "gemini-2.0-flash-001", "model_b": "deepseek-v3-0324", "a_wins": 70, "b_wins": 46, "draws": 52, "count": 168, "a_win_ratio": 0.6}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "mistral-small-3.1-24b", "a_wins": 3, "b_wins": 7, "draws": 13, "count": 23, "a_win_ratio": 0.3}, {"model_a": "llama-3.1-8b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 37, "b_wins": 36, "draws": 58, "count": 131, "a_win_ratio": 0.51}, {"model_a": "qwen3-32b", "model_b": "command-a", "a_wins": 1, "b_wins": 2, "draws": 1, "count": 4, "a_win_ratio": 0.33}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 3, "b_wins": 8, "draws": 5, "count": 16, "a_win_ratio": 0.27}, {"model_a": "gemini-2.0-flash-exp", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 40, "b_wins": 18, "draws": 30, "count": 88, "a_win_ratio": 0.69}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gpt-4o-2024-08-06", "a_wins": 101, "b_wins": 108, "draws": 100, "count": 309, "a_win_ratio": 0.48}, {"model_a": "lfm-40b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 45, "b_wins": 47, "draws": 51, "count": 143, "a_win_ratio": 0.49}, {"model_a": "jamba-1.5-large", "model_b": "deepseek-v3-chat", "a_wins": 3, "b_wins": 1, "draws": 4, "count": 8, "a_win_ratio": 0.75}, {"model_a": "gemini-1.5-pro-001", "model_b": "phi-3.5-mini-instruct", "a_wins": 107, "b_wins": 41, "draws": 117, "count": 265, "a_win_ratio": 0.72}, {"model_a": "llama-3.1-405b", "model_b": "gemma-2-9b-it", "a_wins": 123, "b_wins": 71, "draws": 138, "count": 332, "a_win_ratio": 0.63}, {"model_a": "grok-3-mini-beta", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 3, "b_wins": 0, "draws": 0, "count": 3, "a_win_ratio": 1.0}, {"model_a": "o3-mini", "model_b": "gemma-3-27b", "a_wins": 13, "b_wins": 11, "draws": 14, "count": 38, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-405b", "model_b": "gpt-4.1-mini", "a_wins": 13, "b_wins": 54, "draws": 59, "count": 126, "a_win_ratio": 0.19}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemma-2-27b-it-q8", "a_wins": 2, "b_wins": 1, "draws": 2, "count": 5, "a_win_ratio": 0.67}, {"model_a": "llama-3.3-70b", "model_b": "llama-3.1-8b", "a_wins": 66, "b_wins": 68, "draws": 55, "count": 189, "a_win_ratio": 0.49}, {"model_a": "gemini-1.5-pro-002", "model_b": "mistral-small-3.1-24b", "a_wins": 17, "b_wins": 11, "draws": 22, "count": 50, "a_win_ratio": 0.61}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mistral-large-2411", "a_wins": 69, "b_wins": 69, "draws": 87, "count": 225, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 3, "draws": 0, "count": 6, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-70b", "model_b": "gemma-3-12b", "a_wins": 6, "b_wins": 12, "draws": 10, "count": 28, "a_win_ratio": 0.33}, {"model_a": "qwq-32b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 9, "b_wins": 7, "draws": 17, "count": 33, "a_win_ratio": 0.56}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 71, "b_wins": 97, "draws": 89, "count": 257, "a_win_ratio": 0.42}, {"model_a": "mistral-large-2411", "model_b": "llama-3.3-70b", "a_wins": 69, "b_wins": 81, "draws": 79, "count": 229, "a_win_ratio": 0.46}, {"model_a": "llama-3.1-70b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 49, "b_wins": 48, "draws": 51, "count": 148, "a_win_ratio": 0.51}, {"model_a": "gemma-3-4b", "model_b": "qwq-32b", "a_wins": 11, "b_wins": 8, "draws": 6, "count": 25, "a_win_ratio": 0.58}, {"model_a": "gpt-4o-2024-08-06", "model_b": "llama-3.3-70b", "a_wins": 19, "b_wins": 27, "draws": 25, "count": 71, "a_win_ratio": 0.41}, {"model_a": "lfm-40b", "model_b": "deepseek-r1", "a_wins": 4, "b_wins": 12, "draws": 9, "count": 25, "a_win_ratio": 0.25}, {"model_a": "gpt-4o-2024-08-06", "model_b": "phi-3.5-mini-instruct", "a_wins": 28, "b_wins": 13, "draws": 20, "count": 61, "a_win_ratio": 0.68}, {"model_a": "llama-3.3-70b", "model_b": "command-a", "a_wins": 31, "b_wins": 49, "draws": 46, "count": 126, "a_win_ratio": 0.39}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 10, "b_wins": 13, "draws": 3, "count": 26, "a_win_ratio": 0.43}, {"model_a": "gemma-3-4b", "model_b": "deepseek-v3-0324", "a_wins": 50, "b_wins": 75, "draws": 66, "count": 191, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemma-3-27b", "a_wins": 37, "b_wins": 59, "draws": 40, "count": 136, "a_win_ratio": 0.39}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemma-2-9b-it", "a_wins": 37, "b_wins": 19, "draws": 43, "count": 99, "a_win_ratio": 0.66}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "phi-3.5-mini-instruct", "a_wins": 20, "b_wins": 57, "draws": 69, "count": 146, "a_win_ratio": 0.26}, {"model_a": "deepseek-r1", "model_b": "llama-4-scout", "a_wins": 4, "b_wins": 2, "draws": 3, "count": 9, "a_win_ratio": 0.67}, {"model_a": "aya-expanse-8b", "model_b": "phi-3.5-mini-instruct", "a_wins": 0, "b_wins": 1, "draws": 1, "count": 2, "a_win_ratio": 0.0}, {"model_a": "gpt-4.1-nano", "model_b": "ministral-8b-instruct-2410", "a_wins": 57, "b_wins": 51, "draws": 48, "count": 156, "a_win_ratio": 0.53}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-3.3-70b", "a_wins": 69, "b_wins": 85, "draws": 85, "count": 239, "a_win_ratio": 0.45}, {"model_a": "mistral-nemo-2407", "model_b": "gemma-2-27b-it-q8", "a_wins": 15, "b_wins": 21, "draws": 25, "count": 61, "a_win_ratio": 0.42}, {"model_a": "grok-3-mini-beta", "model_b": "gpt-4.1-mini", "a_wins": 20, "b_wins": 13, "draws": 13, "count": 46, "a_win_ratio": 0.61}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mistral-saba", "a_wins": 2, "b_wins": 4, "draws": 1, "count": 7, "a_win_ratio": 0.33}, {"model_a": "llama-4-scout", "model_b": "mistral-saba", "a_wins": 38, "b_wins": 40, "draws": 48, "count": 126, "a_win_ratio": 0.49}, {"model_a": "llama-3.1-8b", "model_b": "gemma-3-4b", "a_wins": 28, "b_wins": 40, "draws": 44, "count": 112, "a_win_ratio": 0.41}, {"model_a": "gemma-3-27b", "model_b": "gemma-3-12b", "a_wins": 51, "b_wins": 53, "draws": 38, "count": 142, "a_win_ratio": 0.49}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 29, "b_wins": 64, "draws": 92, "count": 185, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-405b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 14, "b_wins": 18, "draws": 19, "count": 51, "a_win_ratio": 0.44}, {"model_a": "o4-mini", "model_b": "qwen3-32b", "a_wins": 42, "b_wins": 62, "draws": 47, "count": 151, "a_win_ratio": 0.4}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "ministral-8b-instruct-2410", "a_wins": 3, "b_wins": 9, "draws": 13, "count": 25, "a_win_ratio": 0.25}, {"model_a": "phi-3.5-mini-instruct", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 5, "b_wins": 4, "draws": 6, "count": 15, "a_win_ratio": 0.56}, {"model_a": "gemma-3-4b", "model_b": "grok-3-mini-beta", "a_wins": 0, "b_wins": 3, "draws": 5, "count": 8, "a_win_ratio": 0.0}, {"model_a": "gpt-4.1-nano", "model_b": "llama-3.1-8b", "a_wins": 26, "b_wins": 29, "draws": 37, "count": 92, "a_win_ratio": 0.47}, {"model_a": "deepseek-r1", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 12, "b_wins": 6, "draws": 12, "count": 30, "a_win_ratio": 0.67}, {"model_a": "phi-4", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 0, "draws": 2, "count": 3, "a_win_ratio": 1.0}, {"model_a": "aya-expanse-32b", "model_b": "command-a", "a_wins": 18, "b_wins": 29, "draws": 31, "count": 78, "a_win_ratio": 0.38}, {"model_a": "gemma-3-27b", "model_b": "command-a", "a_wins": 47, "b_wins": 56, "draws": 55, "count": 158, "a_win_ratio": 0.46}, {"model_a": "gpt-4.1-mini", "model_b": "o4-mini", "a_wins": 33, "b_wins": 27, "draws": 24, "count": 84, "a_win_ratio": 0.55}, {"model_a": "gpt-4.1-mini", "model_b": "mistral-saba", "a_wins": 67, "b_wins": 57, "draws": 59, "count": 183, "a_win_ratio": 0.54}, {"model_a": "llama-4-scout", "model_b": "qwen3-32b", "a_wins": 0, "b_wins": 0, "draws": 2, "count": 2, "a_win_ratio": null}, {"model_a": "gpt-4.1-nano", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 42, "b_wins": 45, "draws": 31, "count": 118, "a_win_ratio": 0.48}, {"model_a": "llama-3.1-405b", "model_b": "deepseek-v3-0324", "a_wins": 19, "b_wins": 53, "draws": 44, "count": 116, "a_win_ratio": 0.26}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "deepseek-v3-chat", "a_wins": 5, "b_wins": 19, "draws": 13, "count": 37, "a_win_ratio": 0.21}, {"model_a": "gemini-2.0-flash-exp", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 10, "b_wins": 2, "draws": 11, "count": 23, "a_win_ratio": 0.83}, {"model_a": "llama-4-scout", "model_b": "llama-3.1-405b", "a_wins": 42, "b_wins": 19, "draws": 44, "count": 105, "a_win_ratio": 0.69}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemini-1.5-pro-002", "a_wins": 31, "b_wins": 80, "draws": 51, "count": 162, "a_win_ratio": 0.28}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemma-3-12b", "a_wins": 39, "b_wins": 63, "draws": 36, "count": 138, "a_win_ratio": 0.38}, {"model_a": "gemini-2.0-flash-exp", "model_b": "claude-3-5-sonnet-v2", "a_wins": 59, "b_wins": 32, "draws": 68, "count": 159, "a_win_ratio": 0.65}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 17, "b_wins": 11, "draws": 11, "count": 39, "a_win_ratio": 0.61}, {"model_a": "llama-3.1-70b", "model_b": "llama-3.1-70b", "a_wins": 10, "b_wins": 10, "draws": 2, "count": 22, "a_win_ratio": 0.5}, {"model_a": "gemma-3-4b", "model_b": "aya-expanse-32b", "a_wins": 273, "b_wins": 183, "draws": 270, "count": 726, "a_win_ratio": 0.6}, {"model_a": "mistral-nemo-2407", "model_b": "gemma-3-4b", "a_wins": 5, "b_wins": 13, "draws": 13, "count": 31, "a_win_ratio": 0.28}, {"model_a": "deepseek-v3-chat", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 60, "b_wins": 29, "draws": 47, "count": 136, "a_win_ratio": 0.67}, {"model_a": "llama-3.1-405b", "model_b": "mistral-small-3.1-24b", "a_wins": 33, "b_wins": 80, "draws": 60, "count": 173, "a_win_ratio": 0.29}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "aya-expanse-8b", "a_wins": 25, "b_wins": 16, "draws": 13, "count": 54, "a_win_ratio": 0.61}, {"model_a": "mistral-large-2411", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 166, "b_wins": 137, "draws": 166, "count": 469, "a_win_ratio": 0.55}, {"model_a": "gemma-3-4b", "model_b": "llama-3.1-70b", "a_wins": 14, "b_wins": 11, "draws": 15, "count": 40, "a_win_ratio": 0.56}, {"model_a": "claude-3-7-sonnet", "model_b": "gpt-4.1-mini", "a_wins": 113, "b_wins": 114, "draws": 129, "count": 356, "a_win_ratio": 0.5}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "qwen2.5-32b-instruct", "a_wins": 3, "b_wins": 4, "draws": 4, "count": 11, "a_win_ratio": 0.43}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "gemma-2-27b-it-q8", "a_wins": 3, "b_wins": 13, "draws": 16, "count": 32, "a_win_ratio": 0.19}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gemini-1.5-pro-002", "a_wins": 0, "b_wins": 1, "draws": 1, "count": 2, "a_win_ratio": 0.0}, {"model_a": "aya-expanse-8b", "model_b": "llama-3.1-8b", "a_wins": 16, "b_wins": 12, "draws": 14, "count": 42, "a_win_ratio": 0.57}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "deepseek-r1", "a_wins": 72, "b_wins": 60, "draws": 70, "count": 202, "a_win_ratio": 0.55}, {"model_a": "deepseek-v3-chat", "model_b": "aya-expanse-8b", "a_wins": 12, "b_wins": 7, "draws": 16, "count": 35, "a_win_ratio": 0.63}, {"model_a": "mistral-small-3.1-24b", "model_b": "o4-mini", "a_wins": 45, "b_wins": 43, "draws": 56, "count": 144, "a_win_ratio": 0.51}, {"model_a": "qwen3-32b", "model_b": "o4-mini", "a_wins": 62, "b_wins": 42, "draws": 47, "count": 151, "a_win_ratio": 0.6}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gemma-2-9b-it", "a_wins": 32, "b_wins": 65, "draws": 72, "count": 169, "a_win_ratio": 0.33}, {"model_a": "o4-mini", "model_b": "gpt-4.1-nano", "a_wins": 16, "b_wins": 5, "draws": 5, "count": 26, "a_win_ratio": 0.76}, {"model_a": "gemini-1.5-pro-002", "model_b": "command-a", "a_wins": 10, "b_wins": 18, "draws": 15, "count": 43, "a_win_ratio": 0.36}, {"model_a": "deepseek-r1", "model_b": "mistral-small-3.1-24b", "a_wins": 31, "b_wins": 26, "draws": 29, "count": 86, "a_win_ratio": 0.54}, {"model_a": "mistral-nemo-2407", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 36, "b_wins": 58, "draws": 48, "count": 142, "a_win_ratio": 0.38}, {"model_a": "deepseek-v3-0324", "model_b": "gpt-4.1-mini", "a_wins": 123, "b_wins": 95, "draws": 103, "count": 321, "a_win_ratio": 0.56}, {"model_a": "phi-4", "model_b": "gpt-4.1-mini", "a_wins": 43, "b_wins": 57, "draws": 50, "count": 150, "a_win_ratio": 0.43}, {"model_a": "gemma-3-4b", "model_b": "qwen3-32b", "a_wins": 0, "b_wins": 1, "draws": 0, "count": 1, "a_win_ratio": 0.0}, {"model_a": "gemma-3-4b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 72, "b_wins": 64, "draws": 69, "count": 205, "a_win_ratio": 0.53}, {"model_a": "gemma-3-12b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 63, "b_wins": 33, "draws": 52, "count": 148, "a_win_ratio": 0.66}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "claude-3-5-sonnet-v2", "a_wins": 46, "b_wins": 56, "draws": 55, "count": 157, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-70b", "model_b": "deepseek-r1", "a_wins": 9, "b_wins": 13, "draws": 11, "count": 33, "a_win_ratio": 0.41}, {"model_a": "lfm-40b", "model_b": "gemma-3-4b", "a_wins": 6, "b_wins": 6, "draws": 5, "count": 17, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-001", "model_b": "llama-3.1-405b", "a_wins": 71, "b_wins": 51, "draws": 127, "count": 249, "a_win_ratio": 0.58}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 17, "b_wins": 36, "draws": 32, "count": 85, "a_win_ratio": 0.32}, {"model_a": "command-a", "model_b": "mistral-large-2411", "a_wins": 54, "b_wins": 35, "draws": 37, "count": 126, "a_win_ratio": 0.61}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "o3-mini", "a_wins": 9, "b_wins": 13, "draws": 10, "count": 32, "a_win_ratio": 0.41}, {"model_a": "o3-mini", "model_b": "mistral-large-2411", "a_wins": 16, "b_wins": 18, "draws": 23, "count": 57, "a_win_ratio": 0.47}, {"model_a": "qwen2-7b-instruct", "model_b": "llama-3.1-70b", "a_wins": 0, "b_wins": 3, "draws": 1, "count": 4, "a_win_ratio": 0.0}, {"model_a": "llama-3.1-405b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 95, "b_wins": 52, "draws": 132, "count": 279, "a_win_ratio": 0.65}, {"model_a": "llama-3.3-70b", "model_b": "gemini-1.5-pro-002", "a_wins": 33, "b_wins": 38, "draws": 42, "count": 113, "a_win_ratio": 0.46}, {"model_a": "mistral-large-2411", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 18, "b_wins": 11, "draws": 10, "count": 39, "a_win_ratio": 0.62}, {"model_a": "llama-3.3-70b", "model_b": "mistral-saba", "a_wins": 44, "b_wins": 34, "draws": 33, "count": 111, "a_win_ratio": 0.56}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "c4ai-command-r-08-2024", "a_wins": 31, "b_wins": 46, "draws": 43, "count": 120, "a_win_ratio": 0.4}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gpt-4.1-mini", "a_wins": 33, "b_wins": 54, "draws": 34, "count": 121, "a_win_ratio": 0.38}, {"model_a": "gemini-2.0-flash-001", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 31, "b_wins": 5, "draws": 14, "count": 50, "a_win_ratio": 0.86}, {"model_a": "phi-3.5-mini-instruct", "model_b": "mistral-large-2411", "a_wins": 1, "b_wins": 1, "draws": 2, "count": 4, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "phi-4", "a_wins": 72, "b_wins": 62, "draws": 68, "count": 202, "a_win_ratio": 0.54}, {"model_a": "gemma-3-27b", "model_b": "lfm-40b", "a_wins": 17, "b_wins": 6, "draws": 12, "count": 35, "a_win_ratio": 0.74}, {"model_a": "phi-4", "model_b": "gemma-3-12b", "a_wins": 31, "b_wins": 71, "draws": 49, "count": 151, "a_win_ratio": 0.3}, {"model_a": "qwen3-32b", "model_b": "gpt-4.1-nano", "a_wins": 0, "b_wins": 1, "draws": 1, "count": 2, "a_win_ratio": 0.0}, {"model_a": "mistral-nemo-2407", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 36, "b_wins": 66, "draws": 55, "count": 157, "a_win_ratio": 0.35}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "llama-3.1-8b", "a_wins": 17, "b_wins": 4, "draws": 15, "count": 36, "a_win_ratio": 0.81}, {"model_a": "llama-3.1-8b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 92, "b_wins": 83, "draws": 83, "count": 258, "a_win_ratio": 0.53}, {"model_a": "aya-expanse-32b", "model_b": "llama-3.1-8b", "a_wins": 21, "b_wins": 21, "draws": 28, "count": 70, "a_win_ratio": 0.5}, {"model_a": "claude-3-7-sonnet", "model_b": "llama-4-scout", "a_wins": 36, "b_wins": 27, "draws": 46, "count": 109, "a_win_ratio": 0.57}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "deepseek-v3-chat", "a_wins": 40, "b_wins": 66, "draws": 51, "count": 157, "a_win_ratio": 0.38}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemma-3-12b", "a_wins": 33, "b_wins": 63, "draws": 52, "count": 148, "a_win_ratio": 0.34}, {"model_a": "gemma-3-4b", "model_b": "gemma-3-4b", "a_wins": 6, "b_wins": 6, "draws": 2, "count": 14, "a_win_ratio": 0.5}, {"model_a": "phi-3.5-mini-instruct", "model_b": "lfm-40b", "a_wins": 21, "b_wins": 24, "draws": 40, "count": 85, "a_win_ratio": 0.47}, {"model_a": "command-a", "model_b": "gemma-3-27b", "a_wins": 56, "b_wins": 47, "draws": 55, "count": 158, "a_win_ratio": 0.54}, {"model_a": "jamba-1.5-large", "model_b": "ministral-8b-instruct-2410", "a_wins": 3, "b_wins": 4, "draws": 2, "count": 9, "a_win_ratio": 0.43}, {"model_a": "lfm-40b", "model_b": "c4ai-command-r-08-2024", "a_wins": 28, "b_wins": 27, "draws": 41, "count": 96, "a_win_ratio": 0.51}, {"model_a": "command-a", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 14, "b_wins": 8, "draws": 16, "count": 38, "a_win_ratio": 0.64}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-3.1-70b", "a_wins": 41, "b_wins": 31, "draws": 44, "count": 116, "a_win_ratio": 0.57}, {"model_a": "gemini-2.0-flash-001", "model_b": "o3-mini", "a_wins": 24, "b_wins": 13, "draws": 21, "count": 58, "a_win_ratio": 0.65}, {"model_a": "mistral-large-2411", "model_b": "llama-3.1-8b", "a_wins": 101, "b_wins": 85, "draws": 81, "count": 267, "a_win_ratio": 0.54}, {"model_a": "claude-3-7-sonnet", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 41, "b_wins": 34, "draws": 37, "count": 112, "a_win_ratio": 0.55}, {"model_a": "gemini-1.5-pro-002", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 160, "b_wins": 139, "draws": 139, "count": 438, "a_win_ratio": 0.54}, {"model_a": "command-a", "model_b": "gemma-3-12b", "a_wins": 49, "b_wins": 42, "draws": 51, "count": 142, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-70b", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 12, "b_wins": 1, "draws": 12, "count": 25, "a_win_ratio": 0.92}, {"model_a": "claude-3-7-sonnet", "model_b": "mistral-saba", "a_wins": 53, "b_wins": 41, "draws": 56, "count": 150, "a_win_ratio": 0.56}, {"model_a": "ministral-8b-instruct-2410", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 9, "b_wins": 3, "draws": 13, "count": 25, "a_win_ratio": 0.75}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gpt-4.1-mini", "a_wins": 41, "b_wins": 50, "draws": 42, "count": 133, "a_win_ratio": 0.45}, {"model_a": "mistral-large-2411", "model_b": "mistral-saba", "a_wins": 51, "b_wins": 31, "draws": 40, "count": 122, "a_win_ratio": 0.62}, {"model_a": "command-a", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 46, "b_wins": 39, "draws": 39, "count": 124, "a_win_ratio": 0.54}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "o3-mini", "a_wins": 14, "b_wins": 12, "draws": 14, "count": 40, "a_win_ratio": 0.54}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 92, "b_wins": 56, "draws": 78, "count": 226, "a_win_ratio": 0.62}, {"model_a": "deepseek-v3-0324", "model_b": "c4ai-command-r-08-2024", "a_wins": 63, "b_wins": 27, "draws": 47, "count": 137, "a_win_ratio": 0.7}, {"model_a": "llama-4-scout", "model_b": "gemini-2.0-flash-001", "a_wins": 31, "b_wins": 51, "draws": 46, "count": 128, "a_win_ratio": 0.38}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemini-2.0-flash-001", "a_wins": 17, "b_wins": 32, "draws": 37, "count": 86, "a_win_ratio": 0.35}, {"model_a": "gemma-3-4b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 11, "b_wins": 4, "draws": 11, "count": 26, "a_win_ratio": 0.73}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "qwq-32b", "a_wins": 10, "b_wins": 6, "draws": 10, "count": 26, "a_win_ratio": 0.62}, {"model_a": "gemma-3-12b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 53, "b_wins": 37, "draws": 63, "count": 153, "a_win_ratio": 0.59}, {"model_a": "ministral-8b-instruct-2410", "model_b": "qwen2.5-7b-instruct", "a_wins": 33, "b_wins": 32, "draws": 70, "count": 135, "a_win_ratio": 0.51}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "o3-mini", "a_wins": 58, "b_wins": 57, "draws": 47, "count": 162, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-70b", "model_b": "ministral-8b-instruct-2410", "a_wins": 70, "b_wins": 83, "draws": 119, "count": 272, "a_win_ratio": 0.46}, {"model_a": "llama-3.3-70b", "model_b": "aya-expanse-8b", "a_wins": 6, "b_wins": 8, "draws": 9, "count": 23, "a_win_ratio": 0.43}, {"model_a": "deepseek-r1", "model_b": "llama-3.1-8b", "a_wins": 14, "b_wins": 15, "draws": 11, "count": 40, "a_win_ratio": 0.48}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "ministral-8b-instruct-2410", "a_wins": 37, "b_wins": 40, "draws": 70, "count": 147, "a_win_ratio": 0.48}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "lfm-40b", "a_wins": 1, "b_wins": 9, "draws": 12, "count": 22, "a_win_ratio": 0.1}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "lfm-40b", "a_wins": 49, "b_wins": 57, "draws": 62, "count": 168, "a_win_ratio": 0.46}, {"model_a": "llama-3.1-8b", "model_b": "o3-mini", "a_wins": 11, "b_wins": 8, "draws": 10, "count": 29, "a_win_ratio": 0.58}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 7, "b_wins": 7, "draws": 4, "count": 18, "a_win_ratio": 0.5}, {"model_a": "c4ai-command-r-08-2024", "model_b": "deepseek-v3-chat", "a_wins": 27, "b_wins": 61, "draws": 46, "count": 134, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-8b", "model_b": "gemma-3-27b", "a_wins": 19, "b_wins": 52, "draws": 48, "count": 119, "a_win_ratio": 0.27}, {"model_a": "llama-3.1-405b", "model_b": "llama-4-scout", "a_wins": 19, "b_wins": 42, "draws": 44, "count": 105, "a_win_ratio": 0.31}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "lfm-40b", "a_wins": 15, "b_wins": 29, "draws": 17, "count": 61, "a_win_ratio": 0.34}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 2, "draws": 3, "count": 8, "a_win_ratio": 0.6}, {"model_a": "gemma-3-4b", "model_b": "mistral-nemo-2407", "a_wins": 13, "b_wins": 5, "draws": 13, "count": 31, "a_win_ratio": 0.72}, {"model_a": "gemini-2.0-flash-001", "model_b": "gpt-4.1-mini", "a_wins": 144, "b_wins": 115, "draws": 101, "count": 360, "a_win_ratio": 0.56}, {"model_a": "qwq-32b", "model_b": "gemma-3-12b", "a_wins": 6, "b_wins": 10, "draws": 5, "count": 21, "a_win_ratio": 0.38}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "aya-expanse-32b", "a_wins": 80, "b_wins": 51, "draws": 62, "count": 193, "a_win_ratio": 0.61}, {"model_a": "deepseek-v3-chat", "model_b": "qwq-32b", "a_wins": 8, "b_wins": 7, "draws": 16, "count": 31, "a_win_ratio": 0.53}, {"model_a": "o4-mini", "model_b": "gemma-3-12b", "a_wins": 7, "b_wins": 9, "draws": 5, "count": 21, "a_win_ratio": 0.44}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "command-a", "a_wins": 39, "b_wins": 46, "draws": 39, "count": 124, "a_win_ratio": 0.46}, {"model_a": "gemma-3-12b", "model_b": "o4-mini", "a_wins": 9, "b_wins": 7, "draws": 5, "count": 21, "a_win_ratio": 0.56}, {"model_a": "deepseek-v3-0324", "model_b": "claude-3-7-sonnet", "a_wins": 84, "b_wins": 69, "draws": 73, "count": 226, "a_win_ratio": 0.55}, {"model_a": "mistral-large-2411", "model_b": "mistral-nemo-2407", "a_wins": 81, "b_wins": 47, "draws": 68, "count": 196, "a_win_ratio": 0.63}, {"model_a": "gemma-2-9b-it", "model_b": "mistral-large-2411", "a_wins": 47, "b_wins": 53, "draws": 60, "count": 160, "a_win_ratio": 0.47}, {"model_a": "llama-3.3-70b", "model_b": "phi-4", "a_wins": 120, "b_wins": 117, "draws": 124, "count": 361, "a_win_ratio": 0.51}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mistral-large-2411", "a_wins": 76, "b_wins": 120, "draws": 110, "count": 306, "a_win_ratio": 0.39}, {"model_a": "mistral-saba", "model_b": "deepseek-r1", "a_wins": 3, "b_wins": 6, "draws": 4, "count": 13, "a_win_ratio": 0.33}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 13, "b_wins": 10, "draws": 3, "count": 26, "a_win_ratio": 0.57}, {"model_a": "qwen3-32b", "model_b": "mistral-saba", "a_wins": 4, "b_wins": 2, "draws": 1, "count": 7, "a_win_ratio": 0.67}, {"model_a": "deepseek-r1", "model_b": "gpt-4.1-mini", "a_wins": 48, "b_wins": 56, "draws": 56, "count": 160, "a_win_ratio": 0.46}, {"model_a": "o4-mini", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 2, "b_wins": 2, "draws": 5, "count": 9, "a_win_ratio": 0.5}, {"model_a": "mistral-nemo-2407", "model_b": "gemma-2-9b-it", "a_wins": 66, "b_wins": 108, "draws": 151, "count": 325, "a_win_ratio": 0.38}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "llama-3.1-405b", "a_wins": 46, "b_wins": 57, "draws": 57, "count": 160, "a_win_ratio": 0.45}, {"model_a": "gemma-2-27b-it-q8", "model_b": "mistral-nemo-2407", "a_wins": 21, "b_wins": 15, "draws": 25, "count": 61, "a_win_ratio": 0.58}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemma-3-27b", "a_wins": 8, "b_wins": 17, "draws": 13, "count": 38, "a_win_ratio": 0.32}, {"model_a": "mistral-nemo-2407", "model_b": "qwen2.5-32b-instruct", "a_wins": 3, "b_wins": 10, "draws": 9, "count": 22, "a_win_ratio": 0.23}, {"model_a": "phi-4", "model_b": "gemma-2-9b-it", "a_wins": 58, "b_wins": 58, "draws": 54, "count": 170, "a_win_ratio": 0.5}, {"model_a": "llama-4-scout", "model_b": "gemma-3-4b", "a_wins": 37, "b_wins": 49, "draws": 41, "count": 127, "a_win_ratio": 0.43}, {"model_a": "gemma-2-9b-it", "model_b": "phi-4", "a_wins": 58, "b_wins": 58, "draws": 54, "count": 170, "a_win_ratio": 0.5}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-4-scout", "a_wins": 38, "b_wins": 48, "draws": 51, "count": 137, "a_win_ratio": 0.44}, {"model_a": "llama-3.1-8b", "model_b": "gemini-1.5-pro-002", "a_wins": 50, "b_wins": 58, "draws": 58, "count": 166, "a_win_ratio": 0.46}, {"model_a": "gemma-3-27b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 17, "b_wins": 8, "draws": 13, "count": 38, "a_win_ratio": 0.68}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 14, "b_wins": 29, "draws": 27, "count": 70, "a_win_ratio": 0.33}, {"model_a": "mistral-large-2411", "model_b": "deepseek-r1", "a_wins": 40, "b_wins": 37, "draws": 44, "count": 121, "a_win_ratio": 0.52}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-3.1-405b", "a_wins": 96, "b_wins": 31, "draws": 81, "count": 208, "a_win_ratio": 0.76}, {"model_a": "o4-mini", "model_b": "mistral-large-2411", "a_wins": 36, "b_wins": 45, "draws": 54, "count": 135, "a_win_ratio": 0.44}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "o4-mini", "a_wins": 2, "b_wins": 3, "draws": 5, "count": 10, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-8b", "model_b": "gpt-4.1-mini", "a_wins": 28, "b_wins": 48, "draws": 46, "count": 122, "a_win_ratio": 0.37}, {"model_a": "gemma-3-27b", "model_b": "o3-mini", "a_wins": 11, "b_wins": 13, "draws": 14, "count": 38, "a_win_ratio": 0.46}, {"model_a": "deepseek-v3-0324", "model_b": "grok-3-mini-beta", "a_wins": 11, "b_wins": 24, "draws": 12, "count": 47, "a_win_ratio": 0.31}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemma-3-4b", "a_wins": 55, "b_wins": 37, "draws": 59, "count": 151, "a_win_ratio": 0.6}, {"model_a": "llama-3.3-70b", "model_b": "mistral-nemo-2407", "a_wins": 45, "b_wins": 21, "draws": 37, "count": 103, "a_win_ratio": 0.68}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "aya-expanse-8b", "a_wins": 14, "b_wins": 28, "draws": 30, "count": 72, "a_win_ratio": 0.33}, {"model_a": "grok-3-mini-beta", "model_b": "c4ai-command-r-08-2024", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 52, "b_wins": 46, "draws": 53, "count": 151, "a_win_ratio": 0.53}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "aya-expanse-8b", "a_wins": 6, "b_wins": 10, "draws": 15, "count": 31, "a_win_ratio": 0.38}, {"model_a": "ministral-8b-instruct-2410", "model_b": "deepseek-r1", "a_wins": 19, "b_wins": 22, "draws": 26, "count": 67, "a_win_ratio": 0.46}, {"model_a": "phi-4", "model_b": "llama-3.1-405b", "a_wins": 122, "b_wins": 104, "draws": 110, "count": 336, "a_win_ratio": 0.54}, {"model_a": "mistral-nemo-2407", "model_b": "gemma-3-27b", "a_wins": 12, "b_wins": 18, "draws": 13, "count": 43, "a_win_ratio": 0.4}, {"model_a": "phi-3.5-mini-instruct", "model_b": "mistral-nemo-2407", "a_wins": 69, "b_wins": 65, "draws": 111, "count": 245, "a_win_ratio": 0.51}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 21, "b_wins": 27, "draws": 29, "count": 77, "a_win_ratio": 0.44}, {"model_a": "gemma-3-27b", "model_b": "gemini-2.0-flash-001", "a_wins": 55, "b_wins": 64, "draws": 51, "count": 170, "a_win_ratio": 0.46}, {"model_a": "gemma-3-4b", "model_b": "gemma-3-27b", "a_wins": 49, "b_wins": 65, "draws": 41, "count": 155, "a_win_ratio": 0.43}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemma-3-12b", "a_wins": 13, "b_wins": 8, "draws": 14, "count": 35, "a_win_ratio": 0.62}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "qwen2.5-7b-instruct", "a_wins": 3, "b_wins": 1, "draws": 1, "count": 5, "a_win_ratio": 0.75}, {"model_a": "llama-3.1-405b", "model_b": "mistral-large-2411", "a_wins": 86, "b_wins": 116, "draws": 124, "count": 326, "a_win_ratio": 0.43}, {"model_a": "gemini-2.0-flash-001", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 89, "b_wins": 59, "draws": 70, "count": 218, "a_win_ratio": 0.6}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gemma-2-9b-it", "a_wins": 46, "b_wins": 24, "draws": 62, "count": 132, "a_win_ratio": 0.66}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemma-2-9b-it", "a_wins": 12, "b_wins": 13, "draws": 8, "count": 33, "a_win_ratio": 0.48}, {"model_a": "mistral-nemo-2407", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 66, "b_wins": 102, "draws": 103, "count": 271, "a_win_ratio": 0.39}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemini-2.0-flash-exp", "a_wins": 38, "b_wins": 64, "draws": 54, "count": 156, "a_win_ratio": 0.37}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gemma-2-27b-it-q8", "a_wins": 18, "b_wins": 24, "draws": 33, "count": 75, "a_win_ratio": 0.43}, {"model_a": "mistral-nemo-2407", "model_b": "ministral-8b-instruct-2410", "a_wins": 71, "b_wins": 105, "draws": 128, "count": 304, "a_win_ratio": 0.4}, {"model_a": "aya-expanse-8b", "model_b": "lfm-40b", "a_wins": 9, "b_wins": 9, "draws": 14, "count": 32, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-2024-08-06", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 40, "b_wins": 22, "draws": 36, "count": 98, "a_win_ratio": 0.65}, {"model_a": "gemini-2.0-flash-001", "model_b": "ministral-8b-instruct-2410", "a_wins": 100, "b_wins": 37, "draws": 73, "count": 210, "a_win_ratio": 0.73}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemini-2.0-flash-exp", "a_wins": 34, "b_wins": 81, "draws": 54, "count": 169, "a_win_ratio": 0.3}, {"model_a": "phi-4", "model_b": "llama-4-scout", "a_wins": 40, "b_wins": 27, "draws": 45, "count": 112, "a_win_ratio": 0.6}, {"model_a": "gemma-3-27b", "model_b": "gemma-2-9b-it", "a_wins": 18, "b_wins": 11, "draws": 9, "count": 38, "a_win_ratio": 0.62}, {"model_a": "gemini-1.5-pro-001", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 13, "b_wins": 1, "draws": 21, "count": 35, "a_win_ratio": 0.93}, {"model_a": "deepseek-v3-chat", "model_b": "llama-3.1-70b", "a_wins": 50, "b_wins": 36, "draws": 44, "count": 130, "a_win_ratio": 0.58}, {"model_a": "phi-4", "model_b": "deepseek-v3-0324", "a_wins": 77, "b_wins": 96, "draws": 86, "count": 259, "a_win_ratio": 0.45}, {"model_a": "jamba-1.5-large", "model_b": "gemma-2-9b-it", "a_wins": 1, "b_wins": 3, "draws": 5, "count": 9, "a_win_ratio": 0.25}, {"model_a": "llama-4-scout", "model_b": "gpt-4.1-nano", "a_wins": 46, "b_wins": 43, "draws": 48, "count": 137, "a_win_ratio": 0.52}, {"model_a": "deepseek-v3-0324", "model_b": "llama-3.1-405b", "a_wins": 53, "b_wins": 19, "draws": 44, "count": 116, "a_win_ratio": 0.74}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gemma-2-9b-it", "a_wins": 42, "b_wins": 65, "draws": 94, "count": 201, "a_win_ratio": 0.39}, {"model_a": "mistral-large-2411", "model_b": "gpt-4.1-mini", "a_wins": 102, "b_wins": 99, "draws": 94, "count": 295, "a_win_ratio": 0.51}, {"model_a": "aya-expanse-8b", "model_b": "phi-4", "a_wins": 27, "b_wins": 29, "draws": 23, "count": 79, "a_win_ratio": 0.48}, {"model_a": "phi-4", "model_b": "gemini-2.0-flash-exp", "a_wins": 32, "b_wins": 51, "draws": 43, "count": 126, "a_win_ratio": 0.39}, {"model_a": "llama-3.1-8b", "model_b": "mistral-large-2411", "a_wins": 85, "b_wins": 101, "draws": 81, "count": 267, "a_win_ratio": 0.46}, {"model_a": "aya-expanse-32b", "model_b": "deepseek-v3-0324", "a_wins": 52, "b_wins": 98, "draws": 69, "count": 219, "a_win_ratio": 0.35}, {"model_a": "c4ai-command-r-08-2024", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "deepseek-v3-chat", "model_b": "phi-3.5-mini-instruct", "a_wins": 2, "b_wins": 0, "draws": 1, "count": 3, "a_win_ratio": 1.0}, {"model_a": "gemma-2-9b-it", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 19, "b_wins": 37, "draws": 43, "count": 99, "a_win_ratio": 0.34}, {"model_a": "qwen2.5-7b-instruct", "model_b": "gpt-4o-2024-08-06", "a_wins": 15, "b_wins": 19, "draws": 15, "count": 49, "a_win_ratio": 0.44}, {"model_a": "command-a", "model_b": "llama-3.1-8b", "a_wins": 63, "b_wins": 27, "draws": 45, "count": 135, "a_win_ratio": 0.7}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "phi-4", "a_wins": 50, "b_wins": 70, "draws": 83, "count": 203, "a_win_ratio": 0.42}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 7, "b_wins": 1, "draws": 6, "count": 14, "a_win_ratio": 0.88}, {"model_a": "o4-mini", "model_b": "deepseek-r1", "a_wins": 54, "b_wins": 83, "draws": 74, "count": 211, "a_win_ratio": 0.39}, {"model_a": "gemini-1.5-pro-002", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 43, "b_wins": 45, "draws": 42, "count": 130, "a_win_ratio": 0.49}, {"model_a": "mistral-large-2411", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 69, "b_wins": 69, "draws": 87, "count": 225, "a_win_ratio": 0.5}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gemini-2.0-flash-exp", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gpt-4o-2024-08-06", "a_wins": 151, "b_wins": 142, "draws": 187, "count": 480, "a_win_ratio": 0.52}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "llama-3.1-8b", "a_wins": 13, "b_wins": 10, "draws": 14, "count": 37, "a_win_ratio": 0.57}, {"model_a": "llama-4-scout", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 38, "b_wins": 40, "draws": 40, "count": 118, "a_win_ratio": 0.49}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemma-2-27b-it-q8", "a_wins": 3, "b_wins": 2, "draws": 12, "count": 17, "a_win_ratio": 0.6}, {"model_a": "command-a", "model_b": "deepseek-r1", "a_wins": 14, "b_wins": 21, "draws": 15, "count": 50, "a_win_ratio": 0.4}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mistral-large-2411", "a_wins": 97, "b_wins": 110, "draws": 125, "count": 332, "a_win_ratio": 0.47}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemma-3-12b", "a_wins": 29, "b_wins": 68, "draws": 64, "count": 161, "a_win_ratio": 0.3}, {"model_a": "gemini-2.0-flash-001", "model_b": "claude-3-5-sonnet-v2", "a_wins": 32, "b_wins": 17, "draws": 37, "count": 86, "a_win_ratio": 0.65}, {"model_a": "deepseek-r1", "model_b": "gemini-2.0-flash-001", "a_wins": 23, "b_wins": 34, "draws": 41, "count": 98, "a_win_ratio": 0.4}, {"model_a": "gemini-1.5-pro-001", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 84, "b_wins": 40, "draws": 106, "count": 230, "a_win_ratio": 0.68}, {"model_a": "jamba-1.5-large", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 5, "b_wins": 1, "draws": 6, "count": 12, "a_win_ratio": 0.83}, {"model_a": "llama-3.3-70b", "model_b": "qwq-32b", "a_wins": 17, "b_wins": 8, "draws": 15, "count": 40, "a_win_ratio": 0.68}, {"model_a": "mistral-large-2411", "model_b": "claude-3-5-sonnet-v2", "a_wins": 110, "b_wins": 97, "draws": 125, "count": 332, "a_win_ratio": 0.53}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "Yi-1.5-9B-Chat", "a_wins": 1, "b_wins": 0, "draws": 3, "count": 4, "a_win_ratio": 1.0}, {"model_a": "deepseek-v3-chat", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 227, "b_wins": 136, "draws": 194, "count": 557, "a_win_ratio": 0.63}, {"model_a": "llama-3.3-70b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 85, "b_wins": 73, "draws": 87, "count": 245, "a_win_ratio": 0.54}, {"model_a": "gemma-2-27b-it-q8", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 1, "b_wins": 2, "draws": 2, "count": 5, "a_win_ratio": 0.33}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 11, "b_wins": 17, "draws": 20, "count": 48, "a_win_ratio": 0.39}, {"model_a": "gemma-3-27b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 73, "b_wins": 24, "draws": 55, "count": 152, "a_win_ratio": 0.75}, {"model_a": "gemma-2-27b-it-q8", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 11, "b_wins": 5, "draws": 5, "count": 21, "a_win_ratio": 0.69}, {"model_a": "gpt-4.1-nano", "model_b": "llama-4-scout", "a_wins": 43, "b_wins": 46, "draws": 48, "count": 137, "a_win_ratio": 0.48}, {"model_a": "mistral-large-2411", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 56, "b_wins": 30, "draws": 57, "count": 143, "a_win_ratio": 0.65}, {"model_a": "llama-3.1-8b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 32, "b_wins": 46, "draws": 34, "count": 112, "a_win_ratio": 0.41}, {"model_a": "phi-4", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 29, "b_wins": 15, "draws": 29, "count": 73, "a_win_ratio": 0.66}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 44, "b_wins": 114, "draws": 91, "count": 249, "a_win_ratio": 0.28}, {"model_a": "command-a", "model_b": "qwq-32b", "a_wins": 16, "b_wins": 10, "draws": 12, "count": 38, "a_win_ratio": 0.62}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "o4-mini", "a_wins": 48, "b_wins": 55, "draws": 66, "count": 169, "a_win_ratio": 0.47}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 24, "b_wins": 14, "draws": 17, "count": 55, "a_win_ratio": 0.63}, {"model_a": "gpt-4.1-nano", "model_b": "deepseek-r1", "a_wins": 15, "b_wins": 18, "draws": 7, "count": 40, "a_win_ratio": 0.45}, {"model_a": "deepseek-v3-chat", "model_b": "gemma-3-12b", "a_wins": 12, "b_wins": 11, "draws": 9, "count": 32, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-8b", "model_b": "mistral-small-3.1-24b", "a_wins": 43, "b_wins": 48, "draws": 44, "count": 135, "a_win_ratio": 0.47}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "gemini-1.5-pro-001", "a_wins": 1, "b_wins": 4, "draws": 4, "count": 9, "a_win_ratio": 0.2}, {"model_a": "ministral-8b-instruct-2410", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 12, "b_wins": 13, "draws": 15, "count": 40, "a_win_ratio": 0.48}, {"model_a": "deepseek-v3-chat", "model_b": "jamba-1.5-large", "a_wins": 1, "b_wins": 3, "draws": 4, "count": 8, "a_win_ratio": 0.25}, {"model_a": "mistral-nemo-2407", "model_b": "llama-3.1-405b", "a_wins": 76, "b_wins": 173, "draws": 217, "count": 466, "a_win_ratio": 0.31}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "llama-3.3-70b", "a_wins": 12, "b_wins": 4, "draws": 13, "count": 29, "a_win_ratio": 0.75}, {"model_a": "mistral-nemo-2407", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 59, "b_wins": 75, "draws": 66, "count": 200, "a_win_ratio": 0.44}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 34, "b_wins": 15, "draws": 28, "count": 77, "a_win_ratio": 0.69}, {"model_a": "command-a", "model_b": "claude-3-5-sonnet-v2", "a_wins": 16, "b_wins": 6, "draws": 18, "count": 40, "a_win_ratio": 0.73}, {"model_a": "gemma-3-12b", "model_b": "gemma-2-9b-it", "a_wins": 12, "b_wins": 8, "draws": 9, "count": 29, "a_win_ratio": 0.6}, {"model_a": "lfm-40b", "model_b": "gpt-4o-2024-08-06", "a_wins": 17, "b_wins": 26, "draws": 23, "count": 66, "a_win_ratio": 0.4}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemma-3-4b", "a_wins": 4, "b_wins": 9, "draws": 6, "count": 19, "a_win_ratio": 0.31}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gpt-4.1-nano", "a_wins": 42, "b_wins": 37, "draws": 47, "count": 126, "a_win_ratio": 0.53}, {"model_a": "gemma-3-27b", "model_b": "llama-3.3-70b", "a_wins": 75, "b_wins": 29, "draws": 52, "count": 156, "a_win_ratio": 0.72}, {"model_a": "gemini-2.0-flash-exp", "model_b": "mistral-large-2411", "a_wins": 53, "b_wins": 37, "draws": 79, "count": 169, "a_win_ratio": 0.59}, {"model_a": "llama-4-scout", "model_b": "gemma-3-27b", "a_wins": 36, "b_wins": 43, "draws": 33, "count": 112, "a_win_ratio": 0.46}, {"model_a": "mistral-nemo-2407", "model_b": "phi-4", "a_wins": 55, "b_wins": 94, "draws": 64, "count": 213, "a_win_ratio": 0.37}, {"model_a": "phi-3.5-mini-instruct", "model_b": "aya-expanse-8b", "a_wins": 1, "b_wins": 0, "draws": 1, "count": 2, "a_win_ratio": 1.0}, {"model_a": "phi-4", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 82, "b_wins": 79, "draws": 85, "count": 246, "a_win_ratio": 0.51}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "command-a", "a_wins": 35, "b_wins": 66, "draws": 52, "count": 153, "a_win_ratio": 0.35}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "lfm-40b", "a_wins": 54, "b_wins": 30, "draws": 55, "count": 139, "a_win_ratio": 0.64}, {"model_a": "phi-4", "model_b": "c4ai-command-r-08-2024", "a_wins": 70, "b_wins": 60, "draws": 81, "count": 211, "a_win_ratio": 0.54}, {"model_a": "o4-mini", "model_b": "gemma-3-27b", "a_wins": 26, "b_wins": 12, "draws": 9, "count": 47, "a_win_ratio": 0.68}, {"model_a": "llama-3.1-70b", "model_b": "gemma-3-4b", "a_wins": 11, "b_wins": 14, "draws": 15, "count": 40, "a_win_ratio": 0.44}, {"model_a": "llama-3.1-70b", "model_b": "gemma-3-27b", "a_wins": 7, "b_wins": 15, "draws": 14, "count": 36, "a_win_ratio": 0.32}, {"model_a": "gpt-4.1-nano", "model_b": "mistral-small-3.1-24b", "a_wins": 66, "b_wins": 63, "draws": 71, "count": 200, "a_win_ratio": 0.51}, {"model_a": "aya-expanse-32b", "model_b": "mistral-saba", "a_wins": 28, "b_wins": 29, "draws": 17, "count": 74, "a_win_ratio": 0.49}, {"model_a": "gemma-3-27b", "model_b": "gpt-4.1-nano", "a_wins": 44, "b_wins": 29, "draws": 49, "count": 122, "a_win_ratio": 0.6}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemini-1.5-pro-002", "a_wins": 26, "b_wins": 28, "draws": 29, "count": 83, "a_win_ratio": 0.48}, {"model_a": "gemma-3-12b", "model_b": "gpt-4.1-mini", "a_wins": 64, "b_wins": 47, "draws": 51, "count": 162, "a_win_ratio": 0.58}, {"model_a": "c4ai-command-r-08-2024", "model_b": "ministral-8b-instruct-2410", "a_wins": 65, "b_wins": 75, "draws": 83, "count": 223, "a_win_ratio": 0.46}, {"model_a": "aya-expanse-8b", "model_b": "gemini-2.0-flash-exp", "a_wins": 13, "b_wins": 24, "draws": 36, "count": 73, "a_win_ratio": 0.35}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 48, "b_wins": 36, "draws": 50, "count": 134, "a_win_ratio": 0.57}, {"model_a": "command-a", "model_b": "deepseek-v3-0324", "a_wins": 40, "b_wins": 44, "draws": 20, "count": 104, "a_win_ratio": 0.48}, {"model_a": "qwen3-32b", "model_b": "deepseek-v3-0324", "a_wins": 3, "b_wins": 4, "draws": 0, "count": 7, "a_win_ratio": 0.43}, {"model_a": "gemma-3-4b", "model_b": "mistral-small-3.1-24b", "a_wins": 55, "b_wins": 43, "draws": 63, "count": 161, "a_win_ratio": 0.56}, {"model_a": "qwq-32b", "model_b": "llama-3.1-70b", "a_wins": 3, "b_wins": 9, "draws": 15, "count": 27, "a_win_ratio": 0.25}, {"model_a": "aya-expanse-8b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 21, "b_wins": 8, "draws": 15, "count": 44, "a_win_ratio": 0.72}, {"model_a": "gemma-2-9b-it", "model_b": "gemma-3-27b", "a_wins": 11, "b_wins": 18, "draws": 9, "count": 38, "a_win_ratio": 0.38}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mistral-small-3.1-24b", "a_wins": 49, "b_wins": 48, "draws": 52, "count": 149, "a_win_ratio": 0.51}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-4-scout", "a_wins": 42, "b_wins": 32, "draws": 27, "count": 101, "a_win_ratio": 0.57}, {"model_a": "ministral-8b-instruct-2410", "model_b": "deepseek-v3-chat", "a_wins": 37, "b_wins": 93, "draws": 66, "count": 196, "a_win_ratio": 0.28}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "llama-3.1-405b", "a_wins": 75, "b_wins": 83, "draws": 84, "count": 242, "a_win_ratio": 0.47}, {"model_a": "lfm-40b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 57, "b_wins": 49, "draws": 62, "count": 168, "a_win_ratio": 0.54}, {"model_a": "lfm-40b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 40, "b_wins": 60, "draws": 43, "count": 143, "a_win_ratio": 0.4}, {"model_a": "deepseek-v3-chat", "model_b": "gpt-4o-2024-08-06", "a_wins": 294, "b_wins": 150, "draws": 270, "count": 714, "a_win_ratio": 0.66}, {"model_a": "gemini-2.0-flash-exp", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 25, "b_wins": 10, "draws": 19, "count": 54, "a_win_ratio": 0.71}, {"model_a": "gemma-2-9b-it", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 28, "b_wins": 39, "draws": 44, "count": 111, "a_win_ratio": 0.42}, {"model_a": "mistral-large-2411", "model_b": "o4-mini", "a_wins": 45, "b_wins": 36, "draws": 54, "count": 135, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "lfm-40b", "a_wins": 6, "b_wins": 8, "draws": 19, "count": 33, "a_win_ratio": 0.43}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 47, "b_wins": 35, "draws": 61, "count": 143, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-405b", "model_b": "gemini-1.5-pro-001", "a_wins": 51, "b_wins": 71, "draws": 127, "count": 249, "a_win_ratio": 0.42}, {"model_a": "phi-3.5-mini-instruct", "model_b": "c4ai-command-r-08-2024", "a_wins": 1, "b_wins": 4, "draws": 6, "count": 11, "a_win_ratio": 0.2}, {"model_a": "gemma-3-4b", "model_b": "claude-3-7-sonnet", "a_wins": 55, "b_wins": 58, "draws": 72, "count": 185, "a_win_ratio": 0.49}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "o3-mini", "a_wins": 4, "b_wins": 17, "draws": 9, "count": 30, "a_win_ratio": 0.19}, {"model_a": "gemma-3-27b", "model_b": "o4-mini", "a_wins": 12, "b_wins": 26, "draws": 9, "count": 47, "a_win_ratio": 0.32}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 1, "b_wins": 0, "draws": 3, "count": 4, "a_win_ratio": 1.0}, {"model_a": "llama-3.1-70b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 64, "b_wins": 34, "draws": 46, "count": 144, "a_win_ratio": 0.65}, {"model_a": "deepseek-v3-0324", "model_b": "phi-4", "a_wins": 96, "b_wins": 77, "draws": 86, "count": 259, "a_win_ratio": 0.55}, {"model_a": "gemma-2-9b-it", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 21, "b_wins": 19, "draws": 24, "count": 64, "a_win_ratio": 0.52}, {"model_a": "mistral-large-2411", "model_b": "claude-3-7-sonnet", "a_wins": 45, "b_wins": 62, "draws": 57, "count": 164, "a_win_ratio": 0.42}, {"model_a": "qwen2.5-32b-instruct", "model_b": "llama-3.1-405b", "a_wins": 4, "b_wins": 3, "draws": 15, "count": 22, "a_win_ratio": 0.57}, {"model_a": "c4ai-command-r-08-2024", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 84, "b_wins": 98, "draws": 94, "count": 276, "a_win_ratio": 0.46}, {"model_a": "grok-3-mini-beta", "model_b": "qwq-32b", "a_wins": 16, "b_wins": 16, "draws": 10, "count": 42, "a_win_ratio": 0.5}, {"model_a": "phi-4", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 62, "b_wins": 72, "draws": 68, "count": 202, "a_win_ratio": 0.46}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemma-2-27b-it-q8", "a_wins": 20, "b_wins": 16, "draws": 26, "count": 62, "a_win_ratio": 0.56}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemma-3-4b", "a_wins": 4, "b_wins": 5, "draws": 11, "count": 20, "a_win_ratio": 0.44}, {"model_a": "gpt-4.1-mini", "model_b": "deepseek-v3-0324", "a_wins": 95, "b_wins": 123, "draws": 103, "count": 321, "a_win_ratio": 0.44}, {"model_a": "qwq-32b", "model_b": "deepseek-v3-0324", "a_wins": 3, "b_wins": 1, "draws": 1, "count": 5, "a_win_ratio": 0.75}, {"model_a": "phi-3.5-mini-instruct", "model_b": "llama-3.1-8b", "a_wins": 47, "b_wins": 53, "draws": 72, "count": 172, "a_win_ratio": 0.47}, {"model_a": "gemma-2-9b-it", "model_b": "gemma-2-9b-it", "a_wins": 14, "b_wins": 14, "draws": 0, "count": 28, "a_win_ratio": 0.5}, {"model_a": "mistral-saba", "model_b": "command-a", "a_wins": 30, "b_wins": 48, "draws": 47, "count": 125, "a_win_ratio": 0.38}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gemini-1.5-pro-001", "a_wins": 5, "b_wins": 13, "draws": 12, "count": 30, "a_win_ratio": 0.28}, {"model_a": "c4ai-command-r-08-2024", "model_b": "qwq-32b", "a_wins": 13, "b_wins": 5, "draws": 11, "count": 29, "a_win_ratio": 0.72}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "aya-expanse-32b", "a_wins": 23, "b_wins": 29, "draws": 37, "count": 89, "a_win_ratio": 0.44}, {"model_a": "gemma-3-4b", "model_b": "command-a", "a_wins": 45, "b_wins": 49, "draws": 31, "count": 125, "a_win_ratio": 0.48}, {"model_a": "llama-3.1-405b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 76, "b_wins": 85, "draws": 117, "count": 278, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemini-1.5-pro-002", "a_wins": 45, "b_wins": 43, "draws": 42, "count": 130, "a_win_ratio": 0.51}, {"model_a": "deepseek-r1", "model_b": "mistral-saba", "a_wins": 6, "b_wins": 3, "draws": 4, "count": 13, "a_win_ratio": 0.67}, {"model_a": "deepseek-v3-chat", "model_b": "gemini-1.5-pro-002", "a_wins": 96, "b_wins": 85, "draws": 88, "count": 269, "a_win_ratio": 0.53}, {"model_a": "qwen3-32b", "model_b": "mistral-small-3.1-24b", "a_wins": 2, "b_wins": 1, "draws": 0, "count": 3, "a_win_ratio": 0.67}, {"model_a": "mistral-nemo-2407", "model_b": "llama-3.1-8b", "a_wins": 84, "b_wins": 92, "draws": 128, "count": 304, "a_win_ratio": 0.48}, {"model_a": "deepseek-r1", "model_b": "deepseek-v3-chat", "a_wins": 16, "b_wins": 15, "draws": 13, "count": 44, "a_win_ratio": 0.52}, {"model_a": "mistral-nemo-2407", "model_b": "mistral-small-3.1-24b", "a_wins": 8, "b_wins": 12, "draws": 14, "count": 34, "a_win_ratio": 0.4}, {"model_a": "qwen2.5-32b-instruct", "model_b": "gemma-2-9b-it", "a_wins": 3, "b_wins": 4, "draws": 5, "count": 12, "a_win_ratio": 0.43}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 36, "b_wins": 48, "draws": 50, "count": 134, "a_win_ratio": 0.43}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "mistral-nemo-2407", "a_wins": 1, "b_wins": 1, "draws": 2, "count": 4, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-70b", "model_b": "gemini-1.5-pro-002", "a_wins": 48, "b_wins": 67, "draws": 72, "count": 187, "a_win_ratio": 0.42}, {"model_a": "llama-3.1-405b", "model_b": "grok-3-mini-beta", "a_wins": 1, "b_wins": 2, "draws": 0, "count": 3, "a_win_ratio": 0.33}, {"model_a": "gemma-2-9b-it", "model_b": "mistral-nemo-2407", "a_wins": 108, "b_wins": 66, "draws": 151, "count": 325, "a_win_ratio": 0.62}, {"model_a": "command-a", "model_b": "o3-mini", "a_wins": 6, "b_wins": 7, "draws": 11, "count": 24, "a_win_ratio": 0.46}, {"model_a": "gpt-4o-2024-08-06", "model_b": "aya-expanse-8b", "a_wins": 29, "b_wins": 33, "draws": 20, "count": 82, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gpt-4o-2024-08-06", "a_wins": 27, "b_wins": 21, "draws": 22, "count": 70, "a_win_ratio": 0.56}, {"model_a": "claude-3-7-sonnet", "model_b": "o4-mini", "a_wins": 62, "b_wins": 48, "draws": 50, "count": 160, "a_win_ratio": 0.56}, {"model_a": "phi-4", "model_b": "mistral-large-2411", "a_wins": 77, "b_wins": 118, "draws": 95, "count": 290, "a_win_ratio": 0.39}, {"model_a": "gpt-4o-2024-08-06", "model_b": "llama-3.1-70b", "a_wins": 51, "b_wins": 42, "draws": 66, "count": 159, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 102, "b_wins": 77, "draws": 102, "count": 281, "a_win_ratio": 0.57}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "llama-3.1-70b", "a_wins": 31, "b_wins": 32, "draws": 39, "count": 102, "a_win_ratio": 0.49}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "jamba-1.5-large", "a_wins": 1, "b_wins": 5, "draws": 6, "count": 12, "a_win_ratio": 0.17}, {"model_a": "qwen3-32b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 4, "b_wins": 2, "draws": 0, "count": 6, "a_win_ratio": 0.67}, {"model_a": "deepseek-v3-chat", "model_b": "llama-3.1-8b", "a_wins": 57, "b_wins": 28, "draws": 51, "count": 136, "a_win_ratio": 0.67}, {"model_a": "gpt-4.1-mini", "model_b": "llama-3.1-8b", "a_wins": 48, "b_wins": 28, "draws": 46, "count": 122, "a_win_ratio": 0.63}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "o4-mini", "a_wins": 1, "b_wins": 3, "draws": 6, "count": 10, "a_win_ratio": 0.25}, {"model_a": "gemma-3-27b", "model_b": "gpt-4.1-mini", "a_wins": 63, "b_wins": 51, "draws": 51, "count": 165, "a_win_ratio": 0.55}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "command-a", "a_wins": 40, "b_wins": 63, "draws": 57, "count": 160, "a_win_ratio": 0.39}, {"model_a": "llama-3.1-70b", "model_b": "o3-mini", "a_wins": 10, "b_wins": 10, "draws": 11, "count": 31, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-8b", "model_b": "claude-3-7-sonnet", "a_wins": 31, "b_wins": 41, "draws": 43, "count": 115, "a_win_ratio": 0.43}, {"model_a": "gemma-3-27b", "model_b": "gemini-1.5-pro-002", "a_wins": 21, "b_wins": 17, "draws": 15, "count": 53, "a_win_ratio": 0.55}, {"model_a": "o3-mini", "model_b": "qwq-32b", "a_wins": 47, "b_wins": 40, "draws": 51, "count": 138, "a_win_ratio": 0.54}, {"model_a": "gemma-3-4b", "model_b": "o4-mini", "a_wins": 10, "b_wins": 7, "draws": 8, "count": 25, "a_win_ratio": 0.59}, {"model_a": "gemma-3-4b", "model_b": "deepseek-v3-chat", "a_wins": 13, "b_wins": 8, "draws": 5, "count": 26, "a_win_ratio": 0.62}, {"model_a": "command-a", "model_b": "mistral-nemo-2407", "a_wins": 17, "b_wins": 3, "draws": 10, "count": 30, "a_win_ratio": 0.85}, {"model_a": "llama-3.3-70b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 4, "b_wins": 12, "draws": 13, "count": 29, "a_win_ratio": 0.25}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "llama-3.3-70b", "a_wins": 49, "b_wins": 36, "draws": 49, "count": 134, "a_win_ratio": 0.58}, {"model_a": "llama-3.1-405b", "model_b": "llama-3.1-405b", "a_wins": 12, "b_wins": 12, "draws": 2, "count": 26, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemma-3-27b", "a_wins": 24, "b_wins": 73, "draws": 55, "count": 152, "a_win_ratio": 0.25}, {"model_a": "o4-mini", "model_b": "llama-3.1-8b", "a_wins": 5, "b_wins": 0, "draws": 2, "count": 7, "a_win_ratio": 1.0}, {"model_a": "deepseek-r1", "model_b": "qwen3-32b", "a_wins": 30, "b_wins": 49, "draws": 21, "count": 100, "a_win_ratio": 0.38}, {"model_a": "gemma-3-27b", "model_b": "ministral-8b-instruct-2410", "a_wins": 94, "b_wins": 30, "draws": 68, "count": 192, "a_win_ratio": 0.76}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemini-2.0-flash-001", "a_wins": 59, "b_wins": 89, "draws": 70, "count": 218, "a_win_ratio": 0.4}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemma-2-9b-it", "a_wins": 19, "b_wins": 21, "draws": 24, "count": 64, "a_win_ratio": 0.48}, {"model_a": "gemini-1.5-pro-002", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 5, "draws": 2, "count": 9, "a_win_ratio": 0.29}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "deepseek-v3-chat", "a_wins": 29, "b_wins": 60, "draws": 47, "count": 136, "a_win_ratio": 0.33}, {"model_a": "gemini-1.5-pro-001", "model_b": "Yi-1.5-9B-Chat", "a_wins": 4, "b_wins": 1, "draws": 4, "count": 9, "a_win_ratio": 0.8}, {"model_a": "gpt-4.1-nano", "model_b": "c4ai-command-r-08-2024", "a_wins": 44, "b_wins": 35, "draws": 45, "count": 124, "a_win_ratio": 0.56}, {"model_a": "llama-4-scout", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 32, "b_wins": 42, "draws": 27, "count": 101, "a_win_ratio": 0.43}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 48, "b_wins": 40, "draws": 55, "count": 143, "a_win_ratio": 0.55}, {"model_a": "llama-4-scout", "model_b": "c4ai-command-r-08-2024", "a_wins": 58, "b_wins": 24, "draws": 48, "count": 130, "a_win_ratio": 0.71}, {"model_a": "o3-mini", "model_b": "mistral-small-3.1-24b", "a_wins": 22, "b_wins": 23, "draws": 18, "count": 63, "a_win_ratio": 0.49}, {"model_a": "phi-4", "model_b": "gemma-3-4b", "a_wins": 45, "b_wins": 60, "draws": 67, "count": 172, "a_win_ratio": 0.43}, {"model_a": "aya-expanse-8b", "model_b": "llama-3.1-70b", "a_wins": 12, "b_wins": 21, "draws": 20, "count": 53, "a_win_ratio": 0.36}, {"model_a": "deepseek-v3-0324", "model_b": "mistral-large-2411", "a_wins": 66, "b_wins": 69, "draws": 58, "count": 193, "a_win_ratio": 0.49}, {"model_a": "llama-3.3-70b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 41, "b_wins": 28, "draws": 43, "count": 112, "a_win_ratio": 0.59}, {"model_a": "llama-3.1-70b", "model_b": "Yi-1.5-9B-Chat", "a_wins": 0, "b_wins": 2, "draws": 1, "count": 3, "a_win_ratio": 0.0}, {"model_a": "llama-3.1-405b", "model_b": "ministral-8b-instruct-2410", "a_wins": 144, "b_wins": 131, "draws": 180, "count": 455, "a_win_ratio": 0.52}, {"model_a": "gemini-2.0-flash-001", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 19, "b_wins": 7, "draws": 16, "count": 42, "a_win_ratio": 0.73}, {"model_a": "phi-3.5-mini-instruct", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 8, "b_wins": 5, "draws": 11, "count": 24, "a_win_ratio": 0.62}, {"model_a": "mistral-saba", "model_b": "o4-mini", "a_wins": 15, "b_wins": 14, "draws": 14, "count": 43, "a_win_ratio": 0.52}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 48, "b_wins": 49, "draws": 54, "count": 151, "a_win_ratio": 0.49}, {"model_a": "mistral-saba", "model_b": "deepseek-v3-0324", "a_wins": 33, "b_wins": 47, "draws": 50, "count": 130, "a_win_ratio": 0.41}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "qwen2.5-7b-instruct", "a_wins": 5, "b_wins": 3, "draws": 8, "count": 16, "a_win_ratio": 0.62}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 27, "b_wins": 21, "draws": 29, "count": 77, "a_win_ratio": 0.56}, {"model_a": "grok-3-mini-beta", "model_b": "llama-4-scout", "a_wins": 2, "b_wins": 0, "draws": 0, "count": 2, "a_win_ratio": 1.0}, {"model_a": "mistral-nemo-2407", "model_b": "command-a", "a_wins": 3, "b_wins": 17, "draws": 10, "count": 30, "a_win_ratio": 0.15}, {"model_a": "mistral-nemo-2407", "model_b": "phi-3.5-mini-instruct", "a_wins": 65, "b_wins": 69, "draws": 111, "count": 245, "a_win_ratio": 0.49}, {"model_a": "jamba-1.5-large", "model_b": "llama-3.1-405b", "a_wins": 2, "b_wins": 4, "draws": 3, "count": 9, "a_win_ratio": 0.33}, {"model_a": "deepseek-v3-chat", "model_b": "gemini-2.0-flash-exp", "a_wins": 51, "b_wins": 48, "draws": 54, "count": 153, "a_win_ratio": 0.52}, {"model_a": "gemma-3-4b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 23, "b_wins": 16, "draws": 23, "count": 62, "a_win_ratio": 0.59}, {"model_a": "gemma-2-27b-it-q8", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 1, "b_wins": 1, "draws": 5, "count": 7, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-2024-08-06", "model_b": "deepseek-v3-chat", "a_wins": 150, "b_wins": 294, "draws": 270, "count": 714, "a_win_ratio": 0.34}, {"model_a": "c4ai-command-r-08-2024", "model_b": "c4ai-command-r-08-2024", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "phi-3.5-mini-instruct", "model_b": "qwen2.5-32b-instruct", "a_wins": 4, "b_wins": 6, "draws": 8, "count": 18, "a_win_ratio": 0.4}, {"model_a": "c4ai-command-r-08-2024", "model_b": "aya-expanse-8b", "a_wins": 7, "b_wins": 6, "draws": 12, "count": 25, "a_win_ratio": 0.54}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemma-3-27b", "a_wins": 24, "b_wins": 68, "draws": 61, "count": 153, "a_win_ratio": 0.26}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemma-3-12b", "a_wins": 61, "b_wins": 37, "draws": 47, "count": 145, "a_win_ratio": 0.62}, {"model_a": "gemma-2-9b-it", "model_b": "gemini-1.5-pro-002", "a_wins": 38, "b_wins": 57, "draws": 56, "count": 151, "a_win_ratio": 0.4}, {"model_a": "aya-expanse-32b", "model_b": "mistral-small-3.1-24b", "a_wins": 27, "b_wins": 22, "draws": 17, "count": 66, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-8b", "model_b": "aya-expanse-8b", "a_wins": 12, "b_wins": 16, "draws": 14, "count": 42, "a_win_ratio": 0.43}, {"model_a": "qwq-32b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 6, "b_wins": 10, "draws": 10, "count": 26, "a_win_ratio": 0.38}, {"model_a": "o3-mini", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 14, "b_wins": 9, "draws": 10, "count": 33, "a_win_ratio": 0.61}, {"model_a": "gemini-1.5-pro-002", "model_b": "deepseek-v3-chat", "a_wins": 85, "b_wins": 96, "draws": 88, "count": 269, "a_win_ratio": 0.47}, {"model_a": "gemini-1.5-pro-001", "model_b": "c4ai-command-r-08-2024", "a_wins": 2, "b_wins": 0, "draws": 5, "count": 7, "a_win_ratio": 1.0}, {"model_a": "jamba-1.5-large", "model_b": "llama-3.1-70b", "a_wins": 6, "b_wins": 1, "draws": 1, "count": 8, "a_win_ratio": 0.86}, {"model_a": "lfm-40b", "model_b": "ministral-8b-instruct-2410", "a_wins": 30, "b_wins": 54, "draws": 51, "count": 135, "a_win_ratio": 0.36}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemma-3-4b", "a_wins": 43, "b_wins": 55, "draws": 63, "count": 161, "a_win_ratio": 0.44}, {"model_a": "mistral-large-2411", "model_b": "aya-expanse-8b", "a_wins": 28, "b_wins": 32, "draws": 25, "count": 85, "a_win_ratio": 0.47}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 41, "b_wins": 37, "draws": 41, "count": 119, "a_win_ratio": 0.53}, {"model_a": "mistral-small-3.1-24b", "model_b": "gpt-4.1-mini", "a_wins": 86, "b_wins": 112, "draws": 99, "count": 297, "a_win_ratio": 0.43}, {"model_a": "qwen3-32b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 45, "b_wins": 36, "draws": 34, "count": 115, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "deepseek-v3-0324", "a_wins": 2, "b_wins": 1, "draws": 5, "count": 8, "a_win_ratio": 0.67}, {"model_a": "deepseek-v3-0324", "model_b": "aya-expanse-32b", "a_wins": 98, "b_wins": 52, "draws": 69, "count": 219, "a_win_ratio": 0.65}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 1, "b_wins": 2, "draws": 2, "count": 5, "a_win_ratio": 0.33}, {"model_a": "gemma-2-9b-it", "model_b": "gemini-1.5-pro-001", "a_wins": 40, "b_wins": 48, "draws": 87, "count": 175, "a_win_ratio": 0.45}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 25, "b_wins": 40, "draws": 40, "count": 105, "a_win_ratio": 0.38}, {"model_a": "lfm-40b", "model_b": "jamba-1.5-large", "a_wins": 1, "b_wins": 0, "draws": 6, "count": 7, "a_win_ratio": 1.0}, {"model_a": "gemma-2-9b-it", "model_b": "claude-3-5-sonnet-v2", "a_wins": 39, "b_wins": 55, "draws": 57, "count": 151, "a_win_ratio": 0.41}, {"model_a": "deepseek-v3-chat", "model_b": "llama-3.1-405b", "a_wins": 66, "b_wins": 50, "draws": 81, "count": 197, "a_win_ratio": 0.57}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemini-2.0-flash-001", "a_wins": 62, "b_wins": 103, "draws": 88, "count": 253, "a_win_ratio": 0.38}, {"model_a": "deepseek-r1", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 16, "b_wins": 8, "draws": 18, "count": 42, "a_win_ratio": 0.67}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "lfm-40b", "a_wins": 45, "b_wins": 31, "draws": 47, "count": 123, "a_win_ratio": 0.59}, {"model_a": "qwen2.5-32b-instruct", "model_b": "phi-3.5-mini-instruct", "a_wins": 6, "b_wins": 4, "draws": 8, "count": 18, "a_win_ratio": 0.6}, {"model_a": "phi-4", "model_b": "mistral-small-3.1-24b", "a_wins": 44, "b_wins": 57, "draws": 50, "count": 151, "a_win_ratio": 0.44}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "deepseek-r1", "a_wins": 8, "b_wins": 16, "draws": 18, "count": 42, "a_win_ratio": 0.33}, {"model_a": "phi-3.5-mini-instruct", "model_b": "qwen2-7b-instruct", "a_wins": 3, "b_wins": 0, "draws": 2, "count": 5, "a_win_ratio": 1.0}, {"model_a": "llama-4-scout", "model_b": "grok-3-mini-beta", "a_wins": 0, "b_wins": 2, "draws": 0, "count": 2, "a_win_ratio": 0.0}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "phi-4", "a_wins": 126, "b_wins": 109, "draws": 129, "count": 364, "a_win_ratio": 0.54}, {"model_a": "mistral-nemo-2407", "model_b": "gemini-1.5-pro-001", "a_wins": 36, "b_wins": 112, "draws": 118, "count": 266, "a_win_ratio": 0.24}, {"model_a": "claude-3-7-sonnet", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 10, "b_wins": 5, "draws": 10, "count": 25, "a_win_ratio": 0.67}, {"model_a": "gemma-2-9b-it", "model_b": "llama-3.1-8b", "a_wins": 86, "b_wins": 63, "draws": 93, "count": 242, "a_win_ratio": 0.58}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 30, "b_wins": 49, "draws": 40, "count": 119, "a_win_ratio": 0.38}, {"model_a": "deepseek-v3-chat", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 44, "b_wins": 28, "draws": 42, "count": 114, "a_win_ratio": 0.61}, {"model_a": "o4-mini", "model_b": "mistral-saba", "a_wins": 14, "b_wins": 15, "draws": 14, "count": 43, "a_win_ratio": 0.48}, {"model_a": "deepseek-r1", "model_b": "claude-3-7-sonnet", "a_wins": 27, "b_wins": 28, "draws": 38, "count": 93, "a_win_ratio": 0.49}, {"model_a": "qwq-32b", "model_b": "claude-3-7-sonnet", "a_wins": 2, "b_wins": 1, "draws": 2, "count": 5, "a_win_ratio": 0.67}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mistral-large-2411", "a_wins": 60, "b_wins": 85, "draws": 75, "count": 220, "a_win_ratio": 0.41}, {"model_a": "qwq-32b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 4, "b_wins": 4, "draws": 8, "count": 16, "a_win_ratio": 0.5}, {"model_a": "o3-mini", "model_b": "phi-4", "a_wins": 17, "b_wins": 9, "draws": 12, "count": 38, "a_win_ratio": 0.65}, {"model_a": "command-a", "model_b": "mistral-small-3.1-24b", "a_wins": 67, "b_wins": 43, "draws": 50, "count": 160, "a_win_ratio": 0.61}, {"model_a": "o4-mini", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 55, "b_wins": 48, "draws": 66, "count": 169, "a_win_ratio": 0.53}, {"model_a": "gemma-3-27b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 59, "b_wins": 37, "draws": 40, "count": 136, "a_win_ratio": 0.61}, {"model_a": "gemini-1.5-pro-001", "model_b": "mistral-nemo-2407", "a_wins": 112, "b_wins": 36, "draws": 118, "count": 266, "a_win_ratio": 0.76}, {"model_a": "qwen3-32b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 3, "b_wins": 2, "draws": 0, "count": 5, "a_win_ratio": 0.6}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "mistral-nemo-2407", "a_wins": 58, "b_wins": 36, "draws": 48, "count": 142, "a_win_ratio": 0.62}, {"model_a": "mistral-saba", "model_b": "gpt-4.1-mini", "a_wins": 57, "b_wins": 67, "draws": 59, "count": 183, "a_win_ratio": 0.46}, {"model_a": "gemini-1.5-pro-001", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 0, "b_wins": 1, "draws": 3, "count": 4, "a_win_ratio": 0.0}, {"model_a": "mistral-nemo-2407", "model_b": "c4ai-command-r-08-2024", "a_wins": 31, "b_wins": 43, "draws": 39, "count": 113, "a_win_ratio": 0.42}, {"model_a": "llama-3.3-70b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 80, "b_wins": 120, "draws": 106, "count": 306, "a_win_ratio": 0.4}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "aya-expanse-8b", "a_wins": 27, "b_wins": 24, "draws": 26, "count": 77, "a_win_ratio": 0.53}, {"model_a": "gemma-3-12b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 6, "b_wins": 1, "draws": 14, "count": 21, "a_win_ratio": 0.86}, {"model_a": "c4ai-command-r-08-2024", "model_b": "phi-3.5-mini-instruct", "a_wins": 4, "b_wins": 1, "draws": 6, "count": 11, "a_win_ratio": 0.8}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "llama-3.1-405b", "a_wins": 10, "b_wins": 33, "draws": 40, "count": 83, "a_win_ratio": 0.23}, {"model_a": "llama-3.1-405b", "model_b": "gemma-3-4b", "a_wins": 25, "b_wins": 50, "draws": 52, "count": 127, "a_win_ratio": 0.33}, {"model_a": "gemma-3-27b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 83, "b_wins": 38, "draws": 65, "count": 186, "a_win_ratio": 0.69}, {"model_a": "mistral-large-2411", "model_b": "mistral-large-2411", "a_wins": 14, "b_wins": 14, "draws": 4, "count": 32, "a_win_ratio": 0.5}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 29, "b_wins": 14, "draws": 27, "count": 70, "a_win_ratio": 0.67}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 107, "b_wins": 118, "draws": 102, "count": 327, "a_win_ratio": 0.48}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 72, "b_wins": 54, "draws": 65, "count": 191, "a_win_ratio": 0.57}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemma-2-9b-it", "a_wins": 55, "b_wins": 39, "draws": 57, "count": 151, "a_win_ratio": 0.59}, {"model_a": "deepseek-r1", "model_b": "phi-4", "a_wins": 12, "b_wins": 15, "draws": 15, "count": 42, "a_win_ratio": 0.44}, {"model_a": "mistral-nemo-2407", "model_b": "gemma-3-12b", "a_wins": 5, "b_wins": 17, "draws": 12, "count": 34, "a_win_ratio": 0.23}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemini-1.5-pro-002", "a_wins": 12, "b_wins": 28, "draws": 22, "count": 62, "a_win_ratio": 0.3}, {"model_a": "gemini-2.0-flash-001", "model_b": "qwq-32b", "a_wins": 8, "b_wins": 6, "draws": 13, "count": 27, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "aya-expanse-8b", "a_wins": 12, "b_wins": 11, "draws": 10, "count": 33, "a_win_ratio": 0.52}, {"model_a": "gpt-4.1-mini", "model_b": "aya-expanse-32b", "a_wins": 21, "b_wins": 26, "draws": 41, "count": 88, "a_win_ratio": 0.45}, {"model_a": "gemma-3-27b", "model_b": "aya-expanse-32b", "a_wins": 31, "b_wins": 25, "draws": 24, "count": 80, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-405b", "model_b": "c4ai-command-r-08-2024", "a_wins": 88, "b_wins": 67, "draws": 70, "count": 225, "a_win_ratio": 0.57}, {"model_a": "o3-mini", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 24, "b_wins": 29, "draws": 31, "count": 84, "a_win_ratio": 0.45}, {"model_a": "o3-mini", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 8, "b_wins": 5, "draws": 13, "count": 26, "a_win_ratio": 0.62}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemma-3-27b", "a_wins": 8, "b_wins": 23, "draws": 21, "count": 52, "a_win_ratio": 0.26}, {"model_a": "o4-mini", "model_b": "grok-3-mini-beta", "a_wins": 81, "b_wins": 104, "draws": 96, "count": 281, "a_win_ratio": 0.44}, {"model_a": "c4ai-command-r-08-2024", "model_b": "Yi-1.5-9B-Chat", "a_wins": 1, "b_wins": 0, "draws": 2, "count": 3, "a_win_ratio": 1.0}, {"model_a": "llama-3.3-70b", "model_b": "aya-expanse-32b", "a_wins": 55, "b_wins": 68, "draws": 62, "count": 185, "a_win_ratio": 0.45}, {"model_a": "llama-4-scout", "model_b": "aya-expanse-32b", "a_wins": 30, "b_wins": 23, "draws": 20, "count": 73, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-405b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 65, "b_wins": 103, "draws": 78, "count": 246, "a_win_ratio": 0.39}, {"model_a": "mistral-saba", "model_b": "qwen3-32b", "a_wins": 2, "b_wins": 4, "draws": 1, "count": 7, "a_win_ratio": 0.33}, {"model_a": "qwen2.5-7b-instruct", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 33, "b_wins": 29, "draws": 53, "count": 115, "a_win_ratio": 0.53}, {"model_a": "gpt-4.1-mini", "model_b": "gemma-3-12b", "a_wins": 47, "b_wins": 64, "draws": 51, "count": 162, "a_win_ratio": 0.42}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemma-3-4b", "a_wins": 16, "b_wins": 23, "draws": 23, "count": 62, "a_win_ratio": 0.41}, {"model_a": "qwq-32b", "model_b": "gemma-3-4b", "a_wins": 8, "b_wins": 11, "draws": 6, "count": 25, "a_win_ratio": 0.42}, {"model_a": "qwen3-32b", "model_b": "phi-4", "a_wins": 0, "b_wins": 1, "draws": 2, "count": 3, "a_win_ratio": 0.0}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gemma-2-27b-it-q8", "a_wins": 8, "b_wins": 15, "draws": 16, "count": 39, "a_win_ratio": 0.35}, {"model_a": "gemini-1.5-pro-001", "model_b": "gemma-2-9b-it", "a_wins": 48, "b_wins": 40, "draws": 87, "count": 175, "a_win_ratio": 0.55}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gemma-2-27b-it-q8", "a_wins": 1, "b_wins": 2, "draws": 3, "count": 6, "a_win_ratio": 0.33}, {"model_a": "o4-mini", "model_b": "deepseek-v3-0324", "a_wins": 45, "b_wins": 61, "draws": 51, "count": 157, "a_win_ratio": 0.42}, {"model_a": "mistral-nemo-2407", "model_b": "deepseek-v3-chat", "a_wins": 20, "b_wins": 60, "draws": 54, "count": 134, "a_win_ratio": 0.25}, {"model_a": "gpt-4o-2024-08-06", "model_b": "llama-3.1-405b", "a_wins": 62, "b_wins": 60, "draws": 84, "count": 206, "a_win_ratio": 0.51}, {"model_a": "gpt-4.1-mini", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 19, "b_wins": 10, "draws": 8, "count": 37, "a_win_ratio": 0.66}, {"model_a": "claude-3-7-sonnet", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 32, "b_wins": 28, "draws": 35, "count": 95, "a_win_ratio": 0.53}, {"model_a": "llama-3.3-70b", "model_b": "gpt-4o-2024-08-06", "a_wins": 27, "b_wins": 19, "draws": 25, "count": 71, "a_win_ratio": 0.59}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "llama-3.1-8b", "a_wins": 56, "b_wins": 73, "draws": 88, "count": 217, "a_win_ratio": 0.43}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gpt-4o-2024-08-06", "a_wins": 23, "b_wins": 29, "draws": 17, "count": 69, "a_win_ratio": 0.44}, {"model_a": "phi-4", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 52, "b_wins": 46, "draws": 33, "count": 131, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-405b", "model_b": "o4-mini", "a_wins": 7, "b_wins": 8, "draws": 10, "count": 25, "a_win_ratio": 0.47}, {"model_a": "o3-mini", "model_b": "mistral-nemo-2407", "a_wins": 24, "b_wins": 8, "draws": 9, "count": 41, "a_win_ratio": 0.75}, {"model_a": "llama-3.1-70b", "model_b": "mistral-large-2411", "a_wins": 50, "b_wins": 55, "draws": 40, "count": 145, "a_win_ratio": 0.48}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemma-3-27b", "a_wins": 17, "b_wins": 21, "draws": 15, "count": 53, "a_win_ratio": 0.45}, {"model_a": "gemma-2-9b-it", "model_b": "mistral-small-3.1-24b", "a_wins": 7, "b_wins": 15, "draws": 19, "count": 41, "a_win_ratio": 0.32}, {"model_a": "deepseek-v3-chat", "model_b": "gemma-3-4b", "a_wins": 8, "b_wins": 13, "draws": 5, "count": 26, "a_win_ratio": 0.38}, {"model_a": "jamba-1.5-large", "model_b": "llama-3.1-8b", "a_wins": 4, "b_wins": 3, "draws": 2, "count": 9, "a_win_ratio": 0.57}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gpt-4o-2024-08-06", "a_wins": 88, "b_wins": 149, "draws": 155, "count": 392, "a_win_ratio": 0.37}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "qwq-32b", "a_wins": 8, "b_wins": 13, "draws": 13, "count": 34, "a_win_ratio": 0.38}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemini-2.0-flash-exp", "a_wins": 24, "b_wins": 54, "draws": 45, "count": 123, "a_win_ratio": 0.31}, {"model_a": "deepseek-v3-0324", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 55, "b_wins": 23, "draws": 45, "count": 123, "a_win_ratio": 0.71}, {"model_a": "mistral-small-3.1-24b", "model_b": "o3-mini", "a_wins": 23, "b_wins": 22, "draws": 18, "count": 63, "a_win_ratio": 0.51}, {"model_a": "llama-3.1-70b", "model_b": "qwen2.5-32b-instruct", "a_wins": 5, "b_wins": 2, "draws": 5, "count": 12, "a_win_ratio": 0.71}, {"model_a": "phi-3.5-mini-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 57, "b_wins": 20, "draws": 69, "count": 146, "a_win_ratio": 0.74}, {"model_a": "llama-4-scout", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 46, "b_wins": 33, "draws": 33, "count": 112, "a_win_ratio": 0.58}, {"model_a": "gemma-2-9b-it", "model_b": "gemini-2.0-flash-exp", "a_wins": 24, "b_wins": 46, "draws": 62, "count": 132, "a_win_ratio": 0.34}, {"model_a": "deepseek-r1", "model_b": "llama-3.1-405b", "a_wins": 26, "b_wins": 25, "draws": 24, "count": 75, "a_win_ratio": 0.51}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "qwq-32b", "a_wins": 68, "b_wins": 69, "draws": 62, "count": 199, "a_win_ratio": 0.5}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemini-2.0-flash-001", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "gemini-2.0-flash-001", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-8b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 28, "b_wins": 14, "draws": 30, "count": 72, "a_win_ratio": 0.67}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mistral-small-3.1-24b", "a_wins": 46, "b_wins": 57, "draws": 48, "count": 151, "a_win_ratio": 0.45}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "ministral-8b-instruct-2410", "a_wins": 39, "b_wins": 50, "draws": 33, "count": 122, "a_win_ratio": 0.44}, {"model_a": "claude-3-7-sonnet", "model_b": "gemini-2.0-flash-001", "a_wins": 50, "b_wins": 65, "draws": 68, "count": 183, "a_win_ratio": 0.43}, {"model_a": "mistral-saba", "model_b": "qwq-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "phi-4", "model_b": "mistral-nemo-2407", "a_wins": 94, "b_wins": 55, "draws": 64, "count": 213, "a_win_ratio": 0.63}, {"model_a": "aya-expanse-32b", "model_b": "gpt-4.1-mini", "a_wins": 26, "b_wins": 21, "draws": 41, "count": 88, "a_win_ratio": 0.55}, {"model_a": "deepseek-v3-chat", "model_b": "deepseek-v3-chat", "a_wins": 52, "b_wins": 52, "draws": 8, "count": 112, "a_win_ratio": 0.5}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "o3-mini", "a_wins": 9, "b_wins": 14, "draws": 10, "count": 33, "a_win_ratio": 0.39}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 23, "b_wins": 41, "draws": 46, "count": 110, "a_win_ratio": 0.36}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 36, "b_wins": 17, "draws": 32, "count": 85, "a_win_ratio": 0.68}, {"model_a": "c4ai-command-r-08-2024", "model_b": "phi-4", "a_wins": 60, "b_wins": 70, "draws": 81, "count": 211, "a_win_ratio": 0.46}, {"model_a": "gemma-2-9b-it", "model_b": "llama-3.1-405b", "a_wins": 71, "b_wins": 123, "draws": 138, "count": 332, "a_win_ratio": 0.37}, {"model_a": "gemma-3-27b", "model_b": "mistral-saba", "a_wins": 37, "b_wins": 25, "draws": 34, "count": 96, "a_win_ratio": 0.6}, {"model_a": "claude-3-7-sonnet", "model_b": "gpt-4.1-nano", "a_wins": 70, "b_wins": 56, "draws": 44, "count": 170, "a_win_ratio": 0.56}, {"model_a": "gemma-2-9b-it", "model_b": "aya-expanse-8b", "a_wins": 21, "b_wins": 13, "draws": 16, "count": 50, "a_win_ratio": 0.62}, {"model_a": "aya-expanse-32b", "model_b": "gemma-3-27b", "a_wins": 25, "b_wins": 31, "draws": 24, "count": 80, "a_win_ratio": 0.45}, {"model_a": "deepseek-r1", "model_b": "gemma-3-27b", "a_wins": 18, "b_wins": 24, "draws": 20, "count": 62, "a_win_ratio": 0.43}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemma-3-27b", "a_wins": 6, "b_wins": 22, "draws": 17, "count": 45, "a_win_ratio": 0.21}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 19, "b_wins": 36, "draws": 43, "count": 98, "a_win_ratio": 0.35}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "llama-3.3-70b", "a_wins": 8, "b_wins": 21, "draws": 31, "count": 60, "a_win_ratio": 0.28}, {"model_a": "gemma-3-12b", "model_b": "command-a", "a_wins": 42, "b_wins": 49, "draws": 51, "count": 142, "a_win_ratio": 0.46}, {"model_a": "qwen2.5-7b-instruct", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 7, "b_wins": 1, "draws": 7, "count": 15, "a_win_ratio": 0.88}, {"model_a": "deepseek-r1", "model_b": "o3-mini", "a_wins": 67, "b_wins": 46, "draws": 64, "count": 177, "a_win_ratio": 0.59}, {"model_a": "phi-4", "model_b": "deepseek-r1", "a_wins": 15, "b_wins": 12, "draws": 15, "count": 42, "a_win_ratio": 0.56}, {"model_a": "gemini-2.0-flash-exp", "model_b": "aya-expanse-8b", "a_wins": 24, "b_wins": 13, "draws": 36, "count": 73, "a_win_ratio": 0.65}, {"model_a": "llama-3.1-8b", "model_b": "deepseek-v3-chat", "a_wins": 28, "b_wins": 57, "draws": 51, "count": 136, "a_win_ratio": 0.33}, {"model_a": "qwq-32b", "model_b": "mistral-nemo-2407", "a_wins": 9, "b_wins": 13, "draws": 6, "count": 28, "a_win_ratio": 0.41}, {"model_a": "deepseek-v3-chat", "model_b": "mistral-small-3.1-24b", "a_wins": 13, "b_wins": 12, "draws": 15, "count": 40, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-3.1-8b", "a_wins": 84, "b_wins": 44, "draws": 75, "count": 203, "a_win_ratio": 0.66}, {"model_a": "mistral-nemo-2407", "model_b": "mistral-nemo-2407", "a_wins": 8, "b_wins": 8, "draws": 0, "count": 16, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "gemini-2.0-flash-exp", "a_wins": 17, "b_wins": 26, "draws": 30, "count": 73, "a_win_ratio": 0.4}, {"model_a": "c4ai-command-r-08-2024", "model_b": "qwen2-7b-instruct", "a_wins": 2, "b_wins": 3, "draws": 7, "count": 12, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-70b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 130, "b_wins": 56, "draws": 133, "count": 319, "a_win_ratio": 0.7}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 9, "b_wins": 9, "draws": 0, "count": 18, "a_win_ratio": 0.5}, {"model_a": "gemma-3-4b", "model_b": "ministral-8b-instruct-2410", "a_wins": 67, "b_wins": 33, "draws": 45, "count": 145, "a_win_ratio": 0.67}, {"model_a": "jamba-1.5-large", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 3, "b_wins": 3, "draws": 5, "count": 11, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 3, "draws": 5, "count": 11, "a_win_ratio": 0.5}, {"model_a": "mistral-saba", "model_b": "gemini-2.0-flash-001", "a_wins": 23, "b_wins": 54, "draws": 40, "count": 117, "a_win_ratio": 0.3}, {"model_a": "gemini-1.5-pro-001", "model_b": "qwen2.5-32b-instruct", "a_wins": 7, "b_wins": 3, "draws": 7, "count": 17, "a_win_ratio": 0.7}, {"model_a": "gemini-1.5-pro-002", "model_b": "deepseek-r1", "a_wins": 18, "b_wins": 16, "draws": 15, "count": 49, "a_win_ratio": 0.53}, {"model_a": "mistral-large-2411", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 15, "b_wins": 11, "draws": 20, "count": 46, "a_win_ratio": 0.58}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "llama-3.1-70b", "a_wins": 56, "b_wins": 130, "draws": 133, "count": 319, "a_win_ratio": 0.3}, {"model_a": "qwen3-32b", "model_b": "gemma-3-12b", "a_wins": 1, "b_wins": 0, "draws": 0, "count": 1, "a_win_ratio": 1.0}, {"model_a": "o3-mini", "model_b": "deepseek-v3-chat", "a_wins": 14, "b_wins": 19, "draws": 19, "count": 52, "a_win_ratio": 0.42}, {"model_a": "gemma-3-12b", "model_b": "c4ai-command-r-08-2024", "a_wins": 68, "b_wins": 29, "draws": 64, "count": 161, "a_win_ratio": 0.7}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gpt-4.1-nano", "a_wins": 35, "b_wins": 44, "draws": 45, "count": 124, "a_win_ratio": 0.44}, {"model_a": "ministral-8b-instruct-2410", "model_b": "aya-expanse-32b", "a_wins": 11, "b_wins": 21, "draws": 14, "count": 46, "a_win_ratio": 0.34}, {"model_a": "gemini-2.0-flash-exp", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 1, "draws": 1, "count": 5, "a_win_ratio": 0.75}, {"model_a": "mistral-saba", "model_b": "ministral-8b-instruct-2410", "a_wins": 45, "b_wins": 46, "draws": 33, "count": 124, "a_win_ratio": 0.49}, {"model_a": "aya-expanse-8b", "model_b": "c4ai-command-r-08-2024", "a_wins": 6, "b_wins": 7, "draws": 12, "count": 25, "a_win_ratio": 0.46}, {"model_a": "mistral-large-2411", "model_b": "lfm-40b", "a_wins": 51, "b_wins": 39, "draws": 52, "count": 142, "a_win_ratio": 0.57}, {"model_a": "mistral-large-2411", "model_b": "aya-expanse-32b", "a_wins": 24, "b_wins": 29, "draws": 20, "count": 73, "a_win_ratio": 0.45}, {"model_a": "deepseek-v3-chat", "model_b": "command-a", "a_wins": 9, "b_wins": 17, "draws": 15, "count": 41, "a_win_ratio": 0.35}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 3, "draws": 1, "count": 6, "a_win_ratio": 0.4}, {"model_a": "gemma-3-4b", "model_b": "llama-4-scout", "a_wins": 49, "b_wins": 37, "draws": 41, "count": 127, "a_win_ratio": 0.57}, {"model_a": "gpt-4.1-nano", "model_b": "mistral-large-2411", "a_wins": 58, "b_wins": 69, "draws": 53, "count": 180, "a_win_ratio": 0.46}, {"model_a": "deepseek-r1", "model_b": "deepseek-r1", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-32b", "model_b": "c4ai-command-r-08-2024", "a_wins": 33, "b_wins": 19, "draws": 25, "count": 77, "a_win_ratio": 0.63}, {"model_a": "llama-3.3-70b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 43, "b_wins": 37, "draws": 49, "count": 129, "a_win_ratio": 0.54}, {"model_a": "gemma-2-27b-it-q8", "model_b": "lfm-40b", "a_wins": 18, "b_wins": 6, "draws": 13, "count": 37, "a_win_ratio": 0.75}, {"model_a": "o3-mini", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 13, "b_wins": 9, "draws": 10, "count": 32, "a_win_ratio": 0.59}, {"model_a": "llama-3.1-405b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 135, "b_wins": 55, "draws": 158, "count": 348, "a_win_ratio": 0.71}, {"model_a": "jamba-1.5-large", "model_b": "lfm-40b", "a_wins": 0, "b_wins": 1, "draws": 6, "count": 7, "a_win_ratio": 0.0}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "deepseek-r1", "a_wins": 15, "b_wins": 15, "draws": 12, "count": 42, "a_win_ratio": 0.5}, {"model_a": "qwen3-32b", "model_b": "llama-3.1-8b", "a_wins": 0, "b_wins": 1, "draws": 0, "count": 1, "a_win_ratio": 0.0}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 41, "b_wins": 23, "draws": 46, "count": 110, "a_win_ratio": 0.64}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemma-2-27b-it-q8", "a_wins": 5, "b_wins": 11, "draws": 5, "count": 21, "a_win_ratio": 0.31}, {"model_a": "claude-3-7-sonnet", "model_b": "phi-4", "a_wins": 34, "b_wins": 53, "draws": 47, "count": 134, "a_win_ratio": 0.39}, {"model_a": "mistral-saba", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 4, "b_wins": 2, "draws": 1, "count": 7, "a_win_ratio": 0.67}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 10, "b_wins": 23, "draws": 21, "count": 54, "a_win_ratio": 0.3}, {"model_a": "phi-3.5-mini-instruct", "model_b": "phi-4", "a_wins": 3, "b_wins": 2, "draws": 1, "count": 6, "a_win_ratio": 0.6}, {"model_a": "aya-expanse-32b", "model_b": "mistral-large-2411", "a_wins": 29, "b_wins": 24, "draws": 20, "count": 73, "a_win_ratio": 0.55}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-3.1-70b", "a_wins": 83, "b_wins": 70, "draws": 119, "count": 272, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemini-2.0-flash-exp", "a_wins": 32, "b_wins": 32, "draws": 23, "count": 87, "a_win_ratio": 0.5}, {"model_a": "deepseek-v3-chat", "model_b": "llama-3.3-70b", "a_wins": 51, "b_wins": 40, "draws": 45, "count": 136, "a_win_ratio": 0.56}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemma-3-27b", "a_wins": 53, "b_wins": 58, "draws": 57, "count": 168, "a_win_ratio": 0.48}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "command-a", "a_wins": 9, "b_wins": 18, "draws": 17, "count": 44, "a_win_ratio": 0.33}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "claude-3-5-sonnet-v2", "a_wins": 66, "b_wins": 64, "draws": 70, "count": 200, "a_win_ratio": 0.51}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-3.3-70b", "a_wins": 67, "b_wins": 83, "draws": 74, "count": 224, "a_win_ratio": 0.45}, {"model_a": "mistral-nemo-2407", "model_b": "qwq-32b", "a_wins": 13, "b_wins": 9, "draws": 6, "count": 28, "a_win_ratio": 0.59}, {"model_a": "deepseek-v3-0324", "model_b": "llama-3.3-70b", "a_wins": 80, "b_wins": 37, "draws": 74, "count": 191, "a_win_ratio": 0.68}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "phi-3.5-mini-instruct", "a_wins": 27, "b_wins": 12, "draws": 22, "count": 61, "a_win_ratio": 0.69}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 41, "b_wins": 107, "draws": 117, "count": 265, "a_win_ratio": 0.28}, {"model_a": "mistral-large-2411", "model_b": "ministral-8b-instruct-2410", "a_wins": 133, "b_wins": 99, "draws": 112, "count": 344, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-8b", "model_b": "llama-3.1-70b", "a_wins": 81, "b_wins": 120, "draws": 132, "count": 333, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-405b", "model_b": "gemini-1.5-pro-002", "a_wins": 58, "b_wins": 64, "draws": 74, "count": 196, "a_win_ratio": 0.48}, {"model_a": "deepseek-v3-chat", "model_b": "lfm-40b", "a_wins": 46, "b_wins": 18, "draws": 30, "count": 94, "a_win_ratio": 0.72}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 12, "b_wins": 12, "draws": 0, "count": 24, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-32b", "model_b": "llama-3.3-70b", "a_wins": 68, "b_wins": 55, "draws": 62, "count": 185, "a_win_ratio": 0.55}, {"model_a": "mistral-saba", "model_b": "gemma-3-27b", "a_wins": 25, "b_wins": 37, "draws": 34, "count": 96, "a_win_ratio": 0.4}, {"model_a": "aya-expanse-32b", "model_b": "gemma-3-4b", "a_wins": 183, "b_wins": 273, "draws": 270, "count": 726, "a_win_ratio": 0.4}, {"model_a": "mistral-saba", "model_b": "c4ai-command-r-08-2024", "a_wins": 40, "b_wins": 29, "draws": 35, "count": 104, "a_win_ratio": 0.58}, {"model_a": "command-a", "model_b": "gpt-4.1-nano", "a_wins": 57, "b_wins": 30, "draws": 39, "count": 126, "a_win_ratio": 0.66}, {"model_a": "qwq-32b", "model_b": "mistral-small-3.1-24b", "a_wins": 11, "b_wins": 20, "draws": 12, "count": 43, "a_win_ratio": 0.35}, {"model_a": "mistral-nemo-2407", "model_b": "mistral-large-2411", "a_wins": 47, "b_wins": 81, "draws": 68, "count": 196, "a_win_ratio": 0.37}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "phi-3.5-mini-instruct", "a_wins": 5, "b_wins": 8, "draws": 11, "count": 24, "a_win_ratio": 0.38}, {"model_a": "qwq-32b", "model_b": "ministral-8b-instruct-2410", "a_wins": 6, "b_wins": 14, "draws": 14, "count": 34, "a_win_ratio": 0.3}, {"model_a": "aya-expanse-8b", "model_b": "llama-3.3-70b", "a_wins": 8, "b_wins": 6, "draws": 9, "count": 23, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-405b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 71, "b_wins": 110, "draws": 121, "count": 302, "a_win_ratio": 0.39}, {"model_a": "gemini-1.5-pro-002", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 80, "b_wins": 31, "draws": 51, "count": 162, "a_win_ratio": 0.72}, {"model_a": "gemma-3-12b", "model_b": "lfm-40b", "a_wins": 5, "b_wins": 2, "draws": 10, "count": 17, "a_win_ratio": 0.71}, {"model_a": "aya-expanse-8b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 10, "b_wins": 6, "draws": 15, "count": 31, "a_win_ratio": 0.62}, {"model_a": "gemini-2.0-flash-001", "model_b": "gemma-3-27b", "a_wins": 64, "b_wins": 55, "draws": 51, "count": 170, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-70b", "model_b": "qwen2-7b-instruct", "a_wins": 3, "b_wins": 0, "draws": 1, "count": 4, "a_win_ratio": 1.0}, {"model_a": "o3-mini", "model_b": "gemma-3-4b", "a_wins": 9, "b_wins": 12, "draws": 11, "count": 32, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "phi-3.5-mini-instruct", "a_wins": 2, "b_wins": 1, "draws": 10, "count": 13, "a_win_ratio": 0.67}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mistral-small-3.1-24b", "a_wins": 25, "b_wins": 24, "draws": 26, "count": 75, "a_win_ratio": 0.51}, {"model_a": "deepseek-v3-chat", "model_b": "o3-mini", "a_wins": 19, "b_wins": 14, "draws": 19, "count": 52, "a_win_ratio": 0.58}, {"model_a": "mistral-small-3.1-24b", "model_b": "mistral-large-2411", "a_wins": 75, "b_wins": 70, "draws": 57, "count": 202, "a_win_ratio": 0.52}, {"model_a": "llama-3.3-70b", "model_b": "qwen3-32b", "a_wins": 3, "b_wins": 2, "draws": 1, "count": 6, "a_win_ratio": 0.6}, {"model_a": "aya-expanse-32b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 2, "b_wins": 1, "draws": 3, "count": 6, "a_win_ratio": 0.67}, {"model_a": "llama-3.1-8b", "model_b": "gemini-1.5-pro-001", "a_wins": 13, "b_wins": 68, "draws": 86, "count": 167, "a_win_ratio": 0.16}, {"model_a": "command-a", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 63, "b_wins": 40, "draws": 57, "count": 160, "a_win_ratio": 0.61}, {"model_a": "llama-3.3-70b", "model_b": "c4ai-command-r-08-2024", "a_wins": 85, "b_wins": 69, "draws": 85, "count": 239, "a_win_ratio": 0.55}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 17, "b_wins": 11, "draws": 20, "count": 48, "a_win_ratio": 0.61}, {"model_a": "gemma-3-27b", "model_b": "deepseek-v3-chat", "a_wins": 20, "b_wins": 21, "draws": 21, "count": 62, "a_win_ratio": 0.49}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 11, "b_wins": 9, "draws": 13, "count": 33, "a_win_ratio": 0.55}, {"model_a": "phi-4", "model_b": "claude-3-5-sonnet-v2", "a_wins": 61, "b_wins": 50, "draws": 82, "count": 193, "a_win_ratio": 0.55}, {"model_a": "mistral-nemo-2407", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 89, "b_wins": 89, "draws": 107, "count": 285, "a_win_ratio": 0.5}, {"model_a": "phi-3.5-mini-instruct", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 1, "b_wins": 2, "draws": 10, "count": 13, "a_win_ratio": 0.33}, {"model_a": "mistral-saba", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 38, "b_wins": 37, "draws": 38, "count": 113, "a_win_ratio": 0.51}, {"model_a": "llama-4-scout", "model_b": "gpt-4.1-mini", "a_wins": 37, "b_wins": 43, "draws": 38, "count": 118, "a_win_ratio": 0.46}, {"model_a": "qwq-32b", "model_b": "gpt-4.1-nano", "a_wins": 1, "b_wins": 2, "draws": 2, "count": 5, "a_win_ratio": 0.33}, {"model_a": "gemma-2-9b-it", "model_b": "qwen2.5-7b-instruct", "a_wins": 35, "b_wins": 30, "draws": 39, "count": 104, "a_win_ratio": 0.54}, {"model_a": "qwen2.5-32b-instruct", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 1, "b_wins": 2, "draws": 11, "count": 14, "a_win_ratio": 0.33}, {"model_a": "gemini-2.0-flash-exp", "model_b": "phi-4", "a_wins": 51, "b_wins": 32, "draws": 43, "count": 126, "a_win_ratio": 0.61}, {"model_a": "deepseek-r1", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 143, "b_wins": 99, "draws": 132, "count": 374, "a_win_ratio": 0.59}, {"model_a": "gemma-3-4b", "model_b": "deepseek-r1", "a_wins": 10, "b_wins": 13, "draws": 12, "count": 35, "a_win_ratio": 0.43}, {"model_a": "deepseek-r1", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 10, "b_wins": 10, "draws": 20, "count": 40, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "deepseek-v3-0324", "a_wins": 42, "b_wins": 66, "draws": 56, "count": 164, "a_win_ratio": 0.39}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemini-1.5-pro-002", "a_wins": 97, "b_wins": 109, "draws": 107, "count": 313, "a_win_ratio": 0.47}, {"model_a": "deepseek-r1", "model_b": "gemma-3-12b", "a_wins": 4, "b_wins": 14, "draws": 21, "count": 39, "a_win_ratio": 0.22}, {"model_a": "command-a", "model_b": "grok-3-mini-beta", "a_wins": 0, "b_wins": 1, "draws": 3, "count": 4, "a_win_ratio": 0.0}, {"model_a": "qwq-32b", "model_b": "mistral-saba", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "gemma-3-27b", "a_wins": 6, "b_wins": 17, "draws": 12, "count": 35, "a_win_ratio": 0.26}, {"model_a": "gpt-4o-2024-08-06", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 29, "b_wins": 23, "draws": 17, "count": 69, "a_win_ratio": 0.56}, {"model_a": "phi-3.5-mini-instruct", "model_b": "deepseek-v3-chat", "a_wins": 0, "b_wins": 2, "draws": 1, "count": 3, "a_win_ratio": 0.0}, {"model_a": "phi-3.5-mini-instruct", "model_b": "Yi-1.5-9B-Chat", "a_wins": 5, "b_wins": 2, "draws": 6, "count": 13, "a_win_ratio": 0.71}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "grok-3-mini-beta", "a_wins": 4, "b_wins": 2, "draws": 2, "count": 8, "a_win_ratio": 0.67}, {"model_a": "phi-3.5-mini-instruct", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 53, "b_wins": 53, "draws": 98, "count": 204, "a_win_ratio": 0.5}, {"model_a": "o4-mini", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 3, "b_wins": 1, "draws": 6, "count": 10, "a_win_ratio": 0.75}, {"model_a": "gpt-4.1-nano", "model_b": "command-a", "a_wins": 30, "b_wins": 57, "draws": 39, "count": 126, "a_win_ratio": 0.34}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "deepseek-r1", "a_wins": 6, "b_wins": 12, "draws": 12, "count": 30, "a_win_ratio": 0.33}, {"model_a": "gemini-1.5-pro-002", "model_b": "gpt-4o-2024-08-06", "a_wins": 160, "b_wins": 152, "draws": 174, "count": 486, "a_win_ratio": 0.51}, {"model_a": "gemini-2.0-flash-001", "model_b": "mistral-small-3.1-24b", "a_wins": 103, "b_wins": 62, "draws": 88, "count": 253, "a_win_ratio": 0.62}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 23, "b_wins": 10, "draws": 21, "count": 54, "a_win_ratio": 0.7}, {"model_a": "ministral-8b-instruct-2410", "model_b": "deepseek-v3-0324", "a_wins": 32, "b_wins": 46, "draws": 46, "count": 124, "a_win_ratio": 0.41}, {"model_a": "llama-3.1-405b", "model_b": "qwen2.5-32b-instruct", "a_wins": 3, "b_wins": 4, "draws": 15, "count": 22, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-8b", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 4, "draws": 2, "count": 9, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-405b", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 15, "b_wins": 0, "draws": 16, "count": 31, "a_win_ratio": 1.0}, {"model_a": "gemini-1.5-pro-002", "model_b": "llama-3.3-70b", "a_wins": 38, "b_wins": 33, "draws": 42, "count": 113, "a_win_ratio": 0.54}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mistral-small-3.1-24b", "a_wins": 43, "b_wins": 53, "draws": 52, "count": 148, "a_win_ratio": 0.45}, {"model_a": "lfm-40b", "model_b": "qwq-32b", "a_wins": 7, "b_wins": 11, "draws": 11, "count": 29, "a_win_ratio": 0.39}, {"model_a": "gpt-4.1-nano", "model_b": "gemma-3-4b", "a_wins": 44, "b_wins": 55, "draws": 49, "count": 148, "a_win_ratio": 0.44}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 50, "b_wins": 39, "draws": 33, "count": 122, "a_win_ratio": 0.56}, {"model_a": "claude-3-7-sonnet", "model_b": "ministral-8b-instruct-2410", "a_wins": 72, "b_wins": 76, "draws": 70, "count": 218, "a_win_ratio": 0.49}, {"model_a": "llama-4-scout", "model_b": "gemma-3-12b", "a_wins": 31, "b_wins": 42, "draws": 34, "count": 107, "a_win_ratio": 0.42}, {"model_a": "llama-3.1-405b", "model_b": "qwen2.5-7b-instruct", "a_wins": 42, "b_wins": 31, "draws": 72, "count": 145, "a_win_ratio": 0.58}, {"model_a": "mistral-small-3.1-24b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 24, "b_wins": 25, "draws": 26, "count": 75, "a_win_ratio": 0.49}, {"model_a": "phi-3.5-mini-instruct", "model_b": "llama-3.1-405b", "a_wins": 44, "b_wins": 120, "draws": 174, "count": 338, "a_win_ratio": 0.27}, {"model_a": "mistral-small-3.1-24b", "model_b": "phi-4", "a_wins": 57, "b_wins": 44, "draws": 50, "count": 151, "a_win_ratio": 0.56}, {"model_a": "lfm-40b", "model_b": "gemma-2-27b-it-q8", "a_wins": 6, "b_wins": 18, "draws": 13, "count": 37, "a_win_ratio": 0.25}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemini-1.5-pro-002", "a_wins": 20, "b_wins": 20, "draws": 2, "count": 42, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 33, "b_wins": 15, "draws": 27, "count": 75, "a_win_ratio": 0.69}, {"model_a": "lfm-40b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 30, "b_wins": 54, "draws": 55, "count": 139, "a_win_ratio": 0.36}, {"model_a": "deepseek-r1", "model_b": "mistral-nemo-2407", "a_wins": 20, "b_wins": 8, "draws": 18, "count": 46, "a_win_ratio": 0.71}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 64, "b_wins": 66, "draws": 70, "count": 200, "a_win_ratio": 0.49}, {"model_a": "gemma-3-27b", "model_b": "mistral-large-2411", "a_wins": 62, "b_wins": 48, "draws": 55, "count": 165, "a_win_ratio": 0.56}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gpt-4o-2024-08-06", "a_wins": 5, "b_wins": 6, "draws": 5, "count": 16, "a_win_ratio": 0.45}, {"model_a": "gemini-2.0-flash-001", "model_b": "aya-expanse-32b", "a_wins": 37, "b_wins": 23, "draws": 33, "count": 93, "a_win_ratio": 0.62}, {"model_a": "qwen3-32b", "model_b": "grok-3-mini-beta", "a_wins": 67, "b_wins": 66, "draws": 50, "count": 183, "a_win_ratio": 0.5}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "qwen2.5-7b-instruct", "a_wins": 1, "b_wins": 7, "draws": 7, "count": 15, "a_win_ratio": 0.12}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemma-3-12b", "a_wins": 7, "b_wins": 12, "draws": 9, "count": 28, "a_win_ratio": 0.37}, {"model_a": "llama-3.1-70b", "model_b": "c4ai-command-r-08-2024", "a_wins": 41, "b_wins": 36, "draws": 44, "count": 121, "a_win_ratio": 0.53}, {"model_a": "llama-3.3-70b", "model_b": "o4-mini", "a_wins": 2, "b_wins": 1, "draws": 4, "count": 7, "a_win_ratio": 0.67}, {"model_a": "llama-3.1-8b", "model_b": "mistral-saba", "a_wins": 30, "b_wins": 32, "draws": 23, "count": 85, "a_win_ratio": 0.48}, {"model_a": "qwq-32b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 8, "b_wins": 10, "draws": 10, "count": 28, "a_win_ratio": 0.44}, {"model_a": "aya-expanse-8b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 24, "b_wins": 27, "draws": 26, "count": 77, "a_win_ratio": 0.47}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "command-a", "a_wins": 6, "b_wins": 16, "draws": 18, "count": 40, "a_win_ratio": 0.27}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "llama-3.1-70b", "a_wins": 53, "b_wins": 48, "draws": 70, "count": 171, "a_win_ratio": 0.52}, {"model_a": "o4-mini", "model_b": "llama-3.1-405b", "a_wins": 8, "b_wins": 7, "draws": 10, "count": 25, "a_win_ratio": 0.53}, {"model_a": "o4-mini", "model_b": "command-a", "a_wins": 3, "b_wins": 1, "draws": 3, "count": 7, "a_win_ratio": 0.75}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 138, "b_wins": 132, "draws": 153, "count": 423, "a_win_ratio": 0.51}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gpt-4o-2024-08-06", "a_wins": 29, "b_wins": 31, "draws": 29, "count": 89, "a_win_ratio": 0.48}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-3.1-8b", "a_wins": 75, "b_wins": 68, "draws": 70, "count": 213, "a_win_ratio": 0.52}, {"model_a": "o3-mini", "model_b": "llama-3.1-405b", "a_wins": 12, "b_wins": 8, "draws": 10, "count": 30, "a_win_ratio": 0.6}, {"model_a": "llama-3.3-70b", "model_b": "gemma-2-9b-it", "a_wins": 28, "b_wins": 34, "draws": 34, "count": 96, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-8b", "model_b": "phi-3.5-mini-instruct", "a_wins": 53, "b_wins": 47, "draws": 72, "count": 172, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-405b", "model_b": "deepseek-v3-chat", "a_wins": 50, "b_wins": 66, "draws": 81, "count": 197, "a_win_ratio": 0.43}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "claude-3-7-sonnet", "a_wins": 34, "b_wins": 41, "draws": 37, "count": 112, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-70b", "model_b": "mistral-small-3.1-24b", "a_wins": 16, "b_wins": 19, "draws": 10, "count": 45, "a_win_ratio": 0.46}, {"model_a": "gemini-2.0-flash-001", "model_b": "phi-4", "a_wins": 67, "b_wins": 39, "draws": 54, "count": 160, "a_win_ratio": 0.63}, {"model_a": "gemma-3-12b", "model_b": "gemma-3-27b", "a_wins": 53, "b_wins": 51, "draws": 38, "count": 142, "a_win_ratio": 0.51}, {"model_a": "gemma-3-12b", "model_b": "grok-3-mini-beta", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "deepseek-v3-chat", "a_wins": 18, "b_wins": 46, "draws": 30, "count": 94, "a_win_ratio": 0.28}, {"model_a": "gemini-2.0-flash-exp", "model_b": "lfm-40b", "a_wins": 26, "b_wins": 17, "draws": 30, "count": 73, "a_win_ratio": 0.6}, {"model_a": "gemini-1.5-pro-002", "model_b": "mistral-large-2411", "a_wins": 64, "b_wins": 77, "draws": 76, "count": 217, "a_win_ratio": 0.45}, {"model_a": "mistral-small-3.1-24b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 57, "b_wins": 46, "draws": 48, "count": 151, "a_win_ratio": 0.55}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "qwq-32b", "a_wins": 5, "b_wins": 4, "draws": 21, "count": 30, "a_win_ratio": 0.56}, {"model_a": "gemini-1.5-pro-002", "model_b": "llama-3.1-70b", "a_wins": 67, "b_wins": 48, "draws": 72, "count": 187, "a_win_ratio": 0.58}, {"model_a": "grok-3-mini-beta", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 3, "b_wins": 2, "draws": 1, "count": 6, "a_win_ratio": 0.6}, {"model_a": "mistral-large-2411", "model_b": "gemini-2.0-flash-exp", "a_wins": 37, "b_wins": 53, "draws": 79, "count": 169, "a_win_ratio": 0.41}, {"model_a": "gemma-3-4b", "model_b": "llama-3.1-8b", "a_wins": 40, "b_wins": 28, "draws": 44, "count": 112, "a_win_ratio": 0.59}, {"model_a": "qwen2.5-7b-instruct", "model_b": "phi-3.5-mini-instruct", "a_wins": 41, "b_wins": 32, "draws": 54, "count": 127, "a_win_ratio": 0.56}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "lfm-40b", "a_wins": 56, "b_wins": 41, "draws": 74, "count": 171, "a_win_ratio": 0.58}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 77, "b_wins": 102, "draws": 102, "count": 281, "a_win_ratio": 0.43}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 86, "b_wins": 46, "draws": 82, "count": 214, "a_win_ratio": 0.65}, {"model_a": "qwq-32b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 15, "b_wins": 8, "draws": 16, "count": 39, "a_win_ratio": 0.65}, {"model_a": "gemma-3-12b", "model_b": "llama-4-scout", "a_wins": 42, "b_wins": 31, "draws": 34, "count": 107, "a_win_ratio": 0.58}, {"model_a": "mistral-small-3.1-24b", "model_b": "ministral-8b-instruct-2410", "a_wins": 54, "b_wins": 51, "draws": 54, "count": 159, "a_win_ratio": 0.51}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemma-2-9b-it", "a_wins": 39, "b_wins": 28, "draws": 44, "count": 111, "a_win_ratio": 0.58}, {"model_a": "qwen2.5-7b-instruct", "model_b": "mistral-nemo-2407", "a_wins": 48, "b_wins": 26, "draws": 52, "count": 126, "a_win_ratio": 0.65}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-3.3-70b", "a_wins": 70, "b_wins": 74, "draws": 95, "count": 239, "a_win_ratio": 0.49}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemini-2.0-flash-001", "a_wins": 47, "b_wins": 52, "draws": 54, "count": 153, "a_win_ratio": 0.47}, {"model_a": "deepseek-r1", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 15, "b_wins": 15, "draws": 12, "count": 42, "a_win_ratio": 0.5}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 7, "b_wins": 27, "draws": 36, "count": 70, "a_win_ratio": 0.21}, {"model_a": "mistral-saba", "model_b": "llama-3.1-8b", "a_wins": 32, "b_wins": 30, "draws": 23, "count": 85, "a_win_ratio": 0.52}, {"model_a": "deepseek-r1", "model_b": "llama-3.3-70b", "a_wins": 9, "b_wins": 20, "draws": 9, "count": 38, "a_win_ratio": 0.31}, {"model_a": "gemma-3-27b", "model_b": "deepseek-r1", "a_wins": 24, "b_wins": 18, "draws": 20, "count": 62, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mistral-small-3.1-24b", "a_wins": 53, "b_wins": 53, "draws": 52, "count": 158, "a_win_ratio": 0.5}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 35, "b_wins": 44, "draws": 36, "count": 115, "a_win_ratio": 0.44}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "c4ai-command-r-08-2024", "a_wins": 98, "b_wins": 84, "draws": 94, "count": 276, "a_win_ratio": 0.54}, {"model_a": "claude-3-7-sonnet", "model_b": "llama-3.3-70b", "a_wins": 34, "b_wins": 27, "draws": 49, "count": 110, "a_win_ratio": 0.56}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 8, "b_wins": 3, "draws": 5, "count": 16, "a_win_ratio": 0.73}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gemini-1.5-pro-002", "a_wins": 64, "b_wins": 38, "draws": 54, "count": 156, "a_win_ratio": 0.63}, {"model_a": "mistral-small-3.1-24b", "model_b": "deepseek-v3-0324", "a_wins": 41, "b_wins": 57, "draws": 67, "count": 165, "a_win_ratio": 0.42}, {"model_a": "o3-mini", "model_b": "command-a", "a_wins": 7, "b_wins": 6, "draws": 11, "count": 24, "a_win_ratio": 0.54}, {"model_a": "phi-4", "model_b": "lfm-40b", "a_wins": 44, "b_wins": 32, "draws": 50, "count": 126, "a_win_ratio": 0.58}, {"model_a": "qwen2-7b-instruct", "model_b": "phi-3.5-mini-instruct", "a_wins": 0, "b_wins": 3, "draws": 2, "count": 5, "a_win_ratio": 0.0}, {"model_a": "mistral-nemo-2407", "model_b": "claude-3-5-sonnet-v2", "a_wins": 53, "b_wins": 91, "draws": 78, "count": 222, "a_win_ratio": 0.37}, {"model_a": "gemma-3-12b", "model_b": "deepseek-v3-0324", "a_wins": 43, "b_wins": 46, "draws": 44, "count": 133, "a_win_ratio": 0.48}, {"model_a": "deepseek-v3-0324", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 1, "b_wins": 2, "draws": 5, "count": 8, "a_win_ratio": 0.33}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "lfm-40b", "a_wins": 47, "b_wins": 45, "draws": 51, "count": 143, "a_win_ratio": 0.51}, {"model_a": "gpt-4.1-mini", "model_b": "claude-3-7-sonnet", "a_wins": 114, "b_wins": 113, "draws": 129, "count": 356, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-8b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 17, "b_wins": 15, "draws": 27, "count": 59, "a_win_ratio": 0.53}, {"model_a": "gemma-3-4b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 5, "b_wins": 4, "draws": 11, "count": 20, "a_win_ratio": 0.56}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mistral-nemo-2407", "a_wins": 89, "b_wins": 89, "draws": 107, "count": 285, "a_win_ratio": 0.5}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemma-3-27b", "a_wins": 10, "b_wins": 15, "draws": 10, "count": 35, "a_win_ratio": 0.4}, {"model_a": "c4ai-command-r-08-2024", "model_b": "deepseek-r1", "a_wins": 11, "b_wins": 17, "draws": 18, "count": 46, "a_win_ratio": 0.39}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "deepseek-r1", "a_wins": 27, "b_wins": 53, "draws": 50, "count": 130, "a_win_ratio": 0.34}, {"model_a": "gemini-1.5-pro-001", "model_b": "ministral-8b-instruct-2410", "a_wins": 36, "b_wins": 23, "draws": 74, "count": 133, "a_win_ratio": 0.61}, {"model_a": "o3-mini", "model_b": "gemma-3-12b", "a_wins": 13, "b_wins": 15, "draws": 16, "count": 44, "a_win_ratio": 0.46}, {"model_a": "grok-3-mini-beta", "model_b": "llama-3.1-405b", "a_wins": 2, "b_wins": 1, "draws": 0, "count": 3, "a_win_ratio": 0.67}, {"model_a": "qwen3-32b", "model_b": "llama-3.3-70b", "a_wins": 2, "b_wins": 3, "draws": 1, "count": 6, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-405b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 87, "b_wins": 100, "draws": 123, "count": 310, "a_win_ratio": 0.47}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 49, "b_wins": 30, "draws": 40, "count": 119, "a_win_ratio": 0.62}, {"model_a": "gemini-2.0-flash-001", "model_b": "gpt-4.1-nano", "a_wins": 77, "b_wins": 68, "draws": 58, "count": 203, "a_win_ratio": 0.53}, {"model_a": "gemini-1.5-pro-002", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 11, "b_wins": 9, "draws": 13, "count": 33, "a_win_ratio": 0.55}, {"model_a": "gpt-4.1-mini", "model_b": "gemini-2.0-flash-001", "a_wins": 115, "b_wins": 144, "draws": 101, "count": 360, "a_win_ratio": 0.44}, {"model_a": "o3-mini", "model_b": "llama-3.3-70b", "a_wins": 18, "b_wins": 11, "draws": 18, "count": 47, "a_win_ratio": 0.62}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "qwen3-32b", "a_wins": 2, "b_wins": 3, "draws": 0, "count": 5, "a_win_ratio": 0.4}, {"model_a": "c4ai-command-r-08-2024", "model_b": "command-a", "a_wins": 43, "b_wins": 70, "draws": 49, "count": 162, "a_win_ratio": 0.38}, {"model_a": "gpt-4.1-mini", "model_b": "gemma-3-27b", "a_wins": 51, "b_wins": 63, "draws": 51, "count": 165, "a_win_ratio": 0.45}, {"model_a": "gemini-2.0-flash-exp", "model_b": "ministral-8b-instruct-2410", "a_wins": 51, "b_wins": 27, "draws": 51, "count": 129, "a_win_ratio": 0.65}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mistral-nemo-2407", "a_wins": 43, "b_wins": 31, "draws": 39, "count": 113, "a_win_ratio": 0.58}, {"model_a": "ministral-8b-instruct-2410", "model_b": "jamba-1.5-large", "a_wins": 4, "b_wins": 3, "draws": 2, "count": 9, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-405b", "model_b": "llama-3.1-70b", "a_wins": 115, "b_wins": 90, "draws": 154, "count": 359, "a_win_ratio": 0.56}, {"model_a": "o3-mini", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 12, "b_wins": 14, "draws": 14, "count": 40, "a_win_ratio": 0.46}, {"model_a": "deepseek-v3-0324", "model_b": "llama-4-scout", "a_wins": 40, "b_wins": 34, "draws": 38, "count": 112, "a_win_ratio": 0.54}, {"model_a": "phi-4", "model_b": "gpt-4.1-nano", "a_wins": 29, "b_wins": 46, "draws": 35, "count": 110, "a_win_ratio": 0.39}, {"model_a": "claude-3-7-sonnet", "model_b": "grok-3-mini-beta", "a_wins": 23, "b_wins": 18, "draws": 26, "count": 67, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "llama-3.1-70b", "a_wins": 12, "b_wins": 11, "draws": 18, "count": 41, "a_win_ratio": 0.52}, {"model_a": "gpt-4o-2024-08-06", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 21, "b_wins": 27, "draws": 22, "count": 70, "a_win_ratio": 0.44}, {"model_a": "phi-4", "model_b": "llama-3.1-8b", "a_wins": 106, "b_wins": 89, "draws": 88, "count": 283, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-8b", "model_b": "gpt-4o-2024-08-06", "a_wins": 58, "b_wins": 44, "draws": 79, "count": 181, "a_win_ratio": 0.57}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "ministral-8b-instruct-2410", "a_wins": 118, "b_wins": 100, "draws": 116, "count": 334, "a_win_ratio": 0.54}, {"model_a": "gpt-4.1-nano", "model_b": "grok-3-mini-beta", "a_wins": 1, "b_wins": 6, "draws": 5, "count": 12, "a_win_ratio": 0.14}, {"model_a": "gemma-2-9b-it", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 58, "b_wins": 42, "draws": 44, "count": 144, "a_win_ratio": 0.58}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemma-3-4b", "a_wins": 4, "b_wins": 11, "draws": 11, "count": 26, "a_win_ratio": 0.27}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "command-a", "a_wins": 8, "b_wins": 18, "draws": 14, "count": 40, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mistral-nemo-2407", "a_wins": 66, "b_wins": 36, "draws": 55, "count": 157, "a_win_ratio": 0.65}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "llama-3.1-8b", "a_wins": 54, "b_wins": 70, "draws": 65, "count": 189, "a_win_ratio": 0.44}, {"model_a": "mistral-small-3.1-24b", "model_b": "qwq-32b", "a_wins": 20, "b_wins": 11, "draws": 12, "count": 43, "a_win_ratio": 0.65}, {"model_a": "mistral-small-3.1-24b", "model_b": "command-a", "a_wins": 43, "b_wins": 67, "draws": 50, "count": 160, "a_win_ratio": 0.39}, {"model_a": "llama-3.1-8b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 68, "b_wins": 75, "draws": 98, "count": 241, "a_win_ratio": 0.48}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 54, "b_wins": 72, "draws": 65, "count": 191, "a_win_ratio": 0.43}, {"model_a": "mistral-nemo-2407", "model_b": "gemini-1.5-pro-002", "a_wins": 41, "b_wins": 92, "draws": 74, "count": 207, "a_win_ratio": 0.31}, {"model_a": "command-a", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 18, "b_wins": 8, "draws": 14, "count": 40, "a_win_ratio": 0.69}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 14, "b_wins": 24, "draws": 17, "count": 55, "a_win_ratio": 0.37}, {"model_a": "gemma-3-27b", "model_b": "llama-4-scout", "a_wins": 43, "b_wins": 36, "draws": 33, "count": 112, "a_win_ratio": 0.54}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "command-a", "a_wins": 3, "b_wins": 7, "draws": 14, "count": 24, "a_win_ratio": 0.3}, {"model_a": "gemma-2-27b-it-q8", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 15, "b_wins": 8, "draws": 16, "count": 39, "a_win_ratio": 0.65}, {"model_a": "o3-mini", "model_b": "lfm-40b", "a_wins": 14, "b_wins": 6, "draws": 19, "count": 39, "a_win_ratio": 0.7}, {"model_a": "mistral-saba", "model_b": "mistral-large-2411", "a_wins": 31, "b_wins": 51, "draws": 40, "count": 122, "a_win_ratio": 0.38}, {"model_a": "llama-3.1-8b", "model_b": "Yi-1.5-9B-Chat", "a_wins": 1, "b_wins": 0, "draws": 2, "count": 3, "a_win_ratio": 1.0}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "o4-mini", "a_wins": 2, "b_wins": 2, "draws": 5, "count": 9, "a_win_ratio": 0.5}, {"model_a": "gemma-2-9b-it", "model_b": "o3-mini", "a_wins": 10, "b_wins": 12, "draws": 5, "count": 27, "a_win_ratio": 0.45}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mistral-large-2411", "a_wins": 30, "b_wins": 56, "draws": 57, "count": 143, "a_win_ratio": 0.35}, {"model_a": "phi-4", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 109, "b_wins": 126, "draws": 129, "count": 364, "a_win_ratio": 0.46}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "llama-3.1-70b", "a_wins": 9, "b_wins": 25, "draws": 20, "count": 54, "a_win_ratio": 0.26}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 44, "b_wins": 39, "draws": 43, "count": 126, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-405b", "model_b": "gpt-4.1-nano", "a_wins": 22, "b_wins": 55, "draws": 51, "count": 128, "a_win_ratio": 0.29}, {"model_a": "o4-mini", "model_b": "gpt-4.1-mini", "a_wins": 27, "b_wins": 33, "draws": 24, "count": 84, "a_win_ratio": 0.45}, {"model_a": "mistral-large-2411", "model_b": "llama-3.1-70b", "a_wins": 55, "b_wins": 50, "draws": 40, "count": 145, "a_win_ratio": 0.52}, {"model_a": "gemma-3-27b", "model_b": "grok-3-mini-beta", "a_wins": 4, "b_wins": 7, "draws": 7, "count": 18, "a_win_ratio": 0.36}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "aya-expanse-8b", "a_wins": 8, "b_wins": 21, "draws": 15, "count": 44, "a_win_ratio": 0.28}, {"model_a": "llama-3.1-8b", "model_b": "llama-3.1-8b", "a_wins": 6, "b_wins": 6, "draws": 2, "count": 14, "a_win_ratio": 0.5}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-3.1-70b", "a_wins": 36, "b_wins": 41, "draws": 44, "count": 121, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-70b", "model_b": "llama-3.1-8b", "a_wins": 120, "b_wins": 81, "draws": 132, "count": 333, "a_win_ratio": 0.6}, {"model_a": "mistral-small-3.1-24b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 23, "b_wins": 8, "draws": 15, "count": 46, "a_win_ratio": 0.74}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 48, "b_wins": 25, "draws": 47, "count": 120, "a_win_ratio": 0.66}, {"model_a": "deepseek-r1", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 8, "b_wins": 12, "draws": 16, "count": 36, "a_win_ratio": 0.4}, {"model_a": "o3-mini", "model_b": "llama-3.1-8b", "a_wins": 8, "b_wins": 11, "draws": 10, "count": 29, "a_win_ratio": 0.42}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gemini-2.0-flash-exp", "a_wins": 0, "b_wins": 8, "draws": 6, "count": 14, "a_win_ratio": 0.0}, {"model_a": "gemini-1.5-pro-002", "model_b": "phi-3.5-mini-instruct", "a_wins": 4, "b_wins": 4, "draws": 7, "count": 15, "a_win_ratio": 0.5}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "aya-expanse-8b", "a_wins": 7, "b_wins": 8, "draws": 16, "count": 31, "a_win_ratio": 0.47}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-3.1-8b", "a_wins": 142, "b_wins": 122, "draws": 140, "count": 404, "a_win_ratio": 0.54}, {"model_a": "gemma-3-12b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 18, "b_wins": 14, "draws": 10, "count": 42, "a_win_ratio": 0.56}, {"model_a": "llama-3.3-70b", "model_b": "llama-3.1-70b", "a_wins": 35, "b_wins": 42, "draws": 36, "count": 113, "a_win_ratio": 0.45}, {"model_a": "gemma-2-9b-it", "model_b": "deepseek-r1", "a_wins": 7, "b_wins": 12, "draws": 9, "count": 28, "a_win_ratio": 0.37}, {"model_a": "gemma-2-9b-it", "model_b": "gemma-2-27b-it-q8", "a_wins": 16, "b_wins": 5, "draws": 18, "count": 39, "a_win_ratio": 0.76}, {"model_a": "qwen2.5-32b-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 3, "b_wins": 7, "draws": 7, "count": 17, "a_win_ratio": 0.3}, {"model_a": "llama-3.1-8b", "model_b": "llama-3.1-405b", "a_wins": 108, "b_wins": 156, "draws": 171, "count": 435, "a_win_ratio": 0.41}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 100, "b_wins": 118, "draws": 116, "count": 334, "a_win_ratio": 0.46}, {"model_a": "command-a", "model_b": "qwen3-32b", "a_wins": 2, "b_wins": 1, "draws": 1, "count": 4, "a_win_ratio": 0.67}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemini-1.5-pro-001", "a_wins": 0, "b_wins": 2, "draws": 5, "count": 7, "a_win_ratio": 0.0}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 36, "b_wins": 19, "draws": 43, "count": 98, "a_win_ratio": 0.65}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemini-1.5-pro-002", "a_wins": 11, "b_wins": 17, "draws": 22, "count": 50, "a_win_ratio": 0.39}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemini-2.0-flash-001", "a_wins": 9, "b_wins": 28, "draws": 20, "count": 57, "a_win_ratio": 0.24}, {"model_a": "deepseek-v3-0324", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 47, "b_wins": 23, "draws": 53, "count": 123, "a_win_ratio": 0.67}, {"model_a": "gemini-1.5-pro-001", "model_b": "gemma-2-27b-it-q8", "a_wins": 20, "b_wins": 20, "draws": 22, "count": 62, "a_win_ratio": 0.5}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "ministral-8b-instruct-2410", "a_wins": 13, "b_wins": 12, "draws": 15, "count": 40, "a_win_ratio": 0.52}, {"model_a": "qwen3-32b", "model_b": "c4ai-command-r-08-2024", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-8b", "model_b": "deepseek-v3-chat", "a_wins": 7, "b_wins": 12, "draws": 16, "count": 35, "a_win_ratio": 0.37}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gemma-2-27b-it-q8", "a_wins": 6, "b_wins": 5, "draws": 5, "count": 16, "a_win_ratio": 0.55}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 14, "b_wins": 6, "draws": 10, "count": 30, "a_win_ratio": 0.7}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "llama-3.3-70b", "a_wins": 36, "b_wins": 43, "draws": 52, "count": 131, "a_win_ratio": 0.46}, {"model_a": "deepseek-v3-0324", "model_b": "ministral-8b-instruct-2410", "a_wins": 46, "b_wins": 32, "draws": 46, "count": 124, "a_win_ratio": 0.59}, {"model_a": "gemma-3-4b", "model_b": "lfm-40b", "a_wins": 6, "b_wins": 6, "draws": 5, "count": 17, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-7b-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 26, "b_wins": 19, "draws": 26, "count": 71, "a_win_ratio": 0.58}, {"model_a": "aya-expanse-32b", "model_b": "phi-4", "a_wins": 72, "b_wins": 58, "draws": 79, "count": 209, "a_win_ratio": 0.55}, {"model_a": "phi-3.5-mini-instruct", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 6, "b_wins": 10, "draws": 10, "count": 26, "a_win_ratio": 0.38}, {"model_a": "lfm-40b", "model_b": "command-a", "a_wins": 10, "b_wins": 12, "draws": 14, "count": 36, "a_win_ratio": 0.45}, {"model_a": "gemma-2-9b-it", "model_b": "command-a", "a_wins": 4, "b_wins": 7, "draws": 11, "count": 22, "a_win_ratio": 0.36}, {"model_a": "claude-3-7-sonnet", "model_b": "llama-3.1-405b", "a_wins": 41, "b_wins": 23, "draws": 47, "count": 111, "a_win_ratio": 0.64}, {"model_a": "qwen3-32b", "model_b": "mistral-large-2411", "a_wins": 3, "b_wins": 0, "draws": 4, "count": 7, "a_win_ratio": 1.0}, {"model_a": "lfm-40b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 29, "b_wins": 40, "draws": 39, "count": 108, "a_win_ratio": 0.42}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemini-2.0-flash-001", "a_wins": 27, "b_wins": 76, "draws": 74, "count": 177, "a_win_ratio": 0.26}, {"model_a": "llama-3.1-405b", "model_b": "lfm-40b", "a_wins": 89, "b_wins": 39, "draws": 93, "count": 221, "a_win_ratio": 0.7}, {"model_a": "qwen2.5-32b-instruct", "model_b": "mistral-nemo-2407", "a_wins": 10, "b_wins": 3, "draws": 9, "count": 22, "a_win_ratio": 0.77}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "mistral-nemo-2407", "a_wins": 68, "b_wins": 54, "draws": 89, "count": 211, "a_win_ratio": 0.56}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "mistral-small-3.1-24b", "a_wins": 9, "b_wins": 8, "draws": 10, "count": 27, "a_win_ratio": 0.53}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "ministral-8b-instruct-2410", "a_wins": 103, "b_wins": 95, "draws": 76, "count": 274, "a_win_ratio": 0.52}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "aya-expanse-32b", "a_wins": 1, "b_wins": 2, "draws": 3, "count": 6, "a_win_ratio": 0.33}, {"model_a": "gpt-4o-2024-08-06", "model_b": "mistral-large-2411", "a_wins": 114, "b_wins": 160, "draws": 167, "count": 441, "a_win_ratio": 0.42}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemini-1.5-pro-001", "a_wins": 12, "b_wins": 11, "draws": 22, "count": 45, "a_win_ratio": 0.52}, {"model_a": "mistral-small-3.1-24b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 8, "b_wins": 9, "draws": 10, "count": 27, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-8b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 44, "b_wins": 84, "draws": 75, "count": 203, "a_win_ratio": 0.34}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gemma-2-9b-it", "a_wins": 67, "b_wins": 62, "draws": 65, "count": 194, "a_win_ratio": 0.52}, {"model_a": "gemma-3-27b", "model_b": "llama-3.1-70b", "a_wins": 15, "b_wins": 7, "draws": 14, "count": 36, "a_win_ratio": 0.68}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gpt-4o-2024-08-06", "a_wins": 9, "b_wins": 16, "draws": 20, "count": 45, "a_win_ratio": 0.36}, {"model_a": "phi-4", "model_b": "aya-expanse-32b", "a_wins": 58, "b_wins": 72, "draws": 79, "count": 209, "a_win_ratio": 0.45}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemini-2.0-flash-001", "a_wins": 10, "b_wins": 16, "draws": 20, "count": 46, "a_win_ratio": 0.38}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "llama-3.1-70b", "a_wins": 1, "b_wins": 12, "draws": 12, "count": 25, "a_win_ratio": 0.08}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "llama-3.1-405b", "a_wins": 18, "b_wins": 14, "draws": 19, "count": 51, "a_win_ratio": 0.56}, {"model_a": "gemma-2-9b-it", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 37, "b_wins": 42, "draws": 42, "count": 121, "a_win_ratio": 0.47}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gpt-4o-2024-08-06", "a_wins": 14, "b_wins": 25, "draws": 18, "count": 57, "a_win_ratio": 0.36}, {"model_a": "gemma-2-9b-it", "model_b": "gemma-3-4b", "a_wins": 3, "b_wins": 10, "draws": 15, "count": 28, "a_win_ratio": 0.23}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-3.1-405b", "a_wins": 110, "b_wins": 71, "draws": 121, "count": 302, "a_win_ratio": 0.61}, {"model_a": "qwq-32b", "model_b": "gemma-2-9b-it", "a_wins": 7, "b_wins": 10, "draws": 10, "count": 27, "a_win_ratio": 0.41}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 9, "b_wins": 11, "draws": 13, "count": 33, "a_win_ratio": 0.45}, {"model_a": "phi-3.5-mini-instruct", "model_b": "qwen2.5-7b-instruct", "a_wins": 32, "b_wins": 41, "draws": 54, "count": 127, "a_win_ratio": 0.44}, {"model_a": "mistral-small-3.1-24b", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 2, "draws": 0, "count": 3, "a_win_ratio": 0.33}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 15, "b_wins": 34, "draws": 28, "count": 77, "a_win_ratio": 0.31}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "llama-3.1-405b", "a_wins": 52, "b_wins": 95, "draws": 132, "count": 279, "a_win_ratio": 0.35}, {"model_a": "gemini-2.0-flash-001", "model_b": "mistral-large-2411", "a_wins": 118, "b_wins": 77, "draws": 86, "count": 281, "a_win_ratio": 0.61}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "lfm-40b", "a_wins": 60, "b_wins": 40, "draws": 43, "count": 143, "a_win_ratio": 0.6}, {"model_a": "gpt-4.1-nano", "model_b": "gemma-3-27b", "a_wins": 29, "b_wins": 44, "draws": 49, "count": 122, "a_win_ratio": 0.4}, {"model_a": "gemma-2-27b-it-q8", "model_b": "llama-3.1-8b", "a_wins": 33, "b_wins": 14, "draws": 41, "count": 88, "a_win_ratio": 0.7}, {"model_a": "mistral-small-3.1-24b", "model_b": "gemma-2-9b-it", "a_wins": 15, "b_wins": 7, "draws": 19, "count": 41, "a_win_ratio": 0.68}, {"model_a": "jamba-1.5-large", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 5, "b_wins": 2, "draws": 5, "count": 12, "a_win_ratio": 0.71}, {"model_a": "llama-3.1-70b", "model_b": "jamba-1.5-large", "a_wins": 1, "b_wins": 6, "draws": 1, "count": 8, "a_win_ratio": 0.14}, {"model_a": "mistral-saba", "model_b": "phi-4", "a_wins": 36, "b_wins": 43, "draws": 34, "count": 113, "a_win_ratio": 0.46}, {"model_a": "mistral-large-2411", "model_b": "gpt-4o-2024-08-06", "a_wins": 160, "b_wins": 114, "draws": 167, "count": 441, "a_win_ratio": 0.58}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "phi-3.5-mini-instruct", "a_wins": 10, "b_wins": 6, "draws": 10, "count": 26, "a_win_ratio": 0.62}, {"model_a": "llama-3.3-70b", "model_b": "deepseek-v3-chat", "a_wins": 40, "b_wins": 51, "draws": 45, "count": 136, "a_win_ratio": 0.44}, {"model_a": "qwen2.5-7b-instruct", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 8, "b_wins": 14, "draws": 23, "count": 45, "a_win_ratio": 0.36}, {"model_a": "jamba-1.5-large", "model_b": "llama-3.3-70b", "a_wins": 1, "b_wins": 0, "draws": 3, "count": 4, "a_win_ratio": 1.0}, {"model_a": "gpt-4.1-mini", "model_b": "mistral-large-2411", "a_wins": 99, "b_wins": 102, "draws": 94, "count": 295, "a_win_ratio": 0.49}, {"model_a": "gpt-4.1-mini", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 43, "b_wins": 26, "draws": 48, "count": 117, "a_win_ratio": 0.62}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 6, "b_wins": 14, "draws": 10, "count": 30, "a_win_ratio": 0.3}, {"model_a": "deepseek-v3-0324", "model_b": "deepseek-r1", "a_wins": 11, "b_wins": 12, "draws": 9, "count": 32, "a_win_ratio": 0.48}, {"model_a": "phi-4", "model_b": "gemini-2.0-flash-001", "a_wins": 39, "b_wins": 67, "draws": 54, "count": 160, "a_win_ratio": 0.37}, {"model_a": "qwq-32b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 4, "b_wins": 5, "draws": 21, "count": 30, "a_win_ratio": 0.44}, {"model_a": "gemma-3-12b", "model_b": "o3-mini", "a_wins": 15, "b_wins": 13, "draws": 16, "count": 44, "a_win_ratio": 0.54}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemini-1.5-pro-002", "a_wins": 35, "b_wins": 51, "draws": 38, "count": 124, "a_win_ratio": 0.41}, {"model_a": "phi-4", "model_b": "llama-3.1-70b", "a_wins": 53, "b_wins": 54, "draws": 51, "count": 158, "a_win_ratio": 0.5}, {"model_a": "o4-mini", "model_b": "mistral-small-3.1-24b", "a_wins": 43, "b_wins": 45, "draws": 56, "count": 144, "a_win_ratio": 0.49}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "lfm-40b", "a_wins": 40, "b_wins": 29, "draws": 39, "count": 108, "a_win_ratio": 0.58}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemini-2.0-flash-exp", "a_wins": 27, "b_wins": 51, "draws": 51, "count": 129, "a_win_ratio": 0.35}, {"model_a": "llama-3.1-405b", "model_b": "llama-3.3-70b", "a_wins": 72, "b_wins": 92, "draws": 72, "count": 236, "a_win_ratio": 0.44}, {"model_a": "gemma-3-12b", "model_b": "mistral-small-3.1-24b", "a_wins": 63, "b_wins": 39, "draws": 36, "count": 138, "a_win_ratio": 0.62}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-4-scout", "a_wins": 40, "b_wins": 38, "draws": 40, "count": 118, "a_win_ratio": 0.51}, {"model_a": "qwen2.5-7b-instruct", "model_b": "lfm-40b", "a_wins": 9, "b_wins": 11, "draws": 17, "count": 37, "a_win_ratio": 0.45}, {"model_a": "mistral-saba", "model_b": "llama-4-scout", "a_wins": 40, "b_wins": 38, "draws": 48, "count": 126, "a_win_ratio": 0.51}, {"model_a": "llama-3.1-8b", "model_b": "llama-3.3-70b", "a_wins": 68, "b_wins": 66, "draws": 55, "count": 189, "a_win_ratio": 0.51}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 2, "b_wins": 1, "draws": 2, "count": 5, "a_win_ratio": 0.67}, {"model_a": "aya-expanse-8b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 8, "b_wins": 7, "draws": 16, "count": 31, "a_win_ratio": 0.53}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 2, "b_wins": 5, "draws": 1, "count": 8, "a_win_ratio": 0.29}, {"model_a": "mistral-large-2411", "model_b": "mistral-small-3.1-24b", "a_wins": 70, "b_wins": 75, "draws": 57, "count": 202, "a_win_ratio": 0.48}, {"model_a": "llama-3.1-70b", "model_b": "gemma-2-27b-it-q8", "a_wins": 8, "b_wins": 13, "draws": 38, "count": 59, "a_win_ratio": 0.38}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 45, "b_wins": 39, "draws": 50, "count": 134, "a_win_ratio": 0.54}, {"model_a": "claude-3-7-sonnet", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 59, "b_wins": 27, "draws": 37, "count": 123, "a_win_ratio": 0.69}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "o3-mini", "a_wins": 29, "b_wins": 24, "draws": 31, "count": 84, "a_win_ratio": 0.55}, {"model_a": "grok-3-mini-beta", "model_b": "grok-3-mini-beta", "a_wins": 2, "b_wins": 2, "draws": 0, "count": 4, "a_win_ratio": 0.5}, {"model_a": "phi-4", "model_b": "jamba-1.5-large", "a_wins": 3, "b_wins": 2, "draws": 3, "count": 8, "a_win_ratio": 0.6}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 27, "b_wins": 7, "draws": 36, "count": 70, "a_win_ratio": 0.79}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gpt-4.1-mini", "a_wins": 10, "b_wins": 19, "draws": 8, "count": 37, "a_win_ratio": 0.34}, {"model_a": "lfm-40b", "model_b": "aya-expanse-8b", "a_wins": 9, "b_wins": 9, "draws": 14, "count": 32, "a_win_ratio": 0.5}, {"model_a": "c4ai-command-r-08-2024", "model_b": "lfm-40b", "a_wins": 27, "b_wins": 28, "draws": 41, "count": 96, "a_win_ratio": 0.49}, {"model_a": "qwen2.5-7b-instruct", "model_b": "ministral-8b-instruct-2410", "a_wins": 32, "b_wins": 33, "draws": 70, "count": 135, "a_win_ratio": 0.49}, {"model_a": "o4-mini", "model_b": "gemma-3-4b", "a_wins": 7, "b_wins": 10, "draws": 8, "count": 25, "a_win_ratio": 0.41}, {"model_a": "o3-mini", "model_b": "deepseek-r1", "a_wins": 46, "b_wins": 67, "draws": 64, "count": 177, "a_win_ratio": 0.41}, {"model_a": "gemma-3-4b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 9, "b_wins": 4, "draws": 6, "count": 19, "a_win_ratio": 0.69}, {"model_a": "phi-4", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 118, "b_wins": 88, "draws": 96, "count": 302, "a_win_ratio": 0.57}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-3.3-70b", "a_wins": 82, "b_wins": 45, "draws": 90, "count": 217, "a_win_ratio": 0.65}, {"model_a": "gemini-2.0-flash-001", "model_b": "deepseek-v3-chat", "a_wins": 36, "b_wins": 37, "draws": 25, "count": 98, "a_win_ratio": 0.49}, {"model_a": "mistral-nemo-2407", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 13, "b_wins": 16, "draws": 11, "count": 40, "a_win_ratio": 0.45}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "llama-3.1-405b", "a_wins": 55, "b_wins": 135, "draws": 158, "count": 348, "a_win_ratio": 0.29}, {"model_a": "qwq-32b", "model_b": "command-a", "a_wins": 10, "b_wins": 16, "draws": 12, "count": 38, "a_win_ratio": 0.38}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "qwen2.5-32b-instruct", "a_wins": 2, "b_wins": 1, "draws": 11, "count": 14, "a_win_ratio": 0.67}, {"model_a": "gemma-2-27b-it-q8", "model_b": "ministral-8b-instruct-2410", "a_wins": 16, "b_wins": 20, "draws": 26, "count": 62, "a_win_ratio": 0.44}, {"model_a": "gemma-2-9b-it", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 89, "b_wins": 75, "draws": 89, "count": 253, "a_win_ratio": 0.54}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 3, "b_wins": 2, "draws": 3, "count": 8, "a_win_ratio": 0.6}, {"model_a": "claude-3-7-sonnet", "model_b": "llama-3.1-8b", "a_wins": 41, "b_wins": 31, "draws": 43, "count": 115, "a_win_ratio": 0.57}, {"model_a": "gemini-1.5-pro-001", "model_b": "qwen2-7b-instruct", "a_wins": 8, "b_wins": 1, "draws": 5, "count": 14, "a_win_ratio": 0.89}, {"model_a": "llama-4-scout", "model_b": "llama-3.3-70b", "a_wins": 43, "b_wins": 32, "draws": 28, "count": 103, "a_win_ratio": 0.57}, {"model_a": "gpt-4.1-nano", "model_b": "llama-3.1-405b", "a_wins": 55, "b_wins": 22, "draws": 51, "count": 128, "a_win_ratio": 0.71}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "lfm-40b", "a_wins": 7, "b_wins": 9, "draws": 16, "count": 32, "a_win_ratio": 0.44}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 64, "b_wins": 29, "draws": 92, "count": 185, "a_win_ratio": 0.69}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-3.1-70b", "a_wins": 22, "b_wins": 11, "draws": 18, "count": 51, "a_win_ratio": 0.67}, {"model_a": "deepseek-r1", "model_b": "gemma-3-4b", "a_wins": 13, "b_wins": 10, "draws": 12, "count": 35, "a_win_ratio": 0.57}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemma-3-12b", "a_wins": 10, "b_wins": 12, "draws": 8, "count": 30, "a_win_ratio": 0.45}, {"model_a": "qwen2-7b-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 1, "b_wins": 2, "draws": 4, "count": 7, "a_win_ratio": 0.33}, {"model_a": "aya-expanse-32b", "model_b": "gemini-2.0-flash-001", "a_wins": 23, "b_wins": 37, "draws": 33, "count": 93, "a_win_ratio": 0.38}, {"model_a": "phi-4", "model_b": "o4-mini", "a_wins": 10, "b_wins": 13, "draws": 7, "count": 30, "a_win_ratio": 0.43}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "deepseek-r1", "a_wins": 99, "b_wins": 143, "draws": 132, "count": 374, "a_win_ratio": 0.41}, {"model_a": "llama-3.1-405b", "model_b": "phi-3.5-mini-instruct", "a_wins": 120, "b_wins": 44, "draws": 174, "count": 338, "a_win_ratio": 0.73}, {"model_a": "lfm-40b", "model_b": "gemini-2.0-flash-001", "a_wins": 11, "b_wins": 27, "draws": 25, "count": 63, "a_win_ratio": 0.29}, {"model_a": "gpt-4o-2024-08-06", "model_b": "claude-3-5-sonnet-v2", "a_wins": 142, "b_wins": 151, "draws": 187, "count": 480, "a_win_ratio": 0.48}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "c4ai-command-r-08-2024", "a_wins": 1, "b_wins": 7, "draws": 6, "count": 14, "a_win_ratio": 0.12}, {"model_a": "gemma-3-12b", "model_b": "ministral-8b-instruct-2410", "a_wins": 65, "b_wins": 39, "draws": 58, "count": 162, "a_win_ratio": 0.62}, {"model_a": "deepseek-v3-chat", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 19, "b_wins": 5, "draws": 13, "count": 37, "a_win_ratio": 0.79}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "claude-3-5-sonnet-v2", "a_wins": 56, "b_wins": 92, "draws": 78, "count": 226, "a_win_ratio": 0.38}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemma-3-4b", "a_wins": 8, "b_wins": 11, "draws": 15, "count": 34, "a_win_ratio": 0.42}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "deepseek-v3-chat", "a_wins": 0, "b_wins": 6, "draws": 6, "count": 12, "a_win_ratio": 0.0}, {"model_a": "gpt-4o-2024-08-06", "model_b": "mistral-nemo-2407", "a_wins": 72, "b_wins": 49, "draws": 73, "count": 194, "a_win_ratio": 0.6}, {"model_a": "phi-4", "model_b": "mistral-saba", "a_wins": 43, "b_wins": 36, "draws": 34, "count": 113, "a_win_ratio": 0.54}, {"model_a": "llama-3.3-70b", "model_b": "gemini-2.0-flash-exp", "a_wins": 7, "b_wins": 34, "draws": 23, "count": 64, "a_win_ratio": 0.17}, {"model_a": "llama-3.1-70b", "model_b": "llama-3.3-70b", "a_wins": 42, "b_wins": 35, "draws": 36, "count": 113, "a_win_ratio": 0.55}, {"model_a": "gemma-2-9b-it", "model_b": "gemini-2.0-flash-001", "a_wins": 10, "b_wins": 32, "draws": 18, "count": 60, "a_win_ratio": 0.24}, {"model_a": "llama-3.1-405b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 83, "b_wins": 75, "draws": 84, "count": 242, "a_win_ratio": 0.53}, {"model_a": "gpt-4.1-mini", "model_b": "deepseek-r1", "a_wins": 56, "b_wins": 48, "draws": 56, "count": 160, "a_win_ratio": 0.54}, {"model_a": "phi-4", "model_b": "claude-3-7-sonnet", "a_wins": 53, "b_wins": 34, "draws": 47, "count": 134, "a_win_ratio": 0.61}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mistral-saba", "a_wins": 43, "b_wins": 35, "draws": 45, "count": 123, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-405b", "model_b": "o3-mini", "a_wins": 8, "b_wins": 12, "draws": 10, "count": 30, "a_win_ratio": 0.4}, {"model_a": "mistral-nemo-2407", "model_b": "aya-expanse-8b", "a_wins": 18, "b_wins": 39, "draws": 32, "count": 89, "a_win_ratio": 0.32}, {"model_a": "llama-3.1-8b", "model_b": "gemma-2-9b-it", "a_wins": 63, "b_wins": 86, "draws": 93, "count": 242, "a_win_ratio": 0.42}, {"model_a": "c4ai-command-r-08-2024", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 7, "b_wins": 1, "draws": 6, "count": 14, "a_win_ratio": 0.88}, {"model_a": "qwen2.5-7b-instruct", "model_b": "gemma-2-27b-it-q8", "a_wins": 11, "b_wins": 13, "draws": 29, "count": 53, "a_win_ratio": 0.46}, {"model_a": "gemini-2.0-flash-001", "model_b": "deepseek-r1", "a_wins": 34, "b_wins": 23, "draws": 41, "count": 98, "a_win_ratio": 0.6}, {"model_a": "gpt-4.1-mini", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 38, "b_wins": 39, "draws": 41, "count": 118, "a_win_ratio": 0.49}, {"model_a": "llama-3.1-70b", "model_b": "command-a", "a_wins": 8, "b_wins": 13, "draws": 8, "count": 29, "a_win_ratio": 0.38}, {"model_a": "gemini-1.5-pro-001", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 5, "b_wins": 5, "draws": 9, "count": 19, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-405b", "model_b": "aya-expanse-32b", "a_wins": 14, "b_wins": 24, "draws": 27, "count": 65, "a_win_ratio": 0.37}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-3.3-70b", "a_wins": 73, "b_wins": 69, "draws": 62, "count": 204, "a_win_ratio": 0.51}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemma-3-12b", "a_wins": 1, "b_wins": 6, "draws": 14, "count": 21, "a_win_ratio": 0.14}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 18, "b_wins": 18, "draws": 0, "count": 36, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 49, "b_wins": 48, "draws": 54, "count": 151, "a_win_ratio": 0.51}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "phi-4", "a_wins": 79, "b_wins": 82, "draws": 85, "count": 246, "a_win_ratio": 0.49}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "c4ai-command-r-08-2024", "a_wins": 37, "b_wins": 41, "draws": 41, "count": 119, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-8b", "model_b": "o4-mini", "a_wins": 0, "b_wins": 5, "draws": 2, "count": 7, "a_win_ratio": 0.0}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gemini-1.5-pro-002", "a_wins": 4, "b_wins": 4, "draws": 7, "count": 15, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-2024-08-06", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 77, "b_wins": 64, "draws": 76, "count": 217, "a_win_ratio": 0.55}, {"model_a": "deepseek-v3-0324", "model_b": "mistral-small-3.1-24b", "a_wins": 57, "b_wins": 41, "draws": 67, "count": 165, "a_win_ratio": 0.58}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "llama-3.1-70b", "a_wins": 2, "b_wins": 0, "draws": 1, "count": 3, "a_win_ratio": 1.0}, {"model_a": "qwq-32b", "model_b": "gpt-4.1-mini", "a_wins": 1, "b_wins": 1, "draws": 1, "count": 3, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 24, "b_wins": 24, "draws": 6, "count": 54, "a_win_ratio": 0.5}, {"model_a": "gpt-4.1-mini", "model_b": "qwen3-32b", "a_wins": 2, "b_wins": 2, "draws": 2, "count": 6, "a_win_ratio": 0.5}, {"model_a": "qwen3-32b", "model_b": "gemma-3-4b", "a_wins": 1, "b_wins": 0, "draws": 0, "count": 1, "a_win_ratio": 1.0}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemini-1.5-pro-002", "a_wins": 46, "b_wins": 52, "draws": 53, "count": 151, "a_win_ratio": 0.47}, {"model_a": "command-a", "model_b": "o4-mini", "a_wins": 1, "b_wins": 3, "draws": 3, "count": 7, "a_win_ratio": 0.25}, {"model_a": "mistral-nemo-2407", "model_b": "gemini-2.0-flash-exp", "a_wins": 10, "b_wins": 61, "draws": 67, "count": 138, "a_win_ratio": 0.14}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "llama-3.1-405b", "a_wins": 1, "b_wins": 4, "draws": 3, "count": 8, "a_win_ratio": 0.2}, {"model_a": "c4ai-command-r-08-2024", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 80, "b_wins": 91, "draws": 78, "count": 249, "a_win_ratio": 0.47}, {"model_a": "phi-4", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 70, "b_wins": 50, "draws": 83, "count": 203, "a_win_ratio": 0.58}, {"model_a": "gemma-3-27b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 23, "b_wins": 8, "draws": 21, "count": 52, "a_win_ratio": 0.74}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "c4ai-command-r-08-2024", "a_wins": 19, "b_wins": 25, "draws": 26, "count": 70, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-70b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 11, "b_wins": 12, "draws": 18, "count": 41, "a_win_ratio": 0.48}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gpt-4o-2024-08-06", "a_wins": 190, "b_wins": 190, "draws": 52, "count": 432, "a_win_ratio": 0.5}, {"model_a": "qwen2-7b-instruct", "model_b": "llama-3.1-405b", "a_wins": 1, "b_wins": 5, "draws": 5, "count": 11, "a_win_ratio": 0.17}, {"model_a": "llama-3.1-8b", "model_b": "gemma-3-12b", "a_wins": 25, "b_wins": 45, "draws": 43, "count": 113, "a_win_ratio": 0.36}, {"model_a": "grok-3-mini-beta", "model_b": "mistral-small-3.1-24b", "a_wins": 4, "b_wins": 6, "draws": 3, "count": 13, "a_win_ratio": 0.4}, {"model_a": "deepseek-r1", "model_b": "qwq-32b", "a_wins": 78, "b_wins": 64, "draws": 62, "count": 204, "a_win_ratio": 0.55}, {"model_a": "gemma-3-4b", "model_b": "phi-4", "a_wins": 60, "b_wins": 45, "draws": 67, "count": 172, "a_win_ratio": 0.57}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gpt-4o-2024-08-06", "a_wins": 82, "b_wins": 106, "draws": 104, "count": 292, "a_win_ratio": 0.44}, {"model_a": "gpt-4o-2024-08-06", "model_b": "qwen2.5-7b-instruct", "a_wins": 19, "b_wins": 15, "draws": 15, "count": 49, "a_win_ratio": 0.56}, {"model_a": "gemma-3-27b", "model_b": "phi-4", "a_wins": 65, "b_wins": 25, "draws": 47, "count": 137, "a_win_ratio": 0.72}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "claude-3-5-sonnet-v2", "a_wins": 35, "b_wins": 47, "draws": 61, "count": 143, "a_win_ratio": 0.43}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "claude-3-5-sonnet-v2", "a_wins": 132, "b_wins": 138, "draws": 153, "count": 423, "a_win_ratio": 0.49}, {"model_a": "mistral-saba", "model_b": "grok-3-mini-beta", "a_wins": 1, "b_wins": 2, "draws": 3, "count": 6, "a_win_ratio": 0.33}, {"model_a": "lfm-40b", "model_b": "llama-3.3-70b", "a_wins": 26, "b_wins": 48, "draws": 40, "count": 114, "a_win_ratio": 0.35}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "c4ai-command-r-08-2024", "a_wins": 91, "b_wins": 80, "draws": 78, "count": 249, "a_win_ratio": 0.53}, {"model_a": "ministral-8b-instruct-2410", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 75, "b_wins": 95, "draws": 94, "count": 264, "a_win_ratio": 0.44}, {"model_a": "command-a", "model_b": "lfm-40b", "a_wins": 12, "b_wins": 10, "draws": 14, "count": 36, "a_win_ratio": 0.55}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "gemma-2-27b-it-q8", "a_wins": 1, "b_wins": 0, "draws": 2, "count": 3, "a_win_ratio": 1.0}, {"model_a": "deepseek-v3-0324", "model_b": "mistral-saba", "a_wins": 47, "b_wins": 33, "draws": 50, "count": 130, "a_win_ratio": 0.59}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "claude-3-5-sonnet-v2", "a_wins": 25, "b_wins": 48, "draws": 47, "count": 120, "a_win_ratio": 0.34}, {"model_a": "qwq-32b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 9, "b_wins": 11, "draws": 19, "count": 39, "a_win_ratio": 0.45}, {"model_a": "qwq-32b", "model_b": "lfm-40b", "a_wins": 11, "b_wins": 7, "draws": 11, "count": 29, "a_win_ratio": 0.61}, {"model_a": "command-a", "model_b": "gemini-1.5-pro-002", "a_wins": 18, "b_wins": 10, "draws": 15, "count": 43, "a_win_ratio": 0.64}, {"model_a": "phi-4", "model_b": "gemini-1.5-pro-002", "a_wins": 55, "b_wins": 58, "draws": 53, "count": 166, "a_win_ratio": 0.49}, {"model_a": "llama-3.1-70b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 25, "b_wins": 9, "draws": 20, "count": 54, "a_win_ratio": 0.74}, {"model_a": "o3-mini", "model_b": "gemma-2-9b-it", "a_wins": 12, "b_wins": 10, "draws": 5, "count": 27, "a_win_ratio": 0.55}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "ministral-8b-instruct-2410", "a_wins": 56, "b_wins": 67, "draws": 99, "count": 222, "a_win_ratio": 0.46}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "phi-4", "a_wins": 50, "b_wins": 61, "draws": 82, "count": 193, "a_win_ratio": 0.45}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mistral-large-2411", "a_wins": 11, "b_wins": 15, "draws": 20, "count": 46, "a_win_ratio": 0.42}, {"model_a": "c4ai-command-r-08-2024", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 14, "b_wins": 11, "draws": 14, "count": 39, "a_win_ratio": 0.56}, {"model_a": "mistral-saba", "model_b": "gemma-3-4b", "a_wins": 41, "b_wins": 40, "draws": 34, "count": 115, "a_win_ratio": 0.51}, {"model_a": "gpt-4o-2024-08-06", "model_b": "llama-3.1-8b", "a_wins": 44, "b_wins": 58, "draws": 79, "count": 181, "a_win_ratio": 0.43}, {"model_a": "deepseek-r1", "model_b": "deepseek-v3-0324", "a_wins": 12, "b_wins": 11, "draws": 9, "count": 32, "a_win_ratio": 0.52}, {"model_a": "llama-4-scout", "model_b": "deepseek-r1", "a_wins": 2, "b_wins": 4, "draws": 3, "count": 9, "a_win_ratio": 0.33}, {"model_a": "gemma-2-9b-it", "model_b": "qwen2.5-32b-instruct", "a_wins": 4, "b_wins": 3, "draws": 5, "count": 12, "a_win_ratio": 0.57}, {"model_a": "lfm-40b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 31, "b_wins": 45, "draws": 47, "count": 123, "a_win_ratio": 0.41}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "c4ai-command-r-08-2024", "a_wins": 40, "b_wins": 25, "draws": 40, "count": 105, "a_win_ratio": 0.62}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mistral-saba", "a_wins": 47, "b_wins": 52, "draws": 33, "count": 132, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-8b", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 0, "draws": 0, "count": 1, "a_win_ratio": 1.0}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gpt-4.1-nano", "a_wins": 6, "b_wins": 5, "draws": 5, "count": 16, "a_win_ratio": 0.55}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 39, "b_wins": 44, "draws": 43, "count": 126, "a_win_ratio": 0.47}, {"model_a": "gemma-2-9b-it", "model_b": "gemma-3-12b", "a_wins": 8, "b_wins": 12, "draws": 9, "count": 29, "a_win_ratio": 0.4}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 35, "b_wins": 29, "draws": 39, "count": 103, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-8b", "model_b": "qwen2.5-7b-instruct", "a_wins": 26, "b_wins": 41, "draws": 52, "count": 119, "a_win_ratio": 0.39}, {"model_a": "llama-3.3-70b", "model_b": "gemma-3-4b", "a_wins": 52, "b_wins": 70, "draws": 85, "count": 207, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-70b", "model_b": "gemini-2.0-flash-exp", "a_wins": 24, "b_wins": 40, "draws": 41, "count": 105, "a_win_ratio": 0.38}, {"model_a": "llama-3.3-70b", "model_b": "lfm-40b", "a_wins": 48, "b_wins": 26, "draws": 40, "count": 114, "a_win_ratio": 0.65}, {"model_a": "gpt-4.1-nano", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 38, "b_wins": 50, "draws": 39, "count": 127, "a_win_ratio": 0.43}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "claude-3-7-sonnet", "a_wins": 28, "b_wins": 32, "draws": 35, "count": 95, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-8b", "model_b": "c4ai-command-r-08-2024", "a_wins": 68, "b_wins": 75, "draws": 70, "count": 213, "a_win_ratio": 0.48}, {"model_a": "command-a", "model_b": "deepseek-v3-chat", "a_wins": 17, "b_wins": 9, "draws": 15, "count": 41, "a_win_ratio": 0.65}, {"model_a": "gpt-4o-2024-08-06", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 149, "b_wins": 88, "draws": 155, "count": 392, "a_win_ratio": 0.63}, {"model_a": "gemma-2-9b-it", "model_b": "ministral-8b-instruct-2410", "a_wins": 88, "b_wins": 77, "draws": 95, "count": 260, "a_win_ratio": 0.53}, {"model_a": "o3-mini", "model_b": "ministral-8b-instruct-2410", "a_wins": 17, "b_wins": 17, "draws": 19, "count": 53, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "mistral-nemo-2407", "a_wins": 64, "b_wins": 60, "draws": 83, "count": 207, "a_win_ratio": 0.52}, {"model_a": "ministral-8b-instruct-2410", "model_b": "o4-mini", "a_wins": 17, "b_wins": 21, "draws": 23, "count": 61, "a_win_ratio": 0.45}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemini-2.0-flash-001", "a_wins": 5, "b_wins": 31, "draws": 14, "count": 50, "a_win_ratio": 0.14}, {"model_a": "command-a", "model_b": "llama-3.1-405b", "a_wins": 56, "b_wins": 20, "draws": 42, "count": 118, "a_win_ratio": 0.74}, {"model_a": "gemma-2-9b-it", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 13, "b_wins": 12, "draws": 8, "count": 33, "a_win_ratio": 0.52}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "grok-3-mini-beta", "a_wins": 0, "b_wins": 3, "draws": 0, "count": 3, "a_win_ratio": 0.0}, {"model_a": "qwq-32b", "model_b": "o4-mini", "a_wins": 26, "b_wins": 19, "draws": 11, "count": 56, "a_win_ratio": 0.58}, {"model_a": "gpt-4.1-mini", "model_b": "phi-4", "a_wins": 57, "b_wins": 43, "draws": 50, "count": 150, "a_win_ratio": 0.57}, {"model_a": "qwen2-7b-instruct", "model_b": "Yi-1.5-9B-Chat", "a_wins": 1, "b_wins": 2, "draws": 7, "count": 10, "a_win_ratio": 0.33}, {"model_a": "gpt-4.1-mini", "model_b": "llama-4-scout", "a_wins": 43, "b_wins": 37, "draws": 38, "count": 118, "a_win_ratio": 0.54}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemma-2-9b-it", "a_wins": 75, "b_wins": 89, "draws": 89, "count": 253, "a_win_ratio": 0.46}, {"model_a": "mistral-nemo-2407", "model_b": "lfm-40b", "a_wins": 60, "b_wins": 64, "draws": 83, "count": 207, "a_win_ratio": 0.48}, {"model_a": "command-a", "model_b": "claude-3-7-sonnet", "a_wins": 46, "b_wins": 40, "draws": 47, "count": 133, "a_win_ratio": 0.53}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemma-3-12b", "a_wins": 39, "b_wins": 65, "draws": 58, "count": 162, "a_win_ratio": 0.38}, {"model_a": "qwen2.5-32b-instruct", "model_b": "llama-3.1-70b", "a_wins": 2, "b_wins": 5, "draws": 5, "count": 12, "a_win_ratio": 0.29}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "c4ai-command-r-08-2024", "a_wins": 95, "b_wins": 63, "draws": 93, "count": 251, "a_win_ratio": 0.6}, {"model_a": "gemini-2.0-flash-exp", "model_b": "llama-3.3-70b", "a_wins": 34, "b_wins": 7, "draws": 23, "count": 64, "a_win_ratio": 0.83}, {"model_a": "o3-mini", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 17, "b_wins": 4, "draws": 9, "count": 30, "a_win_ratio": 0.81}, {"model_a": "gemini-2.0-flash-exp", "model_b": "llama-3.1-8b", "a_wins": 50, "b_wins": 23, "draws": 54, "count": 127, "a_win_ratio": 0.68}, {"model_a": "ministral-8b-instruct-2410", "model_b": "command-a", "a_wins": 38, "b_wins": 56, "draws": 51, "count": 145, "a_win_ratio": 0.4}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 41, "b_wins": 55, "draws": 51, "count": 147, "a_win_ratio": 0.43}, {"model_a": "mistral-nemo-2407", "model_b": "llama-3.1-70b", "a_wins": 65, "b_wins": 155, "draws": 151, "count": 371, "a_win_ratio": 0.3}, {"model_a": "lfm-40b", "model_b": "gemini-1.5-pro-001", "a_wins": 10, "b_wins": 26, "draws": 46, "count": 82, "a_win_ratio": 0.28}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemma-2-9b-it", "a_wins": 42, "b_wins": 58, "draws": 44, "count": 144, "a_win_ratio": 0.42}, {"model_a": "o3-mini", "model_b": "gemini-2.0-flash-001", "a_wins": 13, "b_wins": 24, "draws": 21, "count": 58, "a_win_ratio": 0.35}, {"model_a": "phi-4", "model_b": "qwq-32b", "a_wins": 8, "b_wins": 6, "draws": 13, "count": 27, "a_win_ratio": 0.57}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "ministral-8b-instruct-2410", "a_wins": 95, "b_wins": 75, "draws": 94, "count": 264, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "phi-4", "a_wins": 10, "b_wins": 14, "draws": 8, "count": 32, "a_win_ratio": 0.42}, {"model_a": "gemini-2.0-flash-exp", "model_b": "deepseek-v3-chat", "a_wins": 48, "b_wins": 51, "draws": 54, "count": 153, "a_win_ratio": 0.48}, {"model_a": "gemma-3-4b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 69, "b_wins": 32, "draws": 57, "count": 158, "a_win_ratio": 0.68}, {"model_a": "gemma-3-12b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 45, "b_wins": 45, "draws": 52, "count": 142, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "llama-3.1-405b", "a_wins": 64, "b_wins": 58, "draws": 74, "count": 196, "a_win_ratio": 0.52}, {"model_a": "lfm-40b", "model_b": "qwen2.5-7b-instruct", "a_wins": 11, "b_wins": 9, "draws": 17, "count": 37, "a_win_ratio": 0.55}, {"model_a": "qwq-32b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 13, "b_wins": 8, "draws": 13, "count": 34, "a_win_ratio": 0.62}, {"model_a": "llama-3.1-8b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 73, "b_wins": 56, "draws": 88, "count": 217, "a_win_ratio": 0.57}, {"model_a": "o4-mini", "model_b": "phi-4", "a_wins": 13, "b_wins": 10, "draws": 7, "count": 30, "a_win_ratio": 0.57}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gemini-2.0-flash-exp", "a_wins": 56, "b_wins": 105, "draws": 101, "count": 262, "a_win_ratio": 0.35}, {"model_a": "qwq-32b", "model_b": "gemma-3-27b", "a_wins": 2, "b_wins": 19, "draws": 8, "count": 29, "a_win_ratio": 0.1}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "ministral-8b-instruct-2410", "a_wins": 93, "b_wins": 93, "draws": 98, "count": 284, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-8b", "model_b": "qwq-32b", "a_wins": 6, "b_wins": 7, "draws": 6, "count": 19, "a_win_ratio": 0.46}, {"model_a": "gemini-1.5-pro-001", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 11, "b_wins": 12, "draws": 22, "count": 45, "a_win_ratio": 0.48}, {"model_a": "gemini-1.5-pro-002", "model_b": "ministral-8b-instruct-2410", "a_wins": 61, "b_wins": 50, "draws": 64, "count": 175, "a_win_ratio": 0.55}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "command-a", "a_wins": 8, "b_wins": 14, "draws": 16, "count": 38, "a_win_ratio": 0.36}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "deepseek-v3-chat", "a_wins": 44, "b_wins": 70, "draws": 74, "count": 188, "a_win_ratio": 0.39}, {"model_a": "grok-3-mini-beta", "model_b": "gemma-3-12b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "lfm-40b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 9, "b_wins": 7, "draws": 16, "count": 32, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemma-3-4b", "a_wins": 5, "b_wins": 13, "draws": 7, "count": 25, "a_win_ratio": 0.28}, {"model_a": "command-a", "model_b": "gemma-3-4b", "a_wins": 49, "b_wins": 45, "draws": 31, "count": 125, "a_win_ratio": 0.52}, {"model_a": "gpt-4.1-nano", "model_b": "phi-4", "a_wins": 46, "b_wins": 29, "draws": 35, "count": 110, "a_win_ratio": 0.61}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "qwq-32b", "a_wins": 7, "b_wins": 9, "draws": 17, "count": 33, "a_win_ratio": 0.44}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mistral-small-3.1-24b", "a_wins": 54, "b_wins": 61, "draws": 50, "count": 165, "a_win_ratio": 0.47}, {"model_a": "gemini-2.0-flash-exp", "model_b": "c4ai-command-r-08-2024", "a_wins": 19, "b_wins": 11, "draws": 24, "count": 54, "a_win_ratio": 0.63}, {"model_a": "lfm-40b", "model_b": "gemma-3-12b", "a_wins": 2, "b_wins": 5, "draws": 10, "count": 17, "a_win_ratio": 0.29}, {"model_a": "gpt-4.1-mini", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 54, "b_wins": 33, "draws": 34, "count": 121, "a_win_ratio": 0.62}, {"model_a": "aya-expanse-8b", "model_b": "gemma-2-9b-it", "a_wins": 13, "b_wins": 21, "draws": 16, "count": 50, "a_win_ratio": 0.38}, {"model_a": "jamba-1.5-large", "model_b": "phi-4", "a_wins": 2, "b_wins": 3, "draws": 3, "count": 8, "a_win_ratio": 0.4}, {"model_a": "mistral-nemo-2407", "model_b": "llama-3.3-70b", "a_wins": 21, "b_wins": 45, "draws": 37, "count": 103, "a_win_ratio": 0.32}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mistral-large-2411", "a_wins": 99, "b_wins": 133, "draws": 112, "count": 344, "a_win_ratio": 0.43}, {"model_a": "gemma-3-4b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 13, "b_wins": 5, "draws": 7, "count": 25, "a_win_ratio": 0.72}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "qwen2.5-7b-instruct", "a_wins": 29, "b_wins": 33, "draws": 53, "count": 115, "a_win_ratio": 0.47}, {"model_a": "gemma-3-4b", "model_b": "mistral-large-2411", "a_wins": 49, "b_wins": 44, "draws": 52, "count": 145, "a_win_ratio": 0.53}, {"model_a": "phi-4", "model_b": "ministral-8b-instruct-2410", "a_wins": 95, "b_wins": 87, "draws": 101, "count": 283, "a_win_ratio": 0.52}, {"model_a": "gemma-3-12b", "model_b": "llama-3.1-70b", "a_wins": 12, "b_wins": 6, "draws": 10, "count": 28, "a_win_ratio": 0.67}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "llama-3.1-70b", "a_wins": 46, "b_wins": 62, "draws": 104, "count": 212, "a_win_ratio": 0.43}, {"model_a": "qwen2.5-7b-instruct", "model_b": "llama-3.1-70b", "a_wins": 28, "b_wins": 36, "draws": 66, "count": 130, "a_win_ratio": 0.44}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-4-scout", "a_wins": 51, "b_wins": 31, "draws": 46, "count": 128, "a_win_ratio": 0.62}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "aya-expanse-8b", "a_wins": 25, "b_wins": 19, "draws": 29, "count": 73, "a_win_ratio": 0.57}, {"model_a": "jamba-1.5-large", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 3, "b_wins": 2, "draws": 1, "count": 6, "a_win_ratio": 0.6}, {"model_a": "gemma-3-27b", "model_b": "gemma-3-4b", "a_wins": 65, "b_wins": 49, "draws": 41, "count": 155, "a_win_ratio": 0.57}, {"model_a": "deepseek-r1", "model_b": "gpt-4.1-nano", "a_wins": 18, "b_wins": 15, "draws": 7, "count": 40, "a_win_ratio": 0.55}, {"model_a": "c4ai-command-r-08-2024", "model_b": "aya-expanse-32b", "a_wins": 19, "b_wins": 33, "draws": 25, "count": 77, "a_win_ratio": 0.37}, {"model_a": "gpt-4.1-mini", "model_b": "ministral-8b-instruct-2410", "a_wins": 89, "b_wins": 91, "draws": 97, "count": 277, "a_win_ratio": 0.49}, {"model_a": "qwen2-7b-instruct", "model_b": "llama-3.1-8b", "a_wins": 0, "b_wins": 2, "draws": 4, "count": 6, "a_win_ratio": 0.0}, {"model_a": "gemini-2.0-flash-exp", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 54, "b_wins": 24, "draws": 45, "count": 123, "a_win_ratio": 0.69}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemini-2.0-flash-001", "a_wins": 44, "b_wins": 93, "draws": 73, "count": 210, "a_win_ratio": 0.32}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemma-2-9b-it", "a_wins": 33, "b_wins": 29, "draws": 26, "count": 88, "a_win_ratio": 0.53}, {"model_a": "gemma-3-4b", "model_b": "gemma-3-12b", "a_wins": 32, "b_wins": 59, "draws": 35, "count": 126, "a_win_ratio": 0.35}, {"model_a": "llama-3.3-70b", "model_b": "llama-3.1-405b", "a_wins": 92, "b_wins": 72, "draws": 72, "count": 236, "a_win_ratio": 0.56}, {"model_a": "gemini-1.5-pro-002", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 47, "b_wins": 44, "draws": 63, "count": 154, "a_win_ratio": 0.52}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "mistral-large-2411", "a_wins": 15, "b_wins": 33, "draws": 27, "count": 75, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-405b", "model_b": "gemma-3-27b", "a_wins": 37, "b_wins": 68, "draws": 56, "count": 161, "a_win_ratio": 0.35}, {"model_a": "o4-mini", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 3, "b_wins": 2, "draws": 5, "count": 10, "a_win_ratio": 0.6}, {"model_a": "gemma-2-27b-it-q8", "model_b": "qwen2.5-7b-instruct", "a_wins": 13, "b_wins": 11, "draws": 29, "count": 53, "a_win_ratio": 0.54}, {"model_a": "ministral-8b-instruct-2410", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 93, "b_wins": 93, "draws": 98, "count": 284, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "llama-3.1-8b", "a_wins": 82, "b_wins": 49, "draws": 85, "count": 216, "a_win_ratio": 0.63}, {"model_a": "deepseek-r1", "model_b": "aya-expanse-32b", "a_wins": 1, "b_wins": 1, "draws": 3, "count": 5, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "o3-mini", "a_wins": 11, "b_wins": 8, "draws": 15, "count": 34, "a_win_ratio": 0.58}, {"model_a": "mistral-nemo-2407", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 35, "b_wins": 55, "draws": 62, "count": 152, "a_win_ratio": 0.39}, {"model_a": "qwq-32b", "model_b": "llama-3.1-405b", "a_wins": 11, "b_wins": 10, "draws": 13, "count": 34, "a_win_ratio": 0.52}, {"model_a": "o4-mini", "model_b": "o4-mini", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "deepseek-v3-0324", "model_b": "gemma-3-4b", "a_wins": 75, "b_wins": 50, "draws": 66, "count": 191, "a_win_ratio": 0.6}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemini-2.0-flash-exp", "a_wins": 18, "b_wins": 40, "draws": 30, "count": 88, "a_win_ratio": 0.31}, {"model_a": "gemini-2.0-flash-exp", "model_b": "llama-3.1-405b", "a_wins": 49, "b_wins": 27, "draws": 50, "count": 126, "a_win_ratio": 0.64}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemma-2-9b-it", "a_wins": 42, "b_wins": 37, "draws": 42, "count": 121, "a_win_ratio": 0.53}, {"model_a": "gemma-2-9b-it", "model_b": "phi-3.5-mini-instruct", "a_wins": 65, "b_wins": 32, "draws": 72, "count": 169, "a_win_ratio": 0.67}, {"model_a": "gemma-3-4b", "model_b": "gemini-2.0-flash-001", "a_wins": 37, "b_wins": 55, "draws": 59, "count": 151, "a_win_ratio": 0.4}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemma-2-9b-it", "a_wins": 53, "b_wins": 108, "draws": 138, "count": 299, "a_win_ratio": 0.33}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gemini-1.5-pro-002", "a_wins": 9, "b_wins": 11, "draws": 13, "count": 33, "a_win_ratio": 0.45}, {"model_a": "command-a", "model_b": "llama-4-scout", "a_wins": 42, "b_wins": 22, "draws": 29, "count": 93, "a_win_ratio": 0.66}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "qwq-32b", "a_wins": 10, "b_wins": 8, "draws": 10, "count": 28, "a_win_ratio": 0.56}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "grok-3-mini-beta", "a_wins": 45, "b_wins": 56, "draws": 63, "count": 164, "a_win_ratio": 0.45}, {"model_a": "gpt-4.1-nano", "model_b": "gemini-2.0-flash-001", "a_wins": 68, "b_wins": 77, "draws": 58, "count": 203, "a_win_ratio": 0.47}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "qwen2-7b-instruct", "a_wins": 2, "b_wins": 1, "draws": 4, "count": 7, "a_win_ratio": 0.67}, {"model_a": "command-a", "model_b": "c4ai-command-r-08-2024", "a_wins": 70, "b_wins": 43, "draws": 49, "count": 162, "a_win_ratio": 0.62}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemma-2-9b-it", "a_wins": 57, "b_wins": 38, "draws": 56, "count": 151, "a_win_ratio": 0.6}, {"model_a": "claude-3-7-sonnet", "model_b": "mistral-large-2411", "a_wins": 62, "b_wins": 45, "draws": 57, "count": 164, "a_win_ratio": 0.58}, {"model_a": "gemini-2.0-flash-001", "model_b": "grok-3-mini-beta", "a_wins": 10, "b_wins": 12, "draws": 20, "count": 42, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-405b", "model_b": "qwen2-7b-instruct", "a_wins": 5, "b_wins": 1, "draws": 5, "count": 11, "a_win_ratio": 0.83}, {"model_a": "gemini-1.5-pro-001", "model_b": "lfm-40b", "a_wins": 26, "b_wins": 10, "draws": 46, "count": 82, "a_win_ratio": 0.72}, {"model_a": "mistral-small-3.1-24b", "model_b": "deepseek-r1", "a_wins": 26, "b_wins": 31, "draws": 29, "count": 86, "a_win_ratio": 0.46}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "llama-3.1-70b", "a_wins": 75, "b_wins": 66, "draws": 82, "count": 223, "a_win_ratio": 0.53}, {"model_a": "deepseek-r1", "model_b": "claude-3-5-sonnet-v2", "a_wins": 53, "b_wins": 27, "draws": 50, "count": 130, "a_win_ratio": 0.66}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 44, "b_wins": 35, "draws": 36, "count": 115, "a_win_ratio": 0.56}, {"model_a": "o3-mini", "model_b": "llama-3.1-70b", "a_wins": 10, "b_wins": 10, "draws": 11, "count": 31, "a_win_ratio": 0.5}, {"model_a": "gemma-2-27b-it-q8", "model_b": "phi-3.5-mini-instruct", "a_wins": 24, "b_wins": 18, "draws": 33, "count": 75, "a_win_ratio": 0.57}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "c4ai-command-r-08-2024", "a_wins": 37, "b_wins": 37, "draws": 42, "count": 116, "a_win_ratio": 0.5}, {"model_a": "gemma-3-4b", "model_b": "o3-mini", "a_wins": 12, "b_wins": 9, "draws": 11, "count": 32, "a_win_ratio": 0.57}, {"model_a": "claude-3-7-sonnet", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 2, "draws": 2, "count": 5, "a_win_ratio": 0.33}, {"model_a": "gemini-1.5-pro-001", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 2, "b_wins": 3, "draws": 3, "count": 8, "a_win_ratio": 0.4}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemini-1.5-pro-002", "a_wins": 139, "b_wins": 160, "draws": 139, "count": 438, "a_win_ratio": 0.46}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mistral-large-2411", "a_wins": 77, "b_wins": 93, "draws": 84, "count": 254, "a_win_ratio": 0.45}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemma-3-12b", "a_wins": 37, "b_wins": 53, "draws": 63, "count": 153, "a_win_ratio": 0.41}, {"model_a": "qwen2-7b-instruct", "model_b": "gemma-2-9b-it", "a_wins": 0, "b_wins": 1, "draws": 2, "count": 3, "a_win_ratio": 0.0}, {"model_a": "llama-4-scout", "model_b": "mistral-small-3.1-24b", "a_wins": 53, "b_wins": 46, "draws": 48, "count": 147, "a_win_ratio": 0.54}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 1, "b_wins": 2, "draws": 1, "count": 4, "a_win_ratio": 0.33}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemini-2.0-flash-exp", "a_wins": 16, "b_wins": 49, "draws": 38, "count": 103, "a_win_ratio": 0.25}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "phi-4", "a_wins": 46, "b_wins": 52, "draws": 33, "count": 131, "a_win_ratio": 0.47}, {"model_a": "lfm-40b", "model_b": "gemini-1.5-pro-002", "a_wins": 31, "b_wins": 52, "draws": 43, "count": 126, "a_win_ratio": 0.37}, {"model_a": "qwen2.5-7b-instruct", "model_b": "gemma-2-9b-it", "a_wins": 30, "b_wins": 35, "draws": 39, "count": 104, "a_win_ratio": 0.46}, {"model_a": "claude-3-7-sonnet", "model_b": "qwq-32b", "a_wins": 1, "b_wins": 2, "draws": 2, "count": 5, "a_win_ratio": 0.33}, {"model_a": "llama-3.3-70b", "model_b": "deepseek-r1", "a_wins": 20, "b_wins": 9, "draws": 9, "count": 38, "a_win_ratio": 0.69}, {"model_a": "phi-3.5-mini-instruct", "model_b": "claude-3-5-sonnet-v2", "a_wins": 5, "b_wins": 5, "draws": 4, "count": 14, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-405b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 33, "b_wins": 10, "draws": 40, "count": 83, "a_win_ratio": 0.77}, {"model_a": "gemini-2.0-flash-001", "model_b": "mistral-nemo-2407", "a_wins": 30, "b_wins": 13, "draws": 31, "count": 74, "a_win_ratio": 0.7}, {"model_a": "o3-mini", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 57, "b_wins": 58, "draws": 47, "count": 162, "a_win_ratio": 0.5}, {"model_a": "deepseek-v3-chat", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 33, "b_wins": 13, "draws": 37, "count": 83, "a_win_ratio": 0.72}, {"model_a": "mistral-nemo-2407", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 54, "b_wins": 68, "draws": 89, "count": 211, "a_win_ratio": 0.44}, {"model_a": "qwen2.5-7b-instruct", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 1, "b_wins": 3, "draws": 1, "count": 5, "a_win_ratio": 0.25}, {"model_a": "deepseek-v3-chat", "model_b": "claude-3-5-sonnet-v2", "a_wins": 165, "b_wins": 115, "draws": 139, "count": 419, "a_win_ratio": 0.59}, {"model_a": "lfm-40b", "model_b": "qwen2.5-32b-instruct", "a_wins": 0, "b_wins": 10, "draws": 4, "count": 14, "a_win_ratio": 0.0}, {"model_a": "gemini-2.0-flash-001", "model_b": "command-a", "a_wins": 64, "b_wins": 41, "draws": 55, "count": 160, "a_win_ratio": 0.61}, {"model_a": "llama-3.3-70b", "model_b": "gemini-2.0-flash-001", "a_wins": 45, "b_wins": 82, "draws": 90, "count": 217, "a_win_ratio": 0.35}, {"model_a": "gpt-4.1-nano", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 5, "b_wins": 6, "draws": 5, "count": 16, "a_win_ratio": 0.45}, {"model_a": "gemini-1.5-pro-001", "model_b": "llama-3.1-70b", "a_wins": 61, "b_wins": 48, "draws": 120, "count": 229, "a_win_ratio": 0.56}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "qwen3-32b", "a_wins": 2, "b_wins": 4, "draws": 0, "count": 6, "a_win_ratio": 0.33}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mistral-nemo-2407", "a_wins": 91, "b_wins": 53, "draws": 78, "count": 222, "a_win_ratio": 0.63}, {"model_a": "llama-3.3-70b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 43, "b_wins": 36, "draws": 52, "count": 131, "a_win_ratio": 0.54}, {"model_a": "command-a", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 7, "b_wins": 3, "draws": 14, "count": 24, "a_win_ratio": 0.7}, {"model_a": "llama-3.1-70b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 62, "b_wins": 46, "draws": 104, "count": 212, "a_win_ratio": 0.57}, {"model_a": "lfm-40b", "model_b": "llama-3.1-70b", "a_wins": 34, "b_wins": 68, "draws": 62, "count": 164, "a_win_ratio": 0.33}, {"model_a": "qwq-32b", "model_b": "deepseek-v3-chat", "a_wins": 7, "b_wins": 8, "draws": 16, "count": 31, "a_win_ratio": 0.47}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-4-scout", "a_wins": 33, "b_wins": 46, "draws": 33, "count": 112, "a_win_ratio": 0.42}, {"model_a": "qwen3-32b", "model_b": "gemini-2.0-flash-001", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-8b", "model_b": "mistral-large-2411", "a_wins": 32, "b_wins": 28, "draws": 25, "count": 85, "a_win_ratio": 0.53}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gemma-3-4b", "a_wins": 29, "b_wins": 64, "draws": 53, "count": 146, "a_win_ratio": 0.31}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gemini-1.5-pro-001", "a_wins": 40, "b_wins": 84, "draws": 106, "count": 230, "a_win_ratio": 0.32}, {"model_a": "aya-expanse-32b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 29, "b_wins": 23, "draws": 37, "count": 89, "a_win_ratio": 0.56}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 1, "b_wins": 3, "draws": 0, "count": 4, "a_win_ratio": 0.25}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gemini-2.0-flash-exp", "a_wins": 10, "b_wins": 25, "draws": 19, "count": 54, "a_win_ratio": 0.29}, {"model_a": "gemma-2-27b-it-q8", "model_b": "llama-3.1-70b", "a_wins": 13, "b_wins": 8, "draws": 38, "count": 59, "a_win_ratio": 0.62}, {"model_a": "gemma-2-9b-it", "model_b": "lfm-40b", "a_wins": 39, "b_wins": 29, "draws": 72, "count": 140, "a_win_ratio": 0.57}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-4-scout", "a_wins": 46, "b_wins": 53, "draws": 48, "count": 147, "a_win_ratio": 0.46}, {"model_a": "llama-3.1-405b", "model_b": "claude-3-7-sonnet", "a_wins": 23, "b_wins": 41, "draws": 47, "count": 111, "a_win_ratio": 0.36}, {"model_a": "llama-3.1-70b", "model_b": "deepseek-v3-chat", "a_wins": 36, "b_wins": 50, "draws": 44, "count": 130, "a_win_ratio": 0.42}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 81, "b_wins": 34, "draws": 54, "count": 169, "a_win_ratio": 0.7}, {"model_a": "mistral-large-2411", "model_b": "llama-4-scout", "a_wins": 42, "b_wins": 34, "draws": 33, "count": 109, "a_win_ratio": 0.55}, {"model_a": "lfm-40b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 41, "b_wins": 56, "draws": 74, "count": 171, "a_win_ratio": 0.42}, {"model_a": "mistral-large-2411", "model_b": "grok-3-mini-beta", "a_wins": 13, "b_wins": 9, "draws": 10, "count": 32, "a_win_ratio": 0.59}, {"model_a": "aya-expanse-8b", "model_b": "gpt-4o-2024-08-06", "a_wins": 33, "b_wins": 29, "draws": 20, "count": 82, "a_win_ratio": 0.53}, {"model_a": "llama-3.3-70b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 74, "b_wins": 70, "draws": 95, "count": 239, "a_win_ratio": 0.51}, {"model_a": "o3-mini", "model_b": "o3-mini", "a_wins": 1, "b_wins": 1, "draws": 2, "count": 4, "a_win_ratio": 0.5}, {"model_a": "aya-expanse-32b", "model_b": "gemma-3-12b", "a_wins": 24, "b_wins": 28, "draws": 21, "count": 73, "a_win_ratio": 0.46}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mistral-saba", "a_wins": 46, "b_wins": 45, "draws": 33, "count": 124, "a_win_ratio": 0.51}, {"model_a": "llama-4-scout", "model_b": "qwq-32b", "a_wins": 0, "b_wins": 1, "draws": 0, "count": 1, "a_win_ratio": 0.0}, {"model_a": "qwen3-32b", "model_b": "llama-4-scout", "a_wins": 0, "b_wins": 0, "draws": 2, "count": 2, "a_win_ratio": null}, {"model_a": "o4-mini", "model_b": "llama-3.3-70b", "a_wins": 1, "b_wins": 2, "draws": 4, "count": 7, "a_win_ratio": 0.33}, {"model_a": "o3-mini", "model_b": "claude-3-5-sonnet-v2", "a_wins": 22, "b_wins": 22, "draws": 16, "count": 60, "a_win_ratio": 0.5}, {"model_a": "qwq-32b", "model_b": "llama-3.1-8b", "a_wins": 7, "b_wins": 6, "draws": 6, "count": 19, "a_win_ratio": 0.54}, {"model_a": "jamba-1.5-large", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 2, "b_wins": 3, "draws": 3, "count": 8, "a_win_ratio": 0.4}, {"model_a": "gpt-4.1-mini", "model_b": "llama-3.3-70b", "a_wins": 42, "b_wins": 38, "draws": 45, "count": 125, "a_win_ratio": 0.52}, {"model_a": "gemini-2.0-flash-exp", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 32, "b_wins": 21, "draws": 33, "count": 86, "a_win_ratio": 0.6}, {"model_a": "llama-3.1-70b", "model_b": "gemini-1.5-pro-001", "a_wins": 48, "b_wins": 61, "draws": 120, "count": 229, "a_win_ratio": 0.44}, {"model_a": "gpt-4o-2024-08-06", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 2, "draws": 2, "count": 6, "a_win_ratio": 0.5}, {"model_a": "gemma-2-27b-it-q8", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 13, "b_wins": 3, "draws": 16, "count": 32, "a_win_ratio": 0.81}, {"model_a": "phi-4", "model_b": "command-a", "a_wins": 49, "b_wins": 57, "draws": 43, "count": 149, "a_win_ratio": 0.46}, {"model_a": "o4-mini", "model_b": "claude-3-7-sonnet", "a_wins": 48, "b_wins": 62, "draws": 50, "count": 160, "a_win_ratio": 0.44}, {"model_a": "grok-3-mini-beta", "model_b": "gemma-3-4b", "a_wins": 3, "b_wins": 0, "draws": 5, "count": 8, "a_win_ratio": 1.0}, {"model_a": "deepseek-r1", "model_b": "llama-3.1-70b", "a_wins": 13, "b_wins": 9, "draws": 11, "count": 33, "a_win_ratio": 0.59}, {"model_a": "gpt-4.1-mini", "model_b": "llama-3.1-405b", "a_wins": 54, "b_wins": 13, "draws": 59, "count": 126, "a_win_ratio": 0.81}, {"model_a": "llama-3.3-70b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 36, "b_wins": 49, "draws": 49, "count": 134, "a_win_ratio": 0.42}, {"model_a": "mistral-small-3.1-24b", "model_b": "lfm-40b", "a_wins": 14, "b_wins": 9, "draws": 9, "count": 32, "a_win_ratio": 0.61}, {"model_a": "llama-3.3-70b", "model_b": "gpt-4.1-mini", "a_wins": 38, "b_wins": 42, "draws": 45, "count": 125, "a_win_ratio": 0.48}, {"model_a": "phi-3.5-mini-instruct", "model_b": "ministral-8b-instruct-2410", "a_wins": 22, "b_wins": 49, "draws": 71, "count": 142, "a_win_ratio": 0.31}, {"model_a": "llama-3.3-70b", "model_b": "gemma-3-27b", "a_wins": 29, "b_wins": 75, "draws": 52, "count": 156, "a_win_ratio": 0.28}, {"model_a": "o4-mini", "model_b": "qwq-32b", "a_wins": 19, "b_wins": 26, "draws": 11, "count": 56, "a_win_ratio": 0.42}, {"model_a": "mistral-nemo-2407", "model_b": "qwen2-7b-instruct", "a_wins": 0, "b_wins": 1, "draws": 1, "count": 2, "a_win_ratio": 0.0}, {"model_a": "o3-mini", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 8, "b_wins": 11, "draws": 15, "count": 34, "a_win_ratio": 0.42}, {"model_a": "gemini-1.5-pro-001", "model_b": "gpt-4o-2024-08-06", "a_wins": 13, "b_wins": 5, "draws": 12, "count": 30, "a_win_ratio": 0.72}, {"model_a": "mistral-large-2411", "model_b": "gemma-3-4b", "a_wins": 44, "b_wins": 49, "draws": 52, "count": 145, "a_win_ratio": 0.47}, {"model_a": "gpt-4.1-mini", "model_b": "c4ai-command-r-08-2024", "a_wins": 50, "b_wins": 41, "draws": 42, "count": 133, "a_win_ratio": 0.55}, {"model_a": "llama-3.1-405b", "model_b": "gemma-2-27b-it-q8", "a_wins": 12, "b_wins": 20, "draws": 20, "count": 52, "a_win_ratio": 0.38}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "llama-3.1-405b", "a_wins": 85, "b_wins": 76, "draws": 117, "count": 278, "a_win_ratio": 0.53}, {"model_a": "gemma-3-4b", "model_b": "llama-3.3-70b", "a_wins": 70, "b_wins": 52, "draws": 85, "count": 207, "a_win_ratio": 0.57}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "Yi-1.5-9B-Chat", "a_wins": 5, "b_wins": 2, "draws": 1, "count": 8, "a_win_ratio": 0.71}, {"model_a": "gemma-3-12b", "model_b": "mistral-saba", "a_wins": 49, "b_wins": 22, "draws": 39, "count": 110, "a_win_ratio": 0.69}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "gpt-4o-2024-08-06", "a_wins": 22, "b_wins": 40, "draws": 36, "count": 98, "a_win_ratio": 0.35}, {"model_a": "llama-3.1-405b", "model_b": "mistral-saba", "a_wins": 19, "b_wins": 38, "draws": 36, "count": 93, "a_win_ratio": 0.33}, {"model_a": "o4-mini", "model_b": "c4ai-command-r-08-2024", "a_wins": 0, "b_wins": 4, "draws": 1, "count": 5, "a_win_ratio": 0.0}, {"model_a": "jamba-1.5-large", "model_b": "gemini-1.5-pro-002", "a_wins": 5, "b_wins": 2, "draws": 2, "count": 9, "a_win_ratio": 0.71}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemma-3-4b", "a_wins": 8, "b_wins": 16, "draws": 11, "count": 35, "a_win_ratio": 0.33}, {"model_a": "deepseek-v3-chat", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 94, "b_wins": 46, "draws": 105, "count": 245, "a_win_ratio": 0.67}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-3.1-8b", "a_wins": 56, "b_wins": 33, "draws": 68, "count": 157, "a_win_ratio": 0.63}, {"model_a": "gemma-3-12b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 12, "b_wins": 10, "draws": 8, "count": 30, "a_win_ratio": 0.55}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 68, "b_wins": 113, "draws": 93, "count": 274, "a_win_ratio": 0.38}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "llama-3.3-70b", "a_wins": 37, "b_wins": 43, "draws": 49, "count": 129, "a_win_ratio": 0.46}, {"model_a": "qwq-32b", "model_b": "qwq-32b", "a_wins": 6, "b_wins": 6, "draws": 0, "count": 12, "a_win_ratio": 0.5}, {"model_a": "gpt-4o-2024-08-06", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 31, "b_wins": 29, "draws": 29, "count": 89, "a_win_ratio": 0.52}, {"model_a": "ministral-8b-instruct-2410", "model_b": "claude-3-7-sonnet", "a_wins": 76, "b_wins": 72, "draws": 70, "count": 218, "a_win_ratio": 0.51}, {"model_a": "deepseek-v3-0324", "model_b": "gemini-2.0-flash-001", "a_wins": 46, "b_wins": 70, "draws": 52, "count": 168, "a_win_ratio": 0.4}, {"model_a": "llama-4-scout", "model_b": "deepseek-v3-0324", "a_wins": 34, "b_wins": 40, "draws": 38, "count": 112, "a_win_ratio": 0.46}, {"model_a": "phi-4", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 19, "b_wins": 16, "draws": 12, "count": 47, "a_win_ratio": 0.54}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "deepseek-r1", "a_wins": 4, "b_wins": 10, "draws": 21, "count": 35, "a_win_ratio": 0.29}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemma-3-12b", "a_wins": 4, "b_wins": 9, "draws": 12, "count": 25, "a_win_ratio": 0.31}, {"model_a": "llama-3.1-8b", "model_b": "command-a", "a_wins": 27, "b_wins": 63, "draws": 45, "count": 135, "a_win_ratio": 0.3}, {"model_a": "mistral-large-2411", "model_b": "deepseek-v3-0324", "a_wins": 69, "b_wins": 66, "draws": 58, "count": 193, "a_win_ratio": 0.51}, {"model_a": "deepseek-v3-0324", "model_b": "o4-mini", "a_wins": 61, "b_wins": 45, "draws": 51, "count": 157, "a_win_ratio": 0.58}, {"model_a": "ministral-8b-instruct-2410", "model_b": "grok-3-mini-beta", "a_wins": 7, "b_wins": 6, "draws": 3, "count": 16, "a_win_ratio": 0.54}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "phi-4", "a_wins": 16, "b_wins": 19, "draws": 12, "count": 47, "a_win_ratio": 0.46}, {"model_a": "aya-expanse-32b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 38, "b_wins": 27, "draws": 22, "count": 87, "a_win_ratio": 0.58}, {"model_a": "mistral-large-2411", "model_b": "qwq-32b", "a_wins": 11, "b_wins": 9, "draws": 13, "count": 33, "a_win_ratio": 0.55}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "deepseek-v3-0324", "a_wins": 23, "b_wins": 55, "draws": 45, "count": 123, "a_win_ratio": 0.29}, {"model_a": "phi-4", "model_b": "deepseek-v3-chat", "a_wins": 41, "b_wins": 58, "draws": 50, "count": 149, "a_win_ratio": 0.41}, {"model_a": "phi-4", "model_b": "llama-3.3-70b", "a_wins": 117, "b_wins": 120, "draws": 124, "count": 361, "a_win_ratio": 0.49}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemma-3-4b", "a_wins": 33, "b_wins": 67, "draws": 45, "count": 145, "a_win_ratio": 0.33}, {"model_a": "gemini-1.5-pro-002", "model_b": "claude-3-5-sonnet-v2", "a_wins": 109, "b_wins": 97, "draws": 107, "count": 313, "a_win_ratio": 0.53}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemini-2.0-flash-exp", "a_wins": 11, "b_wins": 19, "draws": 24, "count": 54, "a_win_ratio": 0.37}, {"model_a": "grok-3-mini-beta", "model_b": "deepseek-v3-0324", "a_wins": 24, "b_wins": 11, "draws": 12, "count": 47, "a_win_ratio": 0.69}, {"model_a": "c4ai-command-r-08-2024", "model_b": "gemma-3-4b", "a_wins": 46, "b_wins": 52, "draws": 35, "count": 133, "a_win_ratio": 0.47}, {"model_a": "qwen3-32b", "model_b": "claude-3-7-sonnet", "a_wins": 2, "b_wins": 1, "draws": 2, "count": 5, "a_win_ratio": 0.67}, {"model_a": "gemma-3-27b", "model_b": "gemma-3-27b", "a_wins": 4, "b_wins": 4, "draws": 0, "count": 8, "a_win_ratio": 0.5}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemma-3-27b", "a_wins": 30, "b_wins": 94, "draws": 68, "count": 192, "a_win_ratio": 0.24}, {"model_a": "mistral-large-2411", "model_b": "qwen3-32b", "a_wins": 0, "b_wins": 3, "draws": 4, "count": 7, "a_win_ratio": 0.0}, {"model_a": "qwen2-7b-instruct", "model_b": "c4ai-command-r-08-2024", "a_wins": 3, "b_wins": 2, "draws": 7, "count": 12, "a_win_ratio": 0.6}, {"model_a": "llama-3.1-8b", "model_b": "llama-4-scout", "a_wins": 20, "b_wins": 34, "draws": 32, "count": 86, "a_win_ratio": 0.37}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 14, "b_wins": 14, "draws": 11, "count": 39, "a_win_ratio": 0.5}, {"model_a": "gemma-3-12b", "model_b": "qwen3-32b", "a_wins": 0, "b_wins": 1, "draws": 0, "count": 1, "a_win_ratio": 0.0}, {"model_a": "gpt-4.1-mini", "model_b": "gemma-3-4b", "a_wins": 61, "b_wins": 41, "draws": 42, "count": 144, "a_win_ratio": 0.6}, {"model_a": "grok-3-mini-beta", "model_b": "mistral-large-2411", "a_wins": 9, "b_wins": 13, "draws": 10, "count": 32, "a_win_ratio": 0.41}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gpt-4.1-nano", "a_wins": 50, "b_wins": 38, "draws": 39, "count": 127, "a_win_ratio": 0.57}, {"model_a": "mistral-nemo-2407", "model_b": "jamba-1.5-large", "a_wins": 1, "b_wins": 3, "draws": 2, "count": 6, "a_win_ratio": 0.25}, {"model_a": "qwen3-32b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "llama-4-scout", "model_b": "command-a", "a_wins": 22, "b_wins": 42, "draws": 29, "count": 93, "a_win_ratio": 0.34}, {"model_a": "llama-3.1-405b", "model_b": "phi-4", "a_wins": 104, "b_wins": 122, "draws": 110, "count": 336, "a_win_ratio": 0.46}, {"model_a": "llama-3.3-70b", "model_b": "deepseek-v3-0324", "a_wins": 37, "b_wins": 80, "draws": 74, "count": 191, "a_win_ratio": 0.32}, {"model_a": "grok-3-mini-beta", "model_b": "qwen3-32b", "a_wins": 66, "b_wins": 67, "draws": 50, "count": 183, "a_win_ratio": 0.5}, {"model_a": "gpt-4.1-nano", "model_b": "qwq-32b", "a_wins": 2, "b_wins": 1, "draws": 2, "count": 5, "a_win_ratio": 0.67}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 3, "b_wins": 1, "draws": 0, "count": 4, "a_win_ratio": 0.75}, {"model_a": "deepseek-r1", "model_b": "command-a", "a_wins": 21, "b_wins": 14, "draws": 15, "count": 50, "a_win_ratio": 0.6}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mistral-small-3.1-24b", "a_wins": 15, "b_wins": 22, "draws": 27, "count": 64, "a_win_ratio": 0.41}, {"model_a": "gemma-3-12b", "model_b": "qwq-32b", "a_wins": 10, "b_wins": 6, "draws": 5, "count": 21, "a_win_ratio": 0.62}, {"model_a": "phi-4", "model_b": "aya-expanse-8b", "a_wins": 29, "b_wins": 27, "draws": 23, "count": 79, "a_win_ratio": 0.52}, {"model_a": "llama-3.3-70b", "model_b": "claude-3-7-sonnet", "a_wins": 27, "b_wins": 34, "draws": 49, "count": 110, "a_win_ratio": 0.44}, {"model_a": "mistral-large-2411", "model_b": "command-a", "a_wins": 35, "b_wins": 54, "draws": 37, "count": 126, "a_win_ratio": 0.39}, {"model_a": "aya-expanse-8b", "model_b": "ministral-8b-instruct-2410", "a_wins": 17, "b_wins": 24, "draws": 21, "count": 62, "a_win_ratio": 0.41}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "qwen3-32b", "a_wins": 36, "b_wins": 45, "draws": 34, "count": 115, "a_win_ratio": 0.44}, {"model_a": "qwq-32b", "model_b": "deepseek-r1", "a_wins": 64, "b_wins": 78, "draws": 62, "count": 204, "a_win_ratio": 0.45}, {"model_a": "aya-expanse-32b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 51, "b_wins": 80, "draws": 62, "count": 193, "a_win_ratio": 0.39}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "deepseek-v3-chat", "a_wins": 28, "b_wins": 44, "draws": 42, "count": 114, "a_win_ratio": 0.39}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 39, "b_wins": 45, "draws": 50, "count": 134, "a_win_ratio": 0.46}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 13, "b_wins": 13, "draws": 4, "count": 30, "a_win_ratio": 0.5}, {"model_a": "grok-3-mini-beta", "model_b": "deepseek-r1", "a_wins": 93, "b_wins": 100, "draws": 74, "count": 267, "a_win_ratio": 0.48}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 63, "b_wins": 95, "draws": 93, "count": 251, "a_win_ratio": 0.4}, {"model_a": "lfm-40b", "model_b": "mistral-small-3.1-24b", "a_wins": 9, "b_wins": 14, "draws": 9, "count": 32, "a_win_ratio": 0.39}, {"model_a": "command-a", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 18, "b_wins": 9, "draws": 17, "count": 44, "a_win_ratio": 0.67}, {"model_a": "gemma-3-27b", "model_b": "deepseek-v3-0324", "a_wins": 51, "b_wins": 51, "draws": 56, "count": 158, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "o3-mini", "a_wins": 26, "b_wins": 19, "draws": 15, "count": 60, "a_win_ratio": 0.58}, {"model_a": "gemma-2-27b-it-q8", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 2, "b_wins": 3, "draws": 12, "count": 17, "a_win_ratio": 0.4}, {"model_a": "grok-3-mini-beta", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 2, "b_wins": 4, "draws": 2, "count": 8, "a_win_ratio": 0.33}, {"model_a": "lfm-40b", "model_b": "llama-3.1-8b", "a_wins": 37, "b_wins": 42, "draws": 47, "count": 126, "a_win_ratio": 0.47}, {"model_a": "mistral-small-3.1-24b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 53, "b_wins": 53, "draws": 52, "count": 158, "a_win_ratio": 0.5}, {"model_a": "gemma-3-4b", "model_b": "mistral-saba", "a_wins": 40, "b_wins": 41, "draws": 34, "count": 115, "a_win_ratio": 0.49}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gpt-4.1-mini", "a_wins": 91, "b_wins": 89, "draws": 97, "count": 277, "a_win_ratio": 0.51}, {"model_a": "mistral-large-2411", "model_b": "gemma-3-27b", "a_wins": 48, "b_wins": 62, "draws": 55, "count": 165, "a_win_ratio": 0.44}, {"model_a": "qwen3-32b", "model_b": "deepseek-r1", "a_wins": 49, "b_wins": 30, "draws": 21, "count": 100, "a_win_ratio": 0.62}, {"model_a": "mistral-small-3.1-24b", "model_b": "mistral-nemo-2407", "a_wins": 12, "b_wins": 8, "draws": 14, "count": 34, "a_win_ratio": 0.6}, {"model_a": "gemini-2.0-flash-001", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 52, "b_wins": 47, "draws": 54, "count": 153, "a_win_ratio": 0.53}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemma-2-27b-it-q8", "a_wins": 2, "b_wins": 11, "draws": 13, "count": 26, "a_win_ratio": 0.15}, {"model_a": "llama-3.3-70b", "model_b": "gemma-3-12b", "a_wins": 24, "b_wins": 61, "draws": 48, "count": 133, "a_win_ratio": 0.28}, {"model_a": "o4-mini", "model_b": "aya-expanse-32b", "a_wins": 2, "b_wins": 0, "draws": 4, "count": 6, "a_win_ratio": 1.0}, {"model_a": "gemma-3-12b", "model_b": "llama-3.1-405b", "a_wins": 64, "b_wins": 23, "draws": 59, "count": 146, "a_win_ratio": 0.74}, {"model_a": "llama-3.1-405b", "model_b": "gpt-4o-2024-08-06", "a_wins": 60, "b_wins": 62, "draws": 84, "count": 206, "a_win_ratio": 0.49}, {"model_a": "llama-3.3-70b", "model_b": "mistral-small-3.1-24b", "a_wins": 69, "b_wins": 73, "draws": 62, "count": 204, "a_win_ratio": 0.49}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "command-a", "a_wins": 7, "b_wins": 13, "draws": 9, "count": 29, "a_win_ratio": 0.35}, {"model_a": "ministral-8b-instruct-2410", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 35, "b_wins": 19, "draws": 30, "count": 84, "a_win_ratio": 0.65}, {"model_a": "gemma-3-12b", "model_b": "deepseek-r1", "a_wins": 14, "b_wins": 4, "draws": 21, "count": 39, "a_win_ratio": 0.78}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-3.3-70b", "a_wins": 73, "b_wins": 85, "draws": 87, "count": 245, "a_win_ratio": 0.46}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "mistral-nemo-2407", "a_wins": 22, "b_wins": 25, "draws": 28, "count": 75, "a_win_ratio": 0.47}, {"model_a": "lfm-40b", "model_b": "gemma-2-9b-it", "a_wins": 29, "b_wins": 39, "draws": 72, "count": 140, "a_win_ratio": 0.43}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "claude-3-5-sonnet-v2", "a_wins": 33, "b_wins": 33, "draws": 2, "count": 68, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "phi-4", "a_wins": 58, "b_wins": 55, "draws": 53, "count": 166, "a_win_ratio": 0.51}, {"model_a": "llama-3.1-405b", "model_b": "qwq-32b", "a_wins": 10, "b_wins": 11, "draws": 13, "count": 34, "a_win_ratio": 0.48}, {"model_a": "gemini-2.0-flash-001", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 28, "b_wins": 9, "draws": 20, "count": 57, "a_win_ratio": 0.76}, {"model_a": "phi-4", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 14, "b_wins": 10, "draws": 8, "count": 32, "a_win_ratio": 0.58}, {"model_a": "gemma-2-9b-it", "model_b": "qwen2-7b-instruct", "a_wins": 1, "b_wins": 0, "draws": 2, "count": 3, "a_win_ratio": 1.0}, {"model_a": "grok-3-mini-beta", "model_b": "claude-3-7-sonnet", "a_wins": 18, "b_wins": 23, "draws": 26, "count": 67, "a_win_ratio": 0.44}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "mistral-small-3.1-24b", "a_wins": 8, "b_wins": 23, "draws": 15, "count": 46, "a_win_ratio": 0.26}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "gemini-1.5-pro-001", "a_wins": 14, "b_wins": 76, "draws": 96, "count": 186, "a_win_ratio": 0.16}, {"model_a": "deepseek-r1", "model_b": "lfm-40b", "a_wins": 12, "b_wins": 4, "draws": 9, "count": 25, "a_win_ratio": 0.75}, {"model_a": "llama-3.1-405b", "model_b": "mistral-nemo-2407", "a_wins": 173, "b_wins": 76, "draws": 217, "count": 466, "a_win_ratio": 0.69}, {"model_a": "llama-4-scout", "model_b": "phi-4", "a_wins": 27, "b_wins": 40, "draws": 45, "count": 112, "a_win_ratio": 0.4}, {"model_a": "gpt-4.1-nano", "model_b": "llama-3.3-70b", "a_wins": 41, "b_wins": 40, "draws": 40, "count": 121, "a_win_ratio": 0.51}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "mistral-nemo-2407", "a_wins": 55, "b_wins": 35, "draws": 62, "count": 152, "a_win_ratio": 0.61}, {"model_a": "lfm-40b", "model_b": "phi-3.5-mini-instruct", "a_wins": 24, "b_wins": 21, "draws": 40, "count": 85, "a_win_ratio": 0.53}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "mistral-large-2411", "a_wins": 47, "b_wins": 51, "draws": 54, "count": 152, "a_win_ratio": 0.48}, {"model_a": "mistral-small-3.1-24b", "model_b": "deepseek-v3-chat", "a_wins": 12, "b_wins": 13, "draws": 15, "count": 40, "a_win_ratio": 0.48}, {"model_a": "deepseek-v3-chat", "model_b": "phi-4", "a_wins": 58, "b_wins": 41, "draws": 50, "count": 149, "a_win_ratio": 0.59}, {"model_a": "gemma-3-4b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 64, "b_wins": 29, "draws": 53, "count": 146, "a_win_ratio": 0.69}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemini-2.0-flash-001", "a_wins": 7, "b_wins": 19, "draws": 16, "count": 42, "a_win_ratio": 0.27}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "mistral-nemo-2407", "a_wins": 16, "b_wins": 13, "draws": 11, "count": 40, "a_win_ratio": 0.55}, {"model_a": "aya-expanse-32b", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 1, "draws": 0, "count": 2, "a_win_ratio": 0.5}, {"model_a": "claude-3-7-sonnet", "model_b": "gemma-3-4b", "a_wins": 58, "b_wins": 55, "draws": 72, "count": 185, "a_win_ratio": 0.51}, {"model_a": "gemini-1.5-pro-002", "model_b": "gemma-2-27b-it-q8", "a_wins": 1, "b_wins": 0, "draws": 1, "count": 2, "a_win_ratio": 1.0}, {"model_a": "aya-expanse-8b", "model_b": "mistral-nemo-2407", "a_wins": 39, "b_wins": 18, "draws": 32, "count": 89, "a_win_ratio": 0.68}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "qwq-32b", "a_wins": 4, "b_wins": 4, "draws": 8, "count": 16, "a_win_ratio": 0.5}, {"model_a": "ministral-8b-instruct-2410", "model_b": "phi-3.5-mini-instruct", "a_wins": 49, "b_wins": 22, "draws": 71, "count": 142, "a_win_ratio": 0.69}, {"model_a": "gemini-1.5-pro-001", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 76, "b_wins": 14, "draws": 96, "count": 186, "a_win_ratio": 0.84}, {"model_a": "gpt-4o-2024-08-06", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 16, "b_wins": 9, "draws": 20, "count": 45, "a_win_ratio": 0.64}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-4-scout", "a_wins": 24, "b_wins": 58, "draws": 48, "count": 130, "a_win_ratio": 0.29}, {"model_a": "deepseek-v3-chat", "model_b": "c4ai-command-r-08-2024", "a_wins": 61, "b_wins": 27, "draws": 46, "count": 134, "a_win_ratio": 0.69}, {"model_a": "llama-3.1-8b", "model_b": "grok-3-mini-beta", "a_wins": 0, "b_wins": 1, "draws": 0, "count": 1, "a_win_ratio": 0.0}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "qwen2-7b-instruct", "a_wins": 2, "b_wins": 1, "draws": 7, "count": 10, "a_win_ratio": 0.67}, {"model_a": "lfm-40b", "model_b": "lfm-40b", "a_wins": 9, "b_wins": 9, "draws": 0, "count": 18, "a_win_ratio": 0.5}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 29, "b_wins": 35, "draws": 39, "count": 103, "a_win_ratio": 0.45}, {"model_a": "gemma-2-9b-it", "model_b": "gpt-4o-2024-08-06", "a_wins": 62, "b_wins": 67, "draws": 65, "count": 194, "a_win_ratio": 0.48}, {"model_a": "llama-3.1-8b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 10, "b_wins": 13, "draws": 14, "count": 37, "a_win_ratio": 0.43}, {"model_a": "jamba-1.5-large", "model_b": "gemini-2.0-flash-exp", "a_wins": 1, "b_wins": 3, "draws": 1, "count": 5, "a_win_ratio": 0.25}, {"model_a": "llama-3.1-70b", "model_b": "lfm-40b", "a_wins": 68, "b_wins": 34, "draws": 62, "count": 164, "a_win_ratio": 0.67}, {"model_a": "claude-3-7-sonnet", "model_b": "gemma-3-12b", "a_wins": 28, "b_wins": 33, "draws": 61, "count": 122, "a_win_ratio": 0.46}, {"model_a": "grok-3-mini-beta", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 56, "b_wins": 45, "draws": 63, "count": 164, "a_win_ratio": 0.55}, {"model_a": "aya-expanse-32b", "model_b": "llama-3.1-405b", "a_wins": 24, "b_wins": 14, "draws": 27, "count": 65, "a_win_ratio": 0.63}, {"model_a": "mistral-nemo-2407", "model_b": "chocolatine-14b-instruct-dpo-v1.2-q4", "a_wins": 9, "b_wins": 2, "draws": 14, "count": 25, "a_win_ratio": 0.82}, {"model_a": "mistral-large-2411", "model_b": "phi-4", "a_wins": 118, "b_wins": 77, "draws": 95, "count": 290, "a_win_ratio": 0.61}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mistral-nemo-2407", "a_wins": 75, "b_wins": 59, "draws": 66, "count": 200, "a_win_ratio": 0.56}, {"model_a": "gpt-4.1-mini", "model_b": "qwq-32b", "a_wins": 1, "b_wins": 1, "draws": 1, "count": 3, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "gemma-2-9b-it", "a_wins": 53, "b_wins": 47, "draws": 60, "count": 160, "a_win_ratio": 0.53}, {"model_a": "llama-4-scout", "model_b": "ministral-8b-instruct-2410", "a_wins": 48, "b_wins": 38, "draws": 51, "count": 137, "a_win_ratio": 0.56}, {"model_a": "llama-3.1-405b", "model_b": "jamba-1.5-large", "a_wins": 4, "b_wins": 2, "draws": 3, "count": 9, "a_win_ratio": 0.67}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "gemma-3-27b", "a_wins": 1, "b_wins": 10, "draws": 7, "count": 18, "a_win_ratio": 0.09}, {"model_a": "ministral-8b-instruct-2410", "model_b": "qwq-32b", "a_wins": 14, "b_wins": 6, "draws": 14, "count": 34, "a_win_ratio": 0.7}, {"model_a": "deepseek-v3-0324", "model_b": "qwq-32b", "a_wins": 1, "b_wins": 3, "draws": 1, "count": 5, "a_win_ratio": 0.25}, {"model_a": "mistral-small-3.1-24b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 22, "b_wins": 15, "draws": 27, "count": 64, "a_win_ratio": 0.59}, {"model_a": "qwq-32b", "model_b": "c4ai-command-r-08-2024", "a_wins": 5, "b_wins": 13, "draws": 11, "count": 29, "a_win_ratio": 0.28}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "qwen2-7b-instruct", "a_wins": 2, "b_wins": 1, "draws": 3, "count": 6, "a_win_ratio": 0.67}, {"model_a": "qwq-32b", "model_b": "grok-3-mini-beta", "a_wins": 16, "b_wins": 16, "draws": 10, "count": 42, "a_win_ratio": 0.5}, {"model_a": "deepseek-v3-chat", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 70, "b_wins": 44, "draws": 74, "count": 188, "a_win_ratio": 0.61}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gemini-1.5-pro-001", "a_wins": 20, "b_wins": 20, "draws": 22, "count": 62, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-70b", "model_b": "aya-expanse-8b", "a_wins": 21, "b_wins": 12, "draws": 20, "count": 53, "a_win_ratio": 0.64}, {"model_a": "gemma-3-12b", "model_b": "phi-4", "a_wins": 71, "b_wins": 31, "draws": 49, "count": 151, "a_win_ratio": 0.7}, {"model_a": "mistral-small-3.1-24b", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 61, "b_wins": 54, "draws": 50, "count": 165, "a_win_ratio": 0.53}, {"model_a": "gpt-4o-2024-08-06", "model_b": "ministral-8b-instruct-2410", "a_wins": 106, "b_wins": 82, "draws": 104, "count": 292, "a_win_ratio": 0.56}, {"model_a": "deepseek-v3-0324", "model_b": "gpt-4.1-nano", "a_wins": 78, "b_wins": 42, "draws": 72, "count": 192, "a_win_ratio": 0.65}, {"model_a": "gemma-3-4b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 16, "b_wins": 8, "draws": 11, "count": 35, "a_win_ratio": 0.67}, {"model_a": "llama-3.3-70b", "model_b": "llama-3.3-70b", "a_wins": 13, "b_wins": 13, "draws": 0, "count": 26, "a_win_ratio": 0.5}, {"model_a": "gemini-1.5-pro-002", "model_b": "llama-3.1-8b", "a_wins": 58, "b_wins": 50, "draws": 58, "count": 166, "a_win_ratio": 0.54}, {"model_a": "gemma-3-12b", "model_b": "gemini-2.0-flash-001", "a_wins": 37, "b_wins": 61, "draws": 47, "count": 145, "a_win_ratio": 0.38}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemma-3-4b", "a_wins": 32, "b_wins": 69, "draws": 57, "count": 158, "a_win_ratio": 0.32}, {"model_a": "llama-3.1-8b", "model_b": "deepseek-v3-0324", "a_wins": 20, "b_wins": 40, "draws": 54, "count": 114, "a_win_ratio": 0.33}, {"model_a": "mistral-large-2411", "model_b": "o3-mini", "a_wins": 18, "b_wins": 16, "draws": 23, "count": 57, "a_win_ratio": 0.53}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gpt-4.1-mini", "a_wins": 39, "b_wins": 38, "draws": 41, "count": 118, "a_win_ratio": 0.51}, {"model_a": "gemini-2.0-flash-001", "model_b": "o4-mini", "a_wins": 47, "b_wins": 35, "draws": 26, "count": 108, "a_win_ratio": 0.57}, {"model_a": "gemma-3-12b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 12, "b_wins": 7, "draws": 9, "count": 28, "a_win_ratio": 0.63}, {"model_a": "qwen2.5-7b-instruct", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 3, "b_wins": 5, "draws": 8, "count": 16, "a_win_ratio": 0.38}, {"model_a": "gpt-4.1-nano", "model_b": "qwen3-32b", "a_wins": 1, "b_wins": 0, "draws": 1, "count": 2, "a_win_ratio": 1.0}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "aya-expanse-32b", "a_wins": 27, "b_wins": 38, "draws": 22, "count": 87, "a_win_ratio": 0.42}, {"model_a": "gemini-2.0-flash-001", "model_b": "claude-3-7-sonnet", "a_wins": 65, "b_wins": 50, "draws": 68, "count": 183, "a_win_ratio": 0.57}, {"model_a": "llama-4-scout", "model_b": "mistral-large-2411", "a_wins": 34, "b_wins": 42, "draws": 33, "count": 109, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-70b", "model_b": "gemini-2.0-flash-001", "a_wins": 11, "b_wins": 22, "draws": 18, "count": 51, "a_win_ratio": 0.33}, {"model_a": "o4-mini", "model_b": "gemini-2.0-flash-001", "a_wins": 35, "b_wins": 47, "draws": 26, "count": 108, "a_win_ratio": 0.43}, {"model_a": "gemma-3-12b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 6, "b_wins": 7, "draws": 12, "count": 25, "a_win_ratio": 0.46}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "hermes-3-llama-3.1-405b", "a_wins": 55, "b_wins": 41, "draws": 51, "count": 147, "a_win_ratio": 0.57}, {"model_a": "o4-mini", "model_b": "llama-4-scout", "a_wins": 8, "b_wins": 8, "draws": 5, "count": 21, "a_win_ratio": 0.5}, {"model_a": "llama-3.1-405b", "model_b": "aya-expanse-8b", "a_wins": 25, "b_wins": 27, "draws": 24, "count": 76, "a_win_ratio": 0.48}, {"model_a": "gemma-2-9b-it", "model_b": "c4ai-command-r-08-2024", "a_wins": 29, "b_wins": 33, "draws": 26, "count": 88, "a_win_ratio": 0.47}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "gemma-2-27b-it-q8", "a_wins": 1, "b_wins": 1, "draws": 5, "count": 7, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "llama-3.1-405b", "a_wins": 116, "b_wins": 86, "draws": 124, "count": 326, "a_win_ratio": 0.57}, {"model_a": "c4ai-command-r-08-2024", "model_b": "llama-3.1-405b", "a_wins": 67, "b_wins": 88, "draws": 70, "count": 225, "a_win_ratio": 0.43}, {"model_a": "gpt-4o-2024-08-06", "model_b": "lfm-40b", "a_wins": 26, "b_wins": 17, "draws": 23, "count": 66, "a_win_ratio": 0.6}, {"model_a": "command-a", "model_b": "aya-expanse-32b", "a_wins": 29, "b_wins": 18, "draws": 31, "count": 78, "a_win_ratio": 0.62}, {"model_a": "llama-3.1-70b", "model_b": "llama-3.1-405b", "a_wins": 90, "b_wins": 115, "draws": 154, "count": 359, "a_win_ratio": 0.44}, {"model_a": "llama-3.1-70b", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 32, "b_wins": 31, "draws": 39, "count": 102, "a_win_ratio": 0.51}, {"model_a": "phi-4", "model_b": "gemma-3-27b", "a_wins": 25, "b_wins": 65, "draws": 47, "count": 137, "a_win_ratio": 0.28}, {"model_a": "deepseek-r1", "model_b": "ministral-8b-instruct-2410", "a_wins": 22, "b_wins": 19, "draws": 26, "count": 67, "a_win_ratio": 0.54}, {"model_a": "gpt-4o-2024-08-06", "model_b": "phi-4", "a_wins": 39, "b_wins": 50, "draws": 43, "count": 132, "a_win_ratio": 0.44}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 56, "b_wins": 46, "draws": 55, "count": 157, "a_win_ratio": 0.55}, {"model_a": "llama-3.3-70b", "model_b": "jamba-1.5-large", "a_wins": 0, "b_wins": 1, "draws": 3, "count": 4, "a_win_ratio": 0.0}, {"model_a": "gemini-1.5-pro-001", "model_b": "llama-3.1-8b", "a_wins": 68, "b_wins": 13, "draws": 86, "count": 167, "a_win_ratio": 0.84}, {"model_a": "deepseek-v3-chat", "model_b": "ministral-8b-instruct-2410", "a_wins": 93, "b_wins": 37, "draws": 66, "count": 196, "a_win_ratio": 0.72}, {"model_a": "gemma-3-12b", "model_b": "gpt-4.1-nano", "a_wins": 56, "b_wins": 42, "draws": 42, "count": 140, "a_win_ratio": 0.57}, {"model_a": "deepseek-r1", "model_b": "c4ai-command-r-08-2024", "a_wins": 17, "b_wins": 11, "draws": 18, "count": 46, "a_win_ratio": 0.61}, {"model_a": "gemma-3-12b", "model_b": "aya-expanse-32b", "a_wins": 28, "b_wins": 24, "draws": 21, "count": 73, "a_win_ratio": 0.54}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 1, "b_wins": 7, "draws": 6, "count": 14, "a_win_ratio": 0.12}, {"model_a": "deepseek-r1", "model_b": "o4-mini", "a_wins": 83, "b_wins": 54, "draws": 74, "count": 211, "a_win_ratio": 0.61}, {"model_a": "mistral-small-3.1-24b", "model_b": "claude-3-7-sonnet", "a_wins": 50, "b_wins": 75, "draws": 63, "count": 188, "a_win_ratio": 0.4}, {"model_a": "llama-3.1-8b", "model_b": "gemma-2-27b-it-q8", "a_wins": 14, "b_wins": 33, "draws": 41, "count": 88, "a_win_ratio": 0.3}, {"model_a": "qwq-32b", "model_b": "mistral-large-2411", "a_wins": 9, "b_wins": 11, "draws": 13, "count": 33, "a_win_ratio": 0.45}, {"model_a": "claude-3-7-sonnet", "model_b": "c4ai-command-r-08-2024", "a_wins": 55, "b_wins": 31, "draws": 34, "count": 120, "a_win_ratio": 0.64}, {"model_a": "phi-4", "model_b": "grok-3-mini-beta", "a_wins": 2, "b_wins": 6, "draws": 0, "count": 8, "a_win_ratio": 0.25}, {"model_a": "llama-3.1-8b", "model_b": "phi-4", "a_wins": 89, "b_wins": 106, "draws": 88, "count": 283, "a_win_ratio": 0.46}, {"model_a": "phi-4", "model_b": "phi-3.5-mini-instruct", "a_wins": 2, "b_wins": 3, "draws": 1, "count": 6, "a_win_ratio": 0.4}, {"model_a": "mixtral-8x22b-instruct-v0.1", "model_b": "ministral-8b-instruct-2410", "a_wins": 46, "b_wins": 86, "draws": 82, "count": 214, "a_win_ratio": 0.35}, {"model_a": "deepseek-v3-0324", "model_b": "gemma-3-12b", "a_wins": 46, "b_wins": 43, "draws": 44, "count": 133, "a_win_ratio": 0.52}, {"model_a": "gemma-3-27b", "model_b": "llama-3.1-405b", "a_wins": 68, "b_wins": 37, "draws": 56, "count": 161, "a_win_ratio": 0.65}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "deepseek-v3-chat", "a_wins": 115, "b_wins": 165, "draws": 139, "count": 419, "a_win_ratio": 0.41}, {"model_a": "qwen2.5-7b-instruct", "model_b": "llama-3.1-8b", "a_wins": 41, "b_wins": 26, "draws": 52, "count": 119, "a_win_ratio": 0.61}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "llama-3.1-70b", "a_wins": 34, "b_wins": 64, "draws": 46, "count": 144, "a_win_ratio": 0.35}, {"model_a": "gemma-3-27b", "model_b": "qwq-32b", "a_wins": 19, "b_wins": 2, "draws": 8, "count": 29, "a_win_ratio": 0.9}, {"model_a": "mistral-nemo-2407", "model_b": "gemini-2.0-flash-001", "a_wins": 13, "b_wins": 30, "draws": 31, "count": 74, "a_win_ratio": 0.3}, {"model_a": "gemma-3-27b", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 22, "b_wins": 6, "draws": 17, "count": 45, "a_win_ratio": 0.79}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mistral-saba", "a_wins": 37, "b_wins": 38, "draws": 38, "count": 113, "a_win_ratio": 0.49}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "llama-3.1-70b", "a_wins": 48, "b_wins": 49, "draws": 51, "count": 148, "a_win_ratio": 0.49}, {"model_a": "phi-3.5-mini-instruct", "model_b": "gpt-4o-2024-08-06", "a_wins": 13, "b_wins": 28, "draws": 20, "count": 61, "a_win_ratio": 0.32}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mistral-small-3.1-24b", "a_wins": 51, "b_wins": 54, "draws": 54, "count": 159, "a_win_ratio": 0.49}, {"model_a": "gpt-4.1-mini", "model_b": "gpt-4.1-nano", "a_wins": 103, "b_wins": 59, "draws": 60, "count": 222, "a_win_ratio": 0.64}, {"model_a": "llama-3.1-8b", "model_b": "ministral-8b-instruct-2410", "a_wins": 122, "b_wins": 142, "draws": 140, "count": 404, "a_win_ratio": 0.46}, {"model_a": "llama-3.3-70b", "model_b": "mistral-large-2411", "a_wins": 81, "b_wins": 69, "draws": 79, "count": 229, "a_win_ratio": 0.54}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "grok-3-mini-beta", "a_wins": 2, "b_wins": 3, "draws": 1, "count": 6, "a_win_ratio": 0.4}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 118, "b_wins": 107, "draws": 102, "count": 327, "a_win_ratio": 0.52}, {"model_a": "ministral-8b-instruct-2410", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 95, "b_wins": 103, "draws": 76, "count": 274, "a_win_ratio": 0.48}, {"model_a": "gemma-2-9b-it", "model_b": "llama-3.3-70b", "a_wins": 34, "b_wins": 28, "draws": 34, "count": 96, "a_win_ratio": 0.55}, {"model_a": "gemma-3-27b", "model_b": "claude-3-7-sonnet", "a_wins": 42, "b_wins": 34, "draws": 67, "count": 143, "a_win_ratio": 0.55}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 114, "b_wins": 44, "draws": 91, "count": 249, "a_win_ratio": 0.72}, {"model_a": "deepseek-v3-0324", "model_b": "gemma-3-27b", "a_wins": 51, "b_wins": 51, "draws": 56, "count": 158, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 46, "b_wins": 58, "draws": 45, "count": 149, "a_win_ratio": 0.44}, {"model_a": "command-a", "model_b": "mistral-saba", "a_wins": 48, "b_wins": 30, "draws": 47, "count": 125, "a_win_ratio": 0.62}, {"model_a": "lfm-40b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 8, "b_wins": 6, "draws": 19, "count": 33, "a_win_ratio": 0.57}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "deepseek-v3-0324", "a_wins": 23, "b_wins": 47, "draws": 53, "count": 123, "a_win_ratio": 0.33}, {"model_a": "grok-3-mini-beta", "model_b": "llama-3.1-8b", "a_wins": 1, "b_wins": 0, "draws": 0, "count": 1, "a_win_ratio": 1.0}, {"model_a": "llama-3.3-70b", "model_b": "chocolatine-2-14b-instruct-v2.0.3-q8", "a_wins": 21, "b_wins": 8, "draws": 31, "count": 60, "a_win_ratio": 0.72}, {"model_a": "gpt-4.1-nano", "model_b": "mistral-saba", "a_wins": 60, "b_wins": 49, "draws": 44, "count": 153, "a_win_ratio": 0.55}, {"model_a": "ministral-8b-instruct-2410", "model_b": "gemini-1.5-pro-001", "a_wins": 23, "b_wins": 36, "draws": 74, "count": 133, "a_win_ratio": 0.39}, {"model_a": "gemini-2.0-flash-001", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 99, "b_wins": 66, "draws": 80, "count": 245, "a_win_ratio": 0.6}, {"model_a": "mistral-large-2411", "model_b": "gemini-1.5-pro-002", "a_wins": 77, "b_wins": 64, "draws": 76, "count": 217, "a_win_ratio": 0.55}, {"model_a": "gemma-2-27b-it-q8", "model_b": "llama-3.1-405b", "a_wins": 20, "b_wins": 12, "draws": 20, "count": 52, "a_win_ratio": 0.62}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemini-1.5-pro-002", "a_wins": 44, "b_wins": 47, "draws": 63, "count": 154, "a_win_ratio": 0.48}, {"model_a": "llama-3.1-70b", "model_b": "claude-3-5-sonnet-v2", "a_wins": 48, "b_wins": 53, "draws": 70, "count": 171, "a_win_ratio": 0.48}, {"model_a": "qwen3-32b", "model_b": "ministral-8b-instruct-2410", "a_wins": 0, "b_wins": 0, "draws": 2, "count": 2, "a_win_ratio": null}, {"model_a": "c4ai-command-r-08-2024", "model_b": "claude-3-7-sonnet", "a_wins": 31, "b_wins": 55, "draws": 34, "count": 120, "a_win_ratio": 0.36}, {"model_a": "aya-expanse-8b", "model_b": "gemini-1.5-pro-002", "a_wins": 21, "b_wins": 22, "draws": 30, "count": 73, "a_win_ratio": 0.49}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "phi-4", "a_wins": 15, "b_wins": 29, "draws": 29, "count": 73, "a_win_ratio": 0.34}, {"model_a": "ministral-8b-instruct-2410", "model_b": "mistral-nemo-2407", "a_wins": 105, "b_wins": 71, "draws": 128, "count": 304, "a_win_ratio": 0.6}, {"model_a": "gemma-3-4b", "model_b": "gemini-1.5-pro-002", "a_wins": 11, "b_wins": 8, "draws": 15, "count": 34, "a_win_ratio": 0.58}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "llama-3.1-8b", "a_wins": 36, "b_wins": 37, "draws": 58, "count": 131, "a_win_ratio": 0.49}, {"model_a": "o3-mini", "model_b": "gemini-1.5-pro-002", "a_wins": 19, "b_wins": 26, "draws": 15, "count": 60, "a_win_ratio": 0.42}, {"model_a": "deepseek-v3-chat", "model_b": "mistral-small-24b-instruct-2501", "a_wins": 66, "b_wins": 40, "draws": 51, "count": 157, "a_win_ratio": 0.62}, {"model_a": "claude-3-5-sonnet-v2", "model_b": "phi-3.5-mini-instruct", "a_wins": 5, "b_wins": 5, "draws": 4, "count": 14, "a_win_ratio": 0.5}, {"model_a": "chocolatine-14b-instruct-dpo-v1.2-q4", "model_b": "llama-3.1-405b", "a_wins": 0, "b_wins": 15, "draws": 16, "count": 31, "a_win_ratio": 0.0}, {"model_a": "mistral-saba", "model_b": "mistral-small-3.1-24b", "a_wins": 40, "b_wins": 44, "draws": 28, "count": 112, "a_win_ratio": 0.48}, {"model_a": "mistral-saba", "model_b": "aya-expanse-32b", "a_wins": 29, "b_wins": 28, "draws": 17, "count": 74, "a_win_ratio": 0.51}, {"model_a": "llama-4-scout", "model_b": "llama-3.1-8b", "a_wins": 34, "b_wins": 20, "draws": 32, "count": 86, "a_win_ratio": 0.63}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "ministral-8b-instruct-2410", "a_wins": 19, "b_wins": 35, "draws": 30, "count": 84, "a_win_ratio": 0.35}, {"model_a": "gemma-2-27b-it-q8", "model_b": "gemini-2.0-flash-exp", "a_wins": 2, "b_wins": 1, "draws": 3, "count": 6, "a_win_ratio": 0.67}, {"model_a": "llama-3.1-405b", "model_b": "gemini-2.0-flash-001", "a_wins": 31, "b_wins": 96, "draws": 81, "count": 208, "a_win_ratio": 0.24}, {"model_a": "llama-3.1-8b", "model_b": "gemini-2.0-flash-exp", "a_wins": 23, "b_wins": 50, "draws": 54, "count": 127, "a_win_ratio": 0.32}, {"model_a": "deepseek-v3-chat", "model_b": "mixtral-8x7b-instruct-v0.1", "a_wins": 6, "b_wins": 0, "draws": 6, "count": 12, "a_win_ratio": 1.0}, {"model_a": "deepseek-r1", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 60, "b_wins": 72, "draws": 70, "count": 202, "a_win_ratio": 0.45}, {"model_a": "gpt-4.1-nano", "model_b": "claude-3-7-sonnet", "a_wins": 56, "b_wins": 70, "draws": 44, "count": 170, "a_win_ratio": 0.44}, {"model_a": "gemma-3-12b", "model_b": "llama-3.1-8b", "a_wins": 45, "b_wins": 25, "draws": 43, "count": 113, "a_win_ratio": 0.64}, {"model_a": "command-a", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 13, "b_wins": 7, "draws": 9, "count": 29, "a_win_ratio": 0.65}, {"model_a": "llama-3.1-70b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 66, "b_wins": 75, "draws": 82, "count": 223, "a_win_ratio": 0.47}, {"model_a": "aya-expanse-8b", "model_b": "llama-3.1-nemotron-70b-instruct", "a_wins": 11, "b_wins": 12, "draws": 10, "count": 33, "a_win_ratio": 0.48}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "gemini-2.0-flash-exp", "a_wins": 21, "b_wins": 32, "draws": 33, "count": 86, "a_win_ratio": 0.4}, {"model_a": "claude-3-7-sonnet", "model_b": "aya-expanse-32b", "a_wins": 40, "b_wins": 33, "draws": 31, "count": 104, "a_win_ratio": 0.55}, {"model_a": "gemma-3-27b", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 21, "b_wins": 13, "draws": 27, "count": 61, "a_win_ratio": 0.62}, {"model_a": "mistral-saba", "model_b": "llama-3.1-405b", "a_wins": 38, "b_wins": 19, "draws": 36, "count": 93, "a_win_ratio": 0.67}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "mistral-large-2411", "a_wins": 11, "b_wins": 18, "draws": 10, "count": 39, "a_win_ratio": 0.38}, {"model_a": "Yi-1.5-9B-Chat", "model_b": "mixtral-8x22b-instruct-v0.1", "a_wins": 0, "b_wins": 1, "draws": 3, "count": 4, "a_win_ratio": 0.0}, {"model_a": "gemini-2.0-flash-001", "model_b": "c4ai-command-r-08-2024", "a_wins": 76, "b_wins": 27, "draws": 74, "count": 177, "a_win_ratio": 0.74}, {"model_a": "gemini-2.0-flash-exp", "model_b": "gpt-4o-2024-08-06", "a_wins": 105, "b_wins": 56, "draws": 101, "count": 262, "a_win_ratio": 0.65}, {"model_a": "phi-3.5-mini-instruct", "model_b": "llama-3.1-70b", "a_wins": 45, "b_wins": 86, "draws": 131, "count": 262, "a_win_ratio": 0.34}, {"model_a": "gemma-3-12b", "model_b": "llama-3.3-70b", "a_wins": 61, "b_wins": 24, "draws": 48, "count": 133, "a_win_ratio": 0.72}, {"model_a": "phi-4", "model_b": "gpt-4o-2024-08-06", "a_wins": 50, "b_wins": 39, "draws": 43, "count": 132, "a_win_ratio": 0.56}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "gpt-4o-2024-08-06", "a_wins": 64, "b_wins": 77, "draws": 76, "count": 217, "a_win_ratio": 0.45}, {"model_a": "mistral-small-24b-instruct-2501", "model_b": "gemma-3-12b", "a_wins": 7, "b_wins": 6, "draws": 12, "count": 25, "a_win_ratio": 0.54}, {"model_a": "hermes-3-llama-3.1-405b", "model_b": "deepseek-r1-distill-llama-70b", "a_wins": 12, "b_wins": 10, "draws": 10, "count": 32, "a_win_ratio": 0.55}, {"model_a": "mistral-saba", "model_b": "gemma-3-12b", "a_wins": 22, "b_wins": 49, "draws": 39, "count": 110, "a_win_ratio": 0.31}, {"model_a": "qwen2-7b-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 1, "b_wins": 8, "draws": 5, "count": 14, "a_win_ratio": 0.11}, {"model_a": "gemini-2.0-flash-exp", "model_b": "mistral-nemo-2407", "a_wins": 61, "b_wins": 10, "draws": 67, "count": 138, "a_win_ratio": 0.86}, {"model_a": "llama-3.1-8b", "model_b": "lfm-40b", "a_wins": 42, "b_wins": 37, "draws": 47, "count": 126, "a_win_ratio": 0.53}, {"model_a": "grok-3-mini-beta", "model_b": "ministral-8b-instruct-2410", "a_wins": 6, "b_wins": 7, "draws": 3, "count": 16, "a_win_ratio": 0.46}, {"model_a": "llama-3.1-nemotron-70b-instruct", "model_b": "llama-3.3-70b", "a_wins": 120, "b_wins": 80, "draws": 106, "count": 306, "a_win_ratio": 0.6}, {"model_a": "lfm-40b", "model_b": "phi-4", "a_wins": 32, "b_wins": 44, "draws": 50, "count": 126, "a_win_ratio": 0.42}, {"model_a": "jamba-1.5-large", "model_b": "gpt-4o-mini-2024-07-18", "a_wins": 2, "b_wins": 2, "draws": 1, "count": 5, "a_win_ratio": 0.5}, {"model_a": "jamba-1.5-large", "model_b": "aya-expanse-8b", "a_wins": 1, "b_wins": 2, "draws": 3, "count": 6, "a_win_ratio": 0.33}, {"model_a": "aya-expanse-32b", "model_b": "deepseek-r1", "a_wins": 1, "b_wins": 1, "draws": 3, "count": 5, "a_win_ratio": 0.5}, {"model_a": "qwen2.5-7b-instruct", "model_b": "gemini-1.5-pro-001", "a_wins": 23, "b_wins": 48, "draws": 57, "count": 128, "a_win_ratio": 0.32}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "jamba-1.5-large", "a_wins": 2, "b_wins": 2, "draws": 1, "count": 5, "a_win_ratio": 0.5}, {"model_a": "mistral-large-2411", "model_b": "deepseek-v3-chat", "a_wins": 79, "b_wins": 96, "draws": 94, "count": 269, "a_win_ratio": 0.45}, {"model_a": "llama-3.1-70b", "model_b": "qwen2.5-7b-instruct", "a_wins": 36, "b_wins": 28, "draws": 66, "count": 130, "a_win_ratio": 0.56}, {"model_a": "claude-3-7-sonnet", "model_b": "deepseek-v3-0324", "a_wins": 69, "b_wins": 84, "draws": 73, "count": 226, "a_win_ratio": 0.45}, {"model_a": "deepseek-r1-distill-llama-70b", "model_b": "claude-3-7-sonnet", "a_wins": 5, "b_wins": 10, "draws": 10, "count": 25, "a_win_ratio": 0.33}, {"model_a": "command-a", "model_b": "ministral-8b-instruct-2410", "a_wins": 56, "b_wins": 38, "draws": 51, "count": 145, "a_win_ratio": 0.6}, {"model_a": "gpt-4o-mini-2024-07-18", "model_b": "gemma-3-12b", "a_wins": 14, "b_wins": 18, "draws": 10, "count": 42, "a_win_ratio": 0.44}, {"model_a": "qwq-32b", "model_b": "gemini-2.0-flash-001", "a_wins": 6, "b_wins": 8, "draws": 13, "count": 27, "a_win_ratio": 0.43}, {"model_a": "gemini-1.5-pro-002", "model_b": "c4ai-command-r-08-2024", "a_wins": 51, "b_wins": 35, "draws": 38, "count": 124, "a_win_ratio": 0.59}, {"model_a": "ministral-8b-instruct-2410", "model_b": "phi-4", "a_wins": 87, "b_wins": 95, "draws": 101, "count": 283, "a_win_ratio": 0.48}, {"model_a": "mixtral-8x7b-instruct-v0.1", "model_b": "claude-3-5-sonnet-v2", "a_wins": 9, "b_wins": 16, "draws": 18, "count": 43, "a_win_ratio": 0.36}, {"model_a": "qwen2.5-32b-instruct", "model_b": "lfm-40b", "a_wins": 10, "b_wins": 0, "draws": 4, "count": 14, "a_win_ratio": 1.0}, {"model_a": "gemma-3-4b", "model_b": "gpt-4.1-mini", "a_wins": 41, "b_wins": 61, "draws": 42, "count": 144, "a_win_ratio": 0.4}, {"model_a": "gemini-1.5-pro-002", "model_b": "qwq-32b", "a_wins": 7, "b_wins": 5, "draws": 14, "count": 26, "a_win_ratio": 0.58}, {"model_a": "gpt-4.1-nano", "model_b": "gpt-4.1-mini", "a_wins": 59, "b_wins": 103, "draws": 60, "count": 222, "a_win_ratio": 0.36}, {"model_a": "claude-3-7-sonnet", "model_b": "command-a", "a_wins": 40, "b_wins": 46, "draws": 47, "count": 133, "a_win_ratio": 0.47}, {"model_a": "gemma-3-27b", "model_b": "llama-3.1-8b", "a_wins": 52, "b_wins": 19, "draws": 48, "count": 119, "a_win_ratio": 0.73}, {"model_a": "mistral-saba", "model_b": "claude-3-7-sonnet", "a_wins": 41, "b_wins": 53, "draws": 56, "count": 150, "a_win_ratio": 0.44}, {"model_a": "qwen2.5-coder-32b-instruct", "model_b": "qwq-32b", "a_wins": 8, "b_wins": 15, "draws": 16, "count": 39, "a_win_ratio": 0.35}, {"model_a": "gpt-4o-2024-08-06", "model_b": "gemini-1.5-pro-002", "a_wins": 152, "b_wins": 160, "draws": 174, "count": 486, "a_win_ratio": 0.49}, {"model_a": "chocolatine-2-14b-instruct-v2.0.3-q8", "model_b": "llama-3.1-8b", "a_wins": 15, "b_wins": 17, "draws": 27, "count": 59, "a_win_ratio": 0.47}, {"model_a": "ministral-8b-instruct-2410", "model_b": "aya-expanse-8b", "a_wins": 24, "b_wins": 17, "draws": 21, "count": 62, "a_win_ratio": 0.59}, {"model_a": "ministral-8b-instruct-2410", "model_b": "claude-3-5-sonnet-v2", "a_wins": 67, "b_wins": 56, "draws": 99, "count": 222, "a_win_ratio": 0.54}, {"model_a": "gemini-1.5-pro-002", "model_b": "qwen2.5-coder-32b-instruct", "a_wins": 89, "b_wins": 55, "draws": 88, "count": 232, "a_win_ratio": 0.62}, {"model_a": "jamba-1.5-large", "model_b": "gpt-4o-2024-08-06", "a_wins": 2, "b_wins": 2, "draws": 2, "count": 6, "a_win_ratio": 0.5}, {"model_a": "grok-3-mini-beta", "model_b": "command-a", "a_wins": 1, "b_wins": 0, "draws": 3, "count": 4, "a_win_ratio": 1.0}]}}