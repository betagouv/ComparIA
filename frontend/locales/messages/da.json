{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "accessData": "Adgang til data",
        "contact": "Kontakt os",
        "contactUs": "Kontakt os",
        "copyLink": {
            "do": "Kopiér linket",
            "done": "Link kopieret til udklipsholder"
        },
        "copyMessage": {
            "do": "Kopiér beskeden",
            "done": "Besked kopieret"
        },
        "downloadData": "Download data",
        "home": "Hjemmeside",
        "returnHome": "Tilbage til startsiden",
        "scrollLeft": "Rul til venstre",
        "scrollRight": "Rul til højre",
        "searchModel": "Søg efter en model",
        "seeMore": "Se mere",
        "selectLanguage": "Vælg sprog",
        "vote": "Giv feedback"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 modeller",
            "help": "Hvis du kun vælger én, vil den anden blive valgt tilfældigt",
            "question": "Hvilke modeller vil du gerne sammenligne?"
        },
        "modelSelection": "Modelvalg",
        "prompt": {
            "label": "Skriv din første besked",
            "placeholder": "Skriv din første besked her"
        },
        "selectModels": {
            "help": "Vælg sammenligningstypen",
            "question": "Hvilke modeller vil du gerne sammenligne?"
        },
        "suggestions": {
            "generateAnother": "Generér en ny prompt",
            "title": "Foreslåede prompts"
        },
        "title": "Hvordan kan jeg hjælpe dig i dag?"
    },
    "chatbot": {
        "continuePrompt": "Fortsæt chatten med AI-modellerne",
        "conversation": "Chat",
        "errors": {
            "other": {
                "message": "Der er opstået en midlertidig fejl.",
                "retry": "Du kan prøve at skrive til modellerne igen.",
                "title": "Ups, midlertidig fejl",
                "vote": "Eller afslut oplevelsen ved at angive din præference for disse modeller."
            },
            "tooLong": {
                "message": "Hver model er begrænset i størrelsen af de samtaler, den kan håndtere.",
                "retry": "Du kan genstarte en chat med to nye modeller.",
                "title": "Ups, samtalen er for lang for en af modellerne.",
                "vote": "Du kan stadig angive dine præferencer for disse modeller eller starte en samtale med to nye."
            }
        },
        "loading": "Indlæser svar",
        "reasoning": {
            "finished": "Tænkning afsluttet",
            "inProgress": "Tænker…"
        },
        "revealButton": "Afslør modellerne"
    },
    "closeModal": "Luk pop op-vinduet",
    "components": {
        "pagination": {
            "first": "Første side",
            "label": "Sider",
            "last": "Sidste side",
            "next": "Næste side",
            "nth": "Side {count}",
            "previous": "Forrige side"
        },
        "table": {
            "linePerPage": "Antal linjer per side",
            "pageCount": "{count} linjer per side",
            "triage": "Sortér"
        },
        "theme": {
            "legend": "Vælg et tema for at tilpasse sidens udseende.",
            "options": {
                "dark": "Mørk tema",
                "light": "Lyst tema",
                "system": "System",
                "systemSub": "Brug system indstillinger"
            },
            "title": "Visningsindstillinger"
        }
    },
    "datasets": {
        "access": {
            "catch": "Modeludgivere, forskere, virksomheder, nu er det jeres tur!",
            "desc": "Platformens spørgsmål og præferencer er tilgængelige på dansk, engelsk, fransk og forventes udvidet i fremtiden til blandt andet at inkludere svensk. Spørgsmålene afspejler organisk brug – ikke kunstige spørgsmål. Disse datasæt er offentligt tilgængelige på <a {linkProps}>data.gouv.fr</a> og Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "Alle spørgsmål og svar",
                    "title": "/samtaler"
                },
                "reactions": {
                    "desc": "Alle reaktioner på beskeder",
                    "title": "/reaktioner"
                },
                "votes": {
                    "desc": "Alle de udtrykte præferencer",
                    "title": "/stemmer"
                }
            },
            "share": "Vis os, hvordan du bruger datasættene",
            "title": "Få adgang til AI Arena datasættene"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "desc": "Analyse af brugeres samtaler med registrering af opgaver (oprettelse, informationssøgning osv.), emner (kunst og kultur, uddannelse osv.), komplekse følelser (nysgerrighed, entusiasme osv.), sproglig tone (formel, professionel osv.)",
                    "title": "Få adgang til analysen"
                },
                "conversations": {
                    "desc": "Interaktiv visualisering af samtaler, hvor hver klynge repræsenterer et tilbagevendende tema, som brugerne diskuterer (f.eks. uddannelse, sundhed, miljø eller endda filosofi).",
                    "title": "Udforsk datavisualiseringen"
                },
                "desc": "Bunka.ai-teamet gennemførte en storstilet undersøgelse af bruger-AI-interaktioner på chatbot-området, hvor de kortlagde dominerende temaer, nøgleopgaver og balancen mellem automatisering og menneskelig forstærkning. Deres analyse, der er baseret på 25.000 reelle samtaler, giver en sjælden empirisk indsigt i, hvordan mennesker rent faktisk bruger AI.",
                "method": "Lær mere om metodikken"
            },
            "desc": "Eksempler på brug af AI-arena datasæt",
            "title": "Hvordan bruges disse data?"
        }
    },
    "errors": {
        "404": {
            "desc": "Hvis du har indtastet URL'en i din browser, skal du kontrollere, om den er korrekt. Siden er muligvis ikke længere tilgængelig. <br />Du kan fortsætte ved at besøge vores hjemmeside. <br /> Hvis du har svært ved at finde den side, du leder efter, skal du kontakte os, så vi kan videresende dig til den korrekte URL.",
            "error": "Fejl 404",
            "sorry": "Den side, du søger, kan ikke findes. Vi beklager ulejligheden.",
            "title": "Siden blev ikke fundet"
        },
        "unexpected": {
            "desc": "Prøv at opdatere siden eller prøv igen senere.",
            "error": "Fejl {code}",
            "sorry": "Vi beklager, der er et problem med tjenesten, vi arbejder på at løse det så hurtigt som muligt.",
            "title": "Uventet fejl"
        },
        "unknown": "Der er opstået en fejl"
    },
    "faq": {
        "datasets": {
            "questions": {
                "1": {
                    "desc": "<p> Præferencedata bruges til at forbedre modeller under fremtidig træning. </p><p> Ved blindt at sammenligne svarene fra to modeller udtrykker brugere deres præferencer og angiver, hvilke svar der er mest relevante. Disse præferencedata kan bruges til at forbedre modeljustering, det vil sige at træne dem til at generere svar, der er mere i overensstemmelse med brugernes forventninger og præferencer. </p><p> Dette er en iterativ proces, hvor modellen gradvist lærer at generere bedre svar baseret på feedback fra mennesker om svarenes kvalitet. Ved at blive eksponeret for præferencedata justerer modellerne deres svarstil. </p>",
                    "title": "Har præferencedata en umiddelbar effekt på forbedringen af modellerne?"
                },
                "2": {
                    "desc": "<p>Det særlige ved de data, der indsamles på platformen, er, at de er på dansk, og at de svarer til brugernes faktiske opgaver. Disse data afspejler menneskelige præferencer i en sproglig og kulturel kontekst. De gør det muligt at justere modellerne, så de bliver mere relevante og tilpasset danske brugeres behov, samtidig med at de udjævner eventuelle skævheder eller mangler i de nuværende modeller.</p>",
                    "title": "Hvorfor er præferencedata værdifulde?"
                },
                "3": {
                    "desc": "<p>AI Arenaen positionerer sig som et evaluerings- og tilpasningsværktøj, der er specifikt til dansk, med fokus på svarenes kvalitet og indsamling af præferencedata, og adskiller sig dermed fra den globale rangeringstilgang hos <a href='https://lmarena.ai/' target='_blank'>chatbot arena</a> udviklet af <a href='http://lmsys.org' target='_blank'>lmsys.org</a> og den etiske justering af AI-modeller hos <a href='https://hannahkirk.github.io/prism-alignment/' target='_blank'>Prism Alignment Project</a>.</p>",
                    "title": "Hvad er det særlige ved AI Arenaen sammenlignet med andre lignende initiativer?"
                }
            },
            "title": "Datasæt"
        },
        "ecology": {
            "questions": {
                "1": {
                    "desc": "<p>AI Arenaen bruger den metode, der er udviklet af <a target='_blank' href='https://ecologits.ai/latest/'><strong>Ecologits</strong> (GenAI Impact)</a> til at levere et energiregnskab, der gør det muligt for brugerne at sammenligne den miljømæssige påvirkning fra forskellige AI-modeller for den samme forespørgsel. Denne gennemsigtighed er afgørende for at fremme udviklingen og implementeringen af mere miljøansvarlige AI-modeller.</p><p>Ecologits anvender principperne for livscyklusvurdering (LCA) i overensstemmelse med ISO 14044-standarden ved i øjeblikket at fokusere på påvirkningen fra <strong>inferens</strong> (det vil sige brugen af modeller til at besvare forespørgsler) og <strong>fremstillingen af grafikkort</strong> (udvinding af ressourcer, fremstilling og transport).</p><p>Modellens elforbrug estimeres under hensyntagen til forskellige parametre såsom størrelsen af den anvendte AI-model, placeringen af de servere, hvor modellerne er implementeret, og antallet af output-tokens. Beregningen af indikatoren for globalt opvarmningspotentiale udtrykt i CO2-ækvivalenter er afledt af målingen af modellens elforbrug.</p><p>Det er vigtigt at bemærke, at metoderne til vurdering af AI's miljøpåvirkning stadig er under udvikling, samt at der er tale om estimater.</p>",
                    "title": "Hvordan beregnes miljøindikatorerne?"
                },
                "2": {
                    "desc": "<p>Placeringen af datacentre spiller en rolle for AI's CO2-fodaftryk. Hvis en model trænes eller bruges i et land, der er stærkt afhængigt af fossile brændstoffer, vil dens miljøpåvirkning være større, end hvis den hostes i et land, der primært bruger vedvarende energi.</p><p>Metoden til analyse af AI's miljøpåvirkning udviklet af <a target='_blank' href='https://ecologits.ai/latest/'>Ecologits (fra GenAI Impact)</a>, integrerer data om energimixet i de forskellige lande, hvor serverne befinder sig. Dette gør det muligt at opnå et mere præcist og nuanceret estimat af det faktiske CO2-fodaftryk fra inferens på de forskellige generative AI-modeller.</p>",
                    "title": "Tager miljøindikatorerne hensyn til energimixet i de forskellige lande?"
                },
                "3": {
                    "desc": "<p>De nuværende miljøpåvirkningsindikatorer fokuserer primært på påvirkningen fra <strong>inferens</strong>, det vil sige brugen af AI-modeller til at besvare forespørgsler. Denne tilgang kan give den illusion, at inferens er mindre energikrævende end træning af modeller. Men <strong>virkeligheden er mere kompleks.</strong> Lad os tage analogien med bilen:</p><ul><li>At bygge en bil (træningen) er en engangsproces, der kræver mange ressourcer.</li><li>Hver biltur (inferens) forbruger mindre energi, men disse ture gentages dagligt, og deres antal er potentielt enormt.</li></ul><p>På samme måde <strong>kan den akkumulerede påvirkning fra inferens, i skalaen af millioner af brugere, der foretager daglige forespørgsler, vise sig at være større end påvirkningen fra den indledende træning.</strong> Derfor er det afgørende, at værktøjerne til vurdering af AI's CO2-fodaftryk tager hensyn til <strong>hele livscyklussen</strong> for modellerne, fra træning til brug i produktion</p>",
                    "title": "Tager miljøpåvirkningsindikatorerne hensyn til de ressourcer, der bruges til at træne modellerne?"
                }
            },
            "title": "Miljøindikatorer"
        },
        "i18n": {
            "questions": {
                "1": {
                    "desc": "<p>Ja, internationaliseringen af AI Arenaen er i gang. Projektet havde først success I Frankrig og er nu kommet til Danmark, samt Litauen og Sverige. Denne første fase gør det muligt at teste tilgangen og tilpasse interfacet til forskellige sproglige og kulturelle kontekster i Europa. På sigt kan kredsen udvides til flere europæiske sprog afhængigt af erfaringerne fra disse pilotlande. Målet er gradvist at opbygge en egentligt europæisk digital ressource til menneskelig evaluering af dialogbaserede AI-systemer baseret på en samarbejdsmodel, der skal fastlægges mellem de forskellige deltagende lande.</p>",
                    "title": "Er AI Arenaen kun på dansk, eller er der planer om andre europæiske sprog?"
                },
                "2": {
                    "desc": "<p>Udviklingen af en europæisk platform til sammenligning af dialogbaserede AI-systemer giver flere konkrete fordele. Den gør det muligt at indsamle præferencedata, der afspejler de reelle behov hos europæiske brugere, og dermed forbedre modellernes relevans for dette publikum. Den sikrer således en bedre repræsentation af europæiske sprog og kulturer, som ofte er underrepræsenteret i globale evalueringer domineret af engelsk. Den sikrer også overholdelse af europæiske reguleringer (GDPR, AI Act) og integrerer evalueringskriterier, der er i overensstemmelse med europæiske prioriteter som miljømæssig bæredygtighed og algoritmisk gennemsigtighed. Endelig fremmer den fremkomsten af et konkurrencedygtigt og selvstændigt europæisk AI-økosystem.</p>",
                    "title": "Hvad er fordelene ved en specifikt europæisk platform til indsamling af præferencer?"
                }
            },
            "title": "Internationalisering"
        },
        "models": {
            "questions": {
                "1": {
                    "desc": "<p>Vi vælger modellerne baseret på deres popularitet, diversitet og relevans for brugerne. Vi bestræber os særligt på at gøre såkaldte <em>open weights</em> (semi-åbne) samt <em>open source</em> (åbne) modeller af forskellig størrelse tilgængelige.</p>",
                    "title": "Hvordan vælger I de modeller, der er i sammenligningsværktøjet?"
                },
                "2": {
                    "desc": "<p>Inferens, det vil sige muligheden for at forespørge modellerne, understøttes af projektet. For de fleste modeller går vi gennem Open Router og Hugging Face's API'er, og vi betaler for brugen efter tokens.</p>",
                    "title": "Hvordan er det muligt at gøre denne tjeneste gratis?"
                },
                "3": {
                    "desc": "<p>Kvantiserede modeller er optimeret til at forbruge færre ressourcer ved at forenkle visse beregninger, samtidig med at de sigter mod den bedste svarkvalitet.</p><p>Kvantisering er en optimeringsteknik, der består i at reducere præcisionen af de tal, der bruges til at repræsentere parametrene i en AI-model. Dette gør det muligt at <strong>reducere modellens størrelse</strong> og <strong>fremskynde beregningerne</strong>, hvilket er særligt fordelagtigt ved inferens på maskiner med begrænsede ressourcer. Kvantisering af en model kan således også reducere miljøpåvirkningen.</p>",
                    "title": "Hvad er \"kvanticerede modeller\"?"
                },
                "4": {
                    "desc": "<p><strong>En models evne til at tale flere sprog er knyttet til den sproglige diversitet i dens træningsdata og ikke til landet den var udviklet i</strong>. <strong>LLM'er bruger enorme korpusser på mange sprog</strong>, men fordelingen af sprog i træningsdataene er ikke ensartet. En overrepræsentation af engelsk kan føre til begrænsninger i andre sprog. Disse begrænsninger viser sig for eksempel ved <strong>anglicismer eller en manglende evne til at generere indhold på visse sprog, der er klassificeret som \"truede\" af UNESCO</strong>.</p><p><strong>En models nøjagtighed og ordforrådsrigdom er afhængig af de data, der bruges til dens træning</strong>.</p>",
                    "title": "Er der en sammenhæng mellem nationaliteten af den virksomhed eller det forskningsinstitution, der står bag modellen, og dens evne til at tale flere sprog?"
                },
                "5": {
                    "desc": "<p>Der er få aktører, der er \"transparente\" omkring de datakilder, der bruges i træningskorpusserne. Disse oplysninger er ofte fortrolige af juridiske og kommercielle årsager.</p>",
                    "title": "Kan vi se modellernes træningsdata?"
                }
            },
            "title": "Modeller"
        },
        "title": "Ofte stillede spørgsmål",
        "usage": {
            "questions": {
                "1": {
                    "desc": "<p>De nuværende dialogbaserede sprogmodeller er <strong>ude af stand til at citere kilderne</strong>, som de har brugt til at generere et svar. De fungerer ved at forudsige det næste mest sandsynlige ord baseret på den statistiske fordeling i træningsdataene. Selvom de kan syntetisere information fra forskellige kilder, bevarer de ikke spor af oprindelsen af disse informationer.</p><p>Der findes dog teknikker som <strong>Kildebaseret generering</strong> (Retrieval-Augmented Generation, RAG), der sigter mod at afhjælpe denne begrænsning. Kildebaseret generering gør det muligt for modeller at få adgang til eksterne vidensbaser og <strong>levere kontekstualiseret information med kildehenvisninger</strong>. Denne tilgang er afgørende for at forbedre gennemsigtigheden og pålideligheden af de svar, der genereres af modellerne.</p>",
                    "title": "Kan modellerne citere deres kilder?"
                },
                "2": {
                    "desc": "<p>Du har stillet spørgsmålet \"hvem vandt fodboldkampen i går, og angiv dine kilder\", og du blev skuffet over svarene? Det er normalt…</p><p><strong>De \"rå\" dialogbaserede AI-systemer kan ikke besvare spørgsmål om de seneste nyheder.</strong> De er trænet på statiske datasæt og kan ikke interagere med nettet eller åbne links. De har ikke evnen til at opdatere sig i realtid med de begivenheder, der finder sted i verden. De oplysninger, som modellen har adgang til, er begrænset til datoen for dens seneste træning.</p><p>Hvis du derfor stiller et spørgsmål om en nylig aktuel begivenhed, vil modellen basere sig på potentielt forældede oplysninger med risiko for at generere unøjagtige svar.</p><p>I Perplexity, Copilot eller ChatGPT er de såkaldte \"rå\" dialogbaserede AI-systemer kombineret med andre teknologiske komponenter, der har forbindelse til internettet for at få adgang til realtidsoplysninger. Man taler da om \"dialogbaserede agenter\".</p>",
                    "title": "Hvis jeg stiller et spørgsmål om de seneste nyheder, kan modellen så svare?"
                },
                "3": {
                    "desc": "<p>Hvis du inkluderer en URL i et spørgsmål, kan det dialogbaserede system ikke få adgang til den direkte. Sprogmodellerne behandler teksten i forespørgslen, men har ikke evnen til at interagere med nettet eller åbne links. De er trænet på et fast tekstdatasæt, og deres svar er baseret på disse træningsdata. Når et spørgsmål stilles, bruger modellerne denne træning til at generere et svar, men kan ikke få adgang til nye oplysninger online.</p><p>Som en analogi kan du forestille dig en studerende, der tager en eksamen uden adgang til internettet. Vedkommende kan bruge sin erhvervede viden til at besvare spørgsmålene, men kan ikke tjekke hjemmesider for at få yderligere information.</p>",
                    "title": "Hvis jeg inkluderer et URL-link i et spørgsmål, kan modellen så få adgang til det?"
                },
                "4": {
                    "desc": "<p>Det sker, at modellerne mister tråden i en samtale på grund af deres <strong>begrænsede kontekstvindue.</strong> Dette \"vindue\" repræsenterer mængden af forudgående information, som modellen kan fastholde, og fungerer som en korttidshukommelse. Jo mindre vinduet er, desto mere tilbøjelig er modellen til at glemme nøgleelementer i samtalen, hvilket fører til usammenhængende svar. Lange eller komplekse samtaler kan hurtigt fylde kontekstvinduet, hvilket øger risikoen for et usammenhængende svar.</p><p>Som en analogi kan du forestille dig en person, der kun husker de seneste fem sætninger i en samtale. Hvis samtalen er kort, kan personen følge med. Men hvis samtalen bliver lang, vil personen glemme afgørende information, hvilket vil gøre vedkommendes svar usammenhængende. På samme måde kan en AI-model med et lille kontekstvindue \"miste tråden\" i en samtale, når der udveksles for mange oplysninger, glemme pointer og producere svar, der ikke længere giver mening.</p>",
                    "title": "Hvorfor mister nogle modeller hurtigt tråden i samtalen?"
                },
                "5": {
                    "desc": "<p>Formuleringen af spørgsmål, eller \"prompts\", påvirker samtalens sammenhæng. For at opnå de bedste resultater fra en sprogmodel er det essentielt at mestre kunsten at <em>prompte</em>, det vil sige formuleringen af forespørgsler eller instruktioner. <strong>Klarhed er altafgørende</strong>:</p><ul><li>Brug et simpelt og direkte sprog, og undgå for lange eller komplekse spørgsmål. Opdel forespørgsler i flere simplere spørgsmål for mere præcise svar.</li><li><strong>Præcisér om nødvendigt specifikke formatkrav</strong>: Hvis du har brug for et svar i et bestemt format (liste, tabel, resumé osv.), så angiv det i prompten. Du kan også præcisere de trin, der skal følges, og de ønskede kvalitetskriterier.</li><li><strong>Specificer modellens rolle</strong>: Start for eksempel med \"Agér som en ekspert i...\" eller \"Forestil dig, at du er en lærer...\" for at styre tonen og perspektivet i svaret.</li><li><strong>Kontekstualiser dine spørgsmål</strong>: hvis nødvendigt, giv relevante eksempler for at guide modellen.</li><li><strong>Tilskynd til ræsonnement</strong>: brug opfordring til trin-for-trin ræsonnement (<em>Chain-of-Thought Prompting</em>) for at bede modellen om at forklare sin tankegang, hvilket gør svarene mere robuste.</li></ul><p>Samtalemodeller er følsomme over for variationer i formuleringen: et simpelt sprog, korte spørgsmål og en omformulering hvis nødvendigt kan hjælpe med at guide modellen mod relevante svar. Test og finpuds dine prompts for at finde den mest effektive formulering!</p>",
                    "title": "Hvad er de gode praksisser for at prompte?"
                },
                "6": {
                    "desc": "<p>Chatbots svarer direkte ved at formulere sætninger ud fra et stort datasæt, som modellen er blevet trænet på, mens en søgemaskine foreslår links og ressourcer, som brugeren selv kan udforske.</p>",
                    "title": "Hvad er forskellen mellem at stille et spørgsmål til en chatbot og at lave en søgning på Google?"
                }
            },
            "title": "Brug"
        }
    },
    "footer": {
        "backHome": "TIlbage til hjem - AI Arenaen",
        "dpg": "<p>Tjenesten er anerkendt som et digitalt fællesgode af Digital Public Goods Alliance</p>",
        "helpUs": "Hjælp os med at forbedre produktet!",
        "license": {
            "linkTitle": "Etalab-licens - nyt vindue",
            "mention": "Medmindre andet udtrykkeligt er angivet som tredjeparts intellektuel ejendom, tilbydes indholdet på dette websted under <a {linkProps}>Etalab 2.0-licensen</a>"
        },
        "links": {
            "accessibility": "Tilgængelighed: ikke compliant",
            "legal": "Om os",
            "privacy": "Privatlivspolitik",
            "rgesn": "Miljøvenligt design",
            "sources": "Kildekode",
            "tos": "Brugsbetingelser"
        },
        "writeUs": "Hvis du støder på et problem eller har feedback til AI-Arenaen, er du velkommen til at skrive til os på <a {contactLinkProps}>kontakt@ai-arenaen.dk</a>."
    },
    "general": {
        "a11y": {
            "desc": "Denne tilgængelighedserklæring gælder for webstedet <strong> ai-arenaen.dk </strong> .",
            "disclaimer": "<strong> AI Arenaen </strong> forpligter sig til at gøre sine digitale tjenester tilgængelige i overensstemmelse med artikel 47 i lov nr. 2005-102 a' 11. februar 2005.",
            "improveAdress": "Adresse: DINUM, 20 avenue de Ségur 75007 Paris",
            "improveDelay": "Vi forsøger at svare inden for 2 hverdage.",
            "improveDesc": "Hvis du ikke kan få adgang til noget indhold eller nogen tjeneste, kan du kontakte administratoren af ai-arenaen.dk for at blive henvist til et tilgængeligt alternativ eller få indholdet i et andet format.",
            "improveMail": "E-mail: <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Forbedring og kontakt",
            "remedyAdvocate": "Skriv en besked til <a {linkProps}>Ombudsmanden</a>",
            "remedyAdvocateAdress": "Send et brev med posten: Ombudsmanden - Gratis svar 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Kontakt repræsentanten for <a {linkProps}>ombudsmanden i dit område</a>",
            "remedyDesc": "Denne procedure skal anvendes i følgende tilfælde: Du har rapporteret en tilgængelighedsfejl til webstedsadministratoren, som forhindrer dig i at få adgang til indhold eller en af portalens tjenester, og du har ikke modtaget et tilfredsstillende svar.",
            "remedyList": "Du kan:",
            "remedyTitle": "Klageadgang",
            "stateDesc": "Hjemmesiden ai-arenaen.dk overholder ikke RGAA 4.1. Webstedet er endnu ikke blevet auditeret. <strong>Det er dog designet til at være tilgængeligt for så mange mennesker som muligt </strong> . Du bør derfor kunne:",
            "stateNavigate": "navigere på alle sider på webstedet ved hjælp af et tastatur",
            "statePrefs": "tilpasse siden til dine præferencer (skriftstørrelse, skærmzoom, ændring af typografi osv.) uden tab af indhold",
            "stateScreenReader": "se hjemmesiden med en skærmlæser.",
            "stateTitle": "Overholdelsesstatus",
            "title": "Erklæring om tilgængelighed"
        },
        "legal": {
            "a11yDesc": "Overholdelse af standarder for digital tilgængelighed er et fremtidigt mål, men vi stræber efter at gøre dette websted tilgængeligt for alle.",
            "a11yTitle": "Tilgængelighed",
            "directorDesc": "Romain Delassus, chef for den digitale afdeling i Kulturministeriet",
            "directorTitle": "Publikationens direktør",
            "editorDesc": "Denne hjemmeside er udgivet af det franske kulturministerium, 182 Rue Saint-Honoré, 75001 Paris",
            "editorTitle": "Udgivet",
            "hostingDesc": "Denne hjemmeside hostes af OVH SAS (<a {linkProps}>https://www.ovh.com</a>), som ligger på adressen 2 Rue Kellermann, 59100 Roubaix, Frankrig.",
            "hostingTitle": "Hosting af hjemmesiden",
            "reportA11y": "Hvis du støder på et tilgængelighedsproblem, der forhindrer dig i at få adgang til indhold eller funktioner på webstedet, må du endelig give os besked.",
            "reportA11yDesc": "For at lære mere om statens politik for digital tilgængelighed: <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "reportDesc": "Hvis du ikke modtager et hurtigt svar fra os, har du ret til at indgive din klage eller en anmodning om henvisning til Ombudsmanden.",
            "reportTitle": "Rapporter et problem",
            "securityCertif": "Hjemmesiden er beskyttet af et elektronisk certifikat, der i de fleste browsere vises som et hængelås. Denne beskyttelse hjælper med at sikre fortroligheden af udvekslinger.",
            "securityNoMail": "Under ingen omstændigheder vil de tjenester, der er forbundet med platformen, lede til e-mails, der beder om indtastning af personlige oplysninger.",
            "securityTitle": "Sikkerhed",
            "sources": "Medmindre andet er angivet, er alle tekster på denne hjemmeside underlagt <a {etalabLinkProps}>Etalab Open 2.0-licensen</a>. Kildekoden til denne applikation kan frit genbruges og er tilgængelig på <a {githubLinkProps}>GitHub</a>.",
            "title": "Om os"
        },
        "privacy": {
            "cookiesBannerDesc": "Det er rigtigt, du behøvede ikke at klikke på et vindue, der dækkede halvdelen af siden, for at sige, at du accepterer brugen af cookies – selvom du ikke ved, hvad det betyder!",
            "cookiesBannerNoNeed": "Intet usædvanligt, ingen særlig behandling i forbindelse med et .gouv.fr-domæne. Vi overholder blot loven, som fastslår, at visse værktøjer til publikumssporing, der er korrekt konfigureret til at respektere privatlivets fred, er undtaget fra forudgående godkendelse.",
            "cookiesBannerTitle": "Hvorfor vises der ikke et banner om cookie-samtykke på dette websted?",
            "cookiesBannerTools": "Vi bruger <a {matomoLinkProps}>Matomo</a>, et <a {libreLinkProps}>gratis</a> værktøj, der er konfigureret til at overholde CNIL's \"Cookies\" <a {cnilLinkProps}>anbefaling</a>. Det betyder, at din IP-adresse for eksempel anonymiseres, inden den registreres. Det er derfor umuligt at knytte dine besøg på dette websted til din person.",
            "cookiesDesc": "Denne hjemmeside placerer en lille tekstfil (en \"cookie\") på din computer, når du besøger den. Dette giver os mulighed for at måle antallet af besøg og forstå, hvilke sider der er mest besøgte.",
            "cookiesDescMore": "Du kan fravælge sporing af din browsing på dette websted. Dette beskytter dit privatliv, men det forhindrer også ejeren i at lære af dine handlinger og skabe en bedre oplevelse for dig og andre brugere.",
            "cookiesTitle": "Cookies og samtykke",
            "dataAccessDatasets": "Brugerdialog og præferencedata distribueres under Etalabs Open License 2.0 på Hugging Face-platformen samt på Data.gouv.fr via det franske kulturministeries konto (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "dataAccessDesc": "Selvfølgelig! Webstedets brugsstatistikker er frit tilgængelige på <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessTitle": "Jeg bidrager til at berige dine data, kan jeg få adgang til dem?",
            "dataExtraCountry": "Destinationsland: Frankrig",
            "dataExtraHost": "Underleverandør: OVH",
            "dataExtraTitle": "Hvem hjælper os med at behandle dataene?",
            "dataExtraWarranty": "Garantier: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>",
            "dataExtraWhat": "Udført behandling: Hosting",
            "dataRespDesc": "Det franske kulturministeriums digitale afdeling er ansvarlig for behandlingen af dine personoplysninger.",
            "dataRespTitle": "Hvem er ansvarlig for databehandlingen?",
            "dataTimeDesc": "Data vedrørende brugere og deres samtaler med modeller gemmes fra det tidspunkt, hvor præferenceafstemningen registreres.",
            "dataTimeTitle": "Hvor længe opbevarer vi disse data?",
            "dataUseDesc": "Under alle omstændigheder forpligter udgiveren sig til at implementere foranstaltninger, der sikrer anonymiseringen af dialogdata, inden disse gøres offentligt tilgængelige.",
            "dataUseTitle": "Hvilken behandling foretages der af samtaledataene?",
            "desc": "Tjenesten udgives af det franske kulturministeriums digitale afdeling.",
            "privacyData": "De data, der indsamles på hjemmesiden, er følgende:",
            "privacyDataArena": "Data relateret til brugeres samtaler med modellerne: spørgsmål stillet af brugere, modellernes svar og brugerpræferencer udtrykt mellem de to modeller",
            "privacyDataForm": "Data relateret til spørgeskemaet \"Hjælp os med at forbedre compar:IA\".",
            "privacyDesc": "Tjenesten behandler ikke personoplysninger som defineret af CNIL, dvs. oplysninger, der direkte eller indirekte kan henføres til en identificerbar fysisk person.",
            "privacyResp": "Brugeren er ansvarlig for de data eller det indhold, som vedkommende indtaster i den prompt, som platformen stiller til rådighed. Ved at acceptere <a {linkProps}>brugsbetingelserne</a> accepterer brugeren ikke at overføre oplysninger, der kan identificere vedkommende selv eller en tredjepart.",
            "privacyTitle": "Behandler vi personoplysninger?",
            "title": "Fortrolighedspolitik"
        },
        "rgesn": {
            "1": {
                "1": "The AI-Arenaen giver nem og gratis adgang til en bred vifte af konvertionelle AI-modeller, hvilket øger offentlighedens bevidsthed om de forskellige udfordringer ved generativ AI: diversitet, bias og miljøpåvirkning. Desuden deles datasættene af spørgsmål og præferencer som open source under MIT og creative common (CC-BY) kompatible Etalab 2.0-licensen.",
                "2": "Målgruppen for AI-Arenaen blev identificeret ved at gennemføre brugerinterviews og en indledende fase af behovsundersøgelse.",
                "3": "Således er alle borgere berørte som potentielle brugere, men særligt lærere, elever, studerende, workshopfacilitatorer, AI-trænere samt forskere (datalogi, AI, humaniora og samfundsvidenskab) og virksomheder, der er interesserede i at analysere eller bruge open source-delte datasæt. Derfor imødekommer den digitale tjeneste deres behov gennem følgende funktioner: afprøvning og sammenligning af modeller gratis samt brug af open source-delte data.",
                "4": "AI-Arenaen indgår i en bæredygtig designtilgang, der sigter mod at reducere miljøpåvirkningerne. Med henblik herpå er erklæringen blevet udarbejdet som led i implementeringen af den generelle referenceramme for bæredygtigt design af digitale tjenester (RGESN, version 2024).<br> RGESN, udarbejdet af Arcep og Arcom i samarbejde med ADEME, DINUM, CNIL og Inria, er tilgængelig på <a {linkProps}>Arceps websted</a>.",
                "title": "Formålet med denne tjeneste"
            },
            "2": {
                "1": "Bæredygtigt design af digitale tjenester er en tilgang, der sigter mod at reducere og begrænse miljøpåvirkningerne fra en tjeneste fra dens designfase og gennem hele dens livscyklus.",
                "2": {
                    "1": "Designe mere bæredygtige digitale tjenester for at forlænge enhedernes levetid;",
                    "2": "Fremme en tilgang baseret på miljømæssig soberhed som svar på strategier for indfangning af brugerens opmærksomhed, i overensstemmelse med internationale miljømål;",
                    "3": "Reducere de anvendte IT-ressourcer, optimere datatrafik og brugen af digital infrastruktur;",
                    "4": "Øge transparensniveauet omkring den digitale tjenestes miljøaftryk.",
                    "title": "Implementeringen har fire hovedmål:"
                },
                "3": {
                    "1": "Begrænse størrelsen af en side",
                    "2": "Begrænse kompleksiteten af en side",
                    "3": "Begrænse antallet af serverforespørgsler",
                    "4": "Begrænse det energiforbrug, der kræves for at hoste tjenesten",
                    "title": "I praksis oversættes bæredygtigt design til et sæt best practices, der skal følges før, under og efter oprettelsen eller forbedringen af en tjeneste, hvilket særligt muliggør:"
                },
                "4": "Dette er en proces med løbende og kollektiv forbedring.",
                "title": "Hvad er bæredygtigt design?"
            },
            "3": {
                "1": {
                    "1": "Forenkling af brugeroplevelsen: navigationssti bliver optimeret og centreret omkring AI-Arenaens væsentlige funktioner (arena af modeller at afprøve, datasæt, rangering) med regelmæssig brugertest.",
                    "2": "Øge brugernes bevidsthed om tjenestens økologiske fodaftryk: fremhævelse af energipåvirkningsindikatorer på spørgsmål stillet til AI; fremme af \"sparsommelig\" brug med en tilstand dedikeret til forespørgsler til \"små\" konverserende AI-modeller, hvis miljøpåvirkning er mindre.",
                    "3": "Brug af State Design System (DSFR) grænseflade-komponenter: de er tilgængelige, og deres fodaftryk er minimalt.",
                    "4": "Den mest intuitiv navigation muligt for at reducere den tid, der bruges på at finde information.",
                    "5": "Mobile-first design: alt indhold kan ses på mobile enheder. Siden tilpasser sig alle skærmstørrelser.",
                    "6": "Udelukkelse af videointegration på siden.",
                    "title": "Hovedudfordringen er at sikre, at AI-Arenaen-tjenesten fungerer på de ældst mulige brugerenheder under varierende forbindelsesbetingelser. Flere foranstaltninger blev implementeret under designfasen, herunder:"
                },
                "title": "Implementeret strategi og mål vedrørende reduktion eller begrænsning af miljøpåvirkninger"
            },
            "4": {
                "1": "Selvevalueringen, gennemført mellem maj og oktober 2025 af produktteamet (to personer var ansvarlige for at gennemgå og validere alle kriterier), dækkede alle sider på webstedet. De værktøjer, der blev brugt til at evaluere visse tekniske indikatorer, var browserens udviklerværktøjs \"netværks\"-inspektør, <a {greenitLinkProps}>Green IT Analysis</a>-udvidelsen og <a {ecoindexLinkProps}>EcoIndex</a>-værktøjet.",
                "title": "RGESN-diagnose"
            },
            "5": {
                "1": {
                    "1": "Fremskridtsscore i implementeringen af RGESN pr. 12/10/2025: 89%",
                    "title": "Selvdiagnosen giver en overensstemmelsesrate med <a {linkProps}>RGESN</a> (generel bæredygtig design-reference) på 89%."
                },
                "2": "Den digitale tjeneste AI-Arenaen sigter mod at forbedre denne fremskridtsscore for at nå 95% inden 2027.",
                "3": {
                    "1": "Strategi: 100%",
                    "2": "Specifikationer: 78%",
                    "3": "Arkitektur: 100%",
                    "4": "Brugeroplevelse og grænseflade (UX/UI): 100%",
                    "5": "Indhold: 67%",
                    "6": "Frontend: 82%",
                    "7": "Backend: 64%",
                    "8": "Hosting: 90%",
                    "9": "Algoritmer: 100%",
                    "title": "Score efter tema:"
                },
                "4": {
                    "date": "10. december 2025",
                    "title": "RGESN-revisionstabel"
                },
                "title": "Score i implementeringen af referencerammen"
            },
            "6": {
                "1": "Siden er designet til at være tilgængelig for enhver mobil enhed fra 2017 eller senere.",
                "2": "Minimumsforbindelse påkrævet for komfortabel adgang og brug af tjenesten: 3G på mobil og 512 Kbps på fast forbindelse.",
                "3": "Siden er designet til forskellige skærmstørrelser (minimum 320 pixels bred).",
                "4": "Siden er ikke tilgængelig via Internet Explorer-browseren. ",
                "title": "Minimumskrav for at tilgå siden"
            },
            "7": {
                "1": "Designe tjenesten med et designreview og et kodereview, hvor et af målene er at reducere miljøpåvirkningerne fra hver funktionalitet (kriterium 2.1)",
                "2": "Implementere dataopbevaringsperioder med henblik på sletning eller arkivering: regelmæssig sletning af logs i \"S3\"-filserveren og databasen (Postgres) i særdeleshed.",
                "3": {
                    "1": "Maksimalt antal serverforespørgsler per skærm: 40",
                    "2": "Maksimal ressourcestørrelse per skærm: 600 KB",
                    "title": "For at overholde en maksimal vægt og en grænse for antallet af forespørgsler per skærm (kriterium 6.1):"
                },
                "4": "Konvertere illustrationerne til WebP, AVIF, JPEG XL eller et mere effektivt billedformat til rasterbilleder (kriterium 5.1).",
                "5": "Muliggøre muligheden for at afbryde inferens (indlæsning af svar) under modelsammenligning.",
                "6": "EcoIndex-analysescores demonstrerer en betydelig forskel i påvirkningsmålinger, hvor nogle sider modtager scores op til E eller F (relativt stor sidevægt, kompleksitet og antal forespørgsler); udfordringen nu er at forstå disse resultater mere præcist fra sag til sag, på trods af de implementerede best practices, for at identificere yderligere forbedringer.",
                "desc": "Flere forbedringer er blevet eller er stadig ved at blive identificeret, herunder:",
                "title": "Identificerede forbedringer"
            },
            "desc": "offentliggjort den 11. december 2025",
            "title": "Erklæring om bæredygtigt design"
        },
        "tos": {
            "contactDesc": "Hvis du har spørgsmål om tjenesten, kan du skrive til <a {linkProps}>{contactLink}</a>.",
            "contactTitle": "9. Kontakt",
            "defsEditor": "\"Udgiver\" henviser til Kulturministeriets digitale afdeling.",
            "defsModels": "\"Modeller\" henviser til de store sprogmodeller (LLM'er), der genbruges under deres brugslicens af platformen for at opfylde dens formål.",
            "defsPlatform": "\"Platform\" henviser til den hjemmeside, der gør tjenesterne tilgængelige.",
            "defsServices": "\"Tjenester\" henviser til de funktioner, som platformen tilbyder for at opfylde sine formål.",
            "defsTitle": "2. Definitioner",
            "defsUser": "\"Bruger\" henviser til enhver fysisk person, der bruger platformen og drager fordel af dens tjenester.",
            "descDatasets": "Disse datasæt vil blive gjort tilgængelige under en åben licens, især for at fremme forskningsformål.",
            "descEditor": "Arenaen er en platform, udgivet af det franske kulturministeriums digitale afdeling, til sammenligning af samtalemodeller rettet mod den brede offentlighed med det formål at (1) øge borgernes bevidsthed om store sprogmodeller (LLM'er) og (2) indsamle brugerpræferencer for at skabe afstemningsdatasæt.",
            "descTitle": "3. Platformsbeskrivelse",
            "descUse": "Brugeren stiller et spørgsmål på et givet sprog og modtager svar fra to anonyme store sprogmodeller (LLM'er). Hun/han stemmer på den model, der giver hendes/hans foretrukne svar, og får derefter vist modellernes identitet. Dette deltagende produktionssystem, der er inspireret af platformen \"<a {linkProps}>chatbot arena</a>\" (LMarena), gør det muligt at oprette datasæt med menneskelige præferencer for ægte opgaver på dansk, som kan bruges til modeltræning.",
            "dispoDesc": "Platformen er tilgængelig, undtagen i tilfælde af force majeure eller begivenheder, der ligger uden for udgiverens kontrol.",
            "dispoResp": "I denne henseende kan udgiveren ikke holdes ansvarlig for tab eller skader af nogen art, der måtte opstå som følge af en fejlfunktion eller utilgængelighed af tjenesten. Sådanne situationer giver ikke ret til økonomisk kompensation.",
            "dispoRight": "Udgiveren forbeholder sig ret til uden forudgående varsel at suspendere, afbryde eller begrænse adgangen til alle eller en del af tjenesterne, især i forbindelse med vedligeholdelses- og opdateringsarbejder, der er nødvendige for, at tjenesten og det tilhørende udstyr kan fungere korrekt, eller af andre årsager, herunder tekniske årsager.",
            "dispoTitle": "7. Tjenestens tilgængelighed",
            "dispoWarranty": "Det kan ikke garanteres, at tjenesten vil være fri for fejl eller uregelmæssigheder. Tjenesten leveres derfor uden nogen form for garanti med hensyn til dens tilgængelighed og ydeevne.",
            "evoDesc": "Brugsvilkårene kan til enhver tid ændres eller suppleres uden forudgående varsel, afhængigt af ændringer i tjenesterne, ændringer i lovgivningen eller af andre årsager, der anses for nødvendige.",
            "evoDescMore": "Disse ændringer og opdateringer er bindende for brugeren, som derfor bør regelmæssigt henvise til dette afsnit for at kontrollere de aktuelle generelle vilkår.",
            "evoTitle": "8. Ændringer af brugsbetingelserne",
            "featuresDatasets": "Tjenesten indsamler dialogdata (spørgsmål, svar fra begge modeller) og brugerpræferencer (stemmer, tilhørende metadata). Disse data bruges både til at udgøre åbne datasæt og til at etablere en rangliste over AI-modeller, der vises på platformen, baseret på de afgivne stemmer. Ranglisten vises først når der er nok stemmer.",
            "featuresDatasetsMore": "Udgiveren forbeholder sig ret til at distribuere brugerens dialog- og præferencedata under en etalab 2.0-licens. Datasættet distribueres på Data.gouv og Hugging Face-platformen via det franske kulturministeriums konto (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "featuresDesc": "For at opfylde det dobbelte mål om at øge borgernes bevidsthed om store sprogmodeller og indsamle brugerpræferencer tilbyder platformen følgende tjenester uden adgangsbegrænsninger:",
            "featuresDescMore": "En menneske-maskine-grænseflade, der giver brugerne mulighed for at føre en dialog med to samtalemodeller samtidigt og stemme på det foretrukne svar.",
            "featuresModels": "Modellerne, der er integreret i platformen, er sat op på inferensserverne hos de forskellige hosting-udbydere og modeludgivere. Forespørgslerne håndteres blandt andet af tjenesten Open Router. Betingelserne for inferensstandardisering er oplyst på platformen for at garantere gennemsigtighed i brugen af modellerne.",
            "featuresModelsMore": "Modelsammenligningsgrænsefladen.",
            "featuresTitle": "4. Funktioner",
            "featuresVote": "Når afstemningen er afsluttet, kan brugeren se listen over modeller, der er integreret i AI Arenaen, og få adgang til en liste med oplysninger om disse modeller. Oplysningerne om modellerne er hentet fra forskellige kilder.",
            "featuresVoteMore": "Deling og adgang til datasæt, der er resultatet af indsamling af brugerpræferencer.",
            "licenceCode": "Platformens kildekode er åben og tilgængelig her: <a {linkProps}>https://github.com/betagouv/ComparIA</a>",
            "licenceLLM": "De LLM'er, der bruges til at drive tjenesterne, er underlagt følgende licenser:",
            "licenceLLMEvolution": "Listen over sprogmodeller, der er integreret i platformen, kan ændres over tid og opdateres ved hver ændring.",
            "licenceLLMLicence": "Licens",
            "licenceLLMModel": "Samtale AI-model",
            "licenceLLMNoticeLink": "Link til model-licenserne",
            "licenceLLMUnavailable": "Ikke tilgængelig",
            "licenceTitle": "6. Kode og licenser",
            "respEditor": "Generelt fraskriver udgiveren sig ethvert ansvar i tilfælde af manglende overholdelse af brugsbetingelserne.",
            "respLegal": "Platformen er ikke beregnet til at blive brugt til at generere ulovligt indhold eller indhold, der strider mod den offentlige orden, og mere generelt til generering, der overtræder den gældende lovgivning.",
            "respLegalMore": "I denne henseende må brugeren ikke indtaste indhold eller oplysninger i prompten, der er i strid med gældende lovgivningsmæssige og reguleringsmæssige bestemmelser.",
            "respPrivacy": "Da de data, som brugeren indtaster på platformen, er beregnet til at blive gjort tilgængelige, forpligter brugeren sig til ikke at overføre oplysninger, der kan identificere brugeren eller en tredjepart.",
            "respPrivacyMore": "Under alle omstændigheder forpligter udgiveren sig til at implementere midler til at sikre anonymiseringen af dialogdata, inden disse gøres tilgængelige. Hvis der på trods af udgiverens indsats skulle blive offentliggjort følsomme data i datasættene, kan du straks rapportere det via denne formular: [https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ](https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ).",
            "respTitle": "5. Ansvar",
            "respUser": "Brugeren er ansvarlig for de data eller det indhold, som vedkommende indtaster i den prompt, som platformen stiller til rådighed.",
            "scopeDesc": "Adgang til platformen er gratis, kræver ikke registrering og indebærer anvendelse af specifikke betingelser, der er angivet i disse brugsbetingelser.",
            "scopeTitle": "1. Anvendelsesområde",
            "title": "Brugsbetingelser"
        }
    },
    "generated": {
        "archs": {
            "dense": {
                "desc": "Den dense arkitektur betegner en type neuralt netværk, hvor hver neuron i et lag er forbundet til alle neuroner i det næste lag. Dette gør det muligt for alle parametrene i laget at bidrage til beregningen af outputtet.",
                "name": "Dense",
                "title": "Dense Arkitektur"
            },
            "matformer": {
                "desc": "Forestil dig russiske matryoshka dukker (matryoshkas → matryoshka transformer → Matformer): hver blok indeholder flere indlejrede undermodeller af stigende størrelser, som deler de samme parametre. Dette gør det muligt ved hver forespørgsel at vælge en model med passende kapacitet, afhængigt af den tilgængelige hukommelse eller latens, uden behov for at genoptræne forskellige modeller.",
                "name": "Matformer",
                "title": "Arkitektur Matformer"
            },
            "moe": {
                "desc": "Mixture of Experts (MoE) arkitekturen bruger en routingmekanisme til kun at aktivere visse specialiserede delmængder (\"eksperter\") af det neurale netværk, afhængigt af input. Dette gør det muligt at konstruere meget store modeller, samtidig med at beregningsomkostningerne holdes lave, da kun en del af netværket bruges ved hvert trin.",
                "name": "MoE",
                "title": "Arkitektur MoE"
            },
            "na": {
                "desc": "Udgiveren har ikke offentliggjort informationerne om modellens arkitektur.",
                "name": "Proprietær",
                "title": "Arkitektur N/A"
            }
        },
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "license_desc": "<p>Denne licens gør det muligt at bruge, modificere og distribuere modellen frit, herunder til kommercielle formål. Ud over brugsfriheden garanterer den juridisk beskyttelse ved at inkludere en patentlicensklausul, der fungerer som en forsikring: hvis du bruger denne model, forpligter bidragyderne sig til ikke at sagsøge dig for krænkelse af deres patenter relateret til projektet. Denne gensidige beskyttelse undgår juridiske konflikter mellem brugere og udviklere. Ved distribution af modificerede versioner skal betydelige ændringer angives med passende bemærkninger, hvilket garanterer gennemsigtighed for brugeren.</p>"
                },
                "CC-BY-NC-4.0": {
                    "license_desc": "<p>Denne licens gør det muligt at dele og tilpasse indholdet frit, forudsat at man krediterer ophavsmanden, men forbyder enhver kommerciel brug. Den tilbyder fleksibilitet til ikke-kommercielle formål, samtidig med at den beskytter ophavsmandens rettigheder.</p>",
                    "reuse_specificities": "men kun til ikke-kommercielle formål"
                },
                "Gemma": {
                    "license_desc": "<p>Denne licens er designet til at tilskynde til brug, modificering og redistribution af software, men inkluderer en klausul, der fastslår, at alle modificerede eller forbedrede versioner skal deles med fællesskabet under den samme kildelicens, hvilket således fremmer samarbejde og gennemsigtighed i softwareudvikling.</p>"
                },
                "Jamba Open Model": {
                    "commercial_use_specificities": "under 50 millioner dollars i årlig omsætning",
                    "license_desc": "<p>Denne licens gør det muligt at bruge, reproducere, modificere og distribuere koden frit med kreditering, men pålægger restriktioner for organisationer, der overstiger 50 millioner dollars i årlig omsætning.</p>"
                },
                "LFM 1.0": {
                    "license_desc": "<p>Denne verdensomspændende, gratis og ikke-eksklusive licens tillader brugen, modifikationen og redistributionen af modellen og dens afledte værker, herunder genanvendelse af outputs til at træne andre modeller. Den forbliver liberal, men begrænser enhver kommerciel brug til organisationer, hvis årlige indtægter er under 10 millioner dollars, hvorefter brugen ikke længere er dækket.</p>"
                },
                "Llama 3 Community": {
                    "commercial_use_specificities": "Under 700 millioner brugere",
                    "license_desc": "<p>Denne licens gør det muligt at bruge, modificere og distribuere koden frit med kreditering, men pålægger restriktioner for operationer, der overstiger 700 millioner månedlige brugere, og forbyder genanvendelse af koden eller det genererede indhold til træning eller forbedring af konkurrerende modeller, hvilket således beskytter Metas teknologiske investeringer og brand.</p>"
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "Under 700 millioner brugere",
                    "license_desc": "<p>Denne licens gør det muligt at bruge, reproducere, modificere og distribuere koden frit med kreditering, men pålægger restriktioner for operationer, der overstiger 700 millioner månedlige brugere. Genanvendelse af koden eller det genererede indhold til træning eller forbedring af afledte modeller er tilladt, forudsat at man viser \"built with llama\" og inkluderer \"Llama\" i deres navn ved enhver distribution.</p>"
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "under 700 millioner brugere",
                    "license_desc": "<p>Denne <strong>ikke-eksklusive, globale og royalty-fri</strong> licens gør det muligt at bruge, reproducere, modificere og distribuere koden og Llama 3.3-materialerne frit med kreditering. Den tillader især genanvendelse til forbedring af afledte modeller, men pålægger restriktioner for kommercielle operationer af meget stor skala.</p>"
                },
                "Llama 4": {
                    "commercial_use_specificities": "under 700 millioner brugere\n",
                    "license_desc": "<p>Denne ikke-eksklusive, globale og royalty-fri licens gør det muligt at bruge, reproducere, modificere og distribuere Llama 4-materialerne (modeller og dokumentation) med kreditering. Den pålægger dog to væsentlige restriktioner: (1) virksomheder, der overstiger 700 millioner aktive månedlige brugere, skal indhente en særlig licens fra Meta, og (2) <strong>total udelukkelse</strong> af personer bosiddende i EU og virksomheder med hovedsæde i EU fra direkte brug af de multimodale modeller på grund af regulatorisk usikkerhed relateret til den europæiske AI Act. Europæiske slutbrugere kan dog få adgang til tjenester, der integrerer Llama 4, forudsat at de leveres fra uden for EU.</p>"
                },
                "MIT": {
                    "license_desc": "<p>MIT-licensen er en permissiv fri software-licens: den tillader enhver at genbruge, modificere og distribuere modellen, selv til kommercielle formål, forudsat at den oprindelige licens og ophavsretsnoter inkluderes.</p>"
                },
                "Mistral AI Research License": {
                    "license_desc": "<p>Denne ikke-eksklusive og royalty-fri licens tillader brug, kopiering, modificering og distribution af Mistral-modellerne og deres afledte versioner (inklusiv modificerede eller fintuned versioner). Den er dog strengt begrænset til forskningsformål.</p>",
                    "reuse_specificities": "men kun til ikke-kommercielle formål"
                }
            },
            "proprio": {
                "Alibaba": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via API på Alibaba-selskabets platforme, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur."
                },
                "Amazon": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via Amazon Bedrock, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur.",
                    "reuse_specificities": "undtagen til at destillere eller træne andre modeller på Amazons platforme."
                },
                "Anthropic": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via API på Anthropic-selskabets platforme eller partnerselskabers platforme, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur."
                },
                "Google": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via API på Google-selskabets platforme, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på Googles reserverede infrastruktur.",
                    "reuse_specificities": "undtagen til at træne andre modeller på Vertex AI"
                },
                "Liquid": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via API på Liquid AI-selskabets platforme, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens."
                },
                "Mistral AI": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via Mistral API, Amazon Sagemaker og flere andre hosting-udbydere, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur."
                },
                "OpenAI": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via API på OpenAI-selskabets platforme eller via Microsoft Azure-tjenesterne, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur."
                },
                "xAI": {
                    "license_desc": "Modellen er tilgængelig under en betalt licens og tilgængelig via X og xAI, hvilket kræver betaling pr. brug baseret på antallet af behandlede tokens eller på den reserverede infrastruktur."
                }
            }
        },
        "models": {
            "Apertus 70B Instruct": {
                "desc": "<p>Mellemstor open-source model. Dens vægte og træningskode er open-source. Modellen er udviklet af et konsortium af schweiziske institutioner. Den er blevet trænet på mere end 1.800 sprog ud fra 15.000 milliarder tokens. Modellen er trænet på CSCS' supercomputer Alps i Lugano, som er drevet af CO2 neutral vandkraft og er dermed en mere bæredygtig tilgang til AI-udvikling. Den er blevet designet fra starten til at være i overensstemmelse med AI Act.</p>",
                "fyi": "<p>Modellen er blevet trænet på supercomputeren Alps i Lugano ved hjælp af mere end 10 millioner GPU-timer drevet af CO2-neutral vandkraft. Modellen er blevet prætrænet på 15.000 milliarder tokens udelukkende fra offentlige og permissive kilder, der dækker mere end 1.800 sprog, hvoraf en betydelig del er underrepræsenterede sprog.</p>\n<p>Apertus bygger på en byte-level BPE-tokenizer med 131.000 indgange, afledt af Mistral AI's \"tekken\"-tokenizer, optimeret til flersprogethed, kode og matematiske udtryk. Arkitekturen kombinerer flere innovationer: Rotary Positional Embeddings (RoPE) med udvidet base og NTK-aware justering til lange kontekster, Grouped Query Attention (GQA) for bedre hukommelseseffektivitet, QK-Norm normalisering til at stabilisere træningen, og en xIELU-aktiveringsfunktion (extended Integrated ELU), der forbedrer MLP'ernes performance.</p>\n<p>Modellens endelige finjustering bygger på en alignment-algoritme kaldet QRPO (Quantile Reward-Preferring Optimization), et alternativ til klassisk RLHF, som bruger absolutte belønningssignaler til en mere stabil indlæring, der er bedre alignet med menneskelige præferencer. Selvom den ikke konkurrerer direkte med de mest avancerede proprietære modeller, skiller Apertus sig ud ved sin totale åbenhed, videnskabelige stringens og eksemplariske tilgang til gennemsigtighed og regulatorisk compliance.</p>",
                "size_desc": "<p>Med 70 milliarder parametre hører denne model til de små modeller. Den kan bruges lokalt på en kraftig computer, hvilket garanterer databeskyttelse, eller hostes på en server udstyret med et enkelt grafikkort, hvilket begrænser infrastrukturomkostningerne. Dens kontekstvindue på 65.536 tokens gør det muligt at behandle længere dokumenter.</p>"
            },
            "Apertus 8B Instruct": {
                "desc": "<p>Lille semi-åben og reproducerbar model, udviklet af et konsortium af schweiziske institutioner. Dens vægte og træningskode er publiceret under liberal licens. Den er blevet trænet på mere end 1.800 sprog på over 15.000 milliarder tokens. Træningen fandt sted på CSCS' supercomputer Alps i Lugano, drevet af CO2-neutralt vandkraft.</p>",
                "fyi": "<p>Modellen er blevet trænet på supercomputeren Alps i Lugano ved hjælp af mere end 10 millioner GPU-timer drevet af CO2-neutral vandkraft. Modellen er blevet prætrænet på 15.000 milliarder tokens udelukkende fra offentlige og permissive kilder, der dækker mere end 1.800 sprog, hvoraf en betydelig del er underrepræsenterede sprog.</p>\n<p>Apertus bygger på en byte-level BPE-tokenizer med 131.000 indgange, afledt af Mistral AI's \"tekken\"-tokenizer, optimeret til flersprogethed, kode og matematiske udtryk. Arkitekturen kombinerer flere innovationer: Rotary Positional Embeddings (RoPE) med udvidet base og NTK-aware justering til lange kontekster, Grouped Query Attention (GQA) for bedre hukommelseseffektivitet, QK-Norm normalisering til at stabilisere træningen, og en xIELU-aktiveringsfunktion (extended Integrated ELU), der forbedrer MLP'ernes performance.</p>\n<p>Modellens endelige finjustering bygger på en alignment-algoritme kaldet QRPO (Quantile Reward-Preferring Optimization), et alternativ til klassisk RLHF, som bruger absolutte belønningssignaler til en mere stabil indlæring, der er bedre alignet med menneskelige præferencer. Selvom den ikke konkurrerer direkte med de mest avancerede proprietære modeller, skiller Apertus sig ud ved sin totale åbenhed, videnskabelige stringens og eksemplariske tilgang til gennemsigtighed og regulatorisk compliance.</p>",
                "size_desc": "<p>Med 8 milliarder parametre hører denne model til de små modeller. Den kan bruges lokalt på en kraftig computer, hvilket garanterer databeskyttelse, eller hostes på en server udstyret med et enkelt grafikkort, hvilket begrænser infrastrukturomkostningerne. Dens kontekstvindue på 65.536 tokens gør det muligt at behandle ret lange dokumenter.</p>"
            },
            "Aya 23 8B": {
                "desc": "<p>Lille multilingual model, trænet specifikt i stor udstrækning på sprog, der generelt er underrepræsenterede.</p>",
                "fyi": "<p>Aya 23 8B fra Cohere er en lille model fra Command R-familien, som specielt er blevet trænet på et multilingvalt korpus.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig ydeevne til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "Aya Expanse 32B": {
                "desc": "<p>Mellemstor multilingual model, der er i stand til at behandle 23 sprog.</p>",
                "fyi": "<p>Cohere, den canadiske virksomhed bag denne model, blev grundlagt i 2019 af tidligere forskere fra Google Brain, herunder Aidan Gomez, medforfatter til det berømte paper \"Attention Is All You Need\", som revolutionerede AI. Dens primære særkende ligger i dens eksklusive fokus på generativ AI til virksomheder, særligt regulerede sektorer som finans, sundhed, fremstilling og energi samt den offentlige sektor. Virksomheden er også pioner inden for multilingvale tilgange og opretholder et nonprofit-forskningslaboratorium for at støtte åben innovation.</p>\n<p>Denne model er designet til at tilbyde gode kapaciteter i hvert af de 23 sprog i dens træningskorpus.</p>",
                "size_desc": "<p>Med 32 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan hostes på en server udstyret med et enkelt kraftfuldt grafikkort, hvilket bidrager til at begrænse infrastrukturomkostningerne.</p>\n<p>Den har et kontekstvindue på op til 130.000 tokens, hvilket er nyttigt til analyse af lange dokumenter.</p>"
            },
            "Aya Expanse 8B": {
                "desc": "<p>Lille multilingual model, anden iteration af Aya-serien, trænet specifikt i stor udstrækning på sprog, der generelt er underrepræsenterede.</p>",
                "fyi": "<p>Aya Expanse 8B fra Cohere, en canadisk virksomhed, er en lille model fra Command R-familien, som specielt er blevet trænet på et multilingvalt korpus.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig ydeevne til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "Aya23-35B": {
                "desc": "<p>Mellemstor multilingual model, trænet specifikt i stor udstrækning på sprog, der generelt er underrepræsenterede.</p>",
                "fyi": "<p>Aya 23 35B fra Cohere er en mellemstor model fra Command R-familien, som specielt er blevet trænet på et multilingvalt korpus.</p>",
                "size_desc": "<p>Mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og ydeevne: de er meget mindre ressourcekrævende end store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnement.</p>"
            },
            "Chocolatine 14B": {
                "desc": "<p>Baseret på Microsofts Phi-3 Medium-model er denne model blevet specialiseret i det franske sprog.</p>",
                "fyi": "<p>Baseret på Microsofts Phi-3 Medium-model er denne model blevet specialiseret i det franske sprog. Modellens navn 'Chocolatine' er en hentydning til CroissantLLM-projektet, som var et af de første initiativer til at skabe en open source-model af lille størrelse optimeret til fransk.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig ydeevne til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "Chocolatine 2 14B": {
                "desc": "<p>Baseret på Qwen2.5-modellen fra Alibaba-selskabet er denne model blevet specialiseret i det franske sprog.</p>",
                "fyi": "<p>Baseret på Qwen2.5-modellen fra Alibaba-selskabet er denne model blevet specialiseret i det franske sprog. Modellens navn 'Chocolatine' er en hentydning til CroissantLLM-projektet, som var et af de første initiativer til at skabe en open source-model af lille størrelse optimeret til fransk.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig ydeevne til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "Claude 3.5 Sonnet v2": {
                "desc": "<p>Meget ydeevnestærk model inden for kodning, skabt efter en forbedring af post-training sammenlignet med Claude 3</p>",
                "fyi": "<p>Den bedste model i Claude 3.5-familien, denne model er specialiseret i generering af litterære tekster og en mere naturlig tone. Version v2 blev udgivet i oktober 2024.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til ydeevne og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er af en sådan art, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p>Meget stor multimodal og multilingual model, ydeevnestærk til kodegenerering, med to svarmetoder: brugeren kan vælge mellem en ræsonnementsmetode for mere dybdegående svar eller en hurtig metode til direkte at generere det endelige svar.</p>",
                "fyi": "<p>Claude 4 Opus er den mest avancerede version af Claude 4-familien. Den er optimeret til rå kraft og komplekse opgaver, der kræver vedvarende ræsonnement over lange perioder: den kan for eksempel arbejde på langsigtede opgaver (Anthropic erklærer, at den kan arbejde op til syv timer uafhængigt). Til gengæld er Opus dyrere at bruge, langsommere til at svare og kræver flere ressourcer for at fungere.</p>\n<p>Modellen tilbyder to anvendelsesmetoder: en refleksionsmetode med trin-for-trin ræsonnement til komplekse problemer og en hurtig metode til direkte svar. I modsætning til andre modeller er ræsonnementsmetoden ikke primært blevet trænet på matematiske data, men tilpasset til praktiske use cases.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indicier tyder på, at det er en meget stor model, der kræver servere udstyret med flere kraftfulde grafikkort for at fungere. De tilgængelige estimater er baseret på indirekte indicier som inferensomkostninger og svarlantens. Den har et kontekstvindue på op til 200.000 tokens, egnet til analyse af lange dokumenter eller kode-repositories.</p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p>Meget stor multimodal og multilingual model, der er meget kraftfuld inden for kodning. Brugeren eller udvikleren, som anvender denne model, kan vælge mellem flere niveauer af ræsonnement.</p>",
                "fyi": "<p>Claude 4 Sonnet er en mere kompakt version af Claude 4 Opus optimeret til hastighed, effektivitet og tilgængelighed. Den er lidt mindre god til opgaver, der kræver komplekst ræsonnement i flere trin. Ikke desto mindre er den markant billigere, hurtigere og forbruger mindre energi end Opus 4.</p>\n<p>Modellen tilbyder muligheden for at vælge intensiteten af \"ræsonnement\". I modsætning til andre modeller er ræsonnementsmetoden ikke primært blevet trænet på matematiske data, men især på praktiske use cases.</p>",
                "size_desc": "<p>Den nøjagtige størrelse er ikke kendt. Indicier tyder på, at det er en meget stor model, der kræver servere udstyret med flere kraftfulde grafikkort for at fungere. De tilgængelige estimater er baseret på indirekte indicier som inferensomkostninger og svarlantens. Modellen har et kontekstvindue på op til 1.000.000 tokens, egnet til analyse af meget lange dokumenter eller kode-repositories.</p>"
            },
            "Claude 4.5 Sonnet": {
                "desc": "<p>Meget stor multimodal og multilingual model, ekstremt ydeevnestærk inden for kodning, ræsonnement og matematik. Brugeren eller udvikleren, som anvender denne model, kan vælge mellem flere niveauer af ræsonnement.</p>",
                "fyi": "<p>Claude Sonnet 4.5 er en direkte evolution af Sonnet 4. \".5\" betegner de store ændringer, der er introduceret under post-træningen, som resulterer i betydelige forbedringer inden for ræsonnement, matematik og især i den konkrete brug af computere. På tidspunktet for lanceringen betragtes den som verdens bedste model til kodning og udmærker sig i løsning af lange og komplekse flertrinsopgaver. Dens præstationer på benchmarks som SWE-bench Verified og OSWorld markerer en klar fremgang sammenlignet med tidligere versioner, med en evne til at opretholde sin \"koncentration\" i mere end tredive timer på det samme problem.</p>",
                "size_desc": "<p>Den nøjagtige størrelse er ikke kendt. Alt tyder på, at det er en meget stor model, der kræver servere udstyret med flere kraftfulde grafikkort for at få den til at fungere. Estimaterne af størrelse og energiforbrug er baseret på indirekte indicier som inferensomkostninger og observeret latens. Claude Sonnet 4.5 har et kontekstvindue på op til 1.000.000 tokens, egnet til analyse af hele kode-repositories eller meget store dokumenter.</p>"
            },
            "Command A": {
                "desc": "<p>Stor model, ydeevnestærk til programmering, brug af eksterne værktøjer og \"retrieval augmented generation\" (RAG).</p>",
                "fyi": "<p>Cohere, den canadiske virksomhed bag denne model, blev grundlagt i 2019 af tidligere forskere fra Google Brain, herunder Aidan Gomez, medforfatter til det berømte paper <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> udgivet i 2017, som revolutionerede AI. Virksomheden skiller sig ud ved sit eksklusive fokus på generativ AI til virksomheder, særligt regulerede sektorer som finans, sundhed, fremstilling og energi samt den offentlige sektor. Virksomheden er også pioner inden for multilingvale tilgange og opretholder et nonprofit-forskningslaboratorium for at støtte åben innovation.</p>\n<p>Denne model er designet til at fungere på mere end 23 sprog og til nemt at integrere i virksomhedssystemer. Den er en af de få modeller, der distribueres under <strong>CC-BY-NC 4.0-licensen, som tillader deling og modificering, men forbyder enhver kommerciel brug.</strong> Dette licensvalg afspejler Coheres ønske om at bidrage til forskning og open source-fællesskabet, samtidig med at virksomheden bevarer kontrollen over kommerciel anvendelse for at beskytte sin forretningsmodel... Dette udelukker for eksempel integration af modellen i produkter eller tjenester solgt af en virksomhed til kunder, men tillader akademisk brug, test eller interne projekter begrænset til en ikke-kommerciel ramme.</p>",
                "size_desc": "<p>Med 111 milliarder parametre tilhører denne model de store modeller. Den kræver mindst to kraftfulde grafikkort til hosting, hvilket medfører en betydelig driftsomkostning.</p>\n<p>Dens kontekstvindue når op på 256.000 tokens, egnet til analyse af store dokumentsamlinger eller kodebaser.</p>"
            },
            "Command R": {
                "desc": "<p>Med 111 milliarder parametre tilhører denne model de store modeller. Den kræver mindst to kraftfulde grafikkort til hosting, hvilket medfører en betydelig driftsomkostning.</p>\n<p>Dens kontekstvindue når op på 256.000 tokens, egnet til analyse af store dokumentsamlinger eller kodebaser.</p>",
                "fyi": "<p>Cohere, den canadiske virksomhed bag denne model, blev grundlagt i 2019 af tidligere forskere fra Google Brain, herunder Aidan Gomez, medforfatter til det berømte paper \"Attention Is All You Need\", som revolutionerede AI. Dens primære særkende ligger i dens eksklusive fokus på generativ AI til virksomheder, særligt regulerede sektorer som finans, sundhed, fremstilling og energi samt den offentlige sektor. Virksomheden er også pioner inden for multilingvale tilgange og opretholder et nonprofit-forskningslaboratorium for at støtte åben innovation.</p>\n<p>Denne model er blevet evalueret på mere end 10 sprog. Dens kontekstvindue når op på 128.000 tokens, hvilket letter analysen af lange dokumenter. Dette vindue er blevet fordoblet i den følgende version af modellen (Command A).</p>",
                "size_desc": "<p>Med 35 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan hostes på en server udstyret med et enkelt kraftfuldt grafikkort, hvilket bidrager til at begrænse infrastrukturomkostningerne.</p>"
            },
            "Command R+": {
                "desc": "<p>Multilingual model specialiseret i 10 sprog, specialiseret til business use cases.</p>",
                "fyi": "<p>Storebror i Coheres Command R-familie, denne sprogmodel er orienteret mod professionel brug og designet specifikt til opgaver inden for søgning og informationsudtræk.</p>",
                "size_desc": "<p>Store modeller kræver betydelige ressourcer, men tilbyder den bedste ydeevne til avancerede opgaver som kreativ skrivning, dialogmodellering og applikationer, der kræver en nuanceret forståelse af konteksten.</p>"
            },
            "DeepSeek R1": {
                "desc": "<p>Meget stor model, der er meget ydeevnestærk på matematiske, videnskabelige og programmeringsopgaver, som simulerer et ræsonnemenstrin, før den genererer sit svar.</p>",
                "fyi": "<p>Denne model er baseret på en Mixture of Experts-arkitektur (MoE) med 61 lag. Den har i alt 671 milliarder parametre, hvoraf 37 milliarder aktiveres per token. Træningen har anvendt forstærkningslæring i stor skala med flere trin af SFT-justering (<em>supervised fine-tuning</em>: en superviseret fintuning, hvor modellen lærer fra eksempler på korrekte svar) og bootstrap-data.</p>",
                "size_desc": "<p>Med 671 milliarder parametre er DeepSeek R1 en meget stor model, der kræver flere kraftfulde grafikkort for at fungere. Ræsonnementsmodeller af denne type arbejder længere tid for at producere et svar, hvilket øger energiforbruget. Dog aktiverer Mixture of Experts-arkitekturen (MoE) kun en del af parametrene ved hver token, hvilket begrænser dens energiaftryk. Kontekstvinduet når op på 163.840 tokens, hvilket er egnet til analyse af lange dokumenter.</p>"
            },
            "DeepSeek R1 0528": {
                "desc": "<p>Meget stor model, specialiseret i matematiske, videnskabelige og programmeringsopgaver. Den simulerer et ræsonnemenstrin, før den genererer sit svar, og med opdateringen fra maj 2025 har den opnået større analysedybde og præcision takket være en optimering af post-træningen.</p>",
                "fyi": "<p>Denne model er baseret på en Mixture of Experts-arkitektur (MoE) med 61 lag. Den har i alt 671 milliarder parametre, hvoraf 37 milliarder aktiveres per token. Træningen har anvendt forstærkningslæring i stor skala med flere trin af SFT-justering (<em>supervised fine-tuning</em>: en superviseret fintuning, hvor modellen lærer fra eksempler på korrekte svar) og bootstrap-data. Dens seneste version (DeepSeek-R1-0528) forbedrer betydeligt dens ræsonnementskkapaciteter, reducerer hallucinationsraten og styrker effektiviteten inden for programmering, logik og funktionskald. På AIME 2025-testen er dens score steget fra 70 % til 87,5 %, hvilket bringer den tættere på modeller som o3 og Gemini 2.5 Pro.</p>",
                "size_desc": "<p>Med 671 milliarder parametre er DeepSeek R1 en meget stor model, der kræver flere kraftfulde grafikkort for at fungere. Ræsonnementsmodeller af denne type arbejder længere tid for at producere et svar, hvilket øger energiforbruget. Dog aktiverer Mixture of Experts-arkitekturen (MoE) kun en del af parametrene ved hver token, hvilket begrænser dens energiaftryk. Kontekstvinduet når op på 163.840 tokens, hvilket er egnet til analyse af lange dokumenter.</p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p>Stor model baseret på Meta Llama 3.3 70B, genoptrænet med ræsonnementseksempler fra DeepSeek R1-modellen. Den tilbyder gode kapaciteter inden for matematik og kodning.</p>",
                "fyi": "<p>Modellen er ikke blevet trænet fra bunden. Den er baseret på Llama 3.3 70B, genoptrænet ved brug af resultater genereret af DeepSeek R1. Denne proces har gjort det muligt at give Llama 3.3 70B en evne til at simulere ræsonnement, uden mulighed for brugeren til at vælge at aktivere eller deaktivere denne funktion.</p>\n<p>I overensstemmelse med forpligtelserne i Llama 3.3-licensen skal virksomheden bevare nævnelsen af kildemodellen i modellens navn, som er underlagt den samme licensordning.</p>",
                "size_desc": "<p>Med 70 milliarder parametre er denne model klassificeret blandt de store modeller. Den kræver flere kraftfulde grafikkort for at fungere, hvilket medfører høje driftsomkostninger. Ræsonnementsmodeller arbejder også længere tid for at producere et svar, hvilket øger deres energiforbrug.</p>\n<p>Kontekstvinduet er på 16.000 tokens, hvilket kan være begrænsende for analyse af meget store dokumenter.</p>"
            },
            "DeepSeek V3": {
                "desc": "<p>Meget stor model designet til komplekse opgaver: kodegenerering, brug af værktøjer, analyse af lange dokumenter. Den kan behandle mange sprog, men den er særligt egnet til engelsk og kinesisk.</p>",
                "fyi": "<p>Denne model er baseret på en Mixture of Experts-arkitektur (MoE) med 671 milliarder parametre, men aktiverer kun 37 milliarder per genereret token. Den er effektiv til funktionskald, generering af strukturerede output (JSON) og kodegenerering.</p>",
                "size_desc": "<p>DeepSeek V3 er en meget stor model, der kræver flere grafikkort for at fungere. Mixture of Experts-arkitekturen (MoE) gør det dog muligt kun at aktivere en del af parametrene, hvilket reducerer aftrykket sammenlignet med en tæt model af samme størrelse.</p>\n<p>Kontekstvinduet når op på 128.000 tokens, hvilket er nyttigt til analyse af lange dokumenter.</p>"
            },
            "DeepSeek V3.2": {
                "desc": "<p>Meget stor model designet til komplekse opgaver: agentisk orkestrering, kodegenerering, analyse af lange dokumenter. Denne version er særligt stærk til brug af værktøjer og kan simulere en ræsonneringsfase før den leverer det endelige svar.</p>",
                "fyi": "<p>For denne version er API-omkostningerne og beregningsbehovene reduceret med omkring 50 % eller mere for lange kontekster. Denne forbedring bygger på \"DeepSeek Sparse Attention (DSA)\", en opmærksomhedsmekanisme, der beregner opmærksomhed selektivt for at mindske kompleksiteten på lange sekvenser, samtidig med at det væsentligste af konteksten bevares.</p>\n<p>Modellen er blevet trænet med prioritet på ræsonneringsevner og agentiske anvendelser.</p>",
                "size_desc": "<p>DeepSeek V3.2 er en stor model, som kræver flere grafikkort for at fungere. Mixture of Experts-arkitekturen (MoE) gør det dog muligt kun at aktivere en del af parametrene, hvilket reducerer fodaftrykket sammenlignet med en tæt model af samme størrelse. Kontekstvinduet når op på 163.000 tokens, hvilket er nyttigt til analyse af meget lange dokumenter.</p>"
            },
            "DeepSeek v3": {
                "desc": "<p>Udgivet i december 2024, har DeepSeek V3-modellen en Mixture-of-Experts-arkitektur, der gør det muligt for den at være meget stor, samtidig med at inferensomkostningerne reduceres.</p>",
                "fyi": "<p>Lanceret i december 2024 er denne flagskibsmodel fra det kinesiske firma DeepSeek bygget på en Mixture-of-Experts-arkitektur, som gør det muligt for den at have en meget stor størrelse, samtidig med at inferensomkostningerne reduceres.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til performance og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er så omfattende, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "DeepSeek v3.1": {
                "desc": "<p>Meget stor model designet til komplekse opgaver: kodegenerering, analyse af lange dokumenter. Denne version er særlig stærk til brug af værktøjer og kan simulere en ræsonneringsfase, før den leverer det endelige svar.</p>",
                "fyi": "<p>Denne model er baseret på en Mixture of Experts-arkitektur (MoE), med 671 milliarder parametre, men aktiverer kun 37 milliarder per genereret token. Den er effektiv til værktøjskald, generering af strukturerede outputs (JSON) og kodegenerering. Træningen anvender FP8 microscaling, hvilket reducerer beregnings- og hukommelsesomkostninger, samtidig med at præcisionen opretholdes. Modellen er blevet trænet i to faser: først på sekvenser på 32.000 tokens, derefter udvidet til 163.000 tokens, hvilket muliggør bedre stabilitet og øget performance på meget lange kontekster.</p>",
                "size_desc": "<p>DeepSeek V3.1 er en meget stor model, der kræver flere grafikkort for at fungere. Mixture of Experts-arkitekturen (MoE) gør det dog muligt kun at aktivere en del af parametrene, hvilket reducerer footprintet sammenlignet med en tæt model af samme størrelse.</p>\n<p>Kontekstvinduet når nu op på 163.000 tokens mod 128.000 i den tidligere version, hvilket forbedrer analysen af meget lange dokumenter.</p>"
            },
            "EuroLLM 22B Instruct": {
                "desc": "<p> En flersproget model designet specifikt til europæisk sproglig mangfoldighed med stærke flersprogede oversættelses- og forståelsesevner. </p>",
                "fyi": "<p> EuroLLM-22B er en model skabt af et konsortium bestående af Sorbonne University, Paris-Saclay University, Artefact Research Center, Instituto Superior Técnico – University of Lisbon, Instituto de Telecomunicações, University of Edinburgh, Aveni, Unbabel, University of Amsterdam og Naver Labs. Den blev trænet med det primære mål om balanceret dækning af europæiske sprog. Den dækker de 24 officielle sprog i Den Europæiske Union samt 11 yderligere sprog. Træningen blev udført på cirka 4 billioner tokens ved hjælp af 400 Nvidia H100 GPU'er på MareNostrum 5-supercomputeren, drevet af Barcelona Supercomputing Center. Projektet modtog europæisk institutionel støtte gennem Horizon Europe og EuroHPC inden for rammerne af en \"ekstrem-skala\" computing-tildeling. </p>\n <p> Træningsmaterialet kombinerer webdata, flersproget parallelt data (en–xx og xx–en) og omhyggeligt udvalgte datasæt af høj kvalitet med stærk vægt på balance mellem europæiske sprog. Modellen præsterer særligt godt til flersprogede oversættelsesopgaver. </p>",
                "size_desc": "<p> Med 22 milliarder parametre falder denne model i kategorien af mellemstore modeller. Dens brug kræver en meget kraftfuld personlig computer eller, mere generelt, en server med mindst ét højtydende grafikkort. Kontekstvinduet indeholder 32.000 tokens. </p>"
            },
            "GLM 4.5": {
                "desc": "<p>Meget stor model skabt af Zhipu AI, en kinesisk AI-modeludvikler grundlagt i 2019 af professorer fra Tsinghua-universitetet og støttet af store aktører som Alibaba og Tencent. Modellen har to svarmuligheder: brugeren kan vælge mellem en ræsonneringstilstand for mere dybdegående svar eller en hurtig tilstand til at generere det endelige svar direkte.</p>",
                "fyi": "<p>Denne model har gode agentiske kapaciteter, som gør det muligt for den at udføre funktionskald med stor pålidelighed. Dens kodningspræstationer er høje, og modellen har en god evne til at skabe komplette webapplikationer og generere artefakter, som er enkeltfilsprogrammer, der kan bruges direkte i grænsefladerne for konversationsagenter. Til træningen er der blevet designet en specifik forstærkningslæringsinfrastruktur ved navn slime for at optimere præstationerne på komplekse og agentiske opgaver ved effektivt at håndtere lange arbejdsgange - modellen er i stand til at behandle komplekse og langvarige opgaver, såsom at skabe en applikation fra A til Z, ved at anvende sine værktøjer optimalt og forblive konsistent fra start til slut.</p>",
                "size_desc": "<p>Med 355 milliarder parametre placerer denne model sig i kategorien af meget store modeller. Takket være en Mixture of Experts-arkitektur (MoE) er den mere effektiv end visse andre modeller af lignende størrelse, men den kræver stadig en server med flere meget kraftige grafikkort for at blive hostet. Dens kontekstvindue går op til 128.000 tokens, hvilket gør det muligt at behandle ret lange dokumenter.</p>"
            },
            "GLM 4.6": {
                "desc": "<p>Opdatering af den store model skabt af Zhipu AI - GLM 4.6, en kinesisk AI-modeludvikler grundlagt i 2019 af professorer fra Tsinghua-universitetet og støttet af store aktører som Alibaba og Tencent. Denne opdatering øger størrelsen på kontekstvinduet, forbedrer dens kodningspræstationer, alignerer den bedre med menneskelige præferencer og er merekapabel ved agentiske anvendelser/brug af værktøjer.</p>",
                "fyi": "<p>Denne model har gode agentiske kapaciteter, som gør det muligt for den at udføre funktionskald med stor pålidelighed. Dens kodningspræstationer er høje, og modellen har en god evne til at skabe komplette webapplikationer og generere artefakter, som er enkeltfilsprogrammer, der kan bruges direkte i grænsefladerne for konversationsagenter. Til træningen er der blevet designet en specifik forstærkningslæringsinfrastruktur ved navn slime for at optimere præstationerne på komplekse og agentiske opgaver ved effektivt at håndtere lange arbejdsgange - modellen er i stand til at behandle komplekse og langvarige opgaver, såsom at skabe en applikation fra A til Z, ved at anvende sine værktøjer optimalt og forblive konsistent fra start til slut.</p>",
                "size_desc": "<p>Med 357 milliarder parametre placerer denne model sig i kategorien af meget store modeller. Takket være en Mixture of Experts-arkitektur (MoE) er den mere effektiv end visse andre modeller af lignende størrelse, men den kræver stadig en server med flere meget kraftige grafikkort for at blive hostet. Dens kontekstvindue går op til 200.000 tokens, hvilket gør det muligt at behandle meget lange dokumenter.</p>"
            },
            "GLM 4.7": {
                "desc": "<p> En stor, specialiseret kodemodel skabt af Zhipu AI, en kinesisk AI-modeludvikler grundlagt i 2019 af professorer fra Tsinghua University og støttet af store aktører som Alibaba og Tencent. Denne opdatering forbedrer dens kodeydeevne (især for webgrænseflader), interagerer bedre med AI-assisterede kodemiljøer og præsterer generelt bedre i agentiske sammenhænge. </p>",
                "fyi": "<p> Denne model har gode agentiske evner, hvilket gør det muligt at foretage funktionskald med høj pålidelighed. Dens kodeydeevne er høj, og modellen har en god evne til at skabe applikationer og grænseflader. Til træning blev en specifik forstærkningslæringsinfrastruktur ved navn Slime designet til at optimere ydeevnen på komplekse og agentiske opgaver ved effektivt at håndtere lange arbejdsgange—modellen er i stand til at håndtere komplekse og langvarige opgaver, såsom at skabe en applikation fra start til slut, gøre den bedste brug af sine værktøjer og forblive konsistent fra begyndelse til slut. </p>",
                "size_desc": "<p>Med 357 milliarder parametre placerer denne model sig i kategorien af meget store modeller. Takket være en mixture of experts (MoE)-arkitektur er den mere effektiv end visse andre modeller af tilsvarende størrelse, men den kræver stadig en server udstyret med flere meget kraftfulde grafikkort at hoste. Dens kontekstvindue går op til 200.000 tokens, hvilket gør det muligt at behandle meget lange dokumenter.</p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p>Mindre lettet version af GPT 4.1-modellen, designet til at begrænse omkostninger, samtidig med at den forbliver konkurrencedygtig på de fleste opgaver. Modellen accepterer meget lange forespørgsler, hvilket gør det muligt at bruge den for eksempel til analyse af dokumentkorpusser.</p>",
                "fyi": "<p>Det er en destilleret version af en større model med en delvis overførsel af dens viden. Den kan behandle tekst, billeder og lyd. Dens kontekstvindue kan nå op til 1 million tokens, hvilket gør den særligt velegnet til analyse af tekstkorpusser eller meget lange koderepositories.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en mellemstor model, der kræver et kraftigt grafikkort til at køre. Ikke desto mindre aktiverer den formodede Mixture of Experts-arkitektur (MoE) kun en del af parametrene ved hvert token, hvilket begrænser dens energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>"
            },
            "GPT 5": {
                "desc": "<p>GPT-5 er ikke en enkelt model, men et samlet system bestående af to distinkte modeller: en hurtig model (<code>gpt-5-main</code>) til almindelige forespørgsler og en ræsonneringsmodel (<code>gpt-5-thinking</code>) til komplekse problemer. Sammenlignet med sine forgængere hævder OpenAI, at den er mere nyttig i forespørgsler fra den virkelige verden med bemærkelsesværdige forbedringer inden for områderne skrivning, kodning og sundhed. Den reducerer også fænomenet med hallucinationer. Takket være sit kontekstvindue på 400.000 tokens kan den acceptere lange forespørgsler, hvilket gør det muligt at analysere flere dokumenter på én gang.</p>",
                "fyi": "<p>Udviklere, der bruger denne model, kan konfigurere en verbositetsparameter til at justere længden af ræsonneringsfasen.</p>\n<p>Med hensyn til sikkerhed bruger systemet en ny sikkerhedstilgang kaldet \"safe-completions\" til at forhindre uautoriseret indhold på tidspunktet for svaret snarere end på tidspunktet for forespørgslen. Skaberne af modellen har også brugt \"ræsonnerings\"-træningsfasen til at gøre den mere \"modstandsdygtig\" over for forsøg på at omgå deres sikkerhedsregler (<em>jailbreaking</em>).</p>",
                "size_desc": "<p>GPT-5-systemet er sammensat af modeller i forskellige størrelser, men de nøjagtige størrelser er ikke kendte. Dets arkitektur er designet til at inkludere flere modeller, orkestreret af et internt routingsystem, som vælger den mindste model, der er egnet til opgaven, for at optimere hastigheden og dybden af ræsonnering. Arkitekturen er sandsynligvis baseret på \"Mixture of Experts\" (MoE), hvilket betyder, at kun en del af parametrene aktiveres for hver forespørgsel. Dette muliggør bedre energieffektivitet og høj performance. De tilgængelige estimater af modellernes størrelse bygger på offentlig information og indirekte indikatorer såsom inferensomkostninger og svarlantens.</p>"
            },
            "GPT 5 Mini": {
                "desc": "<p>GPT-5 Mini er en lettet version af den primære GPT-5-model. Den er designet til at blive brugt i miljøer, hvor det er nødvendigt at begrænse omkostningerne, for eksempel i stor skala. Dens ræsonneringsmodel er næsten lige så performant som den primære models (<code>gpt-5-thinking</code>) på trods af dens mindre størrelse. Takket være sit kontekstvindue på 400.000 tokens kan den acceptere lange forespørgsler, hvilket gør det muligt at analysere flere dokumenter på én gang.</p>",
                "fyi": "<p>Systemet bruger en ny sikkerhedstilgang kaldet \"safe-completions\" til at forhindre uautoriseret indhold på tidspunktet for svaret snarere end på tidspunktet for forespørgslen.</p>\n<p>Selvom det er en mindre version, viser den sig meget konkurrencedygtig i forhold til den primære GPT-5-model på mange benchmarks, især inden for det medicinske område.</p>",
                "size_desc": "<p>Mini-modellen er en mere kompakt variant (formodet mellemstor størrelse) af GPT-5-systemet. Den er designet til at fungere optimalt for en god balance mellem performance og omkostninger, takket være et routingsystem, der vælger den til specifikke opgaver. Arkitekturen er sandsynligvis baseret på \"Mixture of Experts\" (MoE), hvilket betyder, at kun en del af parametrene aktiveres for hver forespørgsel. Ikke desto mindre er modellerne sandsynligvis meget store og kræver flere kraftige grafikkort til inferens.</p>"
            },
            "GPT 5 Nano": {
                "desc": "<p>GPT-5 Nano er den mindste og hurtigste version af GPT-5-ræsonneringsmodellen. Den er designet til kontekster, hvor ultralav latens eller omkostning er nødvendig. Takket være sit kontekstvindue på 400.000 tokens kan den acceptere lange forespørgsler, hvilket gør det muligt at analysere flere dokumenter på én gang.</p>",
                "fyi": "<p>Systemet bruger en ny sikkerhedstilgang kaldet \"safe-completions\" til at forhindre uautoriseret indhold på tidspunktet for svaret snarere end på tidspunktet for forespørgslen.</p>",
                "size_desc": "<p>Nano-modellen er den mest kompakte i GPT-5-familien (formodet lille størrelse). Den vælges af routingsystemet til forespørgsler, der kræver ultralav latens og øjeblikkelige svar. Dens arkitektur er sandsynligvis baseret på \"Mixture of Experts\" (MoE), hvilket muliggør bedre energieffektivitet og høj performance, selv på forespørgsler, der kræver et hurtigt svar.</p>"
            },
            "GPT 5.1": {
                "desc": "<p>Anden iteration af GPT 5, med en stil, der vurderes (af udgiveren) som mere naturlig, med bedre resultater i kode og agentopgaver. Modellen har den særlige egenskab at justere sin ræsonneringstid efter sværhedsgraden.</p>",
                "fyi": "<p>Ankomsten af denne nye iteration bringer skifter automatisk (\"routing\") mellem to tilstande. For simple forespørgsler vælger modellen et hurtigt svar. For mere komplekse opgaver skifter den til en tilstand med ræsonnering, før den svarer. Denne logik undgår at spilde tid på ræsonnering på lette forespørgsler.</p>\n<p>Den første udgivelse af GPT 5-familien havde udløst mange negative reaktioner i forhold til dens skrivestil. Mange brugere fandt modellen mindre varm end GPT 4o. Denne iteration sigter mod at vende tilbage til en varmere stil.</p>\n<p>Denne nye version markerer også fremskridt inden for kode og matematik, som bekræftes af benchmarks som SWE Bench for kode og AIME for matematiske kompetencer.</p>",
                "size_desc": "<p>Størrelserne på modellerne bag GPT-5.1-systemet er ikke kendte. Arkitekturen er sandsynligvis baseret på en \"Mixture of Experts\" (MoE), hvilket betyder, at kun en del af parametrene aktiveres for hver forespørgsel. Dette muliggør bedre energieffektivitet og høj ydeevne. De tilgængelige estimater om modellernes størrelse bygger på offentlige oplysninger og indirekte indikationer såsom inferensomkostninger og svartid.</p>"
            },
            "GPT 5.2": {
                "desc": "<p> Tredje iteration af GPT 5, med særlig opmærksomhed på dens anvendelighed i professionelle opgaver. </p>",
                "fyi": "<p>Modellen blev præsenteret som værende i stand til at opnå eller endda overgå niveauet af menneskelige eksperters ydeevne på GDPval-benchmarket, som evaluerer veldefinerede digitale professionelle opgaver. Den tilbyder forskellige niveauer af ræsonneringsindsats, der kan konfigureres af brugeren eller udvikleren, hvilket gør det muligt at afbalancere mellem omkostninger, latenstid og kvalitet af svar afhængigt af opgavens kompleksitet. GPT 5.2 udmærker sig også ved sin evne til at finde præcise oplysninger inden for meget store kontekster, selv når disse er sjældne og spredt i lange korpora (needle in the haystack).</p>",
                "size_desc": "<p> Modelstørrelserne bag GPT-5.2-systemet er ikke kendte. Arkitekturen er sandsynligvis baseret på en \"mixture of experts\" (MoE), hvilket betyder, at kun en delmængde af parametre aktiveres for hver forespørgsel. Dette muliggør større energieffektivitet og høj ydeevne. Tilgængelige estimater af modelstørrelser bygger på offentligt tilgængelig information og indirekte indikatorer såsom inferensomkostninger og svarlatentstid. </p>"
            },
            "GPT OSS-120B": {
                "desc": "<p>Den største af de to første semi-åbne modeller fra OpenAI siden GPT-2. Designet som svar på fremgangen for open source-aktører som Meta (LLaMA) og Mistral, er det en performant ræsonneringsmodel, især til komplekse opgaver og i \"agentiske\" miljøer.</p>",
                "fyi": "<p>Denne model kan køre på en enkelt GPU på 80 GB (såsom NVIDIA H100). Den har et kontekstvindue på 131.000 tokens, hvilket gør den ideel til analyse af store dokumenter.</p>\n<p>I modellens konfigurationer er det muligt at vælge mellem tre ræsonneringsniveauer (<em>low</em>, <em>medium</em> og <em>high</em>), som bestemmer modellens verbositet.</p>",
                "size_desc": "<p>Arkitekturen er baseret på \"Mixture of Experts\"-princippet (MoE), hvilket muliggør bedre energieffektivitet ved kun at aktivere en del af parametrene (5,1 milliarder per token) for hver forespørgsel. Det er en ræsonneringsmodel, så dens energiforbrug er højere, fordi den genererer en intern tankekæde, før den leverer det endelige svar. Den har et kontekstvindue på 131.000 tokens, hvilket gør den ideel til analyse af store dokumenter.</p>"
            },
            "GPT OSS-20B": {
                "desc": "<p>Den mindste af de to semi-åbne modeller fra OpenAI. Den er blevet designet som svar på konkurrencen fra open source og er beregnet til anvendelsestilfælde, der kræver lav latens, samt til lokale eller specialiserede implementeringer.</p>",
                "fyi": "<p>Denne model kan køres lokalt på en high-end bærbar computer udstyret med kun 16 GB VRAM (eller system-RAM). Det gør den til en meget tilgængelig mulighed for udviklere.</p>\n<p>I modellens konfigurationer er det muligt at vælge mellem tre ræsonneringsniveauer (<em>low</em>, <em>medium</em> og <em>high</em>), som bestemmer modellens verbositet.</p>",
                "size_desc": "<p>Med 20 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Arkitekturen er baseret på \"Mixture of Experts\" (MoE), hvilket muliggør bedre energieffektivitet ved kun at aktivere en del af parametrene (3,6 milliarder per token) for hver forespørgsel. Det er en ræsonneringsmodel, hvilket resulterer i et højere energiforbrug, fordi den genererer en intern tankekæde, før den leverer det endelige svar. Den har et kontekstvindue på 131.000 tokens, hvilket gør den ideel til analyse af store dokumenter.</p>"
            },
            "GPT-3.5": {
                "desc": "<p>Model lanceret i marts 2023, GPT-3.5 er en mindre model fra OpenAI, der er tilstrækkelig til forskellige opgaver inden for naturlig sprogbehandling.</p>",
                "fyi": "<p>Model lanceret i marts 2023, GPT-3.5 er en mindre model fra OpenAI, der er tilstrækkelig til forskellige opgaver inden for naturlig sprogbehandling.</p>",
                "size_desc": "<p>De store modeller kræver betydelige ressourcer, men tilbyder de bedste præstationer til avancerede opgaver såsom kreativ skrivning, dialogmodellering og applikationer, der kræver en fin forståelse af kontekst.</p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p>Lettet version af GPT 4.1, men som stadig er af stor størrelse, designet til at begrænse omkostninger, samtidig med at den forbliver konkurrencedygtig på de fleste opgaver. Modellen accepterer meget lange forespørgsler, hvilket gør det muligt at bruge den for eksempel til analyse af dokumentkorpusser.</p>",
                "fyi": "<p>Det er en destilleret version af en større model med en delvis overførsel af dens viden. Den kan behandle tekst, billeder og lyd. Dens kontekstvindue kan nå op til 1 million tokens, hvilket gør den særligt velegnet til analyse af meget lange korpusser eller koderepositories.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en stor model, der kræver et kraftigt grafikkort til at køre. Ikke desto mindre aktiverer den formodede Mixture of Experts-arkitektur (MoE) kun en del af parametrene ved hvert token, hvilket begrænser dens energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>"
            },
            "GPT-4o": {
                "desc": "<p>Den største af de to modeller, som ChatGPT fra OpenAI bygger på, lanceret i august 2024.</p>",
                "fyi": "<p>Model lanceret i august 2024 og efterfølger til GPT-4, GPT-4o er en forbedret version af GPT-4, designet til forskellige opgaver inden for naturlig sprogbehandling via for eksempel ChatGPT-applikationen fra den amerikanske virksomhed OpenAI.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til performance og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er så omfattende, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "GPT-4o mini": {
                "desc": "<p>Den mindste af de to modeller, som ChatGPT fra OpenAI bygger på, lanceret i juli 2024.</p>",
                "fyi": "<p>Model lanceret i juli 2024 og erstatning for GPT-3.5, GPT-4o mini er en mindre version af GPT-4, designet til forskellige opgaver inden for naturlig sprogbehandling via for eksempel ChatGPT-applikationen fra den amerikanske virksomhed OpenAI.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Gemini 1.5 Pro": {
                "desc": "<p>Lanceret i september 2024 er denne multimodale model anvendelig til generering af tekster og billeder, analyse af videoer og transskription af lyd.</p>",
                "fyi": "<p>Lanceret i september 2024 er denne flersprogede og multimodale model i stand til at behandle et meget stort volumen af inputdata, uanset om det drejer sig om tekstdata, billeder, lyd (op til 11 timers audio) eller video (op til en time). Det er LLM-modellen, der driver Googles chatbot Gemini.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til performance og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er så omfattende, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "Gemini 2.0 Flash": {
                "desc": "<p>Lanceret i december 2024 er denne mindre flersprogede og multimodale model fra Gemini Flash-familien, som muliggør meget hurtige svar til mindre avancerede ræsonnementer.</p>",
                "fyi": "<p>Lanceret i december 2024 er denne mindre flersprogede og multimodale model fra Gemini Flash-familien, som muliggør meget hurtige svar til mindre avancerede ræsonnementer end Gemini Pro-modellerne.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p>Stor multimodal og flersproget model med to svarmuligheder: brugeren kan vælge mellem en ræsonneringstilstand for mere dybdegående svar eller en hurtig tilstand til at generere det endelige svar direkte.</p>",
                "fyi": "<p>Denne model bygger på en Mixture of Experts-arkitektur (MoE) og er blevet destilleret ved kun at bevare en approximation af forudsigelserne fra lærermodellen - Gemini 2.5 Pro. Den er blevet trænet på en TPUv5p-arkitektur, der integrerer fremskridt som muligheden for at fortsætte træningen automatisk selv i tilfælde af træningsfejl, datakorruption eller hukommelsesproblemer.</p>\n<p>Gemini 2.5 Flash håndterer kontekster op til 1 million tokens og tre timers videoindhold. Optimeringen af visuel behandling gør det muligt at behandle videoer, der er omkring tre gange længere i det samme kontekstvindue: der er kun brug for 66 visuelle tokens til at generere et billede mod 258 tidligere. Denne model muliggør også native audiogenerering til dialoger og talesyntese.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en stor model, der kræver flere kraftige grafikkort til at fungere. Ikke desto mindre aktiverer Mixture of Experts-arkitekturen (MoE) kun en del af parametrene ved hvert token, hvilket begrænser dens energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens. Dens kontekstvindue går op til 1 million tokens, hvilket gør det muligt at behandle meget store dokumentkorpusser.</p>"
            },
            "Gemini 3 Flash": {
                "desc": "<p> Stor, native, multimodal og flersproget model, destilleret fra Gemini 3 Pro. Den inkorporerer en avanceret ræsonneringsevne (\"Deep Think\"), der kan aktiveres efter behov, med et ræsonneringsniveau, der kan konfigureres af brugeren eller udvikleren. Modellen understøtter nativt tekst, kode, lyd, billeder, video og PDF'er. </p>",
                "fyi": "<p> Modellen blev trænet på en infrastruktur udelukkende sammensat af TPU'er (Tensor Processing Units). Gemini 3 Flash kan behandle store mængder multimodalt indhold per prompt: op til 900 billeder, 900 PDF'er på 900 sider hver, videoer fra 45 minutter til 1 time og op til 8,4 timers lyd. På tidspunktet for sin udgivelse viste Gemini 3 Flash sig at være meget kraftfuld, samtidig med at den var cirka ni gange billigere end Gemini 3 Pro. </p>",
                "size_desc": "<p> Den nøjagtige størrelse af modellen er ikke offentliggjort, men den er baseret på en mixture of experts (MoE)-arkitektur. Denne arkitektur aktiverer kun en delmængde af parametre for hvert token, hvilket reducerer den nødvendige beregningskraft. Dens kontekstvindue strækker sig til 1 million tokens. Modellen er cirka 9 gange billigere end Gemini 3 Pro. Miljøpåvirkningen varierer afhængigt af det valgte ræsonneringsniveau: dybere ræsonnering genererer flere tokens og forbruger derfor flere ressourcer. </p>"
            },
            "Gemini 3 Pro": {
                "desc": "<p>Stor multimodal og flersproget model. Den integrerer en avanceret ræsonneringskapacitet (\"Deep Think\"), der kan aktiveres efter behov til komplekse opgaver (matematik, logik, kodning), som er adskilt fra dens hurtigere standard genereringskapacitet. Modellen understøtter tekst, kode, lyd, billede, video og 3D.</p>",
                "fyi": "<p>Modellen er blevet trænet på en infrastruktur udelukkende bestående af TPU'er (Tensor Processing Units). På tidspunktet for dens udgivelse markerer modellen et meget stort fremskridt i forhold til evalueringerne - for eksempel på Humanity's Last Exam, ARC-AGI-2 og MathArena Apex. Modellen er optimeret til \"agentisk\" brug: den er blevet trænet til at simulere trin af planlægning, værktøjsbrug, kodeudførelse og selvkorrigering inden for kodemiljøer. Dens multimodale forståelse er blevet finjusteret for at reducere antallet af tokens, der er nødvendige til video- og lydkodning.</p>",
                "size_desc": "<p>Den nøjagtige størrelse af modellen er ikke offentliggjort, men den er baseret på en Mixture of Experts (MoE)-arkitektur. Denne arkitektur aktiverer kun en delmængde af parametre for hvert input-token, hvilket kræver mindre regnekraft til at generere. Dens kontekstvindue strækker sig op til 1 million tokens, velegnet til analyse af store dokumentsamlinger, herunder hele kode-repositories og videofiler.</p>"
            },
            "Gemma 2 27B": {
                "desc": "<p>Performant model med en passende størrelse, dens relativt høje omkostninger gør den egnet til specifikke anvendelser, der kræver høj præcision.</p>",
                "fyi": "<p>Med tre gange så mange parametre som sin lillebror fra Gemma 2-familien er denne model mere præcis til at følge instruktioner. Den model, der anvendes her, er den kvantiserede version (q8).</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Gemma 2 2B": {
                "desc": "<p>Meget lille model, der tilbød meget konkurrencedygtige præstationer for sin størrelse og de fleste opgaver.</p>",
                "fyi": "<p>Lillebror i Gemma 2-familien, denne meget lille model lanceret i juli 2024 formår at konkurrere med langt større modeller.</p>",
                "size_desc": "<p>De meget små modeller med mindre end 7 milliarder parametre er de mindst komplekse og mest økonomiske med hensyn til ressourcer og tilbyder tilstrækkelige præstationer til simple opgaver som tekstklassificering.</p>"
            },
            "Gemma 2 9B": {
                "desc": "<p>Lillebror i Gemma 2-familien, denne model lanceret i juni 2024 er trænet til at svare på specifikke instruktioner, behandle komplekse forespørgsler og tilbyde kreative løsninger.</p>",
                "fyi": "<p>Lillebror i Gemma 2-familien, denne model lanceret i juni 2024 er trænet til at svare på specifikke instruktioner, behandle komplekse forespørgsler og tilbyde kreative løsninger.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig performance til forskellige opgaver (resumering, oversættelse, tekstklassificering...)</p>"
            },
            "Gemma 3 12B": {
                "desc": "<p>Lille multimodal model egnet til almindelige opgaver som spørgsmål-svar, resuméer eller fortolkning af billeder.</p>",
                "fyi": "<p>Den behandler tekst og billeder og kan køre lokalt på kraftige bærbare computere eller servere med et enkelt grafikkort. Den er blevet trænet til at kunne interagere med eksterne værktøjer (internetsøgning osv.) via funktionskald, hvilket gør den anvendelig i agentiske kontekster.</p>",
                "size_desc": "<p>Med 12 milliarder parametre hører den til de små modeller. Den kan bruges lokalt på en arbejdsstation for at bevare databeskyttelsen eller på en billig server for at begrænse omkostningerne sammenlignet med en større model.</p>\n<p>Dens kontekstvindue går op til 128.000 tokens, hvilket gør det muligt at behandle lange dokumenter.</p>"
            },
            "Gemma 3 27B": {
                "desc": "<p>Mellemstor multimodal model egnet til almindelige opgaver som spørgsmål-svar, resuméer eller fortolkning af billeder.</p>",
                "fyi": "<p>Den kan behandle tekst og billeder på en server udstyret med et enkelt kraftigt grafikkort. Den er blevet trænet til at kunne interagere med eksterne værktøjer (internetsøgning osv.) via funktionskald, hvilket gør den anvendelig i agentiske kontekster.</p>",
                "size_desc": "<p>Med 27 milliarder parametre tilhører den kategorien af mellemstore modeller. Den kan implementeres på en server med et enkelt grafikkort (GPU).</p>\n<p>Den accepterer kontekster op til 128.000 tokens, hvilket er velegnet til analyse af lange dokumenter.</p>"
            },
            "Gemma 3 4B": {
                "desc": "<p>Meget lille og kompakt multimodal model egnet til almindelige opgaver som spørgsmål-svar, resuméer eller fortolkning af billeder.</p>",
                "fyi": "<p>Den kan behandle tekst og billeder ved at køre på enheder med lav ydeevne, inklusive smartphones og tablets. Den er blevet trænet til at kunne interagere med eksterne værktøjer (internetsøgning osv.) via funktionskald, hvilket gør den anvendelig i agentiske kontekster.</p>",
                "size_desc": "<p>Med 4 milliarder parametre hører den til de meget små modeller. Den kan bruges lokalt for at bevare databeskyttelsen eller på en server for at begrænse omkostningerne sammenlignet med en større model.</p>\n<p>Dens kontekstvindue kan nå op på 128.000 tokens, hvilket gør det muligt at analysere lange dokumenter.</p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p>Meget lille og kompakt multimodal model designet til at køre lokalt på en computer eller smartphone uden brug af en server - den er i stand til at tilpasse sin ydeevne efter enhedens kapacitet og behovet.</p>",
                "fyi": "<p>Denne model kan behandle tekst, billeder og lyd. Den bygger på MatFormer-arkitekturen og et PLE-cachesystem (per-layer embeddings), som kun aktiverer de nødvendige parametre afhængigt af opgaven og tilpasser sig kapaciteten på de maskiner, som modellen kører på.</p>",
                "size_desc": "<p>Med 4 milliarder parametre hører den til de meget små modeller. Den kan bruges lokalt på en computer eller smartphone for at bevare databeskyttelsen eller på en server for at begrænse omkostningerne sammenlignet med en større model.</p>\n<p>Dens kontekstvindue går op til 32.000 tokens.</p>"
            },
            "Grok 3 Mini": {
                "desc": "<p>Lettere version af Grok 3-modellen, som gør det muligt at reducere omkostninger, samtidig med at den bevarer gode præstationer til mange opgaver. Den kan simulere en ræsonneringsfase, før den leverer et endeligt svar.</p>",
                "fyi": "<p>Grok 3 Mini er en destilleret version af Grok 3: den nærmer sig den med hensyn til kapaciteter, samtidig med at den er hurtigere og billigere.\nModellen tilbyder to tilstande: en refleksionstilstand med trin-for-trin-ræsonnering til komplekse problemer og en hurtig tilstand til øjeblikkelige svar.\nDens kontekstvindue når op på 131.000 tokens, hvilket gør den velegnet til analyse af lange dokumenter.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. På trods af sit navn er Grok 3 Mini uden tvivl en meget stor model, der kræver flere kraftige grafikkort for at fungere. Derudover indeholder den en valgfri ræsonneringsfase, som indebærer en længere generering og dermed et højere energiforbrug. Ikke desto mindre aktiverer den formodede Mixture of Experts-arkitektur (MoE) kun en del af parametrene ved hvert token, hvilket begrænser dens energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>"
            },
            "Grok 4 Fast": {
                "desc": "<p>Grok 4 Fast er en model med fokus på balancen mellem performance, hastighed og omkostninger, især til opgaver som informationssøgning og andre \"agentiske\" handlinger.</p>",
                "fyi": "<p>Modellens nøjagtige størrelse er ikke kendt. På trods af sit navn er Grok 4 Fast uden tvivl en meget stor model, der kræver flere kraftige grafikkort for at fungere. Derudover indeholder den en valgfri ræsonneringsfase, som indebærer en længere generering og dermed et højere energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>\n<p>Trænet gennem forstærkning opnår Grok 4 Fast scorer tæt på den meget store model - Grok 4, samtidig med at den forbliver mere økonomisk. Den er blevet trænet til at være performant på webnavigation og specifikt på X-platformen samt kapaciteter til værktøjskald og kodeudførelse.</p>",
                "size_desc": "<p>Udstyret med et kontekstvindue på 2 millioner tokens kombinerer Grok 4 Fast en ræsonneringstilstand og en direkte svartilstand i samme model. Den bruger cirka 40 % færre ræsonneringstokens end Grok 4, hvilket muliggør en betydelig reduktion i udførelsesomkostninger og latens.</p>"
            },
            "Grok 4.1 Fast": {
                "desc": "<p>Grok 4.1 Fast er den fjerde generation af Grok. Baseret på den samme grundmodel nyder den ifølge udgiveren godt af en forbedret post-træningsproces, som giver den en bedre stil, bedre agentisk adfærd og mere konsistens i svarene, især under lange samtaler.</p>",
                "fyi": "<p>De tilgængelige evalueringer har en tendens til at indikere en lavere hallucinationsrate for denne version af modellen. Træningen af Grok 4.1 skulle have integreret en udvidet \"reward modeling\", der ikke kun omfatter menneskelige evalueringer, men også feedback fra modeller.</p>",
                "size_desc": "<p>Grok 4 Fast integrerer en ræsonneringstilstand og en direkte svartilstand. Brugeren eller udvikleren kan kontrollere valget af, om ræsonnering skal aktiveres eller ej. Modellen har et kontekstvindue på 2 millioner tokens, hvilket gør det muligt at analysere meget store dokumentsamlinger.</p>"
            },
            "Hermes 3 405B": {
                "desc": "<p>Meget stor model gentrænet ud fra Llama 3.1 405B, justeret til bedre at imødekomme brugernes forespørgsler og lette brugen af eksterne værktøjer.</p>",
                "fyi": "<p>Denne model er resultatet af en gentræning af alle parametre i Llama 3.1 405B for at gøre dens adfærd mindre begrænset og bedre tage højde for nuancerne i bruger- og systemprompts - brugeren har dermed større kontrol over modellens \"personlighed\" og adfærd. Specifikke ræsonneringsfunktioner som <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> er blevet tilføjet for at simulere ræsonnering på komplekse opgaver. Træningen har brugt et værktøj kaldet AdamW (læringshastighed på 3,5×10⁻⁶), som hjælper modellen med at lære effektivt ved gradvist at justere sine parametre. Derefter er den blevet finjusteret med en metode kaldet DPO (direct preference optimisation), som muliggør forbedring af dens svar baseret på specifikke præferencer. For at gøre denne træning lettere og hurtigere er der blevet brugt LoRA-adaptere; det er mindre moduler, der kun modificerer en del af modellen, hvilket undgår at skulle genbearbejde alle parametre på samme tid.</p>",
                "size_desc": "<p>Med 405 milliarder parametre hører denne model til de meget store modeller. Den kræver en server udstyret med flere kraftige grafikkort, hvilket medfører betydelige driftsomkostninger.</p>"
            },
            "Hermes 4 70B": {
                "desc": "<p>Stor model gentrænet ud fra Llama 3.1 70B, justeret til bedre at imødekomme brugernes forespørgsler og stilistiske instruktioner.</p>",
                "fyi": "<p>Hermes 4-70B er blevet trænet på 56 milliarder tokens ved at kombinere Fully Sharded Data Parallel (FSDP) og Tensor Parallelism for at håndtere dens størrelse. Modellen bygger på basen af Llama 3.1 70B, tilpasset med TorchTitan og beriget med omkring 19 milliarder syntetiske tokens fokuseret på ræsonnering. Dens træning følger en flerfasetilgang med supervised fine-tuning på ræsonneringskæder, der kan overstige 30.000 tokens. Den udnytter også Atropos-miljøet, som bruges til at generere og verificere komplekse trajectories (kode, JSON, agentiske opgaver) takket være massiv rejection sampling, der garanterer datakvaliteten.</p>",
                "size_desc": "<p>Hermes 4-70B er en meget stor model, der kræver mindst et kraftigt grafikkort.</p>\n<p>Kontekstvinduet når op på 40.960 tokens i ræsonnering og 32.768 tokens til andre opgaver med finjusteringsmekanismer, der er blevet brugt til at lære den at \"lukke\" refleksionssekvensen ved omkring 30k tokens.</p>"
            },
            "Jamba 1.5 Large": {
                "desc": "<p>Lanceret i august 2024 er denne model fra virksomheden AI21 en særlig hybridtype kaldet 'SSM' og Mixture of Experts, hvis arkitektur søger at få mest muligt ud af antallet af parametre.</p>",
                "fyi": "<p>Lanceret i august 2024 er denne model fra virksomheden AI21 en særlig hybridtype kaldet 'SSM' (State Space Models) og Mixture of Experts, hvis arkitektur søger at få mest muligt ud af antallet af parametre.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til performance og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er så omfattende, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "Kimi K2": {
                "desc": "<p>Udviklet af Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), en virksomhed baseret i Beijing, er Kimi K2 en meget stor model orienteret mod kode og agentiske anvendelser. Den er anerkendt for udviklingsopgaver i agentiske kontekster (f.eks. i Cursor eller Windsurf), især for sin rolle som orkestrator. Den eksponerer ikke en eksplicit \"ræsonneringstilstand\", men til store opgaver underopdeler den sit svar i trin og veksler mellem handlinger (værktøjskald) og tekstudarbejdelse.</p>",
                "fyi": "<p>For at stabilisere træningen i meget stor skala har Moonshot AI introduceret MuonClip, en \"hastighedsbegrænser\" til træning, som gør det muligt at træne en model af denne størrelse og på et korpus på 15,5 billioner tokens uden at afspore i læringen.</p>\n<p>Med hensyn til data har K2 trænet meget i \"simulator\" med rigtige værktøjer (browser, terminal, kodeudførere, API'er...). Ligesom en pilot på simulator lærer den at planlægge, prøve, fejle og derefter prøve igen samt kæde flere handlinger sammen for at nå et mål. Resultat: den er særligt god til at orkestrere værktøjer og gennemføre opgaver i flere trin.</p>",
                "size_desc": "<p>Med 1 billion parametre er denne model en af de største modeller, der findes. Takket være en Mixture of Experts-arkitektur (MoE) er den mere effektiv end visse andre modeller af lignende størrelse, men den kræver stadig en server med flere meget kraftige grafikkort for at blive hostet. Dens kontekstvindue går op til 128.000 tokens, hvilket gør det muligt at behandle ret lange dokumenter.</p>"
            },
            "Kimi K2 Thinking": {
                "desc": "<p>Denne version af Kimi K2 integrerer en mere avanceret ræsonneringsfase, hvilket forbedrer dens præstationer sammenlignet med den oprindelige iteration. Den er blevet udviklet af Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), et selskab baseret i Beijing.</p>",
                "fyi": "<p>Modellen synes at demonstrere øgede præstationer ved lange ræsonnementer og opgaver, der kræver trin med værktøjsbrug. Disse karakteristika formodes at fremme en forlænget \"autonomi\" i udførelsen af opgaver.</p>",
                "size_desc": "<p>Med 1 billion parametre (hvoraf 32 milliarder er aktive per token) er denne model blandt de allerstørste modeller. Dens Mixture of Experts (MoE)-arkitektur giver dog en bedre effektivitet sammenlignet med visse modeller af tilsvarende størrelse. Desuden er den blevet kvantificeret native på INT4-niveau. Denne kvantificering, som består i at \"afrunde\" modellens vægte til færre cifre for at forenkle beregningerne, præsenteres af Moonshot AI som værende i stand til at reducere inferenstiden med en faktor 2. Kontekstvinduet når op på 256.000 tokens, hvilket giver mulighed for at behandle meget lange dokumenter eller store kodebase.</p>"
            },
            "LFM 2 8B A1B": {
                "desc": "<p>En model specifikt designet til effektiv inferens på lokale enheder (on-device deployment). Dens arkitektur sigter mod at tilbyde en outputkvalitet, der er konkurrencedygtig med større dense modeller, samtidig med at latency og krav til computeressourcer minimeres.</p>",
                "fyi": "<p>LFM2-8B-A1B er baseret på en hybrid arkitektur, der integrerer convolutionsblokke samt Grouped Query Attention (GQA)-mekanismer. Mixture of Experts (MoE)-implementeringen består af 32 eksperter per transformerblok. \"Routeren\" udvælger og aktiverer de 4 bedste eksperter (top-4 gating) til behandling af hvert token. Dette design sigter mod at etablere en reference for Mixture of Experts-modeller, der er optimeret til lokal inferens.</p>",
                "size_desc": "<p>Modellen har i alt 8,3 milliarder parametre. Takket være en Mixture of Experts (MoE)-arkitektur er aktiveringen begrænset til omkring 1,5 milliard parametre per token under inferens. Denne konfiguration gør det muligt at opnå en præstation, der generelt er forbundet med dense modeller i klassen på 3 til 4 milliarder parametre, med en eksekveringshastighed, der kan sammenlignes med modeller på 1,5 milliard parametre. Den tilbydes i kvantificerede varianter, der er optimeret til lokale miljøer (som f.eks. til en smartphone). Kontekstvinduet er begrænset til 32.000 tokens.</p>"
            },
            "LFM 40B": {
                "desc": "<p>Lanceret i september 2024 er denne model fra den amerikanske virksomhed Liquid en Mixture of Experts-type model, hvis arkitektur søger at få mest muligt ud af antallet af parametre.</p>",
                "fyi": "<p>Lanceret i september 2024 er denne model fra den amerikanske virksomhed Liquid en Mixture of Experts-type (MoE) model, hvis arkitektur søger at få mest muligt ud af antallet af parametre.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Llama 3 70B": {
                "desc": "<p>Lanceret i april 2024 er denne model blevet trænet på mere end 15.000 billioner tokens, men understøtter en relativt begrænset kontekst på 8.000 tokens.</p>",
                "fyi": "<p>Lanceret i april 2024 er denne model blevet trænet på mere end 15.000 billioner tokens og derefter specialiseret til dialog ud fra instruktionsdata og annotationer foretaget af mennesker. Den understøtter en kontekst på 8.000 tokens.</p>",
                "size_desc": "<p>De store modeller kræver betydelige ressourcer, men tilbyder de bedste præstationer til avancerede opgaver såsom kreativ skrivning, dialogmodellering og applikationer, der kræver en fin forståelse af kontekst.</p>"
            },
            "Llama 3 8B": {
                "desc": "<p>Lillebror i Llama 3-familien er denne model optimeret til dialoger med særlig fokus på effektivitet og sikkerhed.</p>",
                "fyi": "<p>Lillebror i Llama 3-familien er denne model optimeret til dialoger med særlig fokus på effektivitet og sikkerhed.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig performance til forskellige opgaver (resumering, oversættelse, tekstklassificering...)</p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p>Meget stor model designet til komplekse eller specialiserede opgaver. Ofte brugt som \"lærermodel\" til træning af mere specialiserede modeller.</p>",
                "fyi": "<p>Modellen er blevet trænet på et korpus på 15 billioner tokens med 16.000 H100-grafikkort (et af de mest kraftfulde grafikkort på markedet i 2025). Træningen kombinerede generering af syntetiske data og direct preference optimization (DPO). Denne model bruges selv ofte til at generere syntetiske data til træning af mindre modeller. Modellen bruger som standard 8-bit komprimering for at reducere hukommelseskravene og muliggøre kørsel på en enkelt meget kraftig server.</p>",
                "size_desc": "<p>Med 405 milliarder parametre hører denne model til de meget store modeller. Den kræver en server udstyret med flere kraftige grafikkort, hvilket medfører betydelige driftsomkostninger. Modellen har et kontekstvindue på op til 128.000 tokens, hvilket gør den interessant til opgaver med analyse af lange dokumenter.</p>"
            },
            "Llama 3.1 70B": {
                "desc": "<p>Udstyret med 70 milliarder parametre og lanceret i april 2024 er denne model performant til at generere og forstå komplekse tekster på forskellige sprog.</p>",
                "fyi": "<p>Som de andre modeller i Llama 3.1-familien er denne model, lanceret i april 2024, blevet trænet på data, der går tilbage til december 2023. Det nytter ikke at spørge den om højdepunkterne fra OL i Paris 2024! Med 70 milliarder parametre er denne model performant til at generere og forstå komplekse tekster på forskellige sprog.</p>",
                "size_desc": "<p>De store modeller kræver betydelige ressourcer, men tilbyder de bedste præstationer til avancerede opgaver såsom kreativ skrivning, dialogmodellering og applikationer, der kræver en fin forståelse af kontekst.</p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p>Lille model designet til lokal brug på en bærbar computer, samtidig med at den tilbyder gode kapaciteter til tekstsyntese og simple svar.</p>",
                "fyi": "<p>Denne model er en destilleret version baseret på de større Llama 3-modeller: den er blevet trænet gennem en overførsel af en del af de større modellers viden.</p>",
                "size_desc": "<p>Med 8 milliarder parametre hører denne model til de små modeller. Den kan bruges lokalt på en kraftig computer, hvilket garanterer databeskyttelse, eller hostes på en server udstyret med et enkelt grafikkort, hvilket begrænser infrastrukturomkostningerne. Dens kontekstvindue på 128.000 tokens gør det muligt at behandle lange dokumenter.</p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p>Stor model beregnet til et bredt udvalg af opgaver og kan konkurrere med større modeller.</p>",
                "fyi": "<p>Denne model er en destilleret version baseret på 405B-modellen, som den skylder en del af sine overførte viden. Den har også draget fordel af nyere alignment-teknikker og forstærkningslæring med online-miljøer (online reinforcement learning) - modellen lærte altså ved at forsøge at udføre opgaver online på en autonom måde. Dens træning bygger på 15 billioner tokens.</p>",
                "size_desc": "<p>Med 70 milliarder parametre tilhører denne model kategorien af store modeller. Den kræver flere kraftige grafikkort for at fungere, hvilket medfører betydelige driftsomkostninger. Dens kontekstvindue på 128.000 tokens gør det muligt at behandle lange dokumenter.</p>"
            },
            "Llama 4 Maverick": {
                "desc": "<p>Meget stor model udstyret med et meget bredt kontekstvindue, egnet for eksempel til resumering af flere dokumenter på samme tid.</p>",
                "fyi": "<p>Denne model er blevet kodestilleret med Behemoth, hvilket betyder, at den har lært samtidig med den gigantiske model og ikke bagefter som i en klassisk destillation. Dette gør det muligt at overføre dens kompetencer hurtigere og med mindre beregning. Den er blevet trænet på 30 billioner tokens, der kombinerer tekst på 200 sprog og billeder for at opnå native multimodale kapaciteter - den kan behandle op til 8 billeder samtidigt. Arkitekturen bygger på et Mixture of Experts-system (MoE), med 17 milliarder aktive parametre, 16 eksperter og 109 milliarder parametre i alt. Meta-teamet har udviklet en progressiv post-træningsstrategi, der kombinerer adaptiv datafiltrering - ved kun at beholde de mest komplekse og interessante, målrettet finjustering og online forstærkningslæring for at balancere multimodale præstationer, ræsonnering og samtalekvalitet. Takket være iRoPE-arkitekturen (en optimeret version af positionel kodning) kan den håndtere meget lange kontekstvinduer op til 10 millioner tokens.</p>\n<p>Llama 4 Maverick-modellen blev præsenteret som Metas direkte svar på DeepSeek-modellerne. Men ved lanceringen mente mange brugere, at den ikke indfriede forventningerne, især på programmeringsopgaver og kreativt arbejde.</p>",
                "size_desc": "<p>Med 400 milliarder parametre placerer denne model sig i kategorien af store modeller. Ikke desto mindre kræver den takket være en Mixture of Experts-arkitektur (MoE) færre ressourcer for at fungere end \"tætte\" modeller af denne størrelse. Dens kontekstvindue går op til 1 million tokens, hvilket gør det muligt at behandle meget store dokumentkorpusser.</p>"
            },
            "Llama 4 Scout": {
                "desc": "<p>Stor model udstyret med et meget bredt kontekstvindue, egnet for eksempel til syntese af et sæt dokumenter.</p>",
                "fyi": "<p>Denne model er blevet kodestilleret med Behemoth, hvilket betyder, at den har lært samtidig med den gigantiske model, og ikke bagefter som i en klassisk destillation. Den er blevet trænet på 30 billioner tokens, der kombinerer tekst på 200 sprog og billeder for at opnå native multimodale kapaciteter. Arkitekturen bygger på et mixture of experts-system (MoE - Mixture of Experts), med 17 milliarder aktive parametre, 16 eksperter og 109 milliarder parametre i alt. For at balancere multimodale præstationer, ræsonnering og samtalekvalitet har Meta-teamet udviklet en progressiv post-træningsstrategi, der kombinerer adaptiv datafiltrering (for kun at beholde de mest komplekse og interessante), målrettet finjustering og forstærkningslæring med online-miljøer (online reinforcement learning) - modellen lærte altså ved at forsøge at udføre opgaver online på en autonom måde. Takket være iRoPE-arkitekturen (en optimeret version af positionel kodning) kan den håndtere meget lange kontekstvinduer, op til 10 millioner tokens, og kan behandle op til 8 billeder samtidigt.</p>\n<p>Modellen blev godt modtaget ved lanceringen, især for sit imponerende kontekstvindue, en første i området, samt for sit forhold mellem kvalitet og pris på opgaver som resumering, værktøjskald og augmenteret generering (RAG). Det gør den til et passende valg til automatiserede pipelines.</p>",
                "size_desc": "<p>Med 109 milliarder parametre placerer denne model sig i kategorien af store modeller. Ikke desto mindre kan den takket være en Mixture of Experts-arkitektur (MoE) hostes på en server med et enkelt meget kraftigt grafikkort. Dens kontekstvindue går op til 10 millioner tokens, hvilket gør det muligt at behandle ekstremt lange dokumentkorpusser.</p>"
            },
            "Magistral Medium": {
                "desc": "<p>Mellemstor multimodal og flersproget ræsonneringsmodel. Egnet til programmeringsopgaver eller andre opgaver, der kræver dybdegående analyse, forståelse af komplekse logiske systemer eller planlægning - for eksempel til agentiske anvendelsestilfælde eller udarbejdelse af langt komplekst indhold.</p>",
                "fyi": "<p>Denne model er en del af den første generation af ræsonneringsmodeller fra Mistral AI (sommer 2025). I modsætning til de fleste andre ræsonneringsmodeller kan denne model ræsonnere på flere sprog, herunder engelsk, fransk, spansk, tysk, italiensk, arabisk, russisk og forenklet kinesisk. Den er blevet trænet med forstærkningslæring på Mistral Medium 3 og er ikke blevet destilleret fra eksisterende ræsonneringsmodeller. Denne model arver de multimodale kapaciteter fra Mistral Medium 3, selvom forstærkningslæringen kun er blevet udført på tekst.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en stor model, der kræver mindst flere kraftige grafikkort for at fungere. Ræsonneringsmodeller kræver mere regnekraft for at producere et svar, hvilket øger deres energiforbrug. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>\n<p>Den har et kontekstvindue på op til 40.000 tokens, hvilket er nyttigt til analyse af korte dokumenter, men utilstrækkeligt til at analysere store dokumentkorpusser.</p>"
            },
            "Magistral Small": {
                "desc": "<p>Mellemstor ræsonneringsmodel, multimodal og flersproget. Egnet til opgaver, der kræver dybdegående analyse, forståelse af logiske systemer eller planlægning - for eksempel til agentiske anvendelsestilfælde eller udarbejdelse af langt komplekst indhold.</p>",
                "fyi": "<p>Denne model er en del af den første generation af ræsonneringsmodeller fra Mistral AI (sommer 2025). I modsætning til de fleste andre ræsonneringsmodeller kan denne model ræsonnere på flere sprog, herunder engelsk, fransk, spansk, tysk, italiensk, arabisk, russisk og forenklet kinesisk.</p>\n<p>Træningen er foregået i to faser. Den første, kaldet <em>cold-start</em>-ræsonnering ved destillation (fra Mistral Medium 3 og OpenThoughts/OpenR1), gør det muligt for modellen at erhverve grundlæggende ræsonneringskapaciteter ud fra generelle instruktionsdata (10%). Den anden er en høj-entropi forstærkningslæringsfase (RL, <em>reinforcement learning</em>), hvor modellen opmuntres til at udforske forskellige og varierede løsninger frem for at konvergere mod et enkelt svar og til at generere lange completions (op til 32.000 tokens), hvilket gør det muligt at udvikle ræsonneringskapaciteter, der overgår lærermodellens.</p>",
                "size_desc": "<p>Med 24 milliarder parametre klassificeres denne model blandt de mellemstore modeller. Den kræver et enkelt kraftigt grafikkort for at fungere. Ræsonneringsmodeller kører også længere for at producere et svar, hvilket øger deres energiforbrug.</p>\n<p>Den har et kontekstvindue på op til 40.000 tokens, hvilket er nyttigt til analyse af korte dokumenter, men utilstrækkeligt til at analysere store dokumentkorpusser.</p>"
            },
            "MiniMax M2": {
                "desc": "<p>Model specialiseret i kode med et meget konkurrencedygtigt forhold mellem kvalitet/hastighed/pris. Den er blevet designet af MiniMax, et selskab baseret i Shanghai i Kina.</p>",
                "fyi": "<p>Model designet specifikt til agentiske opgaver (især kode), med træning til at respektere strenge protokoller for agentkontrol (planlægning, værktøjskald, verifikation).</p>",
                "size_desc": "<p>Mixture of Experts-arkitekturmodel med 230 milliarder parametre (hvoraf 10 milliarder er aktive per token-generering). Kontekstvinduet er på 200.000 tokens, hvilket gør det muligt at behandle hele kodebase og lange dokumenter.</p>"
            },
            "Ministral": {
                "desc": "<p>Lille flersproget model designet til at køre på en bærbar computer uden forbindelse til en server, samtidig med at den tilbyder gode kapaciteter til tekstsyntese, besvarelse af simple spørgsmål og brug af værktøjer.</p>",
                "fyi": "<p>Denne model bruger en grouped query attention-metode (GQA) til at begrænse den tekst, der analyseres ved hvert genereringstrin, og vinde i hastighed og hukommelse: beregningerne reduceres uden indvirkning på kvaliteten. Attention-mekanismen forbedres ved at anvende vinduer af forskellige størrelser, hvilket gør det muligt at håndtere lange kontekster (op til 128.000 tokens), samtidig med at den forbliver let. Den store tokenizer (V3-Tekken) komprimerer sprog og kode bedre, hvilket forbedrer dens præstationer på flersprogede opgaver.</p>",
                "size_desc": "<p>Med sine 8 milliarder parametre tilhører denne model kategorien af små modeller (mellem 7 og 20 milliarder parametre). Den kan implementeres lokalt på en ret kraftig computer, hvilket garanterer databeskyttelse, eller hostes på en server med et enkelt grafikkort for at begrænse infrastrukturomkostningerne.</p>"
            },
            "Mistral 3 Large": {
                "desc": "<p> En meget stor, semi-åben, multimodal model, der præsterer godt i flersproget kode og kontekster. </p>",
                "fyi": "<p> Denne model blev trænet på 3.000 Nvidia H200 GPU'er. Den konkurrerer direkte med semi-åbne kinesiske modeller. Ved lanceringen er den konkurrencedygtig med DeepSeek V3.1, Kimi K2, GLM 4.6 og andre benchmarks med hensyn til kode og generelle anvendelsestilfælde. </p>",
                "size_desc": "<p> Med 675 milliarder parametre falder denne model i kategorien af meget store modeller. Takket være en Mixture of Experts (MoE)-arkitektur er den mere effektiv end tætte modeller af tilsvarende størrelse, men den kræver stadig en server med flere meget kraftfulde grafikkort at hoste. Denne model aktiverer 41 milliarder parametre per token-generering. Dens kontekstvindue kan rumme op til 256.000 tokens, hvilket gør det muligt at behandle meget lange dokumenter. </p>"
            },
            "Mistral Large 2": {
                "desc": "<p>Stor model beregnet til at behandle komplekse spørgsmål og opgaver: for eksempel kodegenerering, brug af værktøjer, analyse af lange dokumenter eller præcis sprogforståelse.</p>",
                "fyi": "<p>Denne model er blevet trænet med en høj andel af data i kode (mere end 80 programmeringssprog) og matematik, hvilket forbedrer dens evne til at løse komplekse problemer og bruge eksterne værktøjer.</p>",
                "size_desc": "<p>Med 123 milliarder parametre tilhører denne model kategorien af store modeller. Den kræver en server udstyret med mindst et kraftigt grafikkort, hvilket medfører betydelige driftsomkostninger. Den har et kontekstvindue på op til 128.000 tokens, hvilket er nyttigt til analyse af lange dokumenter.</p>"
            },
            "Mistral Medium 3.1": {
                "desc": "<p>Mellemstor multilingual, multimodal model, der er billigere sammenlignet med andre modeller, som tilbyder lignende ydeevne. Den blev særligt interessant efter en opdatering i august 2025 med betydelige forbedringer af den generelle ydeevne, en \"forbedret\" tone og en bedre evne til at søge information på internettet.</p>",
                "fyi": "<p>Denne model er blevet designet til at tilbyde solid performance til en lavere pris end proprietære eller semi-åbne modeller. Der er blevet lagt særlig vægt på data til professionel brug under dens træning. Den er særligt god sammenlignet med andre modeller af lignende størrelse til at generere kode og udføre matematiske opgaver.</p>\n<p>Denne model har tjent som grundlag for træning af Magistral Medium - en ræsonneringsmodel.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en stor model, der kræver mindst flere kraftige grafikkort for at fungere. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>\n<p>Den har et kontekstvindue på op til 128.000 tokens, hvilket er nyttigt til analyse af lange dokumenter.</p>"
            },
            "Mistral Nemo": {
                "desc": "<p>Optimeret til hurtig reaktionstid er denne model ideel til applikationer, der kræver øjeblikkelige svar, og kan understøtte en kontekst på 128k tokens på over 100 sprog. Lanceret i juli 2024.</p>",
                "fyi": "<p>Lanceret i juli 2024 er denne lille model trænet til ræsonnerings-, almen viden- og programmeringsopgaver. Den bruger Tekken-tokenizeren, som er effektiv til at komprimere tekster op til 128.000 tokens på over 100 sprog.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig performance til forskellige opgaver (resumering, oversættelse, tekstklassificering...)</p>"
            },
            "Mistral Saba": {
                "desc": "<p>Mellemstor model designet til en fin sproglig og kulturel forståelse af sprog fra Mellemøsten og Sydasien, især arabisk, tamil og malayalam.</p>",
                "fyi": "<p>Træningen fokuserede primært på tekster på arabisk, tamil og malayalam. De regionale korpusser blev udvalgt for at afspejle autentiske anvendelser, herunder syntaks, registre og dialektvarianter. Til tokenisering (opdeling af teksten i basisenheder, som modellen kan behandle) blev der anvendt en specialiseret strategi tilpasset sprog med kompleks morfologi som arabisk. Optimeringer sigter mod at undgå overdreven fragmentering af ord og maksimere ordforrådet.</p>",
                "size_desc": "<p>Modellens nøjagtige størrelse er ikke kendt. Indikationer tyder på, at det er en mellemstor model, der kræver mindst et kraftigt grafikkort for at fungere. De tilgængelige estimater bygger på indirekte indikatorer som inferensomkostninger og svarlantens.</p>\n<p>Modellen tilbyder et kontekstvindue på op til 128.000 tokens, som er egnet til analyse af lange dokumenter.</p>"
            },
            "Mistral Small 3": {
                "desc": "<p>Lanceret i januar 2025 er denne model specialiseret i flersprogethed og besidder avancerede ræsonneringskapaciteter.</p>",
                "fyi": "<p>Lanceret i januar 2025 er denne model specialiseret i flersprogethed, har en funktionskaldetilstand og en kontekst på 32.000 tokens.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Mistral Small 3.1 24B": {
                "desc": "<p>Mistral Small 3.1 24B Instruct er en forbedret variant af Mistral Small 3 (januar 2025), udstyret med 24 milliarder parametre og avancerede multimodale kapaciteter.</p>",
                "fyi": "<p>Mistral Small 3.1 24B Instruct er en multimodal model, der tilbyder avancerede præstationer i tekst- og visionsbaserede ræsonneringsopgaver, herunder billedanalyse, programmering, matematisk ræsonnering og flersproget support til snesevis af sprog.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p>På trods af sit navn er det en mellemstor model. Den er multimodal (i stand til at behandle tekst og billeder) og skiller sig ud ved præcis overholdelse af forespørgsler og sin evne til at bruge avancerede værktøjer.</p>",
                "fyi": "<p>Version 3.2 af denne model er optimeret til at generere strukturerede output, især i JSON, samtidig med at gentagelser og uønsket adfærd ved lange genereringer begrænses. Multimodal kan den behandle både tekstuelle input og billeder, hvilket muliggør en fælles analyse.</p>",
                "size_desc": "<p>Med 32 milliarder parametre betragtes denne model som en mellemstor model. Den kan hostes på en server med et enkelt kraftfuldt grafikkort, hvilket begrænser infrastrukturomkostningerne. Den har et kontekstvindue på op til 128.000 tokens, nyttigt til analyse af lange dokumenter.</p>"
            },
            "Mistral Small 3.2 24B": {
                "desc": "<p>Mistral Small 3.2 24B Instruct er en forbedret variant af Mistral Small 3.1 (marts 2025), udstyret med 24 milliarder parametre og avancerede multimodale funktioner.</p>",
                "fyi": "<p>Mistral Small 3.2 24B Instruct er en multimodal model, der tilbyder topmoderne præstationer i opgaver baseret på tekst- og synsbaseret ræsonnement, herunder billedanalyse, programmering, matematisk ræsonnement og flersproget support for snesevis af sprog.</p>",
                "size_desc": "<p>Mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og præstation: de er langt mindre ressourcekrævende end store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnement.</p>"
            },
            "Mixtral 8x22B": {
                "desc": "<p>Denne flersprogede model, der blev udgivet i april 2024, er især blevet trænet på engelsk, fransk, tysk, italiensk og spansk samt på opgaver inden for matematik, programmering og ræsonnement.</p>",
                "fyi": "<p>SMoE-arkitekturen (sparse mixture of experts) i denne model gør den hurtigere og optimerer forholdet mellem dens størrelse og omkostninger. Kun 39 mia. parametre er aktive ud af 141 mia. Kontekstvinduet på 64.000 tokens gør det muligt at hente præcise oplysninger fra store dokumenter. Udgivet i april 2024.</p>",
                "size_desc": "<p>Disse modeller udstyret med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til præstation og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er af en sådan karakter, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "Mixtral-8x7B": {
                "desc": "<p>Denne model, der er trænet på et flersproget korpus, er effektiv til varierede og mindre komplekse opgaver.</p>",
                "fyi": "<p>Lillebror i Mixtral-familien, denne model kan behandle kontekster på 32.000 tokens og understøtter engelsk, fransk, italiensk, tysk og spansk. Takket være SMoE-arkitekturen (sparse mixture of experts) aktiveres kun en brøkdel af parametrene ved hver inferens, hvilket reducerer omkostninger og latens.</p>",
                "size_desc": "<p>Mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og præstation: de er langt mindre ressourcekrævende end store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnement.</p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p>Stor model trænet ud fra Llama 3.1 70B. Denne genoptrænte version (fine-tune) har en tendens til at give flere detaljer og levere mere strukturerede svar.</p>",
                "fyi": "<p>Denne model er resultatet af en genoptræning af Llama 3.1 70B, deraf tilstedeværelsen af dens kildemodel i navnet! Den introducerer forbedringer takket være forstærkningslæring med menneskelig feedback (RLHF) og REINFORCE-algoritmen: modellen udforsker forskellige svar, modtager feedback i form af belønninger og justerer derefter gradvist sine valg for bedre at imødekomme brugernes forventninger. Denne tilpasningsproces bruges ofte, når man vil have modellen til at tilpasse sig menneskelige præferencer, eller når den skal optimere sine svar efter specifikke kriterier.</p>",
                "size_desc": "<p>Med 70 milliarder parametre tilhører denne model kategorien af store modeller. Den kræver flere kraftige grafikkort for at fungere, hvilket medfører bemærkelsesværdige driftsomkostninger.</p>"
            },
            "OLMo-2 32B": {
                "desc": "<p>OLMo 2 32B er en fuldt open source-model (inklusive korpus og træningskode) skabt af Allen AI Institute (Ai2), udgivet i marts 2025.</p>",
                "fyi": "<p>OLMo 2 32B er en fuldt open source-model: korpusset og træningskoden er fuldstændig tilgængelige. Denne OLMo-modelfamilie er blevet designet af Allen Institute for AI (Ai2).</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Olmo 3 32B Think": {
                "desc": "<p>Ræsonneringsmodel, hvis kode og data er fuldstændig åbne. Den er blevet trænet af AI2, et nonprofit-forskningsinstitut.</p>",
                "fyi": "<p>Denne model er et sjældent eksempel på total gennemsigtighed. Koden, dataene (6 billioner tokens), trænings-\"checkpoints\" og evalueringsopskrifterne er alle offentlige. Selvom den fuldstændige reproduktion af træningen er dyr i infrastruktur og sandsynligvis sjælden, gør denne gennemsigtighed det muligt for udviklere bedre at genoptræne (fine-tune) modellen til specifikke opgaver, især ved at blande data med den ønskede adfærd med det eksisterende datasæt, hvilket sikrer mere stabil træning.</p>",
                "size_desc": "<p>Med 32 milliarder parametre er den klassificeret i kategorien af mellemstore modeller. Den kan deployes på en server udstyret med et enkelt grafikkort (GPU). Dens kontekstvindue når op på 65.000 tokens, hvilket gør den egnet til analyse af ret lange dokumenter.</p>"
            },
            "Phi-3-Mini": {
                "desc": "<p>Performant til opgaver med kodegenerering og resumering, understøtter denne kompakte model en begrænset kontekst på 4.000 tokens.</p>",
                "fyi": "<p>Lillebror i Phi3-familien, denne model understøtter en kontekst på 4000 tokens og er blevet trænet på syntetiske datasæt og fra filtreret web.</p>",
                "size_desc": "<p>Meget små modeller med mindre end 7 milliarder parametre er de mindst komplekse og mest økonomiske med hensyn til ressourcer og tilbyder tilstrækkelig præstation til simple opgaver som tekstklassificering.</p>"
            },
            "Phi-3-small-8k-Instruct": {
                "desc": "<p>Optimeret til logisk ræsonnement, denne lille model understøtter en kontekst på 8000 tokens, velegnet til kodegenerering og komplekse opgaver.</p>",
                "fyi": "<p>Storebror i Phi3-familien, denne model understøtter en kontekst på 8000 tokens og er blevet trænet på syntetiske datasæt og fra filtreret web.</p>",
                "size_desc": "<p>En lille model er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig præstation til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "Phi-3.5-mini": {
                "desc": "<p>Ydeevnedygtig til kodegenererings- og resuméopgaver, denne model håndterer en stor kontekst på 128k tokens.</p>",
                "fyi": "<p>Lille model i Phi-familien, der erstatter Phi-3-mini, denne model understøtter en stor kontekst på 128.000 tokens og er blevet trænet på syntetiske datasæt og fra filtreret web.</p>",
                "size_desc": "<p>Meget små modeller med mindre end 7 milliarder parametre er de mindst komplekse og mest økonomiske med hensyn til ressourcer og tilbyder tilstrækkelig præstation til simple opgaver som tekstklassificering.</p>"
            },
            "Phi-4": {
                "desc": "<p>Lille flersproget model, i stand til at bruge værktøjer og yde godt på komplekse opgaver som logik, matematik og kode, samtidig med at den forbliver kompakt.</p>",
                "fyi": "<p>Denne model bruger tiktoken til tokenisering, hvilket forbedrer dens kapaciteter i flersprogede kontekster. Den er blevet trænet på i alt 9,8 <strong>billioner</strong> tokens, hvoraf 400 milliarder specifikt stammer fra højkvalitets syntetiske data, mens resten består af filtrerede organiske data. Træningen foregik på 1.920 H100-grafikkort i 21 dage. Innovative teknikker som selv-evaluering – hvor modellen kritiserer og omskriver sine svar – samt inverterede instruktioner er blevet brugt til at styrke dens forståelse af direktiver og ræsonneringskapaciteter.</p>",
                "size_desc": "<p>Med 14 milliarder parametre tilhører denne model kategorien af små modeller. Den kan implementeres lokalt på en tilstrækkeligt kraftig computer eller hostes på en server med et enkelt grafikkort, hvilket reducerer infrastrukturomkostningerne. Kontekstvinduet på 16.000 tokens kan være begrænsende til analyse af meget lange dokumenter.</p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p>Mellemstor model specialiseret i programmering og brugen af eksterne værktøjer (websøgninger, interaktioner med software osv.).</p>",
                "fyi": "<p>Denne model er blevet trænet på 5,5 billioner tokens og mere end 92 programmeringssprog, herunder specialiserede kodesprog som Haskell eller Racket.</p>\n<p>Takket være dens præstationer i kode er den i stand til at håndtere kald til eksterne værktøjer godt, hvilket er nyttigt til agentiske anvendelser.</p>",
                "size_desc": "<p>Med 32 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan køre på en server udstyret med et enkelt kraftigt grafikkort, hvilket begrænser infrastrukturomkostningerne.</p>\n<p>Dens kontekstvindue på 128.000 tokens gør det muligt at behandle lange dokumenter.</p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p>Mellemstor flersproget model.</p>",
                "fyi": "<p>Denne MoE-model (Mixture of Experts) skiller sig ud ved en konfiguration på 128 eksperter i alt, med kun 8 eksperter aktiveret per token, hvilket muliggør hurtigere og mere effektiv inferens. Den bruger et system kaldet <em>global-batch</em> til at optimere arbejdsfordelingen mellem eksperterne, så de alle bruges på en afbalanceret måde.</p>\n<p>I modsætning til andre modeller som Qwen 2.5-MoE, der genbruger de samme eksperter gennem flere lag i netværket, tildeler Qwen 3 30B A2B unikke eksperter til hvert lag. Konkret betyder dette, at eksperterne fra det første lag aldrig genbruges i de følgende lag - hvert niveau af modellen har sit eget sæt specialiserede eksperter. Denne arkitektur gør det muligt for hver ekspert at fokusere udelukkende på de specifikke opgaver i sin position i det neurale netværk, hvilket resulterer i en finere specialisering og optimerede præstationer for hvert trin i behandlingen af information.</p>",
                "size_desc": "<p>Med 30 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan køre på en server udstyret med et enkelt kraftigt grafikkort, hvilket begrænser infrastrukturomkostningerne. Derudover aktiverer Mixture of Experts-arkitekturen (MoE) kun en del af parametrene ved hvert token, hvilket begrænser dens energiforbrug.</p>"
            },
            "Qwen 3 32B": {
                "desc": "<p>Mellemstor flersproget model med to svarmuligheder: brugeren kan vælge mellem en ræsonneringstilstand for mere dybdegående svar eller en hurtig tilstand til at generere det endelige svar direkte.</p>",
                "fyi": "<p>Denne model er blevet trænet på et meget stort datamængde: 36 billioner tokens på 119 sprog. Træningen fandt sted i tre faser. Modellen lærte først fra 30 billioner tokens med en kontekst på 4.000 tokens. Derefter blev 5 billioner tokens tilføjet for at styrke dens faktuelle viden. Endelig blev den eksponeret for et specifikt korpus for at hjælpe den med bedre at håndtere meget lange tekster. Resultatet: den har ved afslutningen af træningen et kontekstvindue på 128.000 tokens, hvilket er nyttigt til at læse og analysere lange dokumenter.</p>",
                "size_desc": "<p>Med 32 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan køre på en server udstyret med et enkelt kraftfuldt grafikkort, hvilket begrænser infrastrukturomkostningerne.</p>\n<p>Dens kontekstvindue på 128.000 tokens gør det muligt at behandle lange dokumenter.</p>"
            },
            "Qwen 3 8B": {
                "desc": "<p>Lille flersproget tæt model fra Qwen 3-familien, der tilbyder en \"ræsonnements\"-tilstand til komplekse opgaver (matematik, kode) og en \"direkte svar\"-tilstand til hurtigere svar.</p>",
                "fyi": "<p>Qwen 3 8B er blevet trænet på det samme korpus som de større modeller i Qwen-familien: 36 billioner tokens dækkende 119 sprog. Dens læring følger tre trin: præ-træning på 30 billioner tokens med et vindue på 4.000, faktuel berigelse med 5 billioner tokens, derefter en specialiseret fase for lange kontekster.</p>",
                "size_desc": "<p>Med 8 milliarder parametre tilhører den de små modeller. Den kan bruges lokalt på en arbejdsstation for at bevare datafortrolighed eller på en billig server for at begrænse omkostningerne sammenlignet med en større model.</p>\n<p>Dens kontekstvindue går op til 128.000 tokens, hvilket gør det muligt at behandle lange dokumenter.</p>"
            },
            "Qwen 3 Max": {
                "desc": "<p>Blandt de få proprietære modeller fra Qwen er denne den største og mest kraftfulde i tredje generation. Den er blevet trænet med særlig opmærksomhed på virksomhedsbrug og agentiske anvendelsestilfælde.</p>",
                "fyi": "<p>Denne model er blevet trænet på 36 billioner tokens, næsten det dobbelte af Qwen 2.5, og den er officielt i stand til at svare på 100 sprog.</p>",
                "size_desc": "<p>Den nøjagtige størrelse er ikke kendt. Tegn peger på, at det er en meget stor model, der kræver servere udstyret med flere kraftfulde grafikkort for at fungere. De tilgængelige estimater er baseret på indirekte tegn såsom inferensomkostninger og svarlatens. Modellen har et kontekstvindue på op til 256.000 tokens, velegnet til analyse af lange dokumenter eller kodedepoter.</p>"
            },
            "Qwen1.5-32B": {
                "desc": "<p>Mellemstor model med en træningsproces, der er meget målrettet tilpasning til brugerpræferencer.</p>",
                "fyi": "<p>Modellen har gennemgået en alignment-fase med brugerpræferencer via teknikker som DPO (Direct Preference Optimization) og PPO (Proximal Policy Optimization). Ud over disse teknikker, som var meget innovative på tidspunktet for modellens design, har Alibaba-teamet også optimeret træningsdataene for at gøre dem meget flersprogede, især på europæiske, østasiatiske og sydøstasiatiske sprog.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Qwen2-57B-A14B-Instruct": {
                "desc": "<p>Mellemstor model med en Mixture of Experts-arkitektur, performant i kode, matematik og flersprogede opgaver.</p>",
                "fyi": "<p>Denne iteration af Qwen-modellerne har længere kontekstvinduer.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Qwen2-72b-instruct": {
                "desc": "<p>Stor performant model i kode, matematik og flersprogede opgaver.</p>",
                "fyi": "<p>Denne iteration af Qwen-modellerne har længere kontekstvinduer.</p>",
                "size_desc": "<p>De store modeller kræver betydelige ressourcer, men tilbyder de bedste præstationer til avancerede opgaver såsom kreativ skrivning, dialogmodellering og applikationer, der kræver en fin forståelse af kontekst.</p>"
            },
            "Qwen2-7B": {
                "desc": "<p>Med support for en kontekst på 130k tokens er denne lille flersprogede og alsidige model performant til opgaver med oversættelse, resumering, analyse og ræsonnering.</p>",
                "fyi": "<p>Lillebror i Qwen2-familien og produceret af den kinesiske virksomhed Alibaba, kan denne model håndtere op til 130.000 tokens til behandling af lange tekster.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig performance til forskellige opgaver (resumering, oversættelse, tekstklassificering...)</p>"
            },
            "Qwen2.5-32B": {
                "desc": "<p>Med support for en kontekst på 130k tokens er denne flersprogede og alsidige model performant til opgaver med oversættelse, resumering, analyse og ræsonnering.</p>",
                "fyi": "<p>Mellemstor model i Qwen2.5-familien og produceret af den kinesiske virksomhed Alibaba, kan denne model håndtere op til 130.000 tokens til behandling af lange tekster.</p>",
                "size_desc": "<p>De mellemstore modeller tilbyder en god balance mellem kompleksitet, omkostninger og performance: de forbruger langt færre ressourcer end de store modeller, samtidig med at de er i stand til at håndtere komplekse opgaver såsom sentimentanalyse eller ræsonnering.</p>"
            },
            "Qwen2.5-7B": {
                "desc": "<p>Med support for en kontekst på 130k tokens er denne flersprogede og alsidige model performant til opgaver med oversættelse, resumering, analyse og ræsonnering.</p>",
                "fyi": "<p>Lille model i Qwen2.5-familien og produceret af den kinesiske virksomhed Alibaba, kan denne model håndtere op til 130.000 tokens til behandling af lange tekster.</p>",
                "size_desc": "<p>En model af lille størrelse er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig performance til forskellige opgaver (resumering, oversættelse, tekstklassificering...)</p>"
            },
            "Qwen3 Coder 480B A35B": {
                "desc": "<p>Meget stor model specialiseret i kodegenerering, analyse af hele repositories og løsning af multi-trins problemer. Denne version er særligt stærk i brug af værktøjer og kan simulere en ræsonneringsfase, før den leverer det endelige svar.</p>",
                "fyi": "<p>Denne model er blevet prætrænet på 7,5 billioner tokens (hvoraf 70% er kode) og bruger en avanceret post-træningsproces - Code RL (Hard to Solve, Easy to Verify) til at styrke korrekt kodeudførelse og Agent RL (long-horizon reinforcement learning) til at optimere løsning af multi-turn softwareopgaver med et massivt parallelt miljø (20.000 parallelle simuleringer på Alibaba Cloud).</p>",
                "size_desc": "<p>Qwen3-Coder-480B-A35B-Instruct er en meget stor model, der kræver flere grafikkort for at fungere. Mixture of Experts-arkitekturen (MoE) gør det dog muligt kun at aktivere en brøkdel af parametrene (35 mia. ud af 480 mia.), hvilket reducerer miljøpåvirkningen og omkostningerne betydeligt sammenlignet med en tilsvarende dense model.\nKontekstvinduet når native op på 256.000 tokens og kan udvides til op til 1 million takket være ekstrapoleringsteknikker (YaRN), hvilket er ideelt til analyse af store kodebaser.</p>"
            },
            "Yi-1.5 9B": {
                "desc": "<p>Yi 1.5 er en model fra den kinesiske virksomhed 01-ai, specialiseret i kode, matematik, ræsonnement, instruktionsfølgning, med en solid forståelse af sprog.</p>",
                "fyi": "<p>Yi 1.5 er en model fra den kinesiske virksomhed 01-ai, specialiseret i kode, matematik, ræsonnement, instruktionsfølgning, med en solid forståelse af sprog.</p>",
                "size_desc": "<p>En lille model er mindre kompleks og ressourcekrævende sammenlignet med større modeller, samtidig med at den tilbyder tilstrækkelig præstation til forskellige opgaver (resumé, oversættelse, tekstklassificering...)</p>"
            },
            "o3-mini": {
                "desc": "<p>o3-mini er lavet til ræsonnering og kode. Den tilbyder en god balance mellem performance, omkostninger og latens, samtidig med at den er mindre end andre modeller fra OpenAI.</p>",
                "fyi": "<p>Model optimeret til STEM-ræsonneringsopgaver (science, teknologi, ingeniørvirksomhed, matematik) og kodeskrivning. Den skiller sig ud inden for videnskabelige, matematiske og programmeringsmæssige områder.</p>",
                "size_desc": "<p>Disse modeller med flere hundrede milliarder parametre er de mest komplekse og avancerede med hensyn til performance og præcision. De beregnings- og hukommelsesressourcer, der er nødvendige for at implementere disse modeller, er så omfattende, at de er beregnet til de mest avancerede applikationer og højt specialiserede miljøer.</p>"
            },
            "o4 mini": {
                "desc": "<p>Meget stor ræsonneringsmodel, egnet til komplekse videnskabelige og teknologiske opgaver og spørgsmål.</p>",
                "fyi": "<p>Denne model er meget performant til analyse af billeder og grafikker. Den er også blevet trænet til at interagere med andre systemer via funktionskald, hvilket gør det muligt at bruge den til agentiske anvendelsestilfælde. Som en meget kraftig ræsonneringsmodel kan den blandt andet bruges til at fordele opgaver mellem flere mindre og/eller mere specialiserede modeller. Den har et kontekstvindue på op til 200.000 tokens, hvilket letter analysen af lange dokumenter.</p>",
                "size_desc": "<p>På trods af sit navn og det faktum, at den nøjagtige størrelse ikke er kendt, er o4 mini højst sandsynligt en stor model, der kræver servere udstyret med flere grafikkort. Ræsonneringsmodeller som o4 mini kræver mere tid til at svare, fordi en ræsonneringsfase går forud for genereringen af det endelige resultat, hvilket øger deres energiforbrug. Ikke desto mindre aktiverer den formodede Mixture of Experts-arkitektur (MoE) kun en del af parametrene til at generere hvert token, hvilket begrænser dens energiforbrug. Størrelsesestimaterne bygger på indirekte indikatorer såsom inferensomkostninger og svarlantens.</p>"
            },
            "qwq 32B": {
                "desc": "<p>Mellemstor ræsonneringsmodel, specialiseret og meget ydeevnedygtig inden for matematik, kodegenerering og løsning af logiske problemer.</p>",
                "fyi": "<p>Denne model er blevet trænet med en forstærkningslæringsmetode (RL) for at optimere håndteringen af matematikproblemer og programmeringsopgaver. Den bruger flere nyere teknikker til at forbedre kvaliteten af svarene. For eksempel gør RoPE-metoden (Rotary Position Embedding) det muligt for den at forstå rækkefølgen af ord i en tekst bedre. Aktiveringsfunktionen SwiGLU er en mere effektiv måde at håndtere beregninger inden i det neurale netværk på, som hjælper modellen med at producere mere pålidelige svar. QKV-justeringsmetoden (Query Key Value-bias) forbedrer den måde, modellen identificerer og udvælger vigtige oplysninger på. Endelig kan den takket være YaRN-metoden (Yet another RoPE extensioN method) behandle meget lange tekster på op til 130.000 tokens, hvilket gør det muligt for den at arbejde med komplekse eller meget detaljerede dokumenter.</p>",
                "size_desc": "<p>Med 32 milliarder parametre tilhører denne model kategorien af mellemstore modeller. Den kan køre på en server udstyret med et enkelt kraftfuldt grafikkort, hvilket begrænser infrastrukturomkostningerne. Ræsonneringsmodeller af denne type kører dog længere tid for at producere et svar, da en ræsonneringsfase går forud for genereringen af det endelige resultat, hvilket øger energiforbruget.</p>"
            }
        }
    },
    "header": {
        "banner": "Mistral 3 er kommer til arenaen!",
        "chatbot": {
            "newDiscussion": "Ny samtale",
            "step": "Trin",
            "stepOne": {
                "description": "Vær opmærksom på både indhold og form, og vurdér derefter hvert svar.",
                "title": "Hvad synes du om svarene?"
            },
            "stepTwo": {
                "description": "Se den miljømæssige påvirkning af dine samtaler med hver model",
                "title": "Modellerne er afsløres!"
            }
        },
        "help": {
            "link": {
                "content": "Hjælp os med at forbedre AI Arenaen",
                "title": "Giv feedback på arenaen – åbner et nyt vindue"
            }
        },
        "homeTitle": "Hjem - compar:IA",
        "logoAlt": "Den Franske Republik",
        "menu": "Menu",
        "startDiscussion": "Start samtalen",
        "subtitle": "Sammenlign svar fra chatbots",
        "title": "AI-Arenaen",
        "votes": {
            "count": "{count} stemmer",
            "legend": "Legend",
            "objective": "Mål: {count}",
            "tooltip": "Diskuter, stem og hjælp os med at nå dette mål!<br /><strong>Dine stemmer betyder noget</strong>: de leverer data til compar:IA-datasættet, som er frit tilgængeligt, for at hjælpe med at forbedre fremtidige modeller på lavresourcesprog.<br />Denne digitale fælles ressource bidrager til bedre <strong>respekt for sproglig og kulturel mangfoldighed i fremtidige sprogmodeller.</strong>"
        }
    },
    "home": {
        "europe": {
            "desc": "Litauen, Sverige og Danmark slutter sig til Frankrig i at indføre sammenligningsværktøjet for at forbedre fremtidige AI-modeller på deres nationale sprog.",
            "languages": {
                "da": "på dansk",
                "fr": "på fransk",
                "lt": "på litauisk",
                "sv": "på svensk"
            },
            "question": "Vil du gerne have AI Arenaen på dit sprog?",
            "title": "Sammenligneren <span {props}>bliver europæisk!</span>"
        },
        "faq": {
            "discover": "Se andre spørgsmål",
            "title": "Ofte stillede spørgsmål"
        },
        "intro": {
            "desc": "Chat med to ukendte AI'er og vurdér deres svar",
            "steps": {
                "a11yDesc": "1. Jeg chatter med to skjulte AI'er: Chat så længe du vil. 2. Jeg angiver mine præferencer: Dermed hjælper du med at forbedre AI-modellerne. 3. Modellernes identiteter afsløres: Få mere at vide om dem og deres egenskaber.",
                "one": {
                    "desc": "Chat så længe du har lyst",
                    "title": "Chat med to ukendte chatbotter"
                },
                "three": {
                    "desc": "Lær mere om AI-modeller, chatbotter og deres karakteristika",
                    "title": "Se hvilke chatbotter du har chattet med!"
                },
                "title": "Sådan fungerer det",
                "two": {
                    "desc": "Du bidrager dermed til at forbedre chatbotterne",
                    "title": "Giv din feedback"
                }
            },
            "title": "Stol ikke på svarene <span {props}>fra en enkelt AI</span>",
            "tos": {
                "accept": "Jeg accepterer <a {linkProps}>brugsbetingelserne</a>",
                "error": "Du skal acceptere brugsbetingelserne for at fortsætte",
                "help": "Data deles til forskningsformål"
            }
        },
        "origin": {
            "project": {
                "desc": "AI-Arenaen blev designet og udviklet som en del af et statsligt startup-projekt ledet af det franske kulturministerium og integreret i programmet <a {linkProps}>Beta.gouv.fr</a> af det tværministerielle digitale direktorat (DINUM). Dette initiativ støtter franske offentlige myndigheder i at opbygge nyttige, enkle og brugervenlige digitale tjenester.",
                "title": "Hvem tog initiativ til projektet?"
            },
            "team": {
                "desc": "AI Arenaen er et samarbejde mellem Danish Foundation Models, Digitaliseringsstyrelsen og det franske kulturministerium. Danish Foundation Models består af forskere og AI specialister fra Aarhus Universitet, Syddansk Universitet, Københavns Universitet og Alexandra instituttet. Servicen er hostet af det franske kulturministerie hvis team består af AI-eksperter, udviklere, implementeringsspecialister og designere – Missionen med AI Arenaen er at gøre AI mere gennemsigtig og tilgængelig for alle.",
                "title": "Hvem er vi?"
            }
        },
        "usage": {
            "desc": "Værktøjet er også rettet mod AI-eksperter og undervisere til mere specifikke anvendelsestilfælde",
            "educate": {
                "desc": "Brug AI Arenaen som et pædagogisk redskab til at diskutere AI med dit publikum",
                "title": "Træning og awareness"
            },
            "explore": {
                "desc": "Find alle modelspecifikationer og brugsbetingelserne på ét sted",
                "title": "Udforsk modellerne"
            },
            "title": "Specifikke brugscenarier for AI Arenaen",
            "use": {
                "desc": "Udviklere, forskere, modeludgivere – få adgang til AI Arenaen's datasæt for at forbedre modeller til sprog med begrænsede ressourcer",
                "title": "Genbrug data"
            }
        },
        "use": {
            "compare": {
                "alt": "Sammenlign",
                "desc": "Diskuter og udvikl din kritiske tænkning ved at give udtryk for dine præferencer",
                "title": "Sammenlign svarene fra forskellige AI-modeller"
            },
            "desc": "AI Arenaen er et gratis værktøj, der hjælper med at øge borgernes bevidsthed om generativ AI og dens udfordringer.",
            "measure": {
                "alt": "Mål",
                "desc": "Få indsigt i, hvordan det påvirker miljøet at stille spørgsmål til modellerne.",
                "title": "Miljøaftryk af AI-forespørgsler"
            },
            "test": {
                "alt": "Test",
                "desc": "Test forskellige modeller: åbne, proprietære, små, store...",
                "title": "Test de nyeste AI i økosystemet på ét sted"
            },
            "title": "Hvad bruges AI Arenaen til?"
        },
        "vote": {
            "datasetAccess": "Få adgang til datasættene",
            "desc": "Udviklere, forskere, modeludgivere – få adgang til AI-Arenas datasæt for at forbedre modeller til sprog med begrænsede ressourcer.",
            "steps": {
                "datasets": {
                    "desc": "Alle spørgsmål og stemmer samles i datasæt og offentliggøres efter anonymisering.",
                    "title": "Datasæt efter sprog"
                },
                "finetune": {
                    "desc": "Virksomheder og videninstitutioner kan bruge datasættene til at træne nye modeller, der i højere grad tager højde for sproglig og kulturel mangfoldighed.",
                    "title": "Modeller, der er finjusteret til specifikke sprog"
                },
                "prefs": {
                    "desc": "Efter at have snakket med AI'erne, bliver du bedt om at angive hvilket svar du foretrækker ud fra eksempelvis hvor relevant eller brugbart svaret er.",
                    "title": "Dine præferencer"
                }
            },
            "title": "Hvorfor er din stemme vigtig?"
        }
    },
    "models": {
        "arch": {
            "title": "Vidste du?"
        },
        "conditions": {
            "commercialUse": {
                "question": "Er kommerciel brug af modellen tilladt?",
                "title": "Kommerciel brug"
            },
            "reuse": {
                "question": "Kan jeg bruge modellens outputs til at træne nye modeller?",
                "subTitle": "Du kan ikke genbruge dem til at træne andre modeller",
                "title": "Genbrug af genererede resultater"
            },
            "title": "Terms of Use",
            "types": {
                "allowed": "Tilladt",
                "conditions": "Under betingelser",
                "forbidden": "Forbudt"
            }
        },
        "conso": {
            "count": {
                "L": "> 100 Wh",
                "M": "fra 10 til 100 Wh",
                "S": "< 10 Wh"
            },
            "filterLegend": "Filtrer efter gennemsnitligt energiforbrug for 1000 tokens"
        },
        "extra": {
            "experts": {
                "api-only": "For at læse mere, se den <a {linkProps}>officielle modelhjemmeside</a>",
                "open-weights": "For at læse mere, se <a {linkProps}>modelsiden på Hugging Face</a>"
            },
            "impacts": "Miljøpåvirkningsberegninger er baseret på <a {linkProps1}>EcoLogits</a> og <a {linkProps2}>Impact CO<sub>2</sub></a> projekterne.",
            "title": "Få mere at vide"
        },
        "licenses": {
            "type": {
                "openSource": "Open source",
                "proprietary": "Ophavsretligt beskyttet",
                "semiOpen": "Open-weight"
            }
        },
        "list": {
            "filters": {
                "archived": {
                    "checkedLabel": "Synlig",
                    "help": "Som standard er kun aktive modeller i arenaen synlige.",
                    "label": "Arkiverede modeller",
                    "uncheckedLabel": "Ikke synligt"
                },
                "display": "Vis filtre",
                "editor": {
                    "legend": "Udgiver"
                },
                "license": {
                    "legend": "Licens"
                },
                "reset": "Ryd alle filtre",
                "size": {
                    "labels": {
                        "L": "70 til 150 milliarder",
                        "M": "20 til 70 milliarder",
                        "S": "7 til 20 milliarder",
                        "XL": "> 150 milliarder",
                        "XS": "< 7 milliarder"
                    },
                    "legend": "Størrelse (parametre)"
                }
            },
            "intro": "Udforsk de forskellige samtale AI-modeller, deres specifikationer og licenser.",
            "model": "model",
            "models": "modeller",
            "noresults": "Ingen modeller matcher dine søgekriterier.",
            "title": "Udforsk modellerne",
            "triage": {
                "label": "Sortér efter",
                "options": {
                    "date-desc": "Udgivelsesdato (nyeste til ældste)",
                    "name-asc": "Modelnavn (A til Z)",
                    "org-asc": "Udgiver (A til Z)",
                    "params-asc": "Størrelse (mindste til største)"
                }
            }
        },
        "names": {
            "a": "Model A",
            "b": "Model B"
        },
        "openWeight": {
            "tooltips": {
                "copyleft": "Når modellen er ændret, skal den distribueres på ny under samme licens som kildemodellen.",
                "free": "Når modellen er ændret, kan den distribueres under en anden licens end kildemodellen.",
                "openSource": "Træningsdata, koden og vægtene for denne model (dvs. de parametre, der er lært under træningen) kan downloades og ændres af offentligheden, så de kan køre og ændre modellen på deres egen hardware. Om en model er \"open source\" er mere restriktivt end \"open weights\", især på grund af behovet for gennemsigtighed i træningskorpuset. Få modeller betragtes som \"open source\"",
                "openWeight": "En såkaldt \"open weights\"-model, hvor vægtene, dvs. de parametre, der er lært under træningen, kan downloades af offentligheden, så de kan køre modellen på deres egen hardware. Om en model er \"open source\" er mere restriktivt (hovedsageligt i relation til gennemsigtigheden af træningskorpuset). Få modeller betragtes som \"open source\".",
                "params": "Parametre eller vægte, talt i milliarder, er de variabler, som en model lærer under træningen, og som bestemmer dens svar. Jo større antallet af parametre er, jo større er deres læringsevne.",
                "ram": "RAM (random access memory) gemmer data, der behandles af en LLM i realtid. Jo større modellen er, jo mere RAM kræver den for at køre."
            }
        },
        "parameters": "{number} mia. parametre",
        "ram": "{min} til {max} GB",
        "release": "Udgivet {date}",
        "size": {
            "count": {
                "L": "100 til 400 mia.",
                "M": "60 til 100 mia.",
                "S": "15 til 60 mia.",
                "XL": ">400 mia.",
                "XS": "<15 mia."
            },
            "estimated": "Estimeret størrelse ({size})",
            "title": "Størrelse"
        }
    },
    "modes": {
        "big-vs-small": {
            "altLabel": "David mod Goliat-modelvalg",
            "description": "En lille model mod en stor model, begge valgt tilfældigt",
            "label": "David mod Goliat",
            "title": "David mod Goliat-tilstand"
        },
        "custom": {
            "altLabel": "Manuel modeludvælgelse",
            "description": "Kan du genkende de to modeller, du valgte?",
            "label": "Manuel udvælgelse",
            "title": "Manuel udvælgelse"
        },
        "random": {
            "altLabel": "Vilkårligt modelvalg",
            "description": "To modeller valgt vilkårligt fra den fulde liste",
            "label": "Tilfældig",
            "title": "Tilfældig"
        },
        "reasoning": {
            "altLabel": "Valg af ræsonnementmodel",
            "description": "To tilfældigt udvalgte ræsonnementmodeller",
            "label": "Ræsonnement",
            "title": "Ræsonnement"
        },
        "small-models": {
            "altLabel": "Sparsom modeludvælgelse",
            "description": "To små modeller valgt tilfældigt",
            "label": "Sparsom",
            "title": "Sparetilstand"
        }
    },
    "product": {
        "community": {
            "countries": {
                "da": "Danmark",
                "fr": "Frankrig"
            },
            "tabLabel": "Fællesskab",
            "teams": {
                "fr": {
                    "people": {
                        "aurelien": {
                            "date": "Siden juni 2024",
                            "job": "Produktdesigner UX/UI"
                        },
                        "elie": {
                            "date": "Siden november 2025",
                            "job": "Fullstack-udvikler"
                        },
                        "hadrien": {
                            "date": "Juni 2024 - november 2025",
                            "job": "Fullstack-udvikler"
                        },
                        "lucie": {
                            "date": "Januar 2024 - December 2025",
                            "job": "Grundlægger - produktchef"
                        },
                        "mathilde": {
                            "date": "Siden september 2024",
                            "job": "Ansvarlig for Digital Atelier / Product Ops"
                        },
                        "nicolas": {
                            "date": "Siden juni 2025",
                            "job": "Fullstack udvikler"
                        },
                        "simonas": {
                            "date": "Siden december 2024",
                            "job": "Produktchef - tidligere deployment-ansvarlig"
                        }
                    },
                    "title": "Compar:IA-teamet - Frankrig"
                }
            },
            "title": "Partnerne"
        },
        "comparator": {
            "challenges": {
                "bias": {
                    "desc": "Sætter fokus på AI bias, der skyldes underrepræsentation af ikke-engelske data i modellerne, og øger bevidstheden om, hvad det betyder i praksis.",
                    "title": "Kulturel og sproglig bias"
                },
                "impacts": {
                    "desc": "Viser den miljømæssige påvirkning af generativ AI, som stadig er stort set ukendt for offentligheden.",
                    "title": "Miljøpåvirkning"
                },
                "pluralism": {
                    "desc": "Sikrer, at borgerne har adgang til en bred vifte af AI-modeller, så de kan træffe informerede valg og udvikle en kritisk forståelse af disse teknologier.",
                    "title": "Model-diversitet"
                },
                "thinking": {
                    "desc": "Fremmer kritisk tænkning om generativ AI's rolle i privat og arbejdsmæssig sammenhæng.",
                    "title": "Kritisk tænkning og samfundsmæssige spørgsmål"
                },
                "title": "Platformen adresserer flere udfordringer"
            },
            "cta": "Gå til sammenligningsværktøjet",
            "europe": {
                "adventure": "Fra efteråret 2025 tilslutter Litauen, Sverige og Danmark sig initiativet!",
                "catch": "Vil du gerne have AI Arenaen på dit sprog?",
                "desc": "Arenaen er nu tilgængelig for deres borgere på nationale sprog med en central mission: at opbygge præferencedatasæt for at forbedre fremtidige AI-modellers ydeevne på sprog med begrænsede ressourcer.",
                "title": "Arenaen <span {props}>bliver europæisk</span>!"
            },
            "screenshotAlt": "Skærmbillede af compar:ia-arenaen, med det indledende spørgsmål, de to modelsvar og afstemningsknapperne.",
            "tabLabel": "Arenaen",
            "title": "Arenaen gør det muligt at oprette <span {props}>præference-datasæt</span> med fokus på <span {props}>reel brug</span> i <span {props}>europæiske sprog</span>."
        },
        "faq": {
            "tabLabel": "FAQ"
        },
        "history": {
            "steps": {
                "acceleration": {
                    "items": {
                        "1": {
                            "date": "marts 2025",
                            "desc": "Oprettelse og tilgængeliggørelse af det første compar:IA-datasæt på fransk, der samler spørgsmål og præferencer fra platformens brugere.",
                            "title": "50.000 stemmer på sammenligningsplatformen!"
                        },
                        "2": {
                            "date": "maj 2025",
                            "desc": "Og første genanvendelse af datasættet med Bunka.ai, som har gennemført en <a {linkProps}>dybdegående undersøgelse</a> af interaktionerne mellem platformens brugere.",
                            "title": "Målet på 100.000 stemmer nået!"
                        },
                        "3": {
                            "date": "Juni 2025",
                            "desc": "Tilgængeliggørelse af disse tre datasæt, samtaler, reaktioner, stemmer, på <a {hgLinkProps}>HuggingFace</a> og <a {dataLinkProps}>Data.gouv.fr</a>.",
                            "title": "Offentliggørelse af de tre compar:IA-datasæt"
                        }
                    },
                    "tag": "Hastighed"
                },
                "construction": {
                    "items": {
                        "1": {
                            "date": "juni-sep 2024",
                            "desc": "Udvikling af sammenlignerens første funktionaliteter og integration af feedback fra de første betatestere.",
                            "title": "Design af minimum viable product"
                        },
                        "2": {
                            "date": "Oktober 2024",
                            "desc": "Officiel præsentation og første implementeringer af værktøjet.",
                            "title": "Officiel lancering under Frankofoniens topmøde i Villers-Cotterêts!"
                        },
                        "3": {
                            "date": "Januar 2025",
                            "desc": "Lancering af den nye funktionalitet til valg af modelvalgstilstand.",
                            "title": "v2 af AI-arenaen"
                        },
                        "4": {
                            "date": "Februar 2025",
                            "desc": "Mere end 300 personer i forbindelse med konferencer og workshops dedikeret til etiske, kulturelle og miljømæssige udfordringer ved konverserende AI-systemer.",
                            "title": "<a {linkProps}>comparIA-dag på BnF</a> under topmødet for handling om AI"
                        }
                    },
                    "tag": "Konstruktion"
                },
                "i18n": {
                    "items": {
                        "1": {
                            "date": "Sommer 2025",
                            "desc": "Partnerskab med Danmark, Sverige og Litauen for at åbne tjenesten på deres sprog.",
                            "title": "Udvidelse til europæisk skala"
                        },
                        "2": {
                            "date": "September 2025",
                            "desc": "Skabelse af dette nye workshopformat åbent for alle, til at opdage de generative AI'ers bagsiden og reflektere over deres miljømæssige påvirkning, og offentliggørelse af faciliteringsguiden tilknyttet udvidelsen \"AI-dueller\".",
                            "title": "Lancering af <a {linkProps}>\"AI-duellerne\"</a>"
                        },
                        "3": {
                            "desc": "Bygget i partnerskab med Pôle d'Expertise de la Régulation Numérique (<a {linkProps}>PEReN</a>), er compar:IA-rangeringen baseret på alle stemmer og reaktioner indsamlet siden tjenestens offentlige lancering i oktober 2024.",
                            "title": "Offentliggørelse af <a {linkProps}>compar:IA-rangeringen</a>"
                        },
                        "4": {
                            "desc": "Danmark slutter sig til eventyret ved at tilbyde sammenligningsplatformen på sit nationale sprog med et dedikeret domænenavn.",
                            "title": "Åbning af AI-Arenaen <a {linkProps}>på dansk</a>"
                        },
                        "5": {
                            "desc": "Tjenesten er anerkendt som et digitalt fællesgode af Digital Public Goods Alliance.",
                            "title": "AI-arenaen anerkendt som <a {linkProps}>digitalt fællesgode</a>"
                        },
                        "6": {
                            "date": "November 2025",
                            "desc": "Med mere end 500.000 unikke samtaler siden lanceringen af arenaen.",
                            "title": "Målet på 200.000 stemmer nået!"
                        }
                    },
                    "tag": "Internationalisering"
                },
                "investigation": {
                    "items": {
                        "1": {
                            "date": "Jan-marts 2024",
                            "desc": "Samtaler med aktører i økosystemet og første løsningshypoteser om problemstillingen: \"Hvordan lettes adgangen til data på fransk til træning af sprogmodeller?\".",
                            "title": "Undersøgelsesfase"
                        }
                    },
                    "tag": "Undersøgelse"
                }
            },
            "tabLabel": "Nøgledatoer",
            "title": "AI-Arenaen i datoer"
        },
        "partners": {
            "academy": {
                "catch": "Arbejder du med et forskningsprojekt? Har du forslag eller brug for afklaring om vores metodik eller datasæt?",
                "desc": "Vi er fast besluttede på at sikre, at de datasæt, vi genererer, fremmer tværfaglig forskning og bygger bro mellem humaniora, samfundsvidenskab og data science.",
                "title": "Akademiske partnere"
            },
            "diffusion": {
                "catch": "Vil du gerne bruge AI Arenaen i en professionel sammenhæng?",
                "cta": "Giv os besked",
                "desc": "Vi opbygger et netværk af partnere, der integrerer chatbot-området i deres tjenester og uddannelsestilbud.",
                "title": "Kommunikationspartnere"
            },
            "institution": {
                "title": "Institutionelle partnere"
            },
            "services": {
                "desc": "Beregninger af miljøpåvirkningen er baseret på ovenstående værktøjer.",
                "title": "Anvendte tjenester"
            },
            "tabLabel": "Partnere"
        },
        "problem": {
            "alignment": {
                "alignment": {
                    "a": "Justering kommer efter en sprogmodels prætræningsfase og fungerer som det sidste trin til \"forfining\" eller \"polering\". Under prætræningen lærer modellen at forudsige det næste ord og får dermed evnen til at generere sammenhængende tekst – men justeringen er det, der tilpasser den til menneskelige præferencer.",
                    "b": "Justeringsfasen træner modellen til bedre at imødekomme menneskelige behov ved at gøre den <strong>mere relevant</strong> (besvare spørgsmål mere præcist), <strong>mere ærlig</strong> (indrømme, når der mangler tilstrækkelige data), og <strong>mere sikker</strong> (undgå skadeligt eller upassende indhold).",
                    "c": "<strong>Uden justering kan en LLM være teknisk kapabel, men upraktisk at bruge, da den ikke forstår, hvad brugerne virkelig forventer i en samtale.</strong>",
                    "title": "Justering: en kritisk fase efter træningen"
                },
                "datasets": {
                    "a": "Justeringen er afhængig af højt specialiserede datasæt, der er omhyggeligt designet til at lære modellen \"korrekt\" adfærd.",
                    "b": "<strong>Præferencedata</strong> er en vigtig komponent i justeringen og fungerer sammen med <strong>ekspertdata</strong> (ekspertudarbejdede samtaler mellem mennesker og AI med præcise retningslinjer for tone og stil), <strong>sikkerhedsdata</strong> (udvalgte eksempler, der lærer modeller at afvise skadelige anmodninger), og <strong>domænespecifikke datasæt</strong> (tilpasset til områder som medicin, jura eller uddannelse).",
                    "c": "Præferencedata præsenterer flere mulige svar på det samme spørgsmål, rangordnet af menneskelige evaluatorer på baggrund af kriterier som relevans, nytteværdi eller skadepotentiale. Brugerne angiver, hvilket svar der fungerer bedst, og disse kuraterede datasæt bruges derefter til at finjustere modellerne, så de stemmer overens med de udtrykte menneskelige præferencer.",
                    "title": "Specifikke datasæt"
                },
                "desc": "Justering: En teknik til reduktion af bias baseret på crowdsourcing af brugerpræferencer for at forbedre modeladfærd",
                "diversity": {
                    "a": "For at afspejle mangfoldigheden af kulturer og sprog i modelresultaterne skal <strong>justeringsdatasæt indeholde en bred vifte af sprog</strong>, kontekster og virkelige brugeropgaver. Diversificering af justeringsdata forbedrer i sidste ende en models ydeevne på to vigtige måder:",
                    "b": "For det første <strong>reducerer den kulturel bias</strong> ved at forhindre, at et enkelt – ofte engelsksproget – perspektiv dominerer AI'ens svar. Modellen lærer, at gyldige svar varierer alt efter den kulturelle kontekst, og anerkender flere legitime måder at besvare det samme spørgsmål på.",
                    "c": "For det andet muliggør eksponering for sproglig og kulturel mangfoldighed kontekstbevidste svar: en fransk bruger får rådgivning, der er tilpasset de franske systemer, mens en dansk bruger modtager information, der er tilpasset den nationale kontekst.",
                    "d": "Resultatet? En mere inkluderende samtalebaseret AI – en AI, der anerkender og tilpasser sig forskellige kulturelle perspektiver.",
                    "title": "Diversificer datakilder for at reducere bias"
                },
                "english": {
                    "a": "Præferencedata er dyre at producere, fordi <strong>hvert eksempel kræver en dygtig menneskelig evaluering</strong>. Platforme som chat.lmsys.org hjælper med at crowdsource disse datasæt, men få brugere bidrager på deres modersmål, hvilket betyder, at sprog med få ressourcer er underrepræsenterede.",
                    "b": "Der findes kun få eller ingen præferencedatasæt for europæiske sprog. I LMSYS' datasæt udgør franske søgninger for eksempel mindre end 1% af det samlede antal.",
                    "c": "compar:IA er en chatbot-arena, der er designet til at samle multilingvale samtaler, som indfanger regionsspecifikke kulturelle referencer som daglige gøremål, lokale kulinariske traditioner, uddannelsessystemer eller historiske og litterære milepæle.",
                    "title": "De europæiske sprog lider under en mangel på præferencedata"
                },
                "title": "Hvordan kan vi mindske kulturel og sproglig bias i disse modeller?"
            },
            "diversity": {
                "diversity": {
                    "desc": "Disse bias kan føre til ufuldstændige eller direkte forkerte svar, der negligerer mangfoldigheden i de europæiske sprog og kulturer.",
                    "title": "Overset kulturel og sproglig mangfoldighed"
                },
                "english": {
                    "desc": "Samtalebaseret AI er afhængig af store sprogmodeller (LLM'er), der primært er trænet på engelske data, hvilket skaber sproglige og kulturelle skævheder i deres output.",
                    "title": "Træningsdata overvejende på engelsk"
                },
                "stereotypes": {
                    "desc": "Samtalebaserede AI-systemer ser ud til at være flydende i alle sprog – men deres output kan stadig være stereotypisk eller diskriminerende.",
                    "title": "Bias-forstærkende svar"
                }
            },
            "tabLabel": "Det oprindelige problem",
            "title": "Respekterer samtalebaserede AI-modeller <span {props}>mangfoldigheden</span> i de europæiske sprog?"
        },
        "title": "Alt, hvad du behøver at vide om AI Arenaen"
    },
    "ranking": {
        "energy": {
            "desc": "Denne graf viser for hver model sammenhængen mellem tilfredshedsscore (Bradley Terry-score) og det estimerede gennemsnitlige energiforbrug pr. 1000 tokens. Energiforbruget estimeres ved hjælp af <a {linkProps}>Ecologits</a>-metodologien, som tager højde for to faktorer: modellernes <strong>størrelse</strong> (antal parametre) og deres <strong>arkitektur</strong>. Da <strong>proprietære modeller</strong> ikke offentliggør disse oplysninger - eller kun gør det delvist - er de ikke medtaget i grafen nedenfor.",
            "tabLabel": "Energi fokus",
            "title": "Er de mest populære modeller energieffektive?",
            "views": {
                "graph": {
                    "desc": "Vælg en model for at kende dens Bradley-Terry (BT) score og energiforbrug",
                    "legends": {
                        "arch": "Model arkitektur",
                        "size": "filtrer på størrelse",
                        "sizeSub": "(mia. parametre)"
                    },
                    "tabLabel": "Grafisk visning",
                    "title": "Bradley-Terry (BT) tilfredshedsscore VS Gennemsnitsforbrug for 1000 tokens",
                    "tooltip": {
                        "active_params": "Aktive parametre (mia.)",
                        "arch": "Arkitektur",
                        "consumption_wh": "Gennemsnitsforbrug (Wh)",
                        "elo": "BT score",
                        "params": "Parametre (mia.)"
                    },
                    "xLabel": "Forbrug pr. 1000 tokens (Wh)",
                    "yLabel": "Bradley-Terry Score (BT)"
                },
                "methodo": {
                    "1": {
                        "list": {
                            "1": "<strong>Jo højere oppe i grafen en model er placeret</strong>, desto højere er dens Bradley-Terry-tilfredshedsscore. <strong>Jo længere til venstre i grafen en model er placeret</strong>, desto mindre energi forbruger den i forhold til de andre modeller.",
                            "2": "<strong>Øverst til venstre</strong> finder man de modeller, som er populære og forbruger relativt lidt energi sammenlignet med de andre modeller.",
                            "3": "Ud over størrelsen har <strong>arkitekturen</strong> også indflydelse på modellernes gennemsnitlige energiforbrug: for eksempel forbruger Llama 3 405B-modellen (tæt arkitektur, 405 milliarder parametre) i gennemsnit 10 gange mere energi end GLM 4.5-modellen (MOE-arkitektur, 355 milliarder parametre og 32 milliarder aktive parametre), selv om de har en lignende størrelse."
                        },
                        "subTitle": "Eksempler på læsning af grafen",
                        "title": "Hvordan finder man den rette balance mellem opfattet performance og energieffektivitet?"
                    },
                    "2": {
                        "descs": {
                            "1": "Estimeringen af energiforbruget ved inferens af modellerne bygger på Ecologits metode, som tager højde for modellernes størrelse og arkitektur. Disse oplysninger offentliggøres imidlertid ikke af modeludviklerne for såkaldte \"proprietære\" modeller.",
                            "2": "Vi har derfor valgt ikke at medtage de proprietære modeller i grafen, så længe oplysningerne om energiforbrugsberegningen ikke er gennemsigtige."
                        },
                        "title": "Hvorfor vises de proprietære modeller ikke i grafen?"
                    },
                    "3": {
                        "descs": {
                            "1": "compar:IA bruger metodologien udviklet af <a {ecoLinkProps}><strong>Ecologits</strong></a> (<a {genaiLinkProps}><strong>GenAI Impact</strong></a>) til at give et estimat af energiforbruget forbundet med inferens af konversationelle generative AI-modeller. Dette estimat gør det muligt for brugerne at sammenligne forskellige AI-modellers miljøpåvirkning for den samme forespørgsel. Denne gennemsigtighed er afgørende for at fremme udviklingen og anvendelsen af mere miljøansvarlige AI-modeller.",
                            "2": "Ecologits anvender principperne for livscyklusanalyse (LCA) i overensstemmelse med ISO 14044-standarden ved i første omgang at fokusere på påvirkningen fra inferens (dvs. brugen af modeller til at besvare forespørgsler) og <strong>fremstillingen af grafikkort</strong> (udvinding af ressourcer, produktion og transport).",
                            "3": "Modellens elektricitetsforbrug estimeres under hensyntagen til forskellige parametre såsom størrelsen og arkitekturen af den anvendte AI-model, placeringen af de servere, hvor modellerne er implementeret, samt antallet af output-tokens. Beregningen af indikatoren for global opvarmningspotentiale udtrykt i CO2-ækvivalenter udledes af målingen af modellens elektricitetsforbrug.",
                            "4": "Det er vigtigt at bemærke, at metoderne til vurdering af AI's miljøpåvirkning stadig er under udvikling."
                        },
                        "title": "Hvordan beregnes modellernes energipåvirkning?"
                    }
                },
                "table": {
                    "title": "Grafdata i tabelform"
                }
            }
        },
        "methodo": {
            "desc": {
                "1": "Siden 2024 har tusindvis af brugere brugt compar:IA til at sammenligne forskellige modellers svar, hvilket har genereret hundredtusindvis af stemmer. At tælle antallet af sejre er ikke nok til at etablere en rangordning. Et retfærdigt system skal være statistisk robust, justere sig efter hver sammenligning og reelt afspejle værdien af de opnåede præstationer.",
                "2": "Det er i dette perspektiv, at der er blevet etableret en <strong>rangordning baseret på Bradley-Terry-modellen</strong>, udarbejdet i samarbejde med <strong>teamet fra Pôle d'Expertise de la Régulation numérique (<a {perenLinkProps}>PEReN</a>)</strong>, ud fra alle de stemmer og reaktioner, der er indsamlet på platformen. For at gå videre, se vores <a {notebookLinkProps}>metodologiske notesbog</a>."
            },
            "impacts": {
                "elo": {
                    "desc": {
                        "1": "<strong>Bradley-Terry</strong>-modellen omdanner et sæt af lokale og potentielt ufuldstændige sammenligninger til et sammenhængende og statistisk robust globalt rangordningssystem, der hvor den empiriske sejrsrate forbliver begrænset til direkte observationer."
                    },
                    "title": "10 første modeller i rangordningen ifølge den estimerede sejrsrate med Bradley-Terry-modellen"
                },
                "title": "Indvirkningen af valget af metode på rangordningen af modeller",
                "winrate": {
                    "desc": {
                        "1": "Ved kun at basere sig på den <strong>gennemsnitlige sejrsrate</strong> kan man opnå en global rangordning, men denne beregning forudsætter, at hver model har spillet mod alle andre.",
                        "2": "Denne metode er ikke ideel, da den kræver data fra alle kombinationer af modeller, og så snart man øger antallet af modeller, bliver det hurtigt dyrt og tungt at vedligeholde."
                    },
                    "title": "10 første modeller i rangordningen ifølge den \"empiriske\" sejrsrate"
                }
            },
            "methods": {
                "cons": "Hovedproblemer",
                "elo": {
                    "def": "<strong>Definition</strong>: Rangordningssystem, hvor gevinst eller tab af point afhænger af resultatet (sejr/nederlag/uafgjort) <strong>og</strong> det estimerede niveau af modstanderen: hvis en svagere model slår en stærkere model, er dens fremgang i rangordningen større.",
                    "list": {
                        "1": "<strong>Probabilistisk model</strong>: man kan estimere det sandsynlige resultat af enhver sammenligning, selv mellem modeller, der aldrig er blevet direkte sammenlignet.",
                        "2": "<strong>Hensyntagen til sværhedsgraden af kampene</strong>: de estimerede scores fra Bradley Terry-modellen tager højde for niveauet af de mødte modstandere, hvilket muliggør en retfærdig sammenligning mellem modeller.",
                        "3": "<strong>Bedre håndtering af usikkerhed</strong>: konfidensintervallet integrerer hele netværket af sammenligninger. Dette muliggør et mere præcist estimat af usikkerheden, især for modeller med få direkte sammenligninger, men mange fælles modstandere."
                    },
                    "title": "Bradley-Terry (BT) rangordning"
                },
                "pros": "Fordele",
                "title": "To måders at opdele modeller",
                "winrate": {
                    "def": "<strong>Definition</strong>: Empirisk rangordningssystem for modeller baseret på procentdelen af kampe vundet af en model mod alle andre modeller.",
                    "list": {
                        "1": "<strong>Bias fra antal kampe</strong>: en model, der har vundet tre sejre ud af tre \"kampe\", viser en sejrsrate på 100%, men denne score er lidt signifikant, da den er baseret på meget lidt data.",
                        "2": "<strong>Ingen hensyntagen til sværhedsgraden af kampene</strong>: at slå en \"begynder\"-model eller en \"ekspert\"-model tæller det samme. Sejrsraterne er ikke retfærdige, da de ikke tager højde for sværhedsgraden af kampene.",
                        "3": "<strong>Stagnation</strong>: på lang sigt ender mange gode modeller omkring 50% sejrsrate, fordi de møder modeller på deres niveau, hvilket gør rangordningen mindre diskriminerende."
                    },
                    "title": "Rangordning efter sejrsrate"
                }
            },
            "tabLabel": "Metodologi",
            "title": "Hvordan vælger man metoden til rangordning af modeller?"
        },
        "preferences": {
            "desc": "Når du stemmer, kan du kvalificere din præference efter forskellige positive og negative kategorier. Sammenlign deres fordeling fra en model til en anden.",
            "modal": {
                "cta": "Hvad er en præference?",
                "title": "Positive og negative præferencer"
            },
            "tabLabel": "Fokus præferencer",
            "table": {
                "cols": {
                    "clear_formatting": "Tydelig formatering<br>",
                    "complete": "Fuldendt",
                    "creative": "Kreativ",
                    "incorrect": "Forkert",
                    "instructions_not_followed": "Instruktioner ikke overholdt",
                    "n_match": "Samlet antal kampe",
                    "name": "Model",
                    "positive_prefs_ratio": "Fordeling af præferencer",
                    "superficial": "Overfladisk",
                    "total_negative_prefs": "Samlet antal negative præferencer",
                    "total_positive_prefs": "Samlet antal positive præferencer",
                    "useful": "Brugbar"
                },
                "percentLabel": "Præferencer i procent",
                "tooltips": {
                    "positive_prefs_ratio": "Ved afstemningen gjorde badges det muligt at præcisere begrundelserne for din præference. Denne kolonne viser den procentvise fordeling af disse badges (positive eller negative) for alle stemmer."
                }
            },
            "title": "Hvordan fordeler brugernes præferencer sig?"
        },
        "ranking": {
            "desc": "Tak for jeres bidrag!<br> Rangeringen <strong>AI-arenaen</strong> er baseret på alle stemmer og reaktioner fra den <strong>blinde sammenligning</strong> af modellerne og indsamlet siden tjenestens offentlige lancering i oktober 2024.<br> Bygget i partnerskab med Pôle d'Expertise de la Régulation Numérique (<a {linkProps}>PEReN</a>), er modelrangeringen etableret baseret på <strong>tilfredshedsscore</strong> beregnet ud fra den statistiske model <strong>Bradley Terry</strong>, en udbredt metode til at konvertere binære stemmer til probabilistisk rangering.<br> AI-arenaens-rangeringen har ikke til formål at udgøre en officiel anbefaling eller evaluere modellernes tekniske ydeevne. Den afspejler platformens brugeres <strong>subjektive præferencer</strong> og ikke svarenes faktualitet eller sandfærdighed.",
            "tabLabel": "Rangliste"
        },
        "table": {
            "data": {
                "billions": "{count} mia.",
                "cols": {
                    "arch": "Arkitektur",
                    "consumption_wh": "Gns. forbrug<br>(1.000 tokens)",
                    "elo": "BT<br>tilfredshedsscore",
                    "license": "Licens",
                    "n_match": "Samlet antal stemmer",
                    "name": "Model",
                    "organisation": "Organisation",
                    "rank": "Rang",
                    "release": "Udgivelsesdato",
                    "size": "Størrelse<br>(parametre)",
                    "trust_range": "Konfidens (±)"
                },
                "estimation": "(estimering)",
                "tooltips": {
                    "arch": "Et LLM-models arkitektur refererer til designprincipperne, der definerer, hvordan komponenterne i et neuralt netværk er arrangeret og interagerer for at transformere inputdata til prædiktive outputs, herunder måden hvorpå parametre aktiveres (dense vs. sparse), komponentspecialisering og informationsbehandlingsmekanismer (transformers, konvolutionsnetværk, hybride arkitekturer).",
                    "consumption_wh": "Målt i watttimer repræsenterer det forbrugte energi den elektricitet, som modellen bruger til at behandle en forespørgsel og generere det tilsvarende svar. Modellernes energiforbrug afhænger af deres størrelse og arkitektur. Vi vælger at vise de proprietære modeller, som vi ikke har gennemsigtig information om størrelse og arkitektur for, som gråtonede ikke-analyserede (N/A).",
                    "elo": "Estimeret statistisk score i henhold til Bradley-Terry-modellen, som afspejler sandsynligheden for, at en model foretrækkes frem for en anden. Denne score beregnes ud fra alle brugernes stemmer og reaktioner. For at lære mere, gå til metodefanen.",
                    "rank": "Rangordningsplacering tildelt ifølge Bradley-Terry-tilfredshedsscoren",
                    "size": "Modellens størrelse i milliarder af parametre, kategoriseret i fem klasser. For proprietære modeller oplyses denne størrelse ikke.",
                    "trust_range": "Interval, der angiver pålideligheden af rangordningen: jo smallere intervallet er, jo mere pålidelig er rangestimat. Der er 95% sandsynlighed for, at modellens sande rang ligger i dette spænd."
                }
            },
            "lastUpdate": "Updateret {date}",
            "totalModels": "Total antal modeller:",
            "totalVotes": "Total antal stemmer:"
        },
        "title": "Fra stemmer til en rangering af modeller"
    },
    "reveal": {
        "equivalent": {
            "co2": {
                "label": "CO <sub> 2 </sub> udledt",
                "tooltip": "Den udledte CO <sub> 2 </sub> svarer til den mængde kuldioxid, der udledes ved den energi, der bruges til at drive modellen. Den afspejler den miljømæssige påvirkning, der er forbundet med energiforbruget. Beregningen af watt-time/CO <sub> 2 </sub>-ækvivalensen varierer afhængigt af hvert lands energimix. De servere, der anvendes til modelinferens, er dog ikke alle placeret i Europa. Ækvivalensberegningen er således baseret på den globale gennemsnitlige CO <sub> 2 </sub>-emissionsrate pr. forbrugt energi."
            },
            "lightbulb": {
                "label": "LED-pære",
                "tooltip": "Data beregnet på baggrund af forbruget af en standard 5W LED-pære (E14)"
            },
            "streaming": {
                "label": "online videoer",
                "tooltip": "Data beregnet på baggrund af CO2-aftrykket fra en times streaming af video i høj opløsning på et fjernsyn med Wi-Fi-forbindelse (kilde <a {linkProps}>ADEME</a>)"
            },
            "title": "Hvilket svarer til:"
        },
        "feedback": {
            "description": "Del AI Arena'en med andre ved at dele de AI-modeller, du har interageret med! Kun navnene og energipåvirkningen af diskussionen vil være synlige via dette link, uden adgang til beskederne i samtalen.",
            "example": "Eksempel på delte resultater",
            "moreOnVotes": "Lær mere om stemmer",
            "shareResult": "Del dit resultat"
        },
        "impacts": {
            "energy": {
                "label": "energiforbrug",
                "tooltip": "Målt i watt-timer repræsenterer energiforbruget den elektricitet, som modellen bruger til at behandle en forespørgsel og generere det tilsvarende svar. Generelt gælder det, at jo større en model er (i milliarder af parametre), jo mere energi kræves der for at producere et token."
            },
            "size": {
                "count": "milliarder parametre.",
                "estimated": "(ca.)",
                "label": "modelstørrelse",
                "quantized": "(kvantiseret)"
            },
            "title": "Chatens estimerede energiforbrug",
            "tokens": {
                "label": "tekststørrelse",
                "tokens": "tokens",
                "tooltip": "AI analyserer og genererer sætninger ud fra ord eller dele af ord på cirka fire bogstaver; denne tekstdel kaldes et token. Jo længere en tekst er, jo større er antallet af tokens."
            },
            "tooltip": "Estimering baseret på modellens størrelse, dens arkitektur og længden af det genererede svar."
        },
        "thanks": {
            "cta": "Gå til rangeringen",
            "desc": "Oplev hvordan din præference placerer sig i forhold til den generelle rangering af modeller i AI-arenaen.",
            "graphAlt": "Oversigt over grafen der repræsenterer modellernes score og elektricitetsforbrug",
            "rankingAlt": "Oversigt over rangeringen i tabelform",
            "title": "Tak for dit bidrag!"
        }
    },
    "seo": {
        "desc": "compar:IA er et værktøj, der gør det muligt at sammenligne forskellige samtale-AI-modeller blindt, øge bevidstheden om problemerne omkring generativ AI (pluralitet, bias, miljøpåvirkning) og hjælpe med at opbygge datasæt for sprogpræferencer for sprog med færre ressourcer.",
        "title": "compar:IA, arenaen for AI-chatbots",
        "titles": {
            "accessibilite": "Erklæring om tilgængelighed",
            "arene": "Chat",
            "community": "Fællesskab",
            "comparator": "Arenaen",
            "datasets": "Datasæt",
            "donnees-personnelles": "Fortrolighedspolitik",
            "duel": "AI Duel Workshop",
            "faq": "Ofte stillede spørgsmål",
            "history": "Nøgledatoer",
            "home": "Hjem",
            "mentions-legales": "Om os",
            "modalites": "Brugsbetingelser",
            "modeles": "Modelliste",
            "news": "Nyheder og ressourcer",
            "partners": "Partnere",
            "problem": "Den indledende udfordring",
            "product": "Produkt og partnere",
            "ranking": "Rangliste",
            "rgesn": "Erklæring om bæredygtigt design",
            "share": "Mine resultater"
        }
    },
    "vote": {
        "bothEqual": "Begge er lige gode",
        "choices": {
            "altText": "{choice} for modellen {model}",
            "negative": {
                "incorrect": "Forkert",
                "instructions_not_followed": "Instruktionerne er ikke blevet fulgt",
                "question": "Hvorfor kunne du ikke lide svaret?",
                "superficial": "Overfladisk"
            },
            "other": "Andet…",
            "positive": {
                "clear_formatting": "Klar formattering",
                "complete": "Fuldstændigt",
                "creative": "Kreativt",
                "question": "Hvad kunne du lide ved svaret?",
                "useful": "Brugbart"
            }
        },
        "comment": {
            "add": "Tilføj kommentarer",
            "placeholder": "Du kan tilføje detaljer om model {model}s svar"
        },
        "dislike": {
            "label": "Jeg kan ikke lide",
            "selectedLabel": "Jeg kan ikke lide (udvalgt)"
        },
        "introA": "Før vi finder ud af modellernes identitet, har vi brug for din stemme.",
        "introB": "Det giver os mulighed for at forbedre compar:IA-datasættene, hvis formål er at forfine fremtidige AI-modeller på sprog med færre ressourcer",
        "like": {
            "label": "Jeg kan godt lide",
            "selectedLabel": "Jeg kan godt lide (udvalgte)"
        },
        "qualify": {
            "addDetails": "Tilføj detaljer",
            "placeholder": "Svarene fra {model} modellen er...",
            "question": "Hvordan vil du beskrive dens svar?"
        },
        "title": "Hvilken AI-model foretrækker du?",
        "yours": "Din stemme"
    },
    "welcome": {
        "errors": "AI kan begå fejl: vi opfordrer dig til at kontrollere de leverede oplysninger",
        "go": "Så er vi klar",
        "privacy": "Del ikke personlige oplysninger såsom dit navn, efternavn eller adresse.",
        "title": "Velkommen til AI Arenaen!",
        "tos": {
            "desc": "Samtalerne og de præferencer, du udtrykker på AI-arenaen, bruges anonymt til at udgøre datasæt, der er repræsentative for europæiske sprog og anvendelser for at reducere kulturelle bias og tilbyde fremtidige AI-modeller, der er mere inkluderende.",
            "moreInfos": "Lær mere om projektet"
        },
        "use": "Brug ikke arenaen til ulovlige eller skadelige formål."
    },
    "words": {
        "NA": "N/A",
        "activated": "Aktiveret",
        "archived": "Arkiveret",
        "back": "Tilbage",
        "close": "Luk",
        "deactivated": "Deaktiveret",
        "loading": "Indlæser",
        "new": "Ny",
        "random": "Tilfældig",
        "regenerate": "Regenerer",
        "reset": "Nulstil",
        "restart": "Start forfra",
        "retry": "Start forfra",
        "search": "Søg",
        "send": "Send",
        "tooltip": "Tip",
        "validate": "Validér"
    }
}
