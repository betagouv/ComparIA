{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "contact": "Contactez-nous",
        "contactUs": "Nous contacter",
        "copyLink": {
            "do": "Copier le lien",
            "done": "Lien copié dans le presse-papiers"
        },
        "copyMessage": {
            "do": "Copier le message",
            "done": "Message copié"
        },
        "home": "Page d'accueil",
        "returnHome": "Revenir à l'accueil",
        "seeMore": "Voir plus",
        "selectLanguage": "Sélectionner une langue",
        "vote": "Donner mon avis"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 modèles",
            "help": "Si vous n’en choisissez qu’un, le second sera sélectionné de manière aléatoire",
            "question": "Quels modèles voulez-vous comparer ?"
        },
        "modelSelection": "Sélection des modèles",
        "prompt": {
            "label": "Écrivez votre premier message",
            "placeholder": "Écrivez votre premier message ici"
        },
        "selectModels": {
            "help": "Sélectionnez le mode de comparaison qui vous convient",
            "question": "Quels modèles voulez-vous comparer ?"
        },
        "suggestions": {
            "choices": {
                "administrative": {
                    "iconAlt": "Administratif",
                    "title": "Rédiger un document administratif"
                },
                "coach": {
                    "iconAlt": "Conseils",
                    "title": "Obtenir des conseils sur l’alimentation et le sport"
                },
                "explanations": {
                    "iconAlt": "Explications",
                    "title": "Expliquer simplement un concept"
                },
                "iasummit": {
                    "iconAlt": "Sommet pour l'action sur l'IA",
                    "title": "Prompts issus de la consultation citoyenne sur l’IA ",
                    "tooltip": "Ces questions sont issues de la consultation citoyenne sur l’IA qui a lieu du 16/09/2024 au 08/11/2024. Elle visait à associer largement les citoyens et la société civile au Sommet international pour l’action sur l’IA, en collectant leurs idées pour faire de l’intelligence artificielle une opportunité pour toutes et tous, mais aussi de nous prémunir ensemble contre tout usage inapproprié ou abusif de ces technologies."
                },
                "ideas": {
                    "iconAlt": "Idées",
                    "title": "Générer de nouvelles idées"
                },
                "languages": {
                    "iconAlt": "Traduction",
                    "title": "M’exprimer dans une autre langue"
                },
                "recipes": {
                    "iconAlt": "Recettes",
                    "title": "Découvrir une nouvelle recette de cuisine"
                },
                "recommendations": {
                    "iconAlt": "Recommandations",
                    "title": "Proposer des idées de films, livres, musiques"
                },
                "stories": {
                    "iconAlt": "Histoires",
                    "title": "Raconter une histoire"
                }
            },
            "generateAnother": "Générer un autre message",
            "title": "Suggestions de prompts"
        },
        "title": "Comment puis-je vous aider aujourd'hui ?"
    },
    "chatbot": {
        "continuePrompt": "Continuer à discuter avec les modèles",
        "conversation": "Conversation",
        "errors": {
            "other": {
                "message": "Une erreur temporaire est survenue.",
                "retry": "Vous pouvez tenter de réessayer de solliciter les modèles.",
                "title": "Oups, erreur temporaire",
                "vote": "Ou bien conclure votre expérience en donnant votre avis sur les modèles."
            },
            "tooLong": {
                "message": "Chaque modèle est limité dans la taille des conversations qu'il est capable de traiter.",
                "retry": "Vous pouvez recommencer une conversation avec deux nouveaux modèles.",
                "title": "Oups, la conversation est trop longue pour un des modèles.",
                "vote": "Vous pouvez tout de même donner votre avis sur ces modèles ou recommencer une conversation avec deux nouveaux."
            }
        },
        "loading": "Chargement des réponses",
        "reasoning": {
            "finished": "Raisonnement terminé",
            "inProgress": "Raisonnement en cours…"
        },
        "revealButton": "Passer à la révélation des modèles"
    },
    "closeModal": "Fermer la fenêtre modale",
    "components": {
        "pagination": {
            "first": "Première page",
            "label": "Pagination",
            "last": "Dernière page",
            "next": "Page suivante",
            "nth": "Page {count}",
            "previous": "Page précédente"
        },
        "table": {
            "linePerPage": "Nombre de lignes par page",
            "pageCount": "{count} lignes par page",
            "triage": "Trier"
        },
        "theme": {
            "legend": "Choisissez un thème pour personnaliser l’apparence du site.",
            "options": {
                "dark": "Thème sombre",
                "light": "Thème clair",
                "system": "Système",
                "systemSub": "Utilise les paramètres système"
            },
            "title": "Paramètres d'affichage"
        }
    },
    "datasets": {
        "access": {
            "catch": "Editeurs de modèles, chercheurs, chercheuses, entreprises, à vous de jouer !",
            "desc": "Les questions et préférences posées sur la plateforme sont majoritairement en français et reflètent des usages réels et non contraints. Ces jeux de données sont accessibles sur <a {linkProps}>data.gouv</a> et Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "Ensemble des réponses et des questions posées",
                    "title": "/conversations"
                },
                "reactions": {
                    "desc": "Ensemble des réactions exprimées",
                    "title": "/réactions"
                },
                "votes": {
                    "desc": "Ensemble des préférences exprimées",
                    "title": "/votes"
                }
            },
            "share": "Partagez-nous vos réutilisations",
            "title": "Accédez aux jeux de données compar:IA"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "desc": "Analyse des conversations des utilisateurs avec détection des tâches (création, recherche d'informations...), des sujets (arts et culture, éducation...), des émotions complexes (curiosité, enthousiasme...), des types de langage (formel, professionnel...)",
                    "title": "Accéder à l’analyse par indicateur"
                },
                "conversations": {
                    "desc": "Visualisation interactive des conversations où chaque point représente un cluster de discussions évoqué par les utilisateurs (comme l’éducation, la santé, l’environnement, ou encore la philosophie).",
                    "title": "Explorer la visualisation de données"
                },
                "desc": "L'équipe Bunka.ai a mené une étude approfondie sur les interactions entre les utilisateurs de la plateforme Compar:IA et les modèles d'IA, examinant les thématiques privilégiées, les tâches principales et déterminant si ces modèles fonctionnent avant tout comme des outils d'automatisation ou d'augmentation des capacités humaines. Cette analyse repose sur un large échantillon de 25 000 conversations.",
                "method": "En savoir plus sur la méthodologie"
            },
            "desc": "Exemples de réutilisation des jeux de données compar:IA",
            "title": "Comment ces données sont-elles utilisées ?"
        }
    },
    "duel": {
        "1": {
            "cta": "Recevoir le kit facilitateur",
            "desc": "Organisez un <strong>Duel de l’IA</strong> dans votre établissement, association ou entreprise en devenant facilitateur",
            "title": "Vous souhaitez <span {props}>sensibiliser votre public</span> à l’impact environnemental de l’IA générative ?"
        },
        "2": {
            "list": {
                "1": "Durée : 45min à 1h30",
                "2": "Format présentiel ou en ligne",
                "3": "Accès à compar:IA et des supports fournis",
                "4": "Atelier clé en main, accessible et interactif"
            },
            "title": "Les <span {props}>Duels de l’IA</span> : un format d’<span {props}>atelier</span> ouvert à tout public, pour découvrir les coulisses des IA génératives et réfléchir à leur <span {props}>impact environnemental</span>."
        },
        "3": {
            "cards": {
                "1": {
                    "desc": "Pour découvrir concrètement les coulisses de l’IA.",
                    "title": "Collèges et Lycées"
                },
                "2": {
                    "desc": "Pour enrichir les discussions sur l’innovation et le développement durable.",
                    "title": "Universités"
                },
                "3": {
                    "desc": "Pour débattre d’un sujet de société en s’appuyant sur un exemple vécu.",
                    "title": "Associations, Médiation numérique"
                },
                "4": {
                    "desc": "Pour réfléchir à un usage plus responsable des outils numériques.",
                    "title": "Entreprises"
                }
            },
            "desc": "Cet atelier est ouvert à tous, quel que soit le niveau de connaissance initial.",
            "title": "Pour quels publics ?"
        },
        "4": {
            "desc": "Durée approximative : 60 minutes",
            "list": {
                "1": {
                    "desc": "Présentation rapide de l’IA générative et de la plateforme compar:IA pour que tout le monde parte du même point, même sans connaissance préalable.",
                    "duration": "(10min)",
                    "title": "Introduction"
                },
                "2": {
                    "desc": "Les participants posent la même question à deux modèles d’IA anonymes : un « grand » et un « petit » modèle.",
                    "duration": "(10-15min)",
                    "title": "Le duel"
                },
                "3": {
                    "desc": "Moment d’échange pour réfléchir à la pertinence des réponses au regard des besoins des utilisateurs.",
                    "duration": "(10min)",
                    "title": "Comparaison et vote"
                },
                "4": {
                    "desc": "Choc pédagogique et découverte des caractéristiques techniques des modèles : « meilleure réponse » ne rime pas toujours avec « sobriété environnementale » !",
                    "duration": "(10min)",
                    "title": "Révélation des modèles"
                },
                "5": {
                    "desc": "Temps collectif pour réfléchir aux arbitrages entre usage et impact environnemental.",
                    "duration": "(10-15min)",
                    "title": "Débat final"
                },
                "6": {
                    "desc": "Partage de quelques gestes simples pour réduire l’impact lié à l’usage de l’IA.",
                    "duration": "(5min)",
                    "title": "Conclusion et ouverture"
                }
            },
            "title": "Déroulé type d’un atelier"
        },
        "5": {
            "cta": "Recevoir le kit facilitateur",
            "list": {
                "1": "Guide détaillé avec tout le déroulé expliqué",
                "2": "Diapositives prêtes à projeter",
                "3": "FAQ et glossaire pour répondre facilement aux questions",
                "4": "Exension Chrome pour avoir toutes ces informations à portée de main"
            },
            "title": "Le guide facilitateur"
        },
        "6": {
            "cta": "Nous contacter pour en savoir plus",
            "title": "Foire aux questions",
            "questions": {
                "1": {
                    "desc": "Nous avons déjà animé des sessions avec seulement 5 participants, mais aussi devant 300 personnes. Bien sûr, plus le groupe est grand, moins chacun a l’occasion de s’exprimer individuellement. Cela dit, l’atelier reste très flexible : tout dépend de la capacité du facilitateur à gérer le public. Avec un petit groupe, l’expérience est plus personnalisée ; avec un grand nombre de participants, l’énergie et la dynamique sont particulièrement intéressantes.",
                    "title": "Combien de personnes peuvent participer à une session des Duels de l'IA ?"
                },
                "2": {
                    "desc": "Pas du tout ! Le guide facilitateur est conçu pour permettre à toute personne intéressée d’animer un duel. Il fournit les éléments clés pour comprendre le sujet, les enjeux et les outils nécessaires au bon déroulement de l’exercice. Bien sûr, avoir déjà expérimenté un peu avec des modèles de langage peut aider, mais ce n’est en aucun cas indispensable.",
                    "title": "Faut-il être expert du sujet pour faciliter un Duel de l'IA ?"
                },
                "3": {
                    "desc": "Les sessions les plus courtes que nous avons menées ont duré 20 à 30 minutes, mais cela limite un peu l’interactivité. Idéalement, un bon duel prend environ 45 minutes. Et si l’on dispose d’1h30, c’est encore mieux : cela permet d’enchaîner plusieurs duels et d’entrer plus en profondeur dans les discussions.",
                    "title": "Combien de temps dure une session des Duels ?"
                }
            }
        }
    },
    "errors": {
        "404": {
            "desc": "Si vous avez tapé l'adresse web dans le navigateur, vérifiez qu'elle est correcte. La page n’est peut-être plus disponible.<br />Dans ce cas, pour continuer votre visite vous pouvez consulter notre page d’accueil.<br />Sinon contactez-nous pour que l’on puisse vous rediriger vers la bonne information.",
            "error": "Erreur 404",
            "sorry": "La page que vous cherchez est introuvable. Excusez-nous pour la gène occasionnée.",
            "title": "Page non trouvée"
        },
        "unexpected": {
            "desc": "Essayez de rafraîchir la page ou bien ressayez plus tard.",
            "error": "Erreur {code}",
            "sorry": "Désolé, le service rencontre un problème, nous travaillons pour le résoudre le plus rapidement possible.",
            "title": "Erreur inattendue"
        },
        "unknown": "Une erreur est survenue"
    },
    "faq": {
        "datasets": {
            "title": "Jeu de données",
            "questions": {
                "1": {
                    "title": "Les données de préférence ont-elles un effet immédiat pour améliorer les modèles?",
                    "desc": "<p>Les données de préférence servent à améliorer les modèles lors d'entraînements futurs.</p><p>En comparant à l'aveugle les réponses de deux modèles, les utilisateurs de compar:IA expriment leurs préférences, indiquant ainsi quelles réponses sont les plus pertinentes. Ces données de préférence peuvent être utilisées pour affiner l'alignement des modèles, c'est-à-dire pour les entraîner à générer des réponses plus conformes aux attentes et aux préférences des utilisateurs.</p><p>Il s'agit d'un processus itératif, où le modèle apprend progressivement à générer de meilleures réponses en fonction des retours formulés par les humains sur la qualité des réponses. En étant exposés à des données de préférence, les modèles apprennent à les intégrer dans leur processus de génération de réponses.</p>"
                },
                "2": {
                    "title": "Pourquoi les données de préférence collectées sur compar:IA ont-elles de la valeur ?",
                    "desc": "<p>La spécificité des données collectées sur la plateforme compar:IA est qu’elles sont en français et qu’elles correspondent à des tâches réelles des utilisateurs. Ces données reflètent des préférences humaines dans un contexte linguistique et culturel précis. Elles permettent dans un second temps d'ajuster les modèles pour qu’ils soient plus pertinents, précis et adaptés aux usages des utilisateurs, tout en comblant les éventuels biais ou lacunes des modèles actuels.</p>"
                },
                "3": {
                    "title": "Quelle est la spécificité de compar:IA par rapport à d’autres initiatives similaires ?",
                    "desc": "<p>compar:IA se positionne comme un outil d'évaluation et d'alignement spécifique au français, axé sur la qualité des réponses et la collecte de données de préférence, se distinguant ainsi de l'approche de classement global de <a href='https://lmarena.ai/' target='_blank'>chatbot arena</a> développé par <a href='http://lmsys.org' target='_blank'>lmsys.org</a> et de l'alignement éthique des modèles d’IA de <a href='https://hannahkirk.github.io/prism-alignment/' target='_blank'>Prism Alignment Project</a>.</p>"
                }
            }
        },
        "ecology": {
            "title": "Indicateurs écologiques",
            "questions": {
                "1": {
                    "title": "Comment les indicateurs écologiques sont-ils calculés ?",
                    "desc": "<p>compar:IA utilise la méthodologie développée par <a target='_blank' href='https://ecologits.ai/latest/'><strong>Ecologits</strong> (GenAI Impact)</a> pour fournir un bilan énergétique qui permet aux utilisateurs de comparer l'impact environnemental de différents modèles d'IA pour une même requête. Cette transparence est essentielle pour encourager le développement et l'adoption de modèles d'IA plus éco-responsables.</p><p>Ecologits applique les principes de l'analyse du cycle de vie (ACV) conformément à la norme ISO 14044 en se concentrant pour le moment sur l'impact de <strong>l'inférence</strong> (c'est-à-dire l'utilisation des modèles pour répondre aux requêtes) et de la <strong>fabrication des cartes graphiques</strong> (extraction des ressources, fabrication et transport).</p><p>La consommation électrique du modèle est estimée en tenant compte de divers paramètres tels que la taille du modèle d'IA utilisé, la localisation des serveurs où sont déployés les modèles et le nombre de tokens de sortie. Le calcul de l’indicateur de potentiel de réchauffement climatique exprimé en équivalent CO2 est dérivé de la mesure de consommation électrique du modèle.</p><p>Il est important de noter que les méthodologies d'évaluation de l'impact environnemental de l'IA sont encore en développement.</p>"
                },
                "2": {
                    "title": "Les indicateurs écologiques tiennent-ils compte du mix énergétique des différents pays ?",
                    "desc": "<p>La localisation des centres de données joue un rôle dans l'empreinte carbone de l'IA. Si un modèle est entraîné ou utilisé dans un pays fortement dépendant des énergies fossiles, son impact environnemental sera plus important que s'il est hébergé dans un pays utilisant majoritairement des énergies renouvelables.</p><p>La méthode d'analyse de l'impact environnemental de l'IA développée par <a target='_blank' href='https://ecologits.ai/latest/'>Ecologits (de GenAI Impact)</a>, intègre des données sur le mix énergétique des différents pays où se situent les serveurs. Cela permet d'obtenir une estimation plus précise et nuancée de l'empreinte carbone réelle de l’inférence sur les différents modèles d’IA générative.</p>"
                },
                "3": {
                    "title": "Les indicateurs d’impact écologique tiennent-ils compte des ressources utilisées pour entraîner les modèles ?",
                    "desc": "<p>Les indicateurs d'impact écologique actuels se focalisent principalement sur l'impact de <strong>l'inférence</strong>, c'est-à-dire l'utilisation des modèles d'IA pour répondre aux requêtes. Cette approche peut donner l'illusion que l'inférence est moins énergivore que l'entraînement des modèles. Cependant, <strong>la réalité est plus complexe.</strong> Prenons l'analogie de la voiture :</p><ul><li>Construire une voiture (l'entraînement) est un processus ponctuel et gourmand en ressources.</li><li>Chaque trajet en voiture (l'inférence) consomme moins d'énergie, mais ces trajets sont répétés quotidiennement, et leur nombre est potentiellement immense.</li></ul><p>De la même manière, <strong>l'impact cumulé de l'inférence, à l'échelle de millions d'utilisateurs effectuant des requêtes quotidiennement, peut s'avérer supérieur à l'impact de l'entraînement initial.</strong> C'est pourquoi il est crucial que les outils d'évaluation de l'empreinte carbone de l'IA prennent en compte <strong>l'ensemble du cycle de vie</strong> des modèles, de l'entraînement à l'utilisation en production</p>"
                }
            }
        },
        "i18n": {
            "title": "Internationalisation",
            "questions": {
                "1": {
                    "title": "compar:IA s’est d’abord concentré sur le français : y a-t-il des plans pour d'autres langues européennes ?",
                    "desc": "<p>Oui, l'internationalisation de compar:IA est en cours. Nous commençons par un élargissement à trois pays pilotes : la Lituanie, la Suède et le Danemark. Cette première phase permet de tester l’approche et d'adapter l’interface à différents contextes linguistiques et culturels européens. À terme, le cercle pourra s'étendre à davantage de langues européennes selon les retours d'expérience de ces pays pilotes. L'objectif est de construire progressivement un véritable commun numérique européen pour l'évaluation humaine des IA conversationelles, avec une gouvernance collaborative qui reste encore à définir entre les différents pays participants.</p>"
                },
                "2": {
                    "title": "Quels sont les avantages d'une plateforme de collecte de préférences spécifiquement européenne ?",
                    "desc": "<p>Le développement d’un plateforme européenne de comparaison des modèles d’IA conversationnelle offre plusieurs avantages concrets. Elle permet de collecter des données de préférence reflétant les besoins réels des utilisateurs européens, améliorant ainsi la pertinence des modèles pour ce public. Elle garantit ainsi une meilleure représentation des langues et cultures européennes, souvent sous-représentées dans les évaluations globales dominées par l'anglais. Elle assure aussi une conformité avec les réglementations européennes (RGPD, AI Act) et intègre des critères d'évaluation alignés sur les priorités européennes comme la durabilité environnementale et la transparence algorithmique. Enfin, elle favorise l'émergence d'un écosystème d'IA européen compétitif et autonome.</p>"
                }
            }
        },
        "models": {
            "title": "Modèles",
            "questions": {
                "1": {
                    "title": "Comment choisissez-vous les modèles présents dans le comparateur ?",
                    "desc": "<p>Nous choisissons les modèles en fonction de leur popularité, de leur diversité et de la pertinence pour les utilisateurs. Nous veillons particulièrement à rendre accessibles des modèles dits <em>open weights</em> (semi-ouverts) et de taille différentes.</p>"
                },
                "2": {
                    "title": "Comment parvenez-vous à rendre ce service gratuit ?",
                    "desc": "<p>L’inférence, c’est-à-dire le fait de pouvoir interroger les modèles, est rendue possible grâce à des dons des entreprises fournisseuses de cloud qui soutiennent le projet : Google Cloud Platform, Hugging Face, Microsoft Azure, OVH, Scaleway.</p>"
                },
                "3": {
                    "title": "« modèle quantisé », quésaco ?",
                    "desc": "<p>Les modèles quantisés sont optimisés pour consommer moins de ressources en simplifiant certains calculs tout en visant la meilleure qualité de réponse.</p><p>La quantisation est une technique d'optimisation qui consiste à réduire la précision des nombres utilisés pour représenter les paramètres d'un modèle d'IA. Cela permet de <strong>diminuer la taille du modèle</strong> et <strong>d'accélérer les calculs</strong>, ce qui est particulièrement avantageux pour l'inférence sur des machines limitées en ressources.</p>"
                },
                "4": {
                    "title": "Y a-t-il un lien entre la nationalité de l’entreprise ou du laboratoire à l’origine du modèle et sa capacité à parler plusieurs langues ?",
                    "desc": "<p><strong>La capacité d'un modèle à parler plusieurs langues est liée à la diversité linguistique de ses données d'entraînement et non au pays</strong>. Les <strong>LLM utilisent d'énormes corpus dans de nombreuses langues</strong>, mais la répartition des langues dans les données d'entraînement n'est pas uniforme. Une surreprésentation de l'anglais peut entraîner des limitations dans d'autres langues. Ces limitations se traduisent par exemple par des <strong>anglicismes ou une incapacité à générer des contenus dans certaines langues classées « en danger » par l'UNESCO</strong>.</p><p><strong>L'exactitude et la richesse du vocabulaire d'un modèle dépendent des données utilisées pour son apprentissage</strong>.</p>"
                },
                "5": {
                    "title": "Peut-on connaître les données d’entraînement des modèles ?",
                    "desc": "<p>Rares sont les acteurs à être “transparents” sur les sources de données utilisées dans les corpus d’entraînement. Ces informations sont souvent confidentielles pour des raisons légales et commerciales.</p>"
                }
            }
        },
        "usage": {
            "title": "Usage",
            "questions": {
                "1": {
                    "title": "Les modèles peuvent-ils citer leurs sources ?",
                    "desc": "<p>Les modèles de langage conversationnels actuels sont <strong>incapables de citer les sources</strong> qu'ils ont utilisées pour générer une réponse. Ils fonctionnent en prédisant le mot suivant le plus probable en fonction de la distribution statistique des données d'entraînement. Bien qu'ils puissent synthétiser des informations provenant de diverses sources, ils ne conservent pas la trace de l'origine de ces informations.</p><p>Cependant, il existe des techniques comme la <strong>Génération Augmentée par Récupération (RAG)</strong> qui visent à pallier cette limitation. Le RAG permet aux modèles d'accéder à des bases de connaissances externes et de <strong>fournir des informations contextualisées en citant les sources</strong>. Cette approche est essentielle pour améliorer la transparence et la fiabilité des réponses générées par les modèles.</p>"
                },
                "2": {
                    "title": "Si je pose une question sur l’actualité la plus récente, le modèle peut-il répondre ?",
                    "desc": "<p>Vous avez posé la question suivante “explique-moi la motion de censure à l'œuvre actuellement en France à l'Assemblée nationale et cite-moi tes sources” et avez été déçu·e des réponses ? C’est normal…</p><p><strong>Les modèles d'IA conversationnels “bruts” ne peuvent pas répondre aux questions sur l'actualité la plus récente.</strong> Ils sont entraînés sur des ensembles de données statiques et ne peuvent pas interagir avec le web ou ouvrir des liens. Ils n'ont pas la capacité de se mettre à jour en temps réel avec les événements qui se déroulent dans le monde. Les informations auxquelles le modèle a accès sont limitées à la date de son dernier entraînement.</p><p>Par conséquent, si vous posez une question sur un fait d’actualité récent, le modèle s'appuiera sur des informations potentiellement obsolètes, risquant de générer des réponses inexactes.</p><p>Dans le cas de Perplexity, Copilot ou ChatGPT, les modèles d’IA conversationnelle dits “bruts” sont associés à d’autres briques technologiques qui permettent de se connecter à internet pour accéder à des informations en temps réel. On parle alors “d’agents conversationnels”.</p>"
                },
                "3": {
                    "title": "Si j’intègre un lien d’URL dans une requête, le modèle peut-il y accéder ?",
                    "desc": "<p>Si vous intégrez une URL dans une requête, le modèle conversationnel ne peut pas y accéder directement. Les modèles de langage traitent le texte de la requête mais n'ont pas la capacité d'interagir avec le web ou d'ouvrir des liens. Ils sont entraînés sur un ensemble de données textuelles fixes et leurs réponses reposent sur ces données d’entraînement. Lorsqu'une question est posées, les modèles utilisent cet entraînement pour générer une réponse mais ne peuvent pas accéder à de nouvelles informations en ligne.</p><p>Par analogie, imaginez un étudiant passant un examen sans accès à internet. Il peut utiliser ses connaissances acquises pour répondre aux questions, mais ne peut pas consulter de sites web pour obtenir des informations supplémentaires.</p>"
                },
                "4": {
                    "title": "Pourquoi certains modèles perdent-ils rapidement le fil de la conversation ?",
                    "desc": "<p>Il arrive que les modèles perdent le fil d'une conversation en raison de leur <strong>fenêtre de contexte limitée.</strong> Cette « fenêtre » représente la quantité d'informations précédentes que le modèle peut retenir, agissant comme une mémoire à court terme. Plus la fenêtre est petite, plus le modèle est susceptible d'oublier des éléments clés de la conversation, conduisant à des réponses incohérentes. Les conversations longues ou complexes peuvent rapidement saturer la fenêtre de contexte, augmentant le risque d'incohérence.</p><p>Par analogie, imaginez une personne qui ne se souvient que des cinq dernières phrases d'une conversation. Si la conversation est courte, la personne peut suivre. Mais si la conversation devient longue, la personne oubliera des informations cruciales, ce qui rendra ses réponses incohérentes. De même, un modèle d'IA avec une petite fenêtre de contexte peut « perdre le fil » d'une conversation lorsque trop d'informations sont échangées, oubliant des éléments clés et produisant des réponses qui n'ont plus de sens.</p>"
                },
                "5": {
                    "title": "Quelles sont les bonnes pratiques pour prompter ?",
                    "desc": "<p>La formulation des questions, ou « prompts », influence la cohérence de la conversation. Pour obtenir les meilleurs résultats d'un modèle de langage, il est essentiel de maîtriser l'art du <em>prompting</em>, c'est-à-dire la formulation des requêtes ou instructions. <strong>La clarté est primordiale</strong>:</p><ul><li>Utilisez un langage simple et direct, en évitant les questions trop longues ou complexes. Décomposez les requêtes en plusieurs questions plus simples pour des réponses plus précises.</li><li><strong>Précisez si besoin des contraintes de formats spécifiques</strong> : Si vous avez besoin d’une réponse dans un certain format (liste, tableau, résumé, etc.), précisez-le dans le prompt. Vous pouvez également préciser les étapes à suivre et les critères de qualité souhaités.</li><li><strong>Spécifiez le rôle du modèle</strong> : Par exemple, commencez par “Agis comme un expert en…” ou “Imagine que tu es un enseignant…” pour orienter le ton et la perspective de la réponse.</li><li><strong>Contextualisez vos questions</strong> : si nécessaire, fournissez des exemples pertinents pour guider le modèle.</li><li><strong>Encouragez le raisonnement</strong>: utilisez l’incitation au raisonnement pas à pas (<em>Chain-of-Thought Prompting</em>) pour demander au modèle d'expliciter son raisonnement, ce qui rend les réponses plus robustes.</li></ul><p>Les modèles conversationnels sont sensibles aux variations de formulation: un langage simple, des questions courtes et une reformulation si nécessaire peuvent aider à guider le modèle vers des réponses pertinentes. Testez et affinez vos prompts pour trouver la formulation la plus efficace !</p>"
                },
                "6": {
                    "title": "Quelle est la différence entre poser une question à un modèle d’IA conversationnelle et faire une recherche sur Google ?",
                    "desc": "<p>L'IA conversationnelle répond directement en formulant des phrases à partir d’un grand ensemble de données sur lesquelles le modèle a été entraîné, tandis qu’un moteur de recherche propose des liens et des ressources pour que l’internaute les explore lui-même.</p>"
                }
            }
        },
        "title": "Foire aux questions"
    },
    "footer": {
        "backHome": "Retour à l'accueil du site - compar:IA",
        "helpUs": "Aidez-nous à améliorer ce service !",
        "license": {
            "linkTitle": "Licence etalab - nouvelle fenêtre",
            "mention": "Sauf mention explicite de propriété intellectuelle détenue par des tiers, les contenus de ce site sont proposés sous <a {linkProps}>licence etalab-2.0</a>"
        },
        "links": {
            "accessibility": "Accessibilité : non conforme",
            "legal": "Mentions légales",
            "privacy": "Politique de confidentialité",
            "sources": "Code source",
            "tos": "Modalités d'utilisation"
        },
        "writeUs": "Si vous rencontrez un problème ou si vous avez un commentaire sur le comparateur, n'hésitez pas à nous écrire <a {linkProps}>via ce formulaire</a>, nous lisons tous vos messages.<br />Merci !"
    },
    "general": {
        "a11y": {
            "desc": "Cette déclaration d’accessibilité s’applique au site <strong>comparia.beta.gouv.fr</strong>.",
            "disclaimer": "<strong>compar:IA</strong> s’engage à rendre ses services numériques accessibles, conformément à l’article 47 de la loi n° 2005-102 du 11 février 2005.",
            "improveAdress": "Adresse : DINUM, 20 avenue de Ségur 75007 Paris",
            "improveDelay": "Nous essayons de répondre dans les 2 jours ouvrés.",
            "improveDesc": "Si vous n’arrivez pas à accéder à un contenu ou à un service, vous pouvez contacter le responsable de beta.gouv.fr pour être orienté·e vers une alternative accessible ou obtenir le contenu sous une autre forme.",
            "improveMail": "Courriel : <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Amélioration et contact",
            "remedyAdvocate": "Écrire un message au <a {linkProps}>Défenseur des droits</a>",
            "remedyAdvocateAdress": "Envoyer un courrier par la poste (gratuit, ne pas mettre de timbre) : Défenseur des droits - Libre réponse 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Contacter le délégué du <a {linkProps}>Défenseur des droits dans votre région</a>",
            "remedyDesc": "Cette procédure est à utiliser dans le cas suivant : vous avez signalé au responsable du site internet un défaut d’accessibilité qui vous empêche d’accéder à un contenu ou à un des services du portail et vous n’avez pas obtenu de réponse satisfaisante.",
            "remedyList": "Vous pouvez :",
            "remedyTitle": "Voie de recours",
            "stateDesc": "Le site comparia.beta.gouv.fr est non conforme avec le RGAA 4.1. Le site n’a <strong>pas encore été audité</strong>. Il a cependant été conçu pour être accessible au plus grand nombre. Vous devriez donc pouvoir :",
            "stateNavigate": "naviguer sur toutes les pages du site en utilisant un clavier",
            "statePrefs": "adapter le site à votre préférences (taille de la police, zoom écran, changement de typographie…) sans perte de contenu",
            "stateScreenReader": "consulter le site web avec un lecteur d’écran.",
            "stateTitle": "État de conformité",
            "title": "Déclaration d’accessibilité"
        },
        "legal": {
            "a11yDesc": "La conformité aux normes d’accessibilité numérique est un objectif ultérieur mais nous tâchons de rendre ce site accessible à toutes et à tous.",
            "a11yTitle": "Accessibilité",
            "directorDesc": "Monsieur Romain Delassus, chef du service du numérique du Ministère de la Culture",
            "directorTitle": "Directeur de la publication",
            "editorDesc": "Ce site est édité par le Ministère de la culture, 182, rue Saint-Honoré 75001 Paris",
            "editorTitle": "Éditeur",
            "hostingDesc": "Ce site est hébergé par OVH SAS (<a {linkProps}>https://www.ovh.com</a>) dont le siège social est situé au 2 rue Kellermann - 59100 Roubaix - France.",
            "hostingTitle": "Hébergement du site",
            "reportA11y": "Si vous rencontrez un défaut d’accessibilité vous empêchant d’accéder à un contenu ou une fonctionnalité du site, merci de nous en faire part.",
            "reportA11yDesc": "Pour en savoir plus sur la politique d’accessibilité numérique de l’État : <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "reportDesc": "Si vous n’obtenez pas de réponse rapide de notre part, vous êtes en droit de faire parvenir vos doléances ou une demande de saisine au Défenseur des droits.",
            "reportTitle": "Signaler un dysfonctionnement",
            "securityCertif": "Le site est protégé par un certificat électronique, matérialisé pour la grande majorité des navigateurs par un cadenas. Cette protection participe à la confidentialité des échanges.",
            "securityNoMail": "En aucun cas les services associés à la plateforme ne seront à l’origine d’envoi de courriels pour demander la saisie d’informations personnelles.",
            "securityTitle": "Sécurité",
            "sources": "Sauf mention contraire, tous les textes de ce site sont sous <a {etalabLinkProps}>licence Etalab Open 2.0</a>. Le code source de cette application est librement réutilisable et accessible sur <a {githubLinkProps}>GitHub</a>.",
            "title": "Mentions légales"
        },
        "privacy": {
            "cookiesBannerDesc": "C’est vrai, vous n’avez pas eu à cliquer sur un bloc qui recouvre la moitié de la page pour dire que vous êtes d’accord avec le dépôt de cookies — même si vous ne savez pas ce que ça veut dire !",
            "cookiesBannerNoNeed": "Rien d’exceptionnel, pas de passe-droit lié à un .gouv.fr. Nous respectons simplement la loi, qui dit que certains outils de suivi d’audience, correctement configurés pour respecter la vie privée, sont exemptés d’autorisation préalable.",
            "cookiesBannerTitle": "Ce site n’affiche pas de bannière de consentement aux cookies, pourquoi ?",
            "cookiesBannerTools": "Nous utilisons pour cela <a {matomoLinkProps}>Matomo</a>, un outil <a {libreLinkProps}>libre</a>, paramétré pour être en conformité avec la <a {cnilLinkProps}>recommandation « Cookies »</a> de la CNIL. Cela signifie que votre adresse IP, par exemple, est anonymisée avant d’être enregistrée. Il est donc impossible d’associer vos visites sur ce site à votre personne.",
            "cookiesDesc": "Ce site dépose un petit fichier texte (un « cookie ») sur votre ordinateur lorsque vous le consultez. Cela nous permet de mesurer le nombre de visites et de comprendre quelles sont les pages les plus consultées.",
            "cookiesDescMore": "Vous pouvez vous opposer au suivi de votre navigation sur ce site web. Cela protégera votre vie privée, mais empêchera également le propriétaire d'apprendre de vos actions et de créer une meilleure expérience pour vous et les autres utilisateurs.",
            "cookiesTitle": "Cookies déposés et consentement",
            "dataAccessDatasets": "Les données de dialogue et de préférence de l’utilisateur sont distribuées sous la Licence Ouverte 2.0 d'Etalab sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "dataAccessDesc": "Bien sûr ! Les statistiques d’usage du site sont disponibles en accès libre sur <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessTitle": "Je contribue à enrichir vos données, puis-je y accéder ?",
            "dataExtraCountry": "Pays destinataire : France",
            "dataExtraHost": "Sous-traitant : OVH",
            "dataExtraTitle": "Qui nous aide à traiter les données ?",
            "dataExtraWarranty": "Garanties : <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>",
            "dataExtraWhat": "Traitement réalisé : Hébergement",
            "dataRespDesc": "Le service du numérique du ministère de la Culture est le responsable du traitement de vos données à caractère personnel.",
            "dataRespTitle": "Qui est responsable du traitement des données ?",
            "dataTimeDesc": "Les données relatives aux utilisateurs et à leurs conversations avec les modèles sont conservées à compter de l’enregistrement du vote de préférence.",
            "dataTimeTitle": "Pendant combien de temps conservons-nous ces données ?",
            "dataUseDesc": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s’assurer de l’anonymisation des données de dialogue avant leur mise à disposition publique.",
            "dataUseTitle": "Quels sont les traitements réalisés sur les données de conversation ?",
            "desc": "Le service est édité par le service du numérique du ministère de la Culture.",
            "privacyData": "Les données collectées sur le site sont les suivantes :",
            "privacyDataArena": "Données relatives aux conversations des utilisateurs avec les modèles : questions posées par les utilisateurs, réponses des modèles et préférence exprimée par l’utilisateur sur les deux modèles",
            "privacyDataForm": "Données relatives au questionnaire « Nous aider à améliorer compar:IA ».",
            "privacyDesc": "Le service ne traite pas de données à caractère personnel telles que définies par la CNIL, à savoir toute information relative à une personne physique susceptible d'être identifiée, directement ou indirectement.",
            "privacyResp": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme. En acceptant les <a {linkProps}>modalités d’utilisation</a>, l’utilisateur ou l’utilisatrice s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
            "privacyTitle": "Traitons-nous des données à caractère personnel ?",
            "title": "Politique de confidentialité"
        },
        "tos": {
            "contactDesc": "Pour toute question sur le service, vous pouvez écrire à <a {linkProps}>contact@comparia.beta.gouv.fr</a>.",
            "contactTitle": "9. Contact",
            "defsEditor": "« Éditeur » désigne le Service du numérique du Ministère de la Culture.",
            "defsModels": "« Modèles » désigne les grands modèles de langages (LLM) réutilisés dans le cadre de leur licence d’utilisation par la plateforme pour répondre à ses finalités.",
            "defsPlatform": "« Plateforme » désigne le site web qui rend les services accessibles.",
            "defsServices": "« Services » désigne les fonctionnalités offertes par la plateforme pour répondre à ses finalités.",
            "defsTitle": "2. Définitions",
            "defsUser": "« Utilisateur » désigne toute personne physique consultant la plateforme et qui bénéficie de ses services.",
            "descDatasets": "Ces jeux de données seront rendus accessibles sous licence ouverte, notamment pour favoriser des usages de recherche.",
            "descEditor": "Édité par le Service du numérique du Ministère de la Culture, le comparateur est une plateforme de comparaison des modèles conversationnels adressée au grand public dans le but (1) de sensibiliser les citoyens aux grands modèles de langage (LLMs), (2) de collecter les préférences des utilisateurs pour constituer des jeux de données d’alignement.",
            "descTitle": "3. Description de la plateforme",
            "descUse": "L’utilisateur ou l’utilisatrice pose une question en français et obtient des réponses de deux grands modèles de langages (LLM) anonymes. Il ou elle vote pour le modèle qui fournit la réponse qu’il préfère et se voit alors révélée l’identité des modèles. Ce dispositif de production participative inspiré de la plateforme <a {linkProps}>« chatbot arena » (LMSYS)</a> permet de constituer des jeux de données de préférences humaines sur des tâches réelles, en français, utilisables pour l’alignement des modèles.",
            "dispoDesc": "La plateforme est accessible, sauf cas de force majeure ou d’évènement hors de contrôle de son éditeur.",
            "dispoResp": "A ce titre, l’éditeur ne saurait être tenu responsable des pertes ou préjudices, de quelque nature qu’ils soient, qui pourraient être causés à la suite d’un dysfonctionnement ou une indisponibilité du service. De telles situations n&#39;ouvriront droit à aucune compensation financière.",
            "dispoRight": "L’éditeur se réserve le droit de suspendre, d&#39;interrompre ou de limiter, sans avis préalable, l&#39;accès à tout ou partie des services, notamment pour des opérations de maintenance et de mises à jour nécessaires au bon fonctionnement du service et des matériels afférents, ou pour toute autre raison, notamment technique.",
            "dispoTitle": "7. Disponibilité des services",
            "dispoWarranty": "Il n’est pas garanti que le service soit exempt d’anomalies ou erreurs. Le service est donc mis à disposition sans garantie sur sa disponibilité et ses performances.",
            "evoDesc": "Les modalités d’utilisation peuvent être modifiées ou complétées à tout moment, sans préavis, en fonction des modifications apportées aux services, de l’évolution de la législation ou pour tout autre motif jugé nécessaire.",
            "evoDescMore": "Ces modifications et mises à jour s’imposent à l’utilisateur ou l’utilisatrice qui doit, en conséquence, se référer régulièrement à cette rubrique pour vérifier les modalités générales en vigueur.",
            "evoTitle": "8. Évolution des modalités d'utilisation",
            "featuresDatasets": "Le service recueille les données de dialogue et de préférence des utilisateurs. Les jeux de données partagés comprendront les questions de l’utilisateur, les réponses des deux modèles, le vote et les préférences de l’utilisateur.",
            "featuresDatasetsMore": "L’éditeur se réserve le droit de distribuer sous licence ouverte 2.0 les données de dialogue et de préférence de l’utilisateur. Le jeu de données est diffusé sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "featuresDesc": "Afin de répondre au double objectif de sensibiliser les citoyens aux grands modèles de langage et collecter les préférences des utilisateurs et utilisatrices, les services rendus par la plateforme sans restriction d’accès sont les suivants :",
            "featuresDescMore": "Une interface humain-machine permettant de dialoguer simultanément avec deux modèles conversationnels et de voter pour la réponse préférée.",
            "featuresModels": "Les modèles intégrés à la plateforme sont déployés sur les serveurs d’inférence des différents partenaires (Scaleway, OVH, Hugging Face, Google Cloud, Mistral Ai). Les conditions de standardisation d’inférence sont renseignées sur la plateforme pour garantir la transparence d’utilisation des modèles.",
            "featuresModelsMore": "Une interface de comparaison des modèles.",
            "featuresTitle": "4. Fonctionnalités",
            "featuresVote": "À l’issue du parcours de vote, l’utilisateur peut consulter la liste des modèles intégrés au comparateur et accéder à une liste d’informations sur ces modèles. Les informations documentant les modèles sont sourcées.",
            "featuresVoteMore": "Partage et mise à disposition des jeux de données issus de la collecte des préférences des utilisateurs.",
            "licenceCode": "Le code source de la plateforme est libre et disponible ici : <a {linkProps}>https://github.com/betagouv/languia</a>",
            "licenceLLM": "Les LLM utilisés pour alimenter les services sont régis par les licences suivantes :",
            "licenceLLMEvolution": "La liste des modèles de langage intégrés à la plateforme est susceptible d’évoluer au cours du temps et est mise à jour à chaque modification.",
            "licenceLLMLicence": "Licence",
            "licenceLLMModel": "Modèle d’IA conversationnelle",
            "licenceLLMNoticeLink": "Lien vers la notice des modèles",
            "licenceLLMUnavailable": "Non disponible",
            "licenceTitle": "6. Code et licences",
            "respEditor": "De manière générale, l’éditeur se dégage de toute responsabilité en cas d’utilisationnon-conforme aux modalités d’utilisation.",
            "respLegal": "La plateforme n’a pas vocation à être utilisée pour générer des contenus illicites ou contraires à l’ordre public et plus généralement toute génération contrevenant au cadre juridique en vigueur.",
            "respLegalMore": "A cet égard, l’utilisateur ne saisit pas dans l’invite des contenus ou informations contraires aux dispositions légales et réglementaires en vigueur.",
            "respPrivacy": "Les données saisies par l’utilisateur sur la plateforme ayant vocation à être mis à disposition, il ou elle s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
            "respPrivacyMore": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s’assurerde l’anonymisation les données de dialogue avant leur mise à disposition.",
            "respTitle": "5. Responsabilités",
            "respUser": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme.",
            "scopeDesc": "L’accès à la plateforme est gratuit, sans inscription et entraîne l’application de conditions spécifiques, listées dans les présentes modalités d’utilisation.",
            "scopeTitle": "1. Champ d’application",
            "title": "Modalités d’utilisation"
        }
    },
    "generated": {
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le modèle, y compris à des fins commerciales. Outre la liberté d'utilisation, elle garantit la protection juridique en incluant une clause de concession de brevets qui fonctionne comme une assurance : si vous utilisez ce modèle, les contributeurs s'engagent à ne pas vous poursuivre pour violation de leurs brevets liés au projet. Cette protection mutuelle évite les conflits juridiques entre utilisateurs et développeurs. Lors de la distribution de versions modifiées, les changements significatifs doivent être signalés par des mentions appropriées, garantissant la transparence pour l'utilisateur.</p>",
                    "reuse_specificities": ""
                },
                "CC-BY-NC-4.0": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence permet de partager et adapter le contenu librement à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.</p>",
                    "reuse_specificities": "mais que pour des usages non-commerciaux"
                },
                "Gemma": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence est conçue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiées ou améliorées doivent être partagées avec la communauté sous la même licence source, favorisant ainsi la collaboration et la transparence dans le développement logiciel.</p>",
                    "reuse_specificities": ""
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs",
                    "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.</p>",
                    "reuse_specificities": ""
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs",
                    "license_desc": "<p>Cette licence <strong>non-exclusive, mondiale et sans redevance</strong> permet d'utiliser, reproduire, modifier et distribuer librement le code et les Matériaux Llama 3.3 avec attribution. Elle autorise notamment la réutilisation pour l'amélioration de modèles dérivés, mais impose des restrictions pour les opérations commerciales de très grande envergure.</p>",
                    "reuse_specificities": ""
                },
                "Llama 4": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs\n",
                    "license_desc": "<p>Cette licence non-exclusive, mondiale et sans redevance permet d'utiliser, reproduire, modifier et distribuer les Matériaux Llama 4 (modèles et documentation) avec attribution. Cependant, elle impose deux restrictions majeures : (1) les entreprises dépassant 700 millions d'utilisateurs actifs mensuels doivent obtenir une licence spéciale de Meta, et (2) <strong>exclusion totale</strong> des personnes résidant dans l'UE et des entreprises ayant leur siège social dans l'UE pour l'utilisation directe des modèles multimodaux, en raison des incertitudes réglementaires liées à l'AI Act européen. Les utilisateurs finaux européens peuvent néanmoins accéder à des services intégrant Llama 4, à condition qu'ils soient fournis depuis l'extérieur de l'UE.</p>",
                    "reuse_specificities": ""
                },
                "Mistral AI Research License": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence non-exclusive et sans redevance autorise l'utilisation, la copie, la modification et la distribution des modèles Mistral et de leurs dérivés (incluant les versions modifiées ou affinées). Cependant, elle est strictement limitée aux fins de recherche.</p>",
                    "reuse_specificities": "mais que pour des usages non-commerciaux"
                },
                "MIT": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>La licence MIT est une licence de logiciel libre permissive : elle permet à quiconque de réutiliser, modifier et distribuer le modèle, même à des fins commerciales, sous réserve d'inclure la licence d'origine et les mentions de droits d'auteur.</p>",
                    "reuse_specificities": ""
                }
            },
            "proprio": {
                "Alibaba": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Alibaba, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Amazon": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via Amazon Bedrock, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": "sauf pour distiller ou entraîner d’autres modèles sur les plateformes d’Amazon."
                },
                "Anthropic": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic ou des sociétés partenaires, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Cohere": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "DeepSeek": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Google": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Google, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée de Google.",
                    "reuse_specificities": "sauf pour entraîner d’autres modèles sur Vertex AI"
                },
                "Meta": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Microsoft": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Mistral AI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres hébergeurs, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Moonshot AI": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Nous": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Nvidia": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "OpenAI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI ou via les services Microsoft Azure, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "xAI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via X et xAI, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Zhipu": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                }
            }
        },
        "models": {
            "Aya Expanse 32B": {
                "desc": "<p>Modèle de taille moyenne multilingue, capable de traiter 23 langues.</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été conçu pour offrir de bonnes capacités dans chacune des 23 langues de son corpus d’entraînement.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 130 000 jetons, utile pour l’analyse de documents longs.</p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p>Très grand modèle multimodal et multilingue, performant pour la génération de code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Claude 4 Opus est la version la plus avancée de la famille Claude 4. Il est optimisé pour la puissance brute et les tâches complexes nécessitant un raisonnement soutenu sur de longues périodes : il peut par exemple travailler sur des tâches à long terme (Anthropic déclarent qu'il peut travailler jusqu'à sept heures de manière indépendante). En contrepartie, Opus est plus coûteux à utiliser, plus lent à répondre et nécessite davantage de ressources pour fonctionner.</p>\n<p>Le modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été majoritairement entraîné sur des données mathématiques, mais adapté à des cas d’usage réels.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p>Très grand modèle multimodal et multilingue, très puissant en code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisée pour la vitesse, l’efficacité et l’accessibilité. Il est un peu moins à l’aise sur les tâches qui demandent un raisonnement complexe en plusieurs étapes. En contrepartie, il est nettement moins coûteux, plus rapide, peut générer de plus longs textes et consomme moins d’énergie que Opus.</p>\n<p>Le modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été surtout entraîné sur des données mathématiques, mais adapté à des cas d’usage réels.</p>",
                "size_desc": "<p>La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>"
            },
            "Command A": {
                "desc": "<p>Grand modèle, performant pour la programmation, l’utilisation d’outils externes, la “génération augmentée de récupération” (RAG, retrieval augmented generation).</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier <a href=\"https://arxiv.org/abs/1706.03762\">« Attention Is All You Need »</a> paru en 2017 et qui a révolutionné l'IA. L'entreprise se démarque par sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle est conçu pour fonctionner dans plus de 23 langues et pour s’intégrer facilement dans les systèmes d’entreprise.  Il fait partie des rares modèles distribués sous licence <strong>CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.</strong> Ce choix de licence reflète la volonté de Cohere de contribuer à la recherche et la communauté open source, tout en gardant le contrôle sur les usages commerciaux pour protéger son modèle économique... Cela exclut par exemple l’intégration du modèle dans des produits ou services vendus par une entreprise à des clients mais autorise un usage académique, des tests ou des projets internes, restreints à un cadre non-commercial.</p>",
                "size_desc": "<p>Avec 111 milliards de paramètres, ce modèle fait partie des grands modèles. Il nécessite au moins deux cartes graphiques puissantes pour l’hébergement, ce qui entraîne un coût de fonctionnement significatif.</p>\n<p>Sa fenêtre de contexte atteint 256 000 jetons, adaptée à l’analyse de vastes ensembles de documents ou de bases de code.</p>"
            },
            "Command R": {
                "desc": "<p>Modèle de taille moyenne optimisé pour la synthèse, les questions générales, l’utilisation d’outils et efficace dans les systèmes de génération augmentée de récupération (RAG, retrieval augmented generation).</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été évalué dans plus de 10 langues. Sa fenêtre de contexte atteint 128 000 jetons, ce qui facilite l’analyse de documents longs. Cette fenêtre a été doublée sur la version suivante du modèle (Command A).</p>",
                "size_desc": "<p>Avec 35 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>"
            },
            "DeepSeek R1": {
                "desc": "<p>Très grand modèle très performant sur les tâches mathématiques, scientifiques et de programmation, qui simule une étape de raisonnement avant de générer sa réponse.</p>",
                "fyi": "<p>Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (<em>supervised fine-tuning</em> : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage.</p>",
                "size_desc": "<p>Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 128 000 jetons, ce qui est adapté à l’analyse de longs documents.</p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p>Grand modèle basé sur Meta Llama 3.3 70B, ré-entraîné avec des exemples de raisonnement issus du modèle DeepSeek R1. Il offre de bonnes capacités en mathématiques et code.</p>",
                "fyi": "<p>Le modèle n’a pas été entraîné depuis zéro. Il s’appuie sur Llama 3.3 70B, ré-entraîné en utilisant des résultats générés par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B d’une capacité à simuler le raisonnement, sans possibilité pour l’utilisateur de choisir d’activer ou non cette fonction.</p>\n<p>Conformément aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du modèle source dans le nom du modèle, soumis au même régime de licence.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle est classé parmi les modèles de grande taille. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne un coût de fonctionnement élevé. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>La fenêtre de contexte est de 16 000 jetons, ce qui peut être limitant pour l’analyse de très grands documents.</p>"
            },
            "DeepSeek V3": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes : génération de code, utilisation d’outils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particulièrement adapté à l’anglais et au chinois.</p>",
                "fyi": "<p>Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code.</p>",
                "size_desc": "<p>DeepSeek V3 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.</p>\n<p>La fenêtre de contexte atteint 128 000 jetons, ce qui est utile pour l’analyse de longs documents.</p>"
            },
            "DeepSeek v3.1": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes : génération de code, analyse de documents longs. Cette version est particulièrement forte en  utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.</p>",
                "fyi": "<p>Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code. L’entraînement a recours au FP8 microscaling, ce qui réduit les coûts de calcul et de mémoire tout en maintenant la précision. Le modèle a été formé en deux phases : d’abord sur des séquences de 32 000 jetons, puis étendu à 163 000 jetons, permettant une meilleure stabilité et une performance accrue sur les contextes très longs.</p>",
                "size_desc": "<p>DeepSeek V3.1 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.</p>\n<p>La fenêtre de contexte atteint désormais 163 000 jetons, contre 128 000 dans la version précédente, ce qui améliore l’analyse de très longs documents.</p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p>Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale.</p>",
                "fyi": "<p>Ce modèle repose sur une architecture de mélange d’experts (MoE, Mixture of Experts) et a été distillé en ne conservant qu'une approximation des prédictions du modèle enseignant - Gemini 2.5 Pro. Il a été entraîné sur une architecture TPUv5p intégrant des avancées comme la possibilité de poursuivre l'entraînement automatiquement même en cas d’erreurs d’entraînement, de corruption de données ou de problèmes de mémoire.</p>\n<p>Gemini 2.5 Flash gère des contextes allant jusqu'à 1 million de jetons, et trois heures de contenu vidéo. L'optimisation du traitement de la vision permet de traiter des vidéos environ trois fois plus longues dans la même fenêtre de contexte: seuls 66 jetons visuels sont nécessaires pour générer une image contre 258 auparavant. Ce modèle permet  également la génération audio native pour les dialogues et la synthèse vocale.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant plusieurs cartes graphiques puissantes pour le fonctionnement. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>"
            },
            "Gemma 3 12B": {
                "desc": "<p>Petit modèle multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 12 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.</p>"
            },
            "Gemma 3 27B": {
                "desc": "<p>Modèle de taille moyenne multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il peut traiter du texte et des images sur un serveur équipé d’une seule carte graphique puissante. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 27 milliards de paramètres, il appartient à la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur avec une seule carte graphique (GPU). </p>\n<p>Il accepte des contextes jusqu’à 128 000 jetons, ce qui convient pour l’analyse de documents longs.</p>"
            },
            "Gemma 3 4B": {
                "desc": "<p>Très petit modèle multimodal et compact adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte peut atteindre 128 000 jetons, ce qui permet d’analyser de longs documents.</p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p>Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin.</p>",
                "fyi": "<p>Ce modèle peut traiter du texte, des images et de l’audio. Il repose sur l’architecture MatFormer et un système de cache PLE (per-layer embeddings), qui active uniquement les paramètres utiles selon la tâche, s'adaptant à la capacité des machines sur lesquelles fonctionne le modèle.</p>",
                "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement sur un ordinateur ou un smartphone pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand.</p>\n<p>Sa fenêtre de contexte va jusqu’à 32 000 jetons.</p>"
            },
            "GLM 4.5": {
                "desc": "<p>Très grand modèle créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent.  Le modèle a deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications web complètes et de générer des artefacts, qui sont des programmes d’un seul fichier utilisable à l’intérieur même des interfaces des agents conversationnels. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin.</p>",
                "size_desc": "<p>Avec 355 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.</p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p>Plus petite version allégée du modèle GPT 4.1 , conçue pour limiter les coûts tout en restant compétitive sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
                "fyi": "<p>Il s'agit d'une version distillée d’un modèle de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio. Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus de textes ou de dépôts de code très longs.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.  Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT 5": {
                "desc": "<p>Le GPT-5 n'est pas un modèle unique, mais un système unifié composé de deux modèles distincts : un modèle rapide (<code>gpt-5-main</code>) pour les requêtes courantes et un modèle de raisonnement (<code>gpt-5-thinking</code>) pour les problèmes complexes. Comparé à ses prédécesseurs, OpenAI affirme qu'il est plus utile dans les requêtes du monde réel, avec des améliorations notables dans les domaines de l'écriture, du codage et de la santé. Il réduit également le phénomène des hallucinations. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Les développeurs qui utilisent ce modèle peuvent configurer un paramètre de verbosité pour ajuster la longueur de la phase de raisonnement.</p>\n<p>En matière de sécurité, le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête. Les créateurs du modèle ont aussi utilisé la phase d’entraînement au “raisonnement” pour le rendre plus “résistant” aux tentatives de contournement de leurs règles de sécurité (<em>jailbreaking</em>).</p>",
                "size_desc": "<p>Le système GPT-5 est composé de modèles de différentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est conçue pour inclure plusieurs modèles, orchestrés par un système de routage interne, qui sélectionne le plus petit modèle adapté à la tâche pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT 5 Mini": {
                "desc": "<p>Le GPT-5 Mini est une version allégée du modèle GPT-5 principal. Il est conçu pour être utilisé dans des environnements où il est nécessaire de limiter les coûts, par exemple à grande échelle. Son modèle de raisonnement est presque aussi performant que celui du modèle principal (<code>gpt-5-thinking</code>) malgré sa taille plus petite. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>\n<p>Bien qu'il soit une version plus petite, il se montre très compétitif face au modèle GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine médical.</p>",
                "size_desc": "<p>Le modèle Mini est une déclinaison plus compacte (taille moyenne supposée) du système GPT-5. Il est conçu pour fonctionner de manière optimale pour un bon équilibre entre performance et coût, grâce à un système de routage qui le sélectionne pour des tâches spécifiques. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Néanmoins, les modèles sont probablement très grands, nécessitant plusieurs cartes graphiques puissantes pour l’inférence.</p>"
            },
            "GPT 5 Nano": {
                "desc": "<p>Le GPT-5 Nano est la plus petite et la plus rapide version du modèle de raisonnement GPT-5. Il est conçu pour des contextes où une latence ou un coût ultra-faible est nécessaire. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>",
                "size_desc": "<p>Le modèle Nano est le plus compact de la famille GPT-5 (taille petite supposée). Il est sélectionné par le système de routage pour les requêtes nécessitant une latence ultra-faible et des réponses instantanées. Son architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui permet une meilleure efficacité énergétique et des performances élevées, même sur des requêtes nécessitant une réponse rapide.</p>"
            },
            "GPT OSS-120B": {
                "desc": "<p>Le plus grand des deux premiers modèles semi-ouverts d'OpenAI depuis GPT-2. Conçu en réponse à la montée en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modèle de raisonnement performant, notamment sur des tâches complexes et dans des environnements « agentiques ».</p>",
                "fyi": "<p>Ce modèle peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>",
                "size_desc": "<p>L'architecture est basée sur le principe du « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (5,1 milliards par jeton) pour chaque requête. C’est un modèle de raisonnement, donc sa consommation d’énergie est plus élevée car ils génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>"
            },
            "GPT OSS-20B": {
                "desc": "<p>Le plus petit des deux modèles semi-ouverts d'OpenAI. Il a été conçu en réponse à la concurrence de l'open source et est destiné aux cas d'utilisation nécessitant une faible latence ainsi qu'aux déploiements locaux ou spécialisés.</p>",
                "fyi": "<p>Ce modèle peut être exécuté localement sur un ordinateur portable haut de gamme équipé de seulement 16 Go de VRAM (ou de RAM système). Cela en fait une option très accessible pour les développeurs. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>",
                "size_desc": "<p>Avec 20 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. L'architecture est basée sur le « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (3,6 milliards par jeton) pour chaque requête. Il s'agit d'un modèle de raisonnement, ce qui se traduit par une consommation d'énergie plus élevée car il génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p>Version allégée de GPT 4.1 mais qui reste tout de même de grande taille, conçue pour limiter les coûts tout en restant compétitif sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
                "fyi": "<p>Il s'agit d'une version distillée d’un modèle plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio.  Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus très longs ou de dépôts de code.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "Grok 3 Mini": {
                "desc": "<p>Version plus légère du modèle Grok 3, permettant de réduire les coûts tout en conservant de bonnes performances pour de nombreuses tâches. Il peut simuler une phase de raisonnement avant de fournir une réponse finale.</p>",
                "fyi": "<p>Grok 3 Mini est une version distillée de Grok 3: il s’en approche en termes de capacités, tout en étant plus rapide et moins coûteux.\nLe modèle propose deux modes : un mode réflexion avec raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses immédiates.\nSa fenêtre de contexte atteint 131 000 jetons, ce qui le rend adapté à l’analyse de longs documents.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 3 Mini est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "Hermes 3 405B": {
                "desc": "<p>Très grand modèle réentraîné à partir du Llama 3.1 405B, ajusté pour mieux répondre aux demandes des utilisateurs et faciliter l’utilisation d’outils externes.</p>",
                "fyi": "<p>Ce modèle est le résultat d’un réentraînement de l’ensemble des paramètres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et système - l’utilisateur dispose ainsi d’un plus grand contrôle sur la “personnalité” et comportement du modèle. Des fonctions de raisonnement spécifiques telles que <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> ont été ajoutées pour simuler un raisonnement sur les tâches complexes. L'entraînement a utilisé un outil appelé AdamW (vitesse d'apprentissage de 3.5×10⁻⁶), qui aide le modèle à apprendre de manière efficace en ajustant progressivement ses paramètres. Ensuite, il a été affiné avec une méthode appelée DPO (direct preference optimisation), qui permet d'améliorer ses réponses en se basant sur des préférences spécifiques. Pour rendre cet entraînement plus léger et rapide, des adaptateurs LoRA ont été utilisés ; ce sont des modules plus petits qui modifient seulement une partie du modèle, ce qui évite de devoir retravailler tous les paramètres en même temps.</p>",
                "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important.</p>"
            },
            "Hermes 4 70B": {
                "desc": "<p>Grand modèle réentraîné à partir du Llama 3.1 70B, ajusté pour mieux répondre aux demandes et instructions stylistiques des utilisateurs.</p>",
                "fyi": "<p>Hermes 4-70B a été entraîné sur 56 milliards de tokens en combinant Fully Sharded Data Parallel (FSDP) et Tensor Parallelism pour gérer sa taille. Le modèle repose sur la base de Llama 3.1 70B, adaptée avec TorchTitan et enrichie par environ 19 milliards de tokens synthétiques centrés sur le raisonnement. Son entraînement suit une approche multi-phase, avec un supervised fine-tuning sur des chaînes de raisonnement pouvant dépasser les 30 000 jetons. Il exploite également l’environnement Atropos, utilisé pour générer et vérifier des trajectoires complexes (code, JSON, tâches agentiques), grâce à un rejection sampling massif qui garantit la qualité des données.</p>",
                "size_desc": "<p>Hermes 4-70B est un modèle de très grande taille, nécessitant au moins une carte graphique puissante.</p>\n<p>La fenêtre de contexte atteint 40 960 jetons en raisonnement et 32 768 jetons pour d’autres tâches, avec des mécanismes de fine-tuning qui ont été utilisés pour lui apprendre à “fermer” la séquence de réflexion vers 30k jetons. </p>"
            },
            "Kimi K2": {
                "desc": "<p>Développé par Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), une société basée à Pékin, Kimi K2 est un très grand modèle orienté code et usages agentiques. Il est reconnu pour les tâches de développement dans des contextes agentiques (par ex. dans Cursor ou Windsurf) notamment pour son rôle en tant qu’orchestrateur. Il n’expose pas de “mode raisonnement” explicite, mais pour les grandes tâches il sous-divise sa réponse en étapes et alterne entre actions (appels d’outils) et rédaction de texte.</p>",
                "fyi": "<p>Pour stabiliser l’entraînement à très grande échelle, Moonshot AI a introduit MuonClip, un “limiteur de vitesse” pour l’entraînement qui permet d’entraîner un modèle de cette taille et sur un corpus de 15,5 trillions de jetons sans dérailler dans l’apprentissage.</p>\n<p>Côté données, K2 a beaucoup pratiqué en “simulateur” avec de vrais outils (navigateur, terminal, exécuteurs de code, API…). Comme un pilote sur simulateur, il apprend à planifier, essayer, rater puis réessayer, et à enchaîner plusieurs actions pour atteindre un objectif. Résultat: il est particulièrement bon pour orchestrer des outils et réussir des tâches en plusieurs étapes.</p>",
                "size_desc": "<p>Avec 1 billion de paramètres, ce modèle est un des plus grands modèles qui existe. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.</p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes ou spécialisées. Souvent utilisé en tant que “modèle professeur” pour l’entraînement de modèles plus spécialisés.</p>",
                "fyi": "<p>Le modèle a été entraîné sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le marché en 2025). L'entraînement a combiné génération de données synthétiques et optimisation par préférences directes (DPO). Ce modèle est lui-même souvent utilisé pour générer des données synthétiques pour entraîner de plus petits modèles. Le modèle utilise par défaut une compression 8-bit pour réduire les besoins en mémoire et permettre l'exécution sur un seul serveur très puissant.</p>",
                "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important. Le modèle est doté d’une fenêtre de contexte jusqu’à 128 000 jetons, ce qui le rend intéressant pour des tâches d’analyse de longs documents.</p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p>Petit modèle conçu pour un usage local sur ordinateur portable, tout en offrant de bonnes capacités pour la synthèse de texte et les réponses simples.</p>",
                "fyi": "<p>Ce modèle est une version distillée issue des modèles Llama 3 de plus grande tailles : il a été entraîné grâce à un transfert d’une partie des connaissances des plus grands modèles.</p>",
                "size_desc": "<p>Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p>Grand modèle destiné à un large éventail de tâches et pouvant rivaliser avec des modèles plus volumineux.</p>",
                "fyi": "<p>Ce modèle est une version distillée issue du modèle 405B, auquel il doit une partie de ses connaissances transférées. Il a aussi bénéficié de techniques récentes d’alignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome.  Son entraînement s’appuie sur 15 billions de jetons.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation significatifs. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Llama 4 Scout": {
                "desc": "<p>Grand modèle doté d’une très large fenêtre de contexte, adapté par exemple à la synthèse d'un ensemble de documents.</p>",
                "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives . L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. Afin d'équilibrer performances multimodales, raisonnement et qualité conversationnelle, l'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données (pour ne garder que les plus complexes et intéressantes), fine-tuning ciblé et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons et peut traiter jusqu’à 8 images simultanément. </p>\n<p>Le modèle a été bien reçu à son lancement, notamment pour sa fenêtre de contexte impressionnante, une première dans le domaine, ainsi que pour son rapport qualité-prix sur des tâches comme le résumé, l’appel d’outils et la génération augmentée (RAG). Cela en fait un choix adapté pour les pipelines automatisés.</p>",
                "size_desc": "<p>Avec 109 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il peut être hébergé sur un serveur doté d’une seule carte graphique très puissante. Sa fenêtre de contexte va jusqu’à 10 millions de jetons, ce qui permet de traiter des corpus documentaires extrêmement longs.</p>"
            },
            "Llama Maverick": {
                "desc": "<p>Très grand modèle doté d’une très large fenêtre de contexte, adapté par exemple au résumé de plusieurs documents en même temps.</p>",
                "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Cela permet de transférer ses compétences plus vite et avec moins de calcul.  Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives - il peut traiter jusqu’à 8 images simultanément. L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. L'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données - en gardant seulement les plus complexes et intéressantes, fine-tuning ciblé et apprentissage par renforcement en ligne, pour équilibrer performances multimodales, raisonnement et qualité conversationnelle. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons. </p>\n<p>Le modèle Llama 4 Maverick a été présenté comme la réponse directe de Meta aux modèles DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estimé qu’il ne répondait pas aux attentes, en particulier sur les tâches de programmation et les travaux créatifs.</p>",
                "size_desc": "<p>Avec 400 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il nécessite moins de ressources pour fonctionner que les modèles “denses” de cette taille. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>"
            },
            "Magistral Medium": {
                "desc": "<p>Modèle de raisonnement de taille moyenne multimodal et multilingue. Adapté à des tâches de programmation ou autres tâches nécessitant analyse approfondie compréhension de systèmes logiques complexes ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
                "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. Il a été entraîné avec de l’apprentissage par renforcement sur Mistral Medium 3 et n'a pas été distillé à partir de modèles de raisonnement existants. Ce modèle hérite des capacités multimodales de Mistral Medium 3 même si l'apprentissage par renforcement n'a été réalisé que sur du texte.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement requièrent plus de capacité de calcul pour produire une réponse, ce qui augmente leur consommation énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>"
            },
            "Magistral Small": {
                "desc": "<p>Modèle de raisonnement de taille moyenne, multimodal et multilingue. Adapté à des tâches nécessitant une analyse approfondie, compréhension de systèmes logiques ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
                "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. </p>\n<p>L'entraînement s'est fait en deux phases. La première, dite de raisonnement <em>cold-start</em> par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au modèle d'acquérir des capacités de base en raisonnement à partir de données d'instruction générale (10%). La seconde est une phase d'apprentissage par renforcement (RL, <em>renforcement learning</em>) à haute entropie, où le modèle est encouragé à explorer des solutions diverses et variées plutôt que de converger vers une seule réponse, et à générer des complétions longues (jusqu'à 32 000 jetons), ce qui permet de développer des capacités de raisonnement qui dépassent celles du modèle enseignant.</p>",
                "size_desc": "<p>Avec 24 milliards de paramètres, ce modèle est classé parmi les modèles de taille moyenne. Il nécessite une seule carte graphiques puissante pour fonctionner. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>"
            },
            "Ministral": {
                "desc": "<p>Petit modèle multilingue conçu pour fonctionner sur un ordinateur portable sans connexion à un serveur, tout en offrant de bonnes capacités en synthèse de texte, réponses à des questions simples et utilisation d’outils.</p>",
                "fyi": "<p>Ce modèle utilise une méthode d'attention de requête groupée (GQA, grouped query attention) pour limiter le texte analysé à chaque étape de génération et gagner en vitesse et en mémoire: les temps de calculs sont réduits sans incidence sur la qualité. Le mécanisme d'attention est amélioré en appliquant des fenêtres de tailles différentes, ce qui permet de gérer de longs contextes (jusqu’à 128 000 jetons) tout en restant léger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui améliore ses performances sur des tâches multilingues.</p>",
                "size_desc": "<p>Avec ses 8 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles (entre 7 et 20 milliards de paramètres). Il peut être déployé localement sur un ordinateur assez puissant, garantissant la confidentialité des données ou hébergé sur un serveur avec une seule carte graphique pour limiter les coûts d’infrastructure.</p>"
            },
            "Mistral Large 2": {
                "desc": "<p>Grand modèle prévu pour traiter des questions et tâches complexes : par exemple génération de code, utilisation d’outils, analyse de documents longs ou compréhension précise du langage.</p>",
                "fyi": "<p>Ce modèle a été entraîné avec une forte proportion de données en code (plus de 80 langages de programmation) et de mathématiques, ce qui améliore sa capacité à résoudre des problèmes complexes et à utiliser des outils externes.</p>",
                "size_desc": "<p>Avec 123 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite un serveur équipé d’au moins une carte graphique puissante, ce qui implique un coût de fonctionnement important. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Medium 2508": {
                "desc": "<p>Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est devenu particulièrement intéressant après une mise à jour en août 2025 avec des améliorations importantes de performance générale, un ton \"amélioré\" et une meilleure capacité de chercher des informations sur Internet. </p>",
                "fyi": "<p>Ce modèle a été conçu pour offrir des performances solides à un coût inférieur à celui des modèles propriétaires ou semi-ouverts. Une attention particulière a été portée aux données d’usage professionnel pendant son entraînement. Il est particulièrement bon en comparaison à d’autres modèles de taille similaire à générer du code et réaliser des tâches mathématiques.</p>\n<p>Ce modèle a servi de base pour entraîner Magistral Medium - un modèle de raisonnement.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Medium 3.1": {
                "desc": "<p>Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est particulièrement intéressant pour des tâches de programmation ou des tâches de raisonnement, par exemple les mathématiques.</p>",
                "fyi": "<p>Ce modèle a été conçu pour offrir des performances solides à un coût inférieur à celui des modèles propriétaires ou semi-ouverts. Une attention particulière a été portée aux données d’usage professionnel pendant son entraînement. Il est particulièrement bon en comparaison à d’autres modèles de taille similaire à générer du code et réaliser des tâches mathématiques.</p>\n<p>Ce modèle a servi de base pour entraîner Magistral Medium - un modèle de raisonnement.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Saba": {
                "desc": "<p>Modèle de taille moyenne conçu pour une compréhension linguistique et culturelle fine des langues du Moyen-Orient et d’Asie du Sud, notamment l’arabe, le tamoul et le malayalam.</p>",
                "fyi": "<p>L’entraînement a porté principalement sur des textes en arabe, tamoul et malayalam. Les corpus régionaux ont été sélectionnés pour refléter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (découpage du texte en unités de base que le modèle peut traiter), une stratégie spécialisée adaptée aux langues à morphologie complexe comme l'arabe a été employée. Des optimisations visent à éviter la fragmentation excessive des mots et à maximiser la couverture du vocabulaire.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. </p>\n<p>Le modèle propose une fenêtre de contexte allant jusqu’à 128 000 jetons, adaptée à l’analyse de longs documents.</p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p>Malgré son nom, c’est un modèle de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se démarque par un respect précis des requêtes et sa capacité à utiliser des outils avancées.</p>",
                "fyi": "<p>La version 3.2 de ce modèle est optimisée pour générer des sorties structurées, notamment en JSON, tout en limitant la répétitivité et les comportements indésirables lors de longues générations. Multimodal, il traite à la fois des entrées textuelles et des images, permettant une analyse conjointe.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle est considéré comme un modèle de taille moyenne. Il peut être hébergé sur un serveur disposant d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p>Grand modèle entraîné à partir de Llama 3.1 70B. Cette version réentraînée (fine-tune) a tendance à détailler davantage et fournir des réponses plus structurées.</p>",
                "fyi": "<p>Ce modèle est issu d’un réentraînement du Llama 3.1 70B, d'où la présence de son modèle-source dans son nom ! Il introduit des améliorations grâce à l’apprentissage par renforcement avec retour humain (RLHF) et à l’algorithme REINFORCE : le modèle explore différentes réponses, reçoit des retours sous forme de récompenses, puis ajuste ses choix progressivement pour mieux répondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilisé quand on veut que le modèle s’adapte à des préférences humaines ou qu’il optimise ses réponses selon des critères spécifiques.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation notables.</p>"
            },
            "o4 mini": {
                "desc": "<p>Très grand modèle de raisonnement, adapté pour des tâches et questions scientifiques et technologiques complexes.</p>",
                "fyi": "<p>Ce modèle est très performant pour l’analyse d’images et de graphiques. Il a aussi été entraîné pour interagir avec d’autres systèmes via des appels de fonctions, ce qui rend possible son utilisation pour des cas d’usage agentiques. En tant que modèle très puissant de raisonnement, il peut notamment être utilisé pour répartir des tâches entre plusieurs modèles plus petits et/ou plus spécialisés.  Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, ce qui facilite l’analyse de longs documents.</p>",
                "size_desc": "<p>Malgré son nom et le fait que la taille exacte n’est pas connue, o4 mini est très probablement un grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Les modèles de raisonnement comme o4 mini nécessitent plus de temps pour répondre, car une phase de raisonnement précède la génération du résultat final, ce qui accroit leur consommation énergétique. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres pour générer chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "Phi-4": {
                "desc": "<p>Petit modèle multilingue, capable d’utiliser des outils et performant sur des tâches complexes comme la logique, les mathématiques et le code, tout en restant compact.</p>",
                "fyi": "<p>Ce modèle utilise tiktoken pour la tokenisation, ce qui améliore ses capacités en contexte multilingue. Il a été entraîné sur un total de 9,8 <strong>billions</strong> de jetons, dont 400 milliards proviennent spécifiquement de données synthétiques de haute qualité, le reste étant constitué de données organiques filtrées. L'entraînement s'est déroulé sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-évaluation – pendant laquelle le modèle critique et réécrit ses réponses – ainsi que l'inversion des instructions ont été utilisées pour renforcer sa compréhension des consignes et ses capacités de raisonnement.</p>",
                "size_desc": "<p>Avec 14 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles. Il peut être déployé localement sur un ordinateur suffisamment puissant, ou hébergé sur un serveur avec une seule carte graphique, ce qui réduit les coûts d’infrastructure. La fenêtre de contexte, de 16 000 jetons, peut être limitante pour l’analyse de documents très longs.</p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p>Modèle de taille moyenne spécialisé en programmation et dans l’usage d’outils externes (recherches web, interactions avec des logiciels…).</p>",
                "fyi": "<p>Ce modèle a été entraîné sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code spécialisés comme Haskell ou Racket. </p>\n<p>Grâce à ses performances en code, il est  capable de bien gérer les appels à des outils externes, ce qui est utile pour des usages agentiques.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Qwen 2.5 max 0125": {
                "desc": "<p>Très grand modèle de raisonnement spécialisé et très performant en mathématiques, code et résolution de problèmes logiques.</p>",
                "fyi": "<p>La taille exacte du modèle n’est pas connue, mais c’est très probablement un très grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>",
                "size_desc": "<p>Ce modèle propriétaire basé sur une <strong>architecture MoE à grande échelle a été</strong>entraîné sur <strong>plus de 20 billions de jetons</strong>. Il est conçu pour des tâches nécessitant plusieurs étapes de réflexion. </p>\n<p>La fenêtre de contexte va jusqu’à 32 000 jetons.</p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p>Modèle de taille moyenne multilingue.</p>",
                "fyi": "<p>Ce modèle MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activés par jeton, ce qui permet une inférence plus rapide et plus efficace. Il utilise un système appelé <em>global-batch</em> pour optimiser la répartition du travail entre les experts, afin qu'ils soient tous utilisés de manière équilibrée.</p>\n<p>Contrairement à d'autres modèles comme Qwen 2.5-MoE qui recyclent les mêmes experts à travers plusieurs couches du réseau, Qwen 3 30B A2B attribue des experts uniques à chaque couche. Concrètement, cela signifie que les experts de la première couche ne sont jamais réutilisés dans les couches suivantes - chaque niveau du modèle dispose de son propre ensemble d'experts spécialisés. Cette architecture permet à chaque expert de se concentrer exclusivement sur les tâches spécifiques à sa position dans le réseau neuronal, résultant en une spécialisation plus fine et des performances optimisées pour chaque étape du traitement de l'information.</p>",
                "size_desc": "<p>Avec 30 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. De plus l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.</p>"
            },
            "Qwen 3 32B": {
                "desc": "<p>Modèle de taille moyenne multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Ce modèle a été entraîné sur un très grand volume de données : 36 billions de jetons, en 119 langues. L'entraînement s’est fait en trois étapes. Le modèle a d'abord appris à partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont été ajoutés pour renforcer ses connaissances factuelles. Enfin, il a été exposé à un corpus spécifique pour l’aider à mieux gérer les très longs textes. Résultat : il dispose en fin d'entrainement d'une fenêtre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Qwen3 Coder 480B A35B": {
                "desc": "<p>Très grand modèle spécialisé dans la génération de code, analyse de dépôts entiers et résolution de problèmes multi-étapes. Cette version est particulièrement forte en utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale. </p>",
                "fyi": "<p>Ce modèle a été pré-entraîné sur 7,5 trillions de jetons (dont 70 % de code), et utilise un processus de post-entraînement avancée - Code RL (Hard to Solve, Easy to Verify) pour renforcer l’exécution correcte du code et Agent RL (long-horizon reinforcement learning) pour optimiser la résolution de tâches logicielles multi-tours, avec un environnement massivement parallèle (20 000 simulations en parallèle sur Alibaba Cloud).</p>",
                "size_desc": "<p>Qwen3-Coder-480B-A35B-Instruct est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une fraction des paramètres (35 B sur 480 B), ce qui réduit considérablement l'impact environnemental et le coût par rapport à un modèle dense équivalent.\nLa fenêtre de contexte atteint nativement 256 000 jetons et peut être étendue jusqu’à 1 million grâce à des techniques d’extrapolation (YaRN), ce qui est idéal pour l’analyse de bases de code volumineuses.</p>"
            },
            "qwq 32B": {
                "desc": "<p>Modèle de raisonnement de taille moyenne spécialisé et très performant en mathématiques, génération de code, et résolution de problèmes logiques.</p>",
                "fyi": "<p>Ce modèle a été entraîné avec une méthode d’apprentissage par renforcement (RL) pour optimiser la gestion des problèmes de mathématiques et des tâches de programmation. Il utilise plusieurs techniques récentes pour améliorer la qualité des réponses. Par exemple, la méthode RoPE (Rotary Position Embedding) lui permet de mieux comprendre l’ordre des mots dans un texte. La fonction d'activation SwiGLU est une manière plus efficace de gérer les calculs au sein du réseau de neurones qui aide le modèle à produire des réponses plus fiables. La méthode d'ajustement QKV (Query Key Value-biais) améliore la manière dont le modèle repère et sélectionne les informations importantes. Enfin, grâce à la méthode YaRN (Yet another RoPE extensioN method), il peut traiter de très longs textes allant jusqu’à 130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou très détaillés.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Néanmoins, les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse car une phase de raisonnement précède la génération du résultat final, ce qui augmente la consommation énergétique.</p>"
            }
        }
    },
    "header": {
        "banner": "Claude 4, GLM 4.5, GPT OSS et d’autres nouveaux modèles rejoignent l'arène…à tester !",
        "chatbot": {
            "newDiscussion": "Nouvelle discussion",
            "step": "Étape",
            "stepOne": {
                "description": "Prêtez attention au fond et à la forme puis évaluez chaque réponse",
                "title": "Que pensez-vous des réponses ?"
            },
            "stepTwo": {
                "description": "Découvrez l’impact environnemental de vos discussions avec chaque modèle",
                "title": "Les modèles sont démasqués !"
            }
        },
        "help": {
            "link": {
                "content": "Améliorer le comparateur",
                "title": "Donner mon avis sur le comparateur - ouvre une nouvelle fenêtre"
            }
        },
        "homeTitle": "Accueil - compar:IA",
        "logoAlt": "République Française",
        "menu": "Menu",
        "startDiscussion": "Commencer à discuter",
        "subtitle": "Le comparateur d’IA conversationnelles",
        "title": {
            "compar": "compar",
            "ia": "IA"
        },
        "votes": {
            "count": "{count} votes",
            "legend": "Légende",
            "objective": "Obj : {count}",
            "tooltip": "Discutez, votez et aidez-nous à atteindre cet objectif !<br /><strong>Vos votes sont importants</strong> : ils alimentent le jeu de données compar:IA mis à disposition librement pour affiner les prochains modèles sur le français.<br />Ce commun numérique contribue au meilleur <strong>respect de la diversité linguistique et culturelle des futurs modèles de langue.</strong>"
        }
    },
    "home": {
        "europe": {
            "desc": "La Lituanie, la Suède et le Danemark rejoignent la France en adoptant le comparateur dans le but d’affiner les futurs modèles d’IA dans leurs langues nationales.",
            "languages": {
                "da": "en danois",
                "fr": "en français",
                "lt": "en lituanien",
                "sv": "en suédois"
            },
            "question": "Vous souhaitez également disposer du comparateur dans votre langue ?",
            "title": "Le comparateur <span {props}>devient européen !</span>"
        },
        "faq": {
            "discover": "Découvrir les autres questions",
            "title": "Vos questions les plus courantes"
        },
        "intro": {
            "desc": "Discutez avec deux IA à l’aveugle et évaluez leurs réponses",
            "steps": {
                "a11yDesc": "1. Je discute avec deux IA anonymes : échangez aussi longtemps que vous le souhaitez. 2. Je donne mon avis : vous contribuez ainsi à l'amélioration des modèles d’IA. 3. Les modèles sont démasqués : apprenez-en plus sur les modèles d’IA et leurs caractéristiques.",
                "one": {
                    "desc": "Échangez aussi longtemps que vous le souhaitez",
                    "title": "Je discute avec deux IA anonymes"
                },
                "three": {
                    "desc": "Apprenez en plus sur les modèles d’IA et leurs caractéristiques",
                    "title": "Les modèles sont démasqués !"
                },
                "title": "Comment ça marche",
                "two": {
                    "desc": "Vous contribuez ainsi à l'amélioration des modèles d’IA",
                    "title": "Je donne mon avis"
                }
            },
            "title": "Ne vous fiez pas aux réponses <span {props}>d’une seule IA</span>",
            "tos": {
                "accept": "J'accepte les <a {linkProps}>conditions générales d’utilisation</a>",
                "error": "Vous devez accepter les modalités d'utilisation pour continuer",
                "help": "Vos données sont partagées à des fins de recherche"
            }
        },
        "origin": {
            "project": {
                "desc": "Le comparateur a été conçu et développé dans le cadre d’une start-up d’Etat portée par le ministère de la Culture et intégrée au programme <a {linkProps}>Beta.gouv.fr</a> de la Direction interministérielle du numérique (DINUM) qui aide les administrations publiques françaises à construire des services numériques utiles, simples et faciles à utiliser.",
                "title": "Qui est à l’origine du projet ?"
            },
            "team": {
                "desc": "Le comparateur est porté au sein du Ministère de la Culture par une équipe pluridisciplinaire réunissant expert en Intelligence artificielle, développeurs, chargé de déploiement, designer, avec pour mission de rendre les IA conversationnelles plus transparentes et accessibles à toutes et tous.",
                "title": "Qui sommes-nous ?"
            }
        },
        "usage": {
            "desc": "L’outil s’adresse également aux experts IA et aux formateurs pour des usages plus spécifiques",
            "educate": {
                "desc": "Utilisez le comparateur comme un support pédagogique de sensibilisation à l’IA auprès de votre public",
                "title": "Former et sensibiliser"
            },
            "explore": {
                "desc": "Consultez au même endroit toutes les caractéristiques et conditions d’utilisation des modèles",
                "title": "Explorer les modèles"
            },
            "title": "Les usages spécifiques de compar:IA",
            "use": {
                "desc": "Développeurs, chercheurs, éditeurs de modèles… accédez aux jeux de données compar:IA pour améliorer les modèles",
                "title": "Exploiter les données"
            }
        },
        "use": {
            "compare": {
                "alt": "Comparer",
                "desc": "Discutez et développez votre esprit critique en donnant votre préférence",
                "title": "Comparer les réponses de différents modèles d’IA"
            },
            "desc": "compar:IA est un outil gratuit qui permet de sensibiliser les citoyens à l’IA générative et à ses enjeux.",
            "measure": {
                "alt": "Mesurer",
                "desc": "Découvrez l’impact environnemental de vos discussions avec chaque modèle",
                "title": "Mesurer l’empreinte écologique des questions posées aux IA"
            },
            "test": {
                "alt": "Tester",
                "desc": "Testez différents modèles, propriétaires ou non, de petites et grandes tailles...",
                "title": "Tester au même endroit les dernières IA de l’écosystème"
            },
            "title": "À quoi sert compar:IA ?"
        },
        "vote": {
            "datasetAccess": "Accéder aux jeux de données",
            "desc": "L’outil s’adresse également aux experts IA et aux formateurs pour des usages plus spécifiques",
            "steps": {
                "datasets": {
                    "desc": "Toutes les questions posées et les votes sont compilés dans des jeux de données et publiés librement après anonymisation.",
                    "title": "Les jeux de données par langue"
                },
                "finetune": {
                    "desc": "À terme, les entreprises et les acteurs universitaires peuvent exploiter les jeux de données pour entrainer de nouveaux modèles plus respectueux de la diversité linguistique et culturelle.",
                    "title": "Des modèles affinés sur la langue spécifique"
                },
                "prefs": {
                    "desc": "Après discussion avec les IA, vous indiquez votre préférence pour un modèle selon des critères donnés, tels que la pertinence ou l’utilité des réponses.",
                    "title": "Vos préférences"
                }
            },
            "title": "Pourquoi votre vote est-il important ?"
        }
    },
    "models": {
        "arch": {
            "title": "Le saviez-vous ?",
            "types": {
                "dense": {
                    "title": "Architecture Dense",
                    "desc": "L’architecture dense désigne un type de réseau de neurones dans lequel chaque neurone d’une couche est connecté à tous les neurones de la couche suivante. Cela permet à tous les paramètres de la couche de contribuer au calcul de la sortie."
                },
                "matformer": {
                    "title": "Architecture Matformer",
                    "desc": "Imaginez des **poupées russes** (matryoshkas → matryoshka transformer → Matformer) : chaque bloc contient plusieurs **sous-modèles imbriqués** de tailles croissantes, partageant les mêmes paramètres. Cela permet, à chaque requête, de sélectionner un modèle de capacité adaptée, selon la mémoire ou la latence disponibles, sans avoir besoin de ré-entraîner différents modèles."
                },
                "moe": {
                    "title": "Architecture MOE",
                    "desc": "L’architecture Mixture of Experts (MoE) utilise un mécanisme de routage pour n’activer, en fonction de l’entrée, que certains sous-ensembles spécialisés (“experts”) du réseau de neurones. Cela permet de construire des modèles très grands tout en gardant un coût de calcul réduit, car seule une partie du réseau est utilisée à chaque étape."
                },
                "na": {
                    "title": "Architecture N/A",
                    "desc": "L’éditeur n’a pas rendu les informations sur l’architecture du modèle publiques."
                }
            }
        },
        "conditions": {
            "commercialUse": {
                "question": "Les usages commerciaux du modèle sont-ils autorisés ?",
                "title": "Usage commercial"
            },
            "reuse": {
                "question": "Puis-je utiliser les sorties du modèle pour en entrainer de nouveaux ?",
                "subTitle": "Vous ne pouvez pas les réutiliser pour entraîner d’autres modèles",
                "title": "Réutilisation des résultats générés"
            },
            "title": "Conditions d'utilisation",
            "types": {
                "allowed": "Autorisé",
                "conditions": "Sous conditions",
                "forbidden": "Interdit"
            }
        },
        "extra": {
            "experts": {
                "api-only": "Pour les expert·es, consultez la <a {linkProps}>site officiel du modèle</a>",
                "open-weights": "Pour les expert·es, consultez la <a {linkProps}>fiche du modèle sur Hugging Face</a>"
            },
            "impacts": "Les calculs d’impacts environnementaux reposent sur les projets <a {linkProps1}>EcoLogits</a> et <a {linkProps2}>Impact CO<sub>2</sub></a>.",
            "title": "Pour aller plus loin"
        },
        "licenses": {
            "commercial": "Licence commerciale",
            "descriptions": {
                "Apache 2.0": "Cette licence permet d'utiliser, modifier et distribuer librement, même à des fins commerciales. Outre la liberté d’utilisation, elle garantit la protection juridique en incluant une clause de non-atteinte aux brevets et la transparence : toutes les modifications doivent être documentées et sont donc traçables.",
                "CC-BY-NC-4.0": "Cette licence permet de partager et adapter le contenu à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.",
                "Gemma": "Cette licence est conçue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiées ou améliorées doivent être partagée avec la communauté sous la même licence, favorisant ainsi la collaboration et la transparence dans le développement logiciel.",
                "Jamba Open Model": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les organismes dépassant 50 millions de dollars de revenus annuels.",
                "Llama 3 Community": "Cette licence permet d'utiliser, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels et interdit la réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles concurrents, protégeant ainsi les investissements technologiques et la marque de Meta.",
                "Llama 3.1": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
                "Llama 3.3": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
                "Llama 4": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
                "Mistral AI Non-Production": "Cette licence permet de partager et adapter le contenu à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.",
                "MIT": "La licence MIT est une licence de logiciel libre permissive : elle permet à quiconque de réutiliser, modifier et distribuer le modèle, même à des fins commerciales, sous réserve d'inclure la licence d'origine et les mentions de droits d'auteur.",
                "propriétaire Anthropic": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
                "propriétaire Gemini": "Le modèle est disponible sous licence payante et accessible via l'API Gemini disponible sur les plateformes Google AI Studio et Vertex AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
                "propriétaire Liquid": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Liquid AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
                "propriétaire Mistral": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
                "propriétaire OpenAI": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
                "propriétaire xAI": "Le modèle est accessible via API xAI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise."
            },
            "name": "Licence {licence}",
            "noDesc": "Les informations de licence n'ont pas été remplies pour ce modèle.",
            "type": {
                "openSource": "Open source",
                "proprietary": "Propriétaire",
                "semiOpen": "Semi-ouvert"
            }
        },
        "list": {
            "filters": {
                "display": "Afficher les filtres",
                "editor": {
                    "legend": "Éditeur"
                },
                "license": {
                    "legend": "Licence d'utilisation"
                },
                "reset": "Effacer tous les filtres",
                "size": {
                    "labels": {
                        "L": "de 70 à 150 milliards",
                        "M": "de 20 à 70 milliards",
                        "S": "de 7 à 20 milliards",
                        "XL": "> 150 milliards",
                        "XS": "< à 7 milliards"
                    },
                    "legend": "Taille (paramètres)"
                }
            },
            "intro": "Explorez les différents modèles d'IA conversationnels disponibles, leurs caractéristiques et leurs licences.",
            "model": "modèle",
            "models": "modèles",
            "noresults": "Aucun modèle ne correspond à vos critères de recherche.",
            "title": "Découvrez les modèles",
            "triage": {
                "label": "Trier par",
                "options": {
                    "date-desc": "Date de sortie (du plus au moins récent)",
                    "name-asc": "Nom du modèle (A à Z)",
                    "org-asc": "Éditeur (A à Z)",
                    "params-asc": "Taille (du plus petit au plus grand)"
                }
            }
        },
        "names": {
            "a": "Modèle A",
            "b": "Modèle B"
        },
        "openWeight": {
            "conditions": {
                "copyleft": "Copyleft (gauche d'auteur)",
                "free": "Permissive",
                "restricted": "Sous conditions"
            },
            "descriptions": {
                "L": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des grands modèles (entre 70 et 100 milliards de paramètres).",
                "M": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des moyens modèles (entre 20 et 70 milliards de paramètres).",
                "S": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des petits modèles (entre 7 et 20 milliards de paramètres).",
                "XL": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des très grands modèles.",
                "XS": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des modèles très petits (moins de 7 milliards de paramètres)."
            },
            "tooltips": {
                "copyleft": "Une fois modifié, le modèle doit être redistribué sous la même licence que celle du modèle source.",
                "free": "Une fois modifié, le modèle peut être redistribué sous une licence différente du modèle source.",
                "openSource": "Le corpus, le code d'entraînement, et les poids de ce modèle (c’est-à-dire les paramètres appris pendant son entraînement) sont entièrement téléchargeables et modifiables par le public, lui permettant de faire fonctionner et modifier le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant qu'« open weights », notamment à cause de la nécessité de transparence du corpus d'entraînement, et rares sont les modèles qui sont considérés « open source ».",
                "openWeight": "Modèle dit « open weights » dont les poids, c’est-à-dire les paramètres appris pendant son entraînement, sont téléchargeables par le public, lui permettant de faire fonctionner le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant (principalement par rapport à la transparence du corpus d'entraînement), et rares sont les modèles qui sont considérés « open source ».",
                "params": "Les paramètres ou les poids, comptés en milliards, sont les variables, apprises par un modèle au cours de son entrainement, qui déterminent ses réponses. Plus le nombre de paramètres est important, plus il a besoin de ressources pour fonctionner.",
                "ram": "La mémoire vive stocke les données traitées par un LLM en temps réel. Plus le modèle est grand, plus il a besoin de mémoire vive pour fonctionner."
            },
            "use": {
                "attribution": "Attribution requise",
                "commercial": "Utilisation commerciale",
                "licenseType": "Type de licence",
                "modification": "Modification autorisée",
                "requiredRam": "RAM nécessaire"
            }
        },
        "parameters": "{number} mds de paramètres",
        "ram": "{min} à {max} Go",
        "release": "Sortie {date}",
        "size": {
            "descriptions": {
                "L": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
                "M": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
                "S": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
                "XL": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.",
                "XS": "Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte."
            },
            "estimated": "Taille estimée ({size})",
            "title": "Taille"
        }
    },
    "modes": {
        "big-vs-small": {
            "altLabel": "David contre Goliath",
            "description": "Un petit modèle contre un grand, les deux tirés au hasard",
            "label": "David contre Goliath",
            "title": "Mode David contre Goliath"
        },
        "custom": {
            "altLabel": "Sélection manuelle",
            "description": "Reconnaîtrez-vous les deux modèles que vous avez choisis ?",
            "label": "Sélection manuelle",
            "title": "Mode Sélection"
        },
        "random": {
            "altLabel": "Modèles aléatoires",
            "description": "Deux modèles choisis au hasard parmi toute la liste",
            "label": "Aléatoire",
            "title": "Mode Aléatoire"
        },
        "reasoning": {
            "altLabel": "Modèles avec raisonnement",
            "description": "Deux modèles tirés au hasard parmi ceux optimisés pour des tâches complexes",
            "label": "Raisonnement",
            "title": "Mode Raisonnement"
        },
        "small-models": {
            "altLabel": "Modèles frugaux",
            "description": "Deux modèles tirés au hasard parmi ceux de plus petite taille",
            "label": "Frugal",
            "title": "Mode Frugal"
        }
    },
    "product": {
        "comparator": {
            "challenges": {
                "bias": {
                    "desc": "Mettre en avant les biais de l'IA liés à la sous-représentation des données non anglophones dans les modèles et sensibiliser à leurs conséquences.",
                    "title": "Biais culturels et linguistiques"
                },
                "impacts": {
                    "desc": "Révéler les effets écologiques de l'IA générative, encore largement méconnus du grand public.",
                    "title": "Impact environnemental"
                },
                "pluralism": {
                    "desc": "Assurer aux citoyens l'accès à une diversité de modèles d'IA afin qu'ils puissent faire des choix éclairés et développer un regard critique sur ces technologies.",
                    "title": "Pluralisme des modèles"
                },
                "thinking": {
                    "desc": "Inciter au questionnement critique sur la place de l’IA générative dans les pratiques personnelles et professionnelles (éducation, travail).",
                    "title": "Esprit critique et questions sociétales"
                },
                "title": "L’application développée répond à plusieurs enjeux"
            },
            "cta": "Accéder au comparateur",
            "europe": {
                "adventure": "A partir de l’automne, la Lituanie, la Suède et le Danemark rejoignent l’aventure !",
                "catch": "Vous souhaitez disposer du comparateur dans votre langue ?",
                "desc": "Le comparateur sera mis à disposition de leurs citoyens dans leurs langues nationales. L’objectif est de créer des jeux de données de préférence afin d’améliorer les futurs modèles d’IA dans ces langues européennes.",
                "title": "Le comparateur <span {props}>devient européen</span> !"
            },
            "tabLabel": "Le comparateur",
            "title": "Le comparateur permet de créer des <span {props}>jeux de données</span> de préférence centrés sur des <span {props}>usages réels</span> exprimés dans les <span {props}>langues européennes</span>."
        },
        "faq": {
            "tabLabel": "FAQ"
        },
        "history": {
            "tabLabel": "Historique du projet"
        },
        "partners": {
            "academy": {
                "catch": "Vous menez un projet de recherche et avez des suggestions ou besoin de précision sur la démarche et/ou les jeux de données produits ?",
                "desc": "Nous avons à coeur que les jeux de données générés alimentent des travaux de recherche multidisciplinaires mêlant sciences humaines et sociales et data science.",
                "title": "Partenaires académiques"
            },
            "diffusion": {
                "catch": "Vous souhaitez utiliser le comparateur pour répondre à un besoin métier ?",
                "cta": "Dites nous en plus",
                "desc": "Nous créons un réseau de partenaires intégrant le comparateur dans leur offre de services et de formation.",
                "title": "Partenaires de diffusion"
            },
            "institution": {
                "title": "Partenaires institutionnels"
            },
            "services": {
                "desc": "Les calculs d’impacts environnementaux reposent sur les produits ci dessus.",
                "title": "Services mis à contribution"
            },
            "tabLabel": "Partenaires"
        },
        "problem": {
            "alignment": {
                "alignment": {
                    "a": "L'alignement intervient après l'étape de pré-entraînement d'un modèle de langage, comme une étape de « finition » ou de « polissage ». Lors de son pré-entrainement, le modèle apprend à prédire le mot suivant et devient capable de générer du texte cohérent.",
                    "b": "L’étape d’alignement consiste à apprendre au modèle à mieux répondre aux besoins humains, c’est à dire à le rendre plus <strong>pertinent</strong> (le modèle répond « mieux » aux questions), <strong>honnête</strong> (capacité à assumer « qu’il ne sait pas répondre » quand il n’y a pas suffisamment de données), et <strong>inoffensif</strong> (éviter de générer des contenus dangereux ou inappropriés).",
                    "c": "<strong>Sans alignement, un LLM pourrait être techniquement compétent mais difficile à utiliser en pratique, car il ne comprendrait pas vraiment ce qu'on attend de lui dans une conversation.</strong>",
                    "title": "L’alignement, étape décisive d’instruction du modèle"
                },
                "datasets": {
                    "a": "L'alignement utilise des données très spécifiques, spécialement créées pour enseigner au modèle comment « bien » se comporter.",
                    "b": "Les <strong>données de préférence</strong> constituent un type particulier de données d’alignement, aux côtés des <strong>données de démonstration</strong> (exemples de conversations entre humains et assistants IA, rédigées par des annotateurs experts selon des consignes précises de ton et de style), des <strong>données de sécurité</strong> (exemples spécifiques enseignant au modèle à éviter les contenus dangereux en montrant comment refuser les demandes problématiques) ou des <strong>données spécialisées</strong> couvrant des domaines spécifiques (médecine, droit, éducation…).",
                    "c": "Les données de préférence présentent plusieurs réponses possibles à une même question, classées par ordre de qualité par des évaluateurs humains : les utilisateurs indiquent quelle réponse est la meilleure selon des critères donnés, telles que la pertinence, l’utilité, la nocivité. Une fois constitués, ces jeux de données sont utilisés pour entraîner les modèles en les ajustant selon les préférences exprimées par les utilisateurs.",
                    "title": "Des jeux de données spécifiques"
                },
                "desc": "L'alignement : une technique de réduction des biais qui repose sur la collecte des préférences d’utilisateurs",
                "diversity": {
                    "a": "Pour refléter la diversité des cultures et des langues dans les résultats générés par les modèles, <strong>les jeux de données d’alignement doivent inclure une variété de langues</strong>, de contextes et d’exemples issus de tâches courantes des utilisateurs. La diversification des données d'alignement permet d’améliorer à terme les performances d’un modèle à double titre :",
                    "b": "D'une part, elle <strong>réduit les biais culturels</strong> en évitant qu'une seule perspective - souvent anglo-saxonne - domine les réponses de l'IA. Le modèle apprend ainsi à reconnaître qu'il existe plusieurs façons valides d'aborder une même question selon le contexte culturel.",
                    "c": "D'autre part, cette exposition à la diversité de langues et de cultures favorise l’adaptation des réponses à des contextes spécifiques : un utilisateur français recevra des conseils adaptés au système français, tandis qu'un utilisateur danois obtiendra des informations correspondant à son contexte national.",
                    "d": "Le résultat est un modèle d’IA conversationnelle plus inclusif, capable de tenir compte des différentes cultures.",
                    "title": "Diversifier les données pour réduire les biais"
                },
                "english": {
                    "a": "Les données de préférence sont couteuses à produire car <strong>elles nécessitent du travail humain qualifié pour chaque exemple</strong>. Des plateformes telles que https://chat.lmsys.org/ permettent de constituer ces jeux de données de préférence mais peu d’utilisateurs s’en servent dans leur langue d’origine.",
                    "b": "Les jeux de données de préférence sont rares, voire inexistants dans les langues européennes. La part des questions posées en français dans le jeu de données de LMSYS est par exemple inférieure à 1%.",
                    "c": "comparIA est un exemple de dispositif permettant de collecter des conversations dans de multiples langues, incluant des références culturelles spécifiques à chaque région ou pays : tâches courantes, traditions culinaires locales, systèmes éducatifs, références historiques ou littéraires, etc.",
                    "title": "Peu de données de préférence en langues européennes"
                },
                "title": "Comment réduire les biais culturels et linguistiques de ces modèles ?"
            },
            "diversity": {
                "diversity": {
                    "desc": "Ces biais peuvent aussi se traduire par des réponses partielles voire incorrectes négligeant la diversité des langues et des cultures, notamment européennes.",
                    "title": "Diversités culturelles et linguistiques négligées"
                },
                "english": {
                    "desc": "Les IA conversationnelles reposent sur des grands modèles de langage (LLM) entraînés principalement sur des données en anglais, ce qui crée des biais linguistiques et culturels dans les résultats qu'ils produisent.",
                    "title": "Données d’entrainement majoritairement en anglais"
                },
                "stereotypes": {
                    "desc": "Les systèmes d’IA conversationnelle donnent l’impression de parler toutes les langues mais les résultats qu’ils génèrent sont parfois stéréotypés ou discriminants.",
                    "title": "Réponses stéréotypées"
                }
            },
            "tabLabel": "Le problème initial",
            "title": "Les modèles d’IA conversationnelles respectent-ils la <span {props}>diversité</span> des langues européennes ?"
        },
        "title": "Tout savoir sur le comparateur"
    },
    "ranking": {
        "desc": "Découvrez comment les meilleurs modèles d’IA se positionnent à travers leur <strong>score de satisfaction</strong> issus des votes de la communauté compar:IA. Pour en savoir plus, consultez <a {linkProps}>notre méthodologie de classement</a>.",
        "graphs": {
            "title": "Graphiques"
        },
        "table": {
            "data": {
                "cols": {
                    "consumption_wh": "Énergie<br>(1000 tokens)",
                    "elo": "Score satisfaction",
                    "license": "Licence",
                    "name": "Modèle",
                    "organisation": "Organisation",
                    "rank": "Rang",
                    "release": "Date sortie",
                    "size": "Taille<br>(paramètres actifs)",
                    "total_votes": "Total votes",
                    "trust_range": "Confiance (±)"
                }
            },
            "lastUpdate": "Mise à jour le {date}",
            "search": "Rechercher un modèle",
            "totalModels": "Total modèles :",
            "totalVotes": "Total votes :"
        },
        "title": "Classement des modèles"
    },
    "reveal": {
        "equivalent": {
            "co2": {
                "label": "CO<sub>2</sub> émis",
                "tooltip": "Le CO<sub>2</sub> émis équivaut aux émissions de dioxyde de carbone produites par l’énergie utilisée pour faire fonctionner le modèle. Elle traduit l'impact environnemental lié à la consommation énergétique. Le calcul d’équivalence Wattheures/CO<sub>2</sub> diffère selon le mix énergétique de chaque pays. Or, les serveurs utilisés pour l’inférence des modèles ne sont pas tous localisés en France. Ainsi, le calcul d’équivalence repose sur la moyenne mondiale du taux d’émissions de CO<sub>2</sub> par énergie consommée."
            },
            "lightbulb": {
                "label": "ampoule LED",
                "tooltip": "Donnée calculée sur la base de consommation d’une ampoule LED standard de 5 W (E14)"
            },
            "streaming": {
                "label": "vidéos en ligne",
                "tooltip": "Donnée calculée selon l’impact carbone d’une heure de vidéo en ligne en haute définition, sur une télévision, en connexion wifi (source <a {linkProps}>ADEME</a>)"
            },
            "title": "Ce qui correspond à :"
        },
        "feedback": {
            "description": "Faites découvrir compar:IA en partageant les modèles d’IA avec lesquels vous avez échangé ! Seuls les noms et l’impact énergétique de la discussion seront visibles via ce lien, sans accès aux messages échangés.",
            "example": "Exemple de partage de résultat",
            "moreOnVotes": "En savoir plus sur les votes",
            "shareResult": "Partager votre résultat"
        },
        "impacts": {
            "energy": {
                "label": "énergie conso.",
                "tooltip": "Mesurée en wattheures, l’énergie consommée représente l'électricité utilisée par le modèle pour traiter une requête et générer la réponse correspondante. Plus un modèle est grand (en milliards de paramètres), plus il faut d'énergie pour produire un token."
            },
            "size": {
                "count": "milliards param.",
                "estimated": "(est.)",
                "label": "taille du modèle",
                "quantized": "(quantisé)"
            },
            "title": "Impact énergétique de la discussion",
            "tokens": {
                "label": "taille du texte",
                "tokens": "jetons",
                "tooltip": "L’IA analyse et génère des phrases à partir de mots ou de parties de mots d’à peu près quatre lettres, cette unité de texte est appelée token (« jeton »). Plus un texte est long, plus le nombre de tokens est grand."
            }
        }
    },
    "seo": {
        "desc": "compar:IA est un outil permettant de comparer à l’aveugle différents modèles d'IA conversationnelle pour sensibiliser aux enjeux de l'IA générative (biais, impact environmental) et constituer des jeux de données de préférence en français.",
        "title": "compar:IA, le comparateur d'IA conversationnelles",
        "titles": {
            "accessibilite": "Déclaration d’accessibilité",
            "arene": "Discussion",
            "comparator": "Le comparateur",
            "datasets": "Jeux de données",
            "donnees-personnelles": "Politique de confidentialité",
            "duel": "Atelier Duel de l’IA",
            "faq": "FAQ",
            "history": "Historique du projet",
            "home": "Accueil",
            "mentions-legales": "Mentions légales",
            "modalites": "Modalités d’utilisation",
            "modeles": "Liste des modèles",
            "news": "Nouveautés",
            "partners": "Partenaires",
            "problem": "Le problème initial",
            "product": "Produit et partenaires",
            "ranking": "Classement",
            "share": "Mon bilan"
        }
    },
    "vote": {
        "bothEqual": "Les deux se valent",
        "choices": {
            "altText": "{choice} pour le modèle {model}",
            "negative": {
                "incorrect": "Incorrecte",
                "instructions-not-followed": "Instructions non suivies",
                "question": "Pourquoi la réponse ne convient-elle pas ?",
                "superficial": "Superficielle"
            },
            "positive": {
                "clear-formatting": "Mise en forme claire",
                "complete": "Complète",
                "creative": "Créative",
                "question": "Qu'avez-vous apprécié dans la réponse ?",
                "useful": "Utile"
            }
        },
        "comment": {
            "add": "Ajouter des commentaires",
            "placeholder": "Vous pouvez ajouter des précisions sur cette réponse du modèle {model}"
        },
        "dislike": {
            "label": "je n'apprécie pas",
            "selectedLabel": "je n'apprécie pas (sélectionné)"
        },
        "introA": "Avant de découvrir l’identité des modèles, nous avons besoin de votre préférence.",
        "introB": "Elle permet d'enrichir les jeux de données compar:IA dont l’objectif est d’affiner les futurs modèles d’IA sur le français",
        "like": {
            "label": "j'apprécie",
            "selectedLabel": "j'apprécie (sélectionné)"
        },
        "qualify": {
            "addDetails": "Ajouter des détails",
            "placeholder": "Les réponses du modèle {model} sont...",
            "question": "Comment qualifiez-vous ses réponses ?"
        },
        "title": "Quel modèle d’IA préférez-vous ?",
        "yours": "Votre vote"
    },
    "welcome": {
        "errors": "Les IA peuvent faire des erreurs : nous vous encourageons à vérifier les informations communiquées.",
        "go": "C'est parti",
        "privacy": "Ne communiquez pas d’informations personnelles comme votre nom, prénom ou adresse.",
        "title": "Bienvenue sur compar:IA !",
        "tos": {
            "desc": "Les conversations et les préférences que vous exprimez sur compar:IA sont utilisées de manière anonyme pour constituer des jeux de données représentatifs des langues et usages européens, afin de réduire les biais culturels et proposer de futurs modèles d’IA plus inclusifs.",
            "moreInfos": "En savoir plus sur le projet."
        },
        "use": "N’utilisez pas l’arène à des fins illégales ou nuisibles."
    },
    "words": {
        "back": "Retour",
        "close": "Fermer",
        "loading": "Chargement",
        "NA": "N/A",
        "random": "Aléatoire",
        "regenerate": "Regénérer",
        "reset": "Réinitialiser",
        "restart": "Recommencer",
        "retry": "Recommencer",
        "search": "Rechercher",
        "send": "Envoyer",
        "tooltip": "Infobulle",
        "validate": "Valider"
    }
}
