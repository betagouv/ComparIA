{
  "a11y": {
    "externalLink": "{text}"
  },
  "seo": {
    "title": "compar:IA, le comparateur d'IA conversationnelles",
    "desc": "compar:IA est un outil permettant de comparer à l’aveugle différents modèles d'IA conversationnelle pour sensibiliser aux enjeux de l'IA générative (biais, impact environmental) et constituer des jeux de données de préférence en français.",
    "titles": {
      "home": "Accueil",
      "product": "Produit et partenaires",
      "ranking": "Classement",
      "modeles": "Liste des modèles",
      "datasets": "Jeux de données",
      "comparator": "Le comparateur",
      "problem": "Le problème initial",
      "history": "Historique du projet",
      "faq": "FAQ",
      "partners": "Partenaires",
      "news": "Nouveautés",
      "mentions-legales": "Mentions légales",
      "modalites": "Modalités d’utilisation",
      "donnees-personnelles": "Politique de confidentialité",
      "accessibilite": "Déclaration d’accessibilité",
      "arene": "Discussion",
      "share": "Mon bilan"
    }
  },
  "header": {
    "title": {
      "compar": "compar",
      "ia": "IA"
    },
    "subtitle": "Le comparateur d’IA conversationnelles",
    "homeTitle": "Accueil - compar:IA",
    "logoAlt": "République Française",
    "startDiscussion": "Commencer à discuter",
    "help": {
      "link": {
        "title": "Donner mon avis sur le comparateur - ouvre une nouvelle fenêtre",
        "content": "Nous aider à améliorer compar:IA"
      }
    },
    "banner": "Le comparateur est désormais disponible en lituanien 🇱🇹 en suédois 🇸🇪 et en danois 🇩🇰 !",
    "votes": {
      "count": "{count} votes",
      "objective": "Obj : {count}",
      "legend": "Légende",
      "tooltip": "Discutez, votez et aidez-nous à atteindre cet objectif !<br /><strong>Vos votes sont importants</strong> : ils alimentent le jeu de données compar:IA mis à disposition librement pour affiner les prochains modèles sur le français.<br />Ce commun numérique contribue au meilleur <strong>respect de la diversité linguistique et culturelle des futurs modèles de langue.</strong>"
    },
    "chatbot": {
      "step": "Étape",
      "stepOne": {
        "title": "Que pensez-vous des réponses ?",
        "description": "Prêtez attention au fond et à la forme puis évaluez chaque réponse"
      },
      "stepTwo": {
        "title": "Les modèles sont démasqués !",
        "description": "Découvrez l’impact environnemental de vos discussions avec chaque modèle"
      },
      "newDiscussion": "Nouvelle discussion"
    },
    "menu": "Menu"
  },
  "footer": {
    "backHome": "Retour à l'accueil du site - compar:IA",
    "helpUs": "Aidez-nous à améliorer ce service !",
    "writeUs": "Si vous rencontrez un problème ou si vous avez un commentaire sur le comparateur, n'hésitez pas à nous écrire <a {linkProps}>via ce formulaire</a>, nous lisons tous vos messages.<br />Merci !",
    "links": {
      "legal": "Mentions légales",
      "tos": "Modalités d'utilisation",
      "privacy": "Politique de confidentialité",
      "accessibility": "Accessibilité : non conforme",
      "sources": "Code source"
    },
    "license": {
      "mention": "Sauf mention explicite de propriété intellectuelle détenue par des tiers, les contenus de ce site sont proposés sous <a {linkProps}>licence etalab-2.0</a>",
      "linkTitle": "Licence etalab - nouvelle fenêtre"
    }
  },
  "general": {
    "legal": {
      "title": "Mentions légales",
      "editorTitle": "Éditeur",
      "editorDesc": "Ce site est édité par le Ministère de la culture, 182, rue Saint-Honoré 75001 Paris",
      "directorTitle": "Directeur de la publication",
      "directorDesc": "Monsieur Romain Delassus, chef du service du numérique du Ministère de la Culture",
      "hostingTitle": "Hébergement du site",
      "hostingDesc": "Ce site est hébergé par OVH SAS (<a {linkProps}>https://www.ovh.com</a>) dont le siège social est situé au 2 rue Kellermann - 59100 Roubaix - France.",
      "a11yTitle": "Accessibilité",
      "a11yDesc": "La conformité aux normes d’accessibilité numérique est un objectif ultérieur mais nous tâchons de rendre ce site accessible à toutes et à tous.",
      "reportTitle": "Signaler un dysfonctionnement",
      "reportA11y": "Si vous rencontrez un défaut d’accessibilité vous empêchant d’accéder à un contenu ou une fonctionnalité du site, merci de nous en faire part.",
      "reportDesc": "Si vous n’obtenez pas de réponse rapide de notre part, vous êtes en droit de faire parvenir vos doléances ou une demande de saisine au Défenseur des droits.",
      "reportA11yDesc": "Pour en savoir plus sur la politique d’accessibilité numérique de l’État : <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
      "securityTitle": "Sécurité",
      "securityCertif": "Le site est protégé par un certificat électronique, matérialisé pour la grande majorité des navigateurs par un cadenas. Cette protection participe à la confidentialité des échanges.",
      "securityNoMail": "En aucun cas les services associés à la plateforme ne seront à l’origine d’envoi de courriels pour demander la saisie d’informations personnelles.",
      "sources": "Sauf mention contraire, tous les textes de ce site sont sous <a {etalabLinkProps}>licence Etalab Open 2.0</a>. Le code source de cette application est librement réutilisable et accessible sur <a {githubLinkProps}>GitHub</a>."
    },
    "tos": {
      "title": "Modalités d’utilisation",
      "scopeTitle": "1. Champ d’application",
      "scopeDesc": "L’accès à la plateforme est gratuit, sans inscription et entraîne l’application de conditions spécifiques, listées dans les présentes modalités d’utilisation.",
      "defsTitle": "2. Définitions",
      "defsUser": "« Utilisateur » désigne toute personne physique consultant la plateforme et qui bénéficie de ses services.",
      "defsEditor": "« Éditeur » désigne le Service du numérique du Ministère de la Culture.",
      "defsPlatform": "« Plateforme » désigne le site web qui rend les services accessibles.",
      "defsModels": "« Modèles » désigne les grands modèles de langages (LLM) réutilisés dans le cadre de leur licence d’utilisation par la plateforme pour répondre à ses finalités.",
      "defsServices": "« Services » désigne les fonctionnalités offertes par la plateforme pour répondre à ses finalités.",
      "descTitle": "3. Description de la plateforme",
      "descEditor": "Édité par le Service du numérique du Ministère de la Culture, le comparateur est une plateforme de comparaison des modèles conversationnels adressée au grand public dans le but (1) de sensibiliser les citoyens aux grands modèles de langage (LLMs), (2) de collecter les préférences des utilisateurs pour constituer des jeux de données d’alignement.",
      "descUse": "L’utilisateur ou l’utilisatrice pose une question en français et obtient des réponses de deux grands modèles de langages (LLM) anonymes. Il ou elle vote pour le modèle qui fournit la réponse qu’il préfère et se voit alors révélée l’identité des modèles. Ce dispositif de production participative inspiré de la plateforme <a {linkProps}>« chatbot arena » (LMSYS)</a> permet de constituer des jeux de données de préférences humaines sur des tâches réelles, en français, utilisables pour l’alignement des modèles.",
      "descDatasets": "Ces jeux de données seront rendus accessibles sous licence ouverte, notamment pour favoriser des usages de recherche.",
      "featuresTitle": "4. Fonctionnalités",
      "featuresDesc": "Afin de répondre au double objectif de sensibiliser les citoyens aux grands modèles de langage et collecter les préférences des utilisateurs et utilisatrices, les services rendus par la plateforme sans restriction d’accès sont les suivants :",
      "featuresDescMore": "Une interface humain-machine permettant de dialoguer simultanément avec deux modèles conversationnels et de voter pour la réponse préférée.",
      "featuresModels": "Les modèles intégrés à la plateforme sont déployés sur les serveurs d’inférence des différents partenaires (Scaleway, OVH, Hugging Face, Google Cloud, Mistral Ai). Les conditions de standardisation d’inférence sont renseignées sur la plateforme pour garantir la transparence d’utilisation des modèles.",
      "featuresModelsMore": "Une interface de comparaison des modèles.",
      "featuresVote": "A l’issue du parcours de vote, l’utilisateur peut consulter la liste des modèles intégrés au comparateur et accéder à une liste d’informations sur ces modèles. Les informations documentant les modèles sont sourcées.",
      "featuresVoteMore": "Partage et mise à disposition des jeux de données issus de la collecte des préférences des utilisateurs.",
      "featuresDatasets": "Le service recueille les données de dialogue et de préférence des utilisateurs. Les jeux de données partagés comprendront les questions de l’utilisateur, les réponses des deux modèles, le vote et les préférences de l’utilisateur.",
      "featuresDatasetsMore": "L’éditeur se réserve le droit de distribuer sous licence ouverte 2.0 les données de dialogue et de préférence de l’utilisateur. Le jeu de données est diffusé sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "respTitle": "5. Responsabilités",
      "respUser": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme.",
      "respLegal": "La plateforme n’a pas vocation à être utilisée pour générer des contenus illicites ou contraires à l’ordre public et plus généralement toute génération contrevenant au cadre juridique en vigueur.",
      "respLegalMore": "A cet égard, l’utilisateur ne saisit pas dans l’invite des contenus ou informations contraires aux dispositions légales et réglementaires en vigueur.",
      "respPrivacy": "Les données saisies par l’utilisateur sur la plateforme ayant vocation à être mis à disposition, il ou elle s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
      "respPrivacyMore": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s’assurerde l’anonymisation les données de dialogue avant leur mise à disposition.",
      "respEditor": "De manière générale, l’éditeur se dégage de toute responsabilité en cas d’utilisationnon-conforme aux modalités d’utilisation.",
      "licenceTitle": "6. Code et licences",
      "licenceCode": "Le code source de la plateforme est libre et disponible ici : <a {linkProps}>https://github.com/betagouv/languia</a>",
      "licenceLLM": "Les LLM utilisés pour alimenter les services sont régis par les licences suivantes :",
      "licenceLLMModel": "Modèle d’IA conversationnelle",
      "licenceLLMNoticeLink": "Lien vers la notice des modèles",
      "licenceLLMLicence": "Licence",
      "licenceLLMUnavailable": "Non disponible",
      "licenceLLMEvolution": "La liste des modèles de langage intégrés à la plateforme est susceptible d’évoluer au cours du temps et est mise à jour à chaque modification.",
      "dispoTitle": "7. Disponibilité des services",
      "dispoDesc": "La plateforme est accessible, sauf cas de force majeure ou d’évènement hors de contrôle de son éditeur.",
      "dispoRight": "L’éditeur se réserve le droit de suspendre, d&#39;interrompre ou de limiter, sans avis préalable, l&#39;accès à tout ou partie des services, notamment pour des opérations de maintenance et de mises à jour nécessaires au bon fonctionnement du service et des matériels afférents, ou pour toute autre raison, notamment technique.",
      "dispoWarranty": "Il n’est pas garanti que le service soit exempt d’anomalies ou erreurs. Le service est donc mis à disposition sans garantie sur sa disponibilité et ses performances.",
      "dispoResp": "A ce titre, l’éditeur ne saurait être tenu responsable des pertes ou préjudices, de quelque nature qu’ils soient, qui pourraient être causés à la suite d’un dysfonctionnement ou une indisponibilité du service. De telles situations n&#39;ouvriront droit à aucune compensation financière.",
      "evoTitle": "8. Évolution des modalités d'utilisation",
      "evoDesc": "Les modalités d’utilisation peuvent être modifiées ou complétées à tout moment, sans préavis, en fonction des modifications apportées aux services, de l’évolution de la législation ou pour tout autre motif jugé nécessaire.",
      "evoDescMore": "Ces modifications et mises à jour s’imposent à l’utilisateur ou l’utilisatrice qui doit, en conséquence, se référer régulièrement à cette rubrique pour vérifier les modalités générales en vigueur.",
      "contactTitle": "9. Contact",
      "contactDesc": "Pour toute question sur le service, vous pouvez écrire à <a {linkProps}>contact@comparia.beta.gouv.fr</a>."
    },
    "privacy": {
      "title": "Politique de confidentialité",
      "desc": "Le service est édité par le service du numérique du ministère de la Culture.",
      "cookiesTitle": "Cookies déposés et consentement",
      "cookiesDesc": "Ce site dépose un petit fichier texte (un « cookie ») sur votre ordinateur lorsque vous le consultez. Cela nous permet de mesurer le nombre de visites et de comprendre quelles sont les pages les plus consultées.",
      "cookiesDescMore": "Vous pouvez vous opposer au suivi de votre navigation sur ce site web. Cela protégera votre vie privée, mais empêchera également le propriétaire d'apprendre de vos actions et de créer une meilleure expérience pour vous et les autres utilisateurs.",
      "cookiesBannerTitle": "Ce site n’affiche pas de bannière de consentement aux cookies, pourquoi ?",
      "cookiesBannerDesc": "C’est vrai, vous n’avez pas eu à cliquer sur un bloc qui recouvre la moitié de la page pour dire que vous êtes d’accord avec le dépôt de cookies — même si vous ne savez pas ce que ça veut dire !",
      "cookiesBannerNoNeed": "Rien d’exceptionnel, pas de passe-droit lié à un .gouv.fr. Nous respectons simplement la loi, qui dit que certains outils de suivi d’audience, correctement configurés pour respecter la vie privée, sont exemptés d’autorisation préalable.",
      "cookiesBannerTools": "Nous utilisons pour cela <a {matomoLinkProps}>Matomo</a>, un outil <a {libreLinkProps}>libre</a>, paramétré pour être en conformité avec la <a {cnilLinkProps}>recommandation « Cookies »</a> de la CNIL. Cela signifie que votre adresse IP, par exemple, est anonymisée avant d’être enregistrée. Il est donc impossible d’associer vos visites sur ce site à votre personne.",
      "dataAccessTitle": "Je contribue à enrichir vos données, puis-je y accéder ?",
      "dataAccessDesc": "Bien sûr ! Les statistiques d’usage du site sont disponibles en accès libre sur <a {linkProps}>stats.beta.gouv.fr</a>.",
      "dataAccessDatasets": "Les données de dialogue et de préférence de l’utilisateur sont distribuées sous la Licence Ouverte 2.0 d'Etalab sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "privacyTitle": "Traitons-nous des données à caractère personnel ?",
      "privacyDesc": "Le service ne traite pas de données à caractère personnel telles que définies par la CNIL, à savoir toute information relative à une personne physique susceptible d'être identifiée, directement ou indirectement.",
      "privacyData": "Les données collectées sur le site sont les suivantes :",
      "privacyDataArena": "Données relatives aux conversations des utilisateurs avec les modèles : questions posées par les utilisateurs, réponses des modèles et préférence exprimée par l’utilisateur sur les deux modèles",
      "privacyDataForm": "Données relatives au questionnaire « Nous aider à améliorer compar:IA ».",
      "privacyResp": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme. En acceptant les <a {linkProps}>modalités d’utilisation</a>, l’utilisateur ou l’utilisatrice s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
      "dataUseTitle": "Quels sont les traitements réalisés sur les données de conversation ?",
      "dataUseDesc": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s’assurer de l’anonymisation des données de dialogue avant leur mise à disposition publique.",
      "dataTimeTitle": "Pendant combien de temps conservons-nous ces données ?",
      "dataTimeDesc": "Les données relatives aux utilisateurs et à leurs conversations avec les modèles sont conservées à compter de l’enregistrement du vote de préférence.",
      "dataRespTitle": "Qui est responsable du traitement des données ?",
      "dataRespDesc": "Le service du numérique du ministère de la Culture est le responsable du traitement de vos données à caractère personnel.",
      "dataExtraTitle": "Qui nous aide à traiter les données ?",
      "dataExtraHost": "Sous-traitant : OVH",
      "dataExtraCountry": "Pays destinataire : France",
      "dataExtraWhat": "Traitement réalisé : Hébergement",
      "dataExtraWarranty": "Garanties : <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>"
    },
    "a11y": {
      "disclaimer": "<strong>compar:IA</strong> s’engage à rendre ses services numériques accessibles, conformément à l’article 47 de la loi n° 2005-102 du 11 février 2005.",
      "title": "Déclaration d’accessibilité",
      "desc": "Cette déclaration d’accessibilité s’applique au site <strong>comparia.beta.gouv.fr</strong>.",
      "stateTitle": "État de conformité",
      "stateDesc": "Le site comparia.beta.gouv.fr est non conforme avec le RGAA 4.1. Le site n’a <strong>pas encore été audité</strong>. Il a cependant été conçu pour être accessible au plus grand nombre. Vous devriez donc pouvoir :",
      "stateNavigate": "naviguer sur toutes les pages du site en utilisant un clavier",
      "stateScreenReader": "consulter le site web avec un lecteur d’écran.",
      "statePrefs": "adapter le site à votre préférences (taille de la police, zoom écran, changement de typographie…) sans perte de contenu",
      "improveTitle": "Amélioration et contact",
      "improveDesc": "Si vous n’arrivez pas à accéder à un contenu ou à un service, vous pouvez contacter le responsable de beta.gouv.fr pour être orienté·e vers une alternative accessible ou obtenir le contenu sous une autre forme.",
      "improveMail": "E-mail : <a {linkProps}>contact@beta.gouv.fr</a>",
      "improveAdress": "Adresse : DINUM, 20 avenue de Ségur 75007 Paris",
      "improveDelay": "Nous essayons de répondre dans les 2 jours ouvrés.",
      "remedyTitle": "Voie de recours",
      "remedyDesc": "Cette procédure est à utiliser dans le cas suivant : vous avez signalé au responsable du site internet un défaut d’accessibilité qui vous empêche d’accéder à un contenu ou à un des services du portail et vous n’avez pas obtenu de réponse satisfaisante.",
      "remedyList": "Vous pouvez :",
      "remedyAdvocate": "Écrire un message au <a {linkProps}>Défenseur des droits</a>",
      "remedyDelegateAdvocate": "Contacter le délégué du <a {linkProps}>Défenseur des droits dans votre région</a>",
      "remedyAdvocateAdress": "Envoyer un courrier par la poste (gratuit, ne pas mettre de timbre) : Défenseur des droits - Libre réponse 71120 75342 Paris CEDEX 07"
    }
  },
  "welcome": {
    "title": "Bienvenue dans compar:IA !",
    "goodPractices": "Voici quelques bonnes pratiques :",
    "errors": "Les IA peuvent faire des erreurs : nous vous encourageons à vérifier les informations communiquées",
    "privacy": "Ne communiquez pas d'informations personnelles comme votre nom, prénom ou adresse",
    "use": "N'utilisez pas le comparateur à des fins illégales ou nuisibles",
    "go": "C'est parti"
  },
  "home": {
    "intro": {
      "title": "Ne vous fiez pas aux réponses <span {props}>d’une seule IA</span>",
      "desc": "Discutez avec deux IA à l’aveugle et évaluez leurs réponses",
      "tos": {
        "accept": "J'accepte les <a {linkProps}>conditions générales d’utilisation</a>",
        "help": "Les données sont partagées à des fins de recherche",
        "error": "Vous devez accepter les modalités d'utilisation pour continuer"
      },
      "steps": {
        "title": "Comment ça marche",
        "a11yDesc": "1. Je discute avec deux IA anonymes : échangez aussi longtemps que vous le souhaitez. 2. Je donne mon avis : vous contribuez ainsi à l'amélioration des modèles d’IA. 3. Les modèles sont démasqués : apprenez-en plus sur les modèles d’IA et leurs caractéristiques.",
        "one": {
          "title": "Je discute avec deux IA anonymes",
          "a": "Échangez aussi longtemps que",
          "b": "vous le souhaitez"
        },
        "two": {
          "title": "Je donne mon avis",
          "a": "Vous contribuez ainsi à",
          "b": "l'amélioration des modèles d’IA"
        },
        "three": {
          "title": "Les modèles sont démasqués !",
          "a": "Apprenez en plus sur les modèles",
          "b": "d’IA et leurs caractéristiques"
        }
      }
    },
    "use": {
      "title": "À quoi sert compar:IA ?",
      "desc": "compar:IA est un outil gratuit qui permet de sensibiliser les citoyens à l’IA générative et à ses enjeux",
      "compare": {
        "title": "Comparer les réponses de différents modèles d’IA",
        "desc": "Discutez et développez votre esprit critique en donnant votre préférence",
        "alt": "Comparer"
      },
      "test": {
        "title": "Tester au même endroit les dernières IA de l’écosystème",
        "desc": "Testez différents modèles, propriétaires ou non, de petites et grandes tailles",
        "alt": "Tester"
      },
      "measure": {
        "title": "Mesurer l’empreinte écologique des questions posées aux IA",
        "desc": "Découvrez l’impact environnemental de vos discussions avec chaque modèle",
        "alt": "Mesurer"
      }
    },
    "europe": {
      "title": "Le comparateur <span {props}>devient européen !</span>",
      "desc": "La Lituanie, la Suède et le Danemark rejoignent la France en adoptant le comparateur dans le but d’affiner les futurs modèles d’IA dans leurs langues nationales.",
      "question": "Vous souhaitez également disposer du comparateur dans votre langue ?",
      "languages": {
        "da": "en danois",
        "fr": "en français",
        "lt": "en lituanien",
        "sv": "en suédois"
      }
    },
    "vote": {
      "title": "Pourquoi votre vote est-il important ?",
      "desc": "Votre préférence enrichit les jeux de données compar:IA dont l’objectif est d’affiner les futurs modèles d’IA sur le français, le suédois, le lituanien et le danois",
      "steps": {
        "prefs": {
          "title": "Vos préférences",
          "desc": "Après discussion avec les IA, vous indiquez votre préférence pour un modèle selon des critères donnés, tels que la pertinence ou l’utilité des réponses."
        },
        "datasets": {
          "title": "Les jeux de données par langue",
          "desc": "Toutes les questions posées et les votes sont compilées dans des jeux de données et publiés librement après anonmymisation."
        },
        "finetune": {
          "title": "Des modèles affinés sur la langue spécifique",
          "desc": "A terme, les acteurs industriels et académiques peuvent exploiter les jeux de données pour entrainer de nouveaux modèles plus respectueux de la diversité linguistique et culturelle."
        }
      },
      "datasetAccess": "Accéder aux jeux de données"
    },
    "usage": {
      "title": "Les usages spécifiques de compar:IA",
      "desc": "L’outil s’adresse également aux experts IA et aux formateurs pour des usages plus spécifiques",
      "use": {
        "title": "Exploiter les données",
        "desc": "Développeurs, chercheurs, éditeurs de modèles… accédez aux jeux de données compar:IA pour améliorer les modèles"
      },
      "explore": {
        "title": "Explorer les modèles",
        "desc": "Consultez au même endroit toutes les caractéristiques et conditions d’utilisation des modèles"
      },
      "educate": {
        "title": "Former et sensibiliser",
        "desc": "Utilisez le comparateur comme un support pédagogique de sensibilisation à l’IA auprès de votre public"
      }
    },
    "origin": {
      "team": {
        "title": "Qui sommes-nous ?",
        "desc": "Le comparateur est porté au sein du Ministère de la Culture par une équipe pluridisciplinaire réunissant expert en Intelligence artificielle, développeurs, chargé de déploiement, designer, avec pour mission de rendre les IA conversationnelles plus transparentes et accessibles à toutes et tous."
      },
      "project": {
        "title": "Qui est à l’origine du projet ?",
        "desc": "Le comparateur a été conçu et développé dans le cadre d’une start-up d’Etat portée par le ministère de la Culture et intégrée au programme <a {linkProps}>Beta.gouv.fr</a> de la Direction interministérielle du numérique (DINUM) qui aide les administrations publiques françaises à construire des services numériques utiles, simples et faciles à utiliser."
      }
    },
    "faq": {
      "title": "Vos questions les plus courantes",
      "discover": "Découvrir les autres questions"
    }
  },
  "ranking": {
    "title": "Classement des modèles",
    "desc": "Découvrez comment les meilleurs modèles d’IA se positionnent à travers leur <strong>score de satisfaction</strong> issus des votes de la communauté compar:IA. Pour en savoir plus, consultez <a {linkProps}>notre méthodologie de classement</a>.",
    "table": {
      "search": "Rechercher un modèle",
      "lastUpdate": "Mise à jour le {date}",
      "totalModels": "Total modèles :",
      "totalVotes": "Total votes :",
      "data": {
        "cols": {
          "rank": "Rang",
          "name": "Modèle",
          "elo": "Score satisfaction",
          "trust_range": "Confiance (±)",
          "total_votes": "Total votes",
          "consumption_wh": "Énergie<br>(1000 tokens)",
          "size": "Taille<br>(paramètres actifs)",
          "release": "Date sortie",
          "organisation": "Organisation",
          "license": "Licence"
        }
      }
    }
  },
  "product": {
    "title": "Tout savoir sur le comparateur",
    "comparator": {
      "title": "Le comparateur permet de créer des <span {props}>jeux de données</span> de préférence centrés sur des <span {props}>usages réels</span> exprimés dans les <span {props}>langues européennes</span>.",
      "cta": "Accéder au comparateur",
      "challenges": {
        "title": "L’application développée répond à plusieurs enjeux",
        "bias": {
          "title": "Biais culturels et linguistiques",
          "desc": "Mettre en avant les biais de l'IA liés à la sous-représentation des données non anglophones dans les modèles et sensibiliser à leurs conséquences."
        },
        "impacts": {
          "title": "Impact environnemental",
          "desc": "Révéler les effets écologiques de l'IA générative, encore largement méconnus du grand public."
        },
        "pluralism": {
          "title": "Pluralisme des modèles",
          "desc": "Assurer aux citoyens l'accès à une diversité de modèles d'IA afin qu'ils puissent faire des choix éclairés et développer un regard critique sur ces technologies."
        },
        "thinking": {
          "title": "Esprit critique et questions sociétales",
          "desc": "Inciter au questionnement critique sur la place de l’IA générative dans les pratiques personnelles et professionnelles (éducation, travail)."
        }
      },
      "europe": {
        "title": "Le comparateur <span {props}>devient européen</span> !",
        "adventure": "Depuis l’été 2025, la Lituanie, la Suède et le Danemark rejoignent l’aventure !",
        "desc": "Le comparateur est mis à disposition de leurs citoyens dans leurs langues nationales. L’objectif est de créer des jeux de données de préférence afin d’améliorer les futurs modèles d’IA dans ces langues européennes.",
        "catch": "Vous souhaitez disposer du comparateur dans votre langue ?"
      }
    },
    "problem": {
      "title": "Les modèles d’IA conversationnelles respectent-ils la <span {props}>diversité</span> des langues européennes ?",
      "diversity": {
        "stereotypes": {
          "title": "Réponses stéréotypées",
          "desc": "Les systèmes d’IA conversationnelle donnent l’impression de parler toutes les langues mais les résultats qu’ils génèrent sont parfois stéréotypés ou discriminants."
        },
        "english": {
          "title": "Données d’entrainement majoritairement en anglais",
          "desc": "Les IA conversationnelles reposent sur des grands modèles de langage (LLM) entraînés principalement sur des données en anglais, ce qui crée des biais linguistiques et culturels dans les résultats qu'ils produisent."
        },
        "diversity": {
          "title": "Diversités culturelles et linguistiques négligées",
          "desc": "Ces biais peuvent aussi se traduire par des réponses partielles voire incorrectes négligeant la diversité des langues et des cultures, notamment européennes."
        }
      },
      "alignment": {
        "title": "Comment réduire les biais culturels et linguistiques de ces modèles ?",
        "desc": "L'alignement : une technique de réduction des biais qui repose sur la collecte des préférences d’utilisateurs",
        "alignment": {
          "title": "L’alignement, étape décisive d’instruction du modèle",
          "a": "L'alignement intervient après l'étape de pré-entraînement d'un modèle de langage, comme une étape de « finition » ou de « polissage ». Lors de son pré-entrainement, le modèle apprend à prédire le mot suivant et devient capable de générer du texte cohérent.",
          "b": "L’étape d’alignement consiste à apprendre au modèle à mieux répondre aux besoins humains, c’est à dire à le rendre plus <strong>pertinent</strong> (le modèle répond « mieux » aux questions), <strong>honnête</strong> (capacité à assumer « qu’il ne sait pas répondre » quand il n’y a pas suffisamment de données), et <strong>inoffensif</strong> (éviter de générer des contenus dangereux ou inappropriés).",
          "c": "<strong>Sans alignement, un LLM pourrait être techniquement compétent mais difficile à utiliser en pratique, car il ne comprendrait pas vraiment ce qu'on attend de lui dans une conversation.</strong>"
        },
        "datasets": {
          "title": "Des jeux de données spécifiques",
          "a": "L'alignement utilise des données très spécifiques, spécialement créées pour enseigner au modèle comment « bien » se comporter.",
          "b": "Les <strong>données de préférence</strong> constituent un type particulier de données d’alignement, aux côtés des <strong>données de démonstration</strong> (exemples de conversations entre humains et assistants IA, rédigées par des annotateurs experts selon des consignes précises de ton et de style), des <strong>données de sécurité</strong> (exemples spécifiques enseignant au modèle à éviter les contenus dangereux en montrant comment refuser les demandes problématiques) ou des <strong>données spécialisées</strong> couvrant des domaines spécifiques (médecine, droit, éducation…).",
          "c": "Les données de préférence présentent plusieurs réponses possibles à une même question, classées par ordre de qualité par des évaluateurs humains: les utilisateurs indiquent quelle réponse est la meilleure selon des critères donnés, telles que la pertinence, l’utilité, la nocivité. Une fois constitués, ces jeux de données sont utilisés pour entrainer les modèles en les ajustant selon les préférences exprimées par les utilisateurs."
        },
        "english": {
          "title": "Peu de données de préférence en langues européennes",
          "a": "Les données de préférence sont couteuses à produire car <strong>elles nécessitent du travail humain qualifié pour chaque exemple</strong>. Des plateformes telles que https://chat.lmsys.org/ permettent de constituer ces jeux de données de préférence mais peu d’utilisateurs s’en servent dans leur langue d’origine.",
          "b": "Les jeux de données de préférence sont rares, voire inexistants dans les langues européennes. La part des questions posées en français dans le jeu de données de LMSYS est par exemple inférieure à 1%.",
          "c": "comparIA est un exemple de dispositif permettant de collecter des conversations dans de multiples langues, incluant des références culturelles spécifiques à chaque région ou pays : tâches courantes, traditions culinaires locales, systèmes éducatifs, références historiques ou littéraires, etc."
        },
        "diversity": {
          "title": "Diversifier les données pour réduire les biais",
          "a": "Pour refléter la diversité des cultures et des langues dans les résultats générés par les modèles, <strong>les jeux de données d’alignement doivent inclure une variété de langues</strong>, de contextes et d’exemples issus de tâches courantes des utilisateurs. La diversification des données d'alignement permet d’améiorer à terme les performances d’un modèle à double titre :",
          "b": "D'une part, elle <strong>réduit les biais culturels</strong> en évitant qu'une seule perspective - souvent anglo-saxonne - domine les réponses de l'IA. Le modèle apprend ainsi à reconnaître qu'il existe plusieurs façons valides d'aborder une même question selon le contexte culturel.",
          "c": "D'autre part, cette exposition à la diversité de langues et de cultures favorise l’adaptation des réponses à des contextes spécifiques: un utilisateur français recevra des conseils adaptés au système français, tandis qu'un utilisateur danois obtiendra des informations correspondant à son contexte national.",
          "d": "Le résultat est un modèle d’IA conversationnelle plus inclusif, capable de tenir compte des différentes cultures."
        }
      }
    },
    "partners": {
      "institution": {
        "title": "Partenaires institutionnels"
      },
      "diffusion": {
        "title": "Partenaires de diffusion",
        "desc": "Nous créons un réseau de partenaires intégrant le comparateur dans leur offre de services et de formation.",
        "catch": "Vous souhaitez utiliser le comparateur pour répondre à un besoin métier ?",
        "cta": "Dites nous en plus"
      },
      "academy": {
        "title": "Partenaires académiques",
        "desc": "Nous avons à coeur que les jeux de données générés alimentent des travaux de recherche multidisciplinaires mêlant sciences humaines et sociales et data science.",
        "catch": "Vous menez un projet de recherche et avez des suggestions ou besoin de précision sur la démarche et/ou les jeux de données produits ?"
      },
      "services": {
        "title": "Services mis à contribution",
        "desc": "Les calculs d’impacts environnementaux reposent sur les produits ci dessus."
      }
    }
  },
  "datasets": {
    "access": {
      "title": "Accédez aux jeux de données compar:IA",
      "desc": "Les questions et préférences posées sur la plateforme sont majoritairement en français et reflètent des usages réels et non contraints. Ces jeux de données sont accessibles sur <a {linkProps}>data.gouv</a> et Hugging Face.",
      "catch": "Editeurs de modèles, chercheurs, chercheuses, entreprises, à vous de jouer !",
      "share": "Partagez-nous vos réutilisations",
      "repos": {
        "conversations": {
          "title": "/conversations",
          "desc": "Ensemble des réponses et des questions posées"
        },
        "reactions": {
          "title": "/réactions",
          "desc": "Ensemble des réactions exprimées"
        },
        "votes": {
          "title": "/votes",
          "desc": "Ensemble des préférences exprimées"
        }
      }
    },
    "reuse": {
      "title": "Comment ces données sont-elles utilisées ?",
      "desc": "Exemples de réutilisation des jeux de données compar:IA",
      "bunka": {
        "desc": "L'équipe Bunka.ai a mené une étude approfondie sur les interactions entre les utilisateurs de la plateforme Compar:IA et les modèles d'IA, examinant les thématiques privilégiées, les tâches principales et déterminant si ces modèles fonctionnent avant tout comme des outils d'automatisation ou d'augmentation des capacités humaines. Cette analyse repose sur un large échantillon de 25 000 conversations.",
        "conversations": {
          "title": "Explorer la visualisation de données",
          "desc": "Visualisation interactive des conversations où chaque point représente un cluster de discussions évoqué par les utilisateurs (comme l’éducation, la santé, l’environnement, ou encore la philosophie)."
        },
        "analyze": {
          "title": "Accéder à l’analyse par indicateur",
          "desc": "Analyse des conversations des utilisateurs avec détection des tâches (création, recherche d'informations...), des sujets (arts et culture, éducation...), des émotions complexes (curiosité, enthousiasme...), des types de langage (formel, professionnel...)"
        },
        "method": "En savoir plus sur la méthodologie"
      }
    }
  },
  "arenaHome": {
    "title": "Comment puis-je vous aider aujourd'hui ?",
    "modelSelection": "Sélection des modèles",
    "prompt": {
      "label": "Écrivez votre premier message",
      "placeholder": "Écrivez votre premier message ici"
    },
    "selectModels": {
      "question": "Quels modèles voulez-vous comparer ?",
      "help": "Sélectionnez le mode de comparaison qui vous convient"
    },
    "compareModels": {
      "question": "Quels modèles voulez-vous comparer ?",
      "count": "{count}/2 modèles",
      "help": "Si vous n’en choisissez qu’un, le second sera sélectionné de manière aléatoire"
    },
    "suggestions": {
      "title": "Suggestions de prompts",
      "generateAnother": "Générer un autre message",
      "choices": {
        "iasummit": {
          "iconAlt": "Sommet pour l'action sur l'IA",
          "title": "Prompts issus de la consultation citoyenne sur l’IA ",
          "tooltip": "Ces questions sont issues de la consultation citoyenne sur l’IA qui a lieu du 16/09/2024 au 08/11/2024. Elle visait à associer largement les citoyens et la société civile au Sommet international pour l’action sur l’IA, en collectant leurs idées pour faire de l’intelligence artificielle une opportunité pour toutes et tous, mais aussi de nous prémunir ensemble contre tout usage inapproprié ou abusif de ces technologies."
        },
        "ideas": {
          "iconAlt": "Idées",
          "title": "Générer de nouvelles idées"
        },
        "explanations": {
          "iconAlt": "Explications",
          "title": "Expliquer simplement un concept"
        },
        "languages": {
          "iconAlt": "Traduction",
          "title": "M’exprimer dans une autre langue"
        },
        "administrative": {
          "iconAlt": "Administratif",
          "title": "Rédiger un document administratif"
        },
        "recipes": {
          "iconAlt": "Recettes",
          "title": "Découvrir une nouvelle recette de cuisine"
        },
        "coach": {
          "iconAlt": "Conseils",
          "title": "Obtenir des conseils sur l’alimentation et le sport"
        },
        "stories": {
          "iconAlt": "Histoires",
          "title": "Raconter une histoire"
        },
        "recommendations": {
          "iconAlt": "Recommandations",
          "title": "Proposer des idées de films, livres, musiques"
        }
      }
    }
  },
  "chatbot": {
    "continuePrompt": "Continuer à discuter avec les deux modèles d'IA",
    "revealButton": "Passer à la révélation des modèles",
    "conversation": "Conversation",
    "errors": {
      "tooLong": {
        "title": "Oups, la conversation est trop longue pour un des modèles.",
        "message": "Chaque modèle est limité dans la taille des conversations qu'il est capable de traiter.",
        "vote": "Vous pouvez tout de même donner votre avis sur ces modèles ou recommencer une conversation avec deux nouveaux.",
        "retry": "Vous pouvez recommencer une conversation avec deux nouveaux modèles."
      },
      "other": {
        "title": "Oups, erreur temporaire",
        "message": "Une erreur temporaire est survenue.",
        "vote": "Ou bien conclure votre expérience en donnant votre avis sur les modèles.",
        "retry": "Vous pouvez tenter de réessayer de solliciter les modèles."
      }
    },
    "loading": "Chargement des réponses"
  },
  "closeModal": "Fermer la fenêtre modale",
  "models": {
    "conditions": "Conditions d'utilisation",
    "licenses": {
      "type": {
        "proprietary": "Propriétaire",
        "openSource": "Open source",
        "semiOpen": "Semi-ouvert"
      },
      "name": "Licence {licence}",
      "commercial": "Licence commerciale",
      "noDesc": "Les informations de licence n'ont pas été remplies pour ce modèle.",
      "descriptions": {
        "MIT": "La licence MIT est une licence de logiciel libre permissive : elle permet à quiconque de réutiliser, modifier et distribuer le modèle, même à des fins commerciales, sous réserve d'inclure la licence d'origine et les mentions de droits d'auteur.",
        "Apache 2.0": "Cette licence permet d'utiliser, modifier et distribuer librement, même à des fins commerciales. Outre la liberté d’utilisation, elle garantit la protection juridique en incluant une clause de non-atteinte aux brevets et la transparence : toutes les modifications doivent être documentées et sont donc traçables.",
        "Gemma": "Cette licence est conçue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiées ou améliorées doivent être partagée avec la communauté sous la même licence, favorisant ainsi la collaboration et la transparence dans le développement logiciel.",
        "Llama 3 Community": "Cette licence permet d'utiliser, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels et interdit la réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles concurrents, protégeant ainsi les investissements technologiques et la marque de Meta.",
        "Llama 3.1": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
        "Llama 3.3": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
        "Llama 4": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.",
        "Jamba Open Model": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les organismes dépassant 50 millions de dollars de revenus annuels.",
        "CC-BY-NC-4.0": "Cette licence permet de partager et adapter le contenu à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.",
        "propriétaire Gemini": "Le modèle est disponible sous licence payante et accessible via l'API Gemini disponible sur les plateformes Google AI Studio et Vertex AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
        "propriétaire Mistral": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
        "propriétaire xAI": "Le modèle est accessible via API xAI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
        "propriétaire Liquid": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Liquid AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
        "propriétaire OpenAI": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
        "propriétaire Anthropic": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités ou selon les termes de l'entreprise.",
        "Mistral AI Non-Production": "Cette licence permet de partager et adapter le contenu à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur."
      }
    },
    "release": "Sortie {date}",
    "size": {
      "title": "Taille",
      "estimated": "Taille estimée ({size})",
      "descriptions": {
        "XS": "Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.",
        "S": "Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)",
        "M": "Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "L": "Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.",
        "XL": "Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés."
      }
    },
    "openWeight": {
      "conditions": {
        "free": "Permissive",
        "copyleft": "Copyleft",
        "restricted": "Sous conditions"
      },
      "tooltips": {
        "openSource": "Le corpus, le code d'entraînement, et les poids de ce modèle (c’est-à-dire les paramètres appris pendant son entraînement) sont entièrement téléchargeables et modifiables par le public, lui permettant de faire fonctionner et modifier le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant qu'« open weights », notamment à cause de la nécessité de transparence du corpus d'entraînement, et rares sont les modèles qui sont considérés « open source ».",
        "openWeight": "Modèle dit « open weights » dont les poids, c’est-à-dire les paramètres appris pendant son entraînement, sont téléchargeables par le public, lui permettant de faire fonctionner le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant (principalement par rapport à la transparence du corpus d'entraînement), et rares sont les modèles qui sont considérés « open source ».",
        "params": "Les paramètres ou les poids, comptés en milliards, sont les variables, apprises par un modèle au cours de son entrainement, qui déterminent ses réponses. Plus le nombre de paramètres est important, plus il est capable d’effectuer des tâches complexes.",
        "free": "Une fois modifié, le modèle peut être redistribué sous une licence différente du modèle source.",
        "copyleft": "Une fois modifié, le modèle doit être redistribué sous la même licence que celle du modèle source.",
        "ram": "La RAM (mémoire vive) stocke les données traitées par un LLM en temps réel. Plus le modèle est grand, plus il a besoin de RAM pour fonctionner."
      },
      "descriptions": {
        "XS": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des modèles très petits (moins de 7 milliards de paramètres).",
        "S": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des petits modèles (entre 7 et 20 milliards de paramètres).",
        "M": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des moyens modèles (entre 20 et 70 milliards de paramètres).",
        "L": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des grands modèles (entre 70 et 100 milliards de paramètres).",
        "XL": "Doté de {paramsCount} milliards de paramètres, ce modèle fait partie de la classe des très grands modèles."
      },
      "use": {
        "commercial": "Utilisation commerciale",
        "modification": "Modification autorisée",
        "attribution": "Attribution requise",
        "licenseType": "Type de licence",
        "requiredRam": "RAM nécessaire"
      }
    },
    "parameters": "{number} mds de paramètres",
    "ram": "{min} à {max} Go",
    "names": {
      "a": "Modèle A",
      "b": "Modèle B"
    },
    "extra": {
      "title": "Pour aller plus loin",
      "experts": {
        "open-weights": "Pour les expert·es, consultez la <a {linkProps}>fiche du modèle sur Hugging Face</a>",
        "api-only": "Pour les expert·es, consultez la <a {linkProps}>site officiel du modèle</a>"
      },
      "impacts": "Les calculs d’impacts environnementaux reposent sur les projets <a {linkProps1}>EcoLogits</a> et <a {linkProps2}>Impact CO<sub>2</sub></a>."
    },
    "list": {
      "title": "Découvrez les modèles",
      "intro": "Explorez les différents modèles d'IA conversationnels disponibles, leurs caractéristiques et leurs licences.",
      "filters": {
        "editor": {
          "legend": "Éditeur"
        },
        "size": {
          "legend": "Taille (en milliards de paramètres)",
          "labels": {
            "XS": "< à 7 milliards",
            "S": "de 7 à 20 milliards",
            "M": "de 20 à 70 milliards",
            "L": "de 70 à 150 milliards",
            "XL": "> 150 milliards"
          }
        },
        "license": {
          "legend": "Licence d'utilisation"
        },
        "display": "Afficher les filtres"
      },
      "triage": {
        "label": "Trier par",
        "options": {
          "name-asc": "Nom du modèle (A à Z)",
          "date-desc": "Date de sortie (du plus au moins récent)",
          "params-asc": "Taille (du plus petit au plus grand)",
          "org-asc": "Éditeur (A à Z)"
        }
      },
      "model": "modèle",
      "models": "modèles",
      "noresults": "Aucun modèle ne correspond à vos critères de recherche."
    }
  },
  "modes": {
    "random": {
      "title": "Mode Aléatoire",
      "label": "Aléatoire",
      "altLabel": "Modèles aléatoires",
      "description": "Deux modèles choisis au hasard parmi toute la liste"
    },
    "custom": {
      "title": "Mode Sélection",
      "label": "Sélection manuelle",
      "altLabel": "Sélection manuelle",
      "description": "Reconnaîtrez-vous les deux modèles que vous avez choisis ?"
    },
    "small-models": {
      "title": "Mode Frugal",
      "label": "Frugal",
      "altLabel": "Modèles frugaux",
      "description": "Deux modèles tirés au hasard parmi ceux de plus petite taille"
    },
    "big-vs-small": {
      "title": "Mode David contre Goliath",
      "label": "David contre Goliath",
      "altLabel": "David contre Goliath",
      "description": "Un petit modèle contre un grand, les deux tirés au hasard"
    },
    "reasoning": {
      "title": "Mode Raisonnement",
      "label": "Raisonnement",
      "altLabel": "Modèles avec raisonnement",
      "description": "Deux modèles tirés au hasard parmi ceux optimisés pour des tâches complexes"
    }
  },
  "vote": {
    "title": "Quel modèle d’IA préférez-vous ?",
    "introA": "Avant de découvrir l’identité des modèles, nous avons besoin de votre préférence.",
    "introB": "Elle permet d'enrichir les jeux de données compar:IA dont l’objectif est d’affiner les futurs modèles d’IA sur le français",
    "bothEqual": "Les deux se valent",
    "comment": {
      "add": "Ajouter des commentaires",
      "placeholder": "Vous pouvez ajouter des précisions sur cette réponse du modèle {model}"
    },
    "choices": {
      "positive": {
        "question": "Qu'avez-vous apprécié dans la réponse ?",
        "useful": "Utile",
        "complete": "Complète",
        "creative": "Créative",
        "clear-formatting": "Mise en forme claire"
      },
      "negative": {
        "question": "Pourquoi la réponse ne convient-elle pas ?",
        "incorrect": "Incorrecte",
        "superficial": "Superficielle",
        "instructions-not-followed": "Instructions non suivies"
      },
      "altText": "{choice} pour le modèle {model}"
    },
    "qualify": {
      "question": "Comment qualifiez-vous ses réponses ?",
      "placeholder": "Les réponses du modèle {model} sont...",
      "addDetails": "Ajouter des détails"
    },
    "like": {
      "label": "j'apprécie",
      "selectedLabel": "j'apprécie (sélectionné)"
    },
    "dislike": {
      "label": "je n'apprécie pas",
      "selectedLabel": "je n'apprécie pas (sélectionné)"
    },
    "yours": "Votre vote"
  },
  "reveal": {
    "impacts": {
      "title": "Impact énergétique de la discussion",
      "size": {
        "label": "taille du modèle",
        "count": "milliards param.",
        "estimated": "(est.)",
        "quantized": "(quantisé)"
      },
      "tokens": {
        "label": "taille du texte",
        "tooltip": "L’IA analyse et génère des phrases à partir de mots ou de parties de mots d’à peu près quatre lettres, cette unité de texte est appelée token (« jeton »). Plus un texte est long, plus le nombre de tokens est grand.",
        "tokens": "tokens"
      },
      "energy": {
        "label": "énergie conso.",
        "tooltip": "Mesurée en wattheures, l’énergie consommée représente l'électricité utilisée par le modèle pour traiter une requête et générer la réponse correspondante. Plus un modèle est grand (en milliards de paramètres), plus il faut d'énergie pour produire un token."
      }
    },
    "equivalent": {
      "title": "Ce qui correspond à :",
      "co2": {
        "label": "CO<sub>2</sub> émis",
        "tooltip": "Le CO<sub>2</sub> émis équivaut aux émissions de dioxyde de carbone produites par l’énergie utilisée pour faire fonctionner le modèle. Elle traduit l'impact environnemental lié à la consommation énergétique. Le calcul d’équivalence Wattheures/CO<sub>2</sub> diffère selon le mix énergétique de chaque pays. Or, les serveurs utilisés pour l’inférence des modèles ne sont pas tous localisés en France. Ainsi, le calcul d’équivalence repose sur la moyenne mondiale du taux d’émissions de CO<sub>2</sub> par énergie consommée."
      },
      "lightbulb": {
        "label": "ampoule LED",
        "tooltip": "Donnée calculée sur la base de consommation d’une ampoule LED standard de 5W (E14)"
      },
      "streaming": {
        "label": "vidéos en ligne",
        "tooltip": "Donnée calculée selon l’impact carbone d’une heure de vidéo en ligne en haute définition, sur une télévision, en connexion wifi (source <a {linkProps}>ADEME</a>)"
      }
    },
    "feedback": {
      "shareResult": "Partager votre résultat",
      "moreOnVotes": "En savoir plus sur les votes",
      "description": "Faites découvrir compar:IA en partageant les modèles d’IA avec lesquels vous avez échangé ! Seuls les noms et l’impact énergétique de la discussion seront visibles via ce lien, sans accès aux messages échangés.",
      "example": "Exemple de partage de résultat"
    }
  },
  "errors": {
    "unknown": "Une erreur est survenue",
    "404": {
      "title": "Page non trouvée",
      "error": "Erreur 404",
      "sorry": "La page que vous cherchez est introuvable. Excusez-nous pour la gène occasionnée.",
      "desc": "Si vous avez tapé l'adresse web dans le navigateur, vérifiez qu'elle est correcte. La page n’est peut-être plus disponible.<br />Dans ce cas, pour continuer votre visite vous pouvez consulter notre page d’accueil.<br />Sinon contactez-nous pour que l’on puisse vous rediriger vers la bonne information."
    },
    "unexpected": {
      "title": "Erreur inattendue",
      "error": "Erreur {code}",
      "sorry": "Désolé, le service rencontre un problème, nous travaillons pour le résoudre le plus rapidement possible.",
      "desc": "Essayez de rafraîchir la page ou bien ressayez plus tard."
    }
  },
  "actions": {
    "copyMessage": {
      "do": "Copier le message",
      "done": "Message copié"
    },
    "copyLink": {
      "do": "Copier le lien",
      "done": "Lien copié dans le presse-papiers"
    },
    "contact": "Contactez-nous",
    "contactUs": "Nous contacter",
    "home": "Page d'accueil",
    "returnHome": "Revenir à l'accueil",
    "seeMore": "Voir plus",
    "selectLanguage": "Sélectionner une langue"
  },
  "words": {
    "back": "Retour",
    "close": "Fermer",
    "NA": "N/A",
    "random": "Aléatoire",
    "regenerate": "Regénérer",
    "reset": "Réinitialiser",
    "restart": "Recommencer",
    "retry": "Recommencer",
    "search": "Rechercher",
    "send": "Envoyer",
    "tooltip": "Infobulle",
    "validate": "Valider"
  },
  "generated": {
    "licenses": {
      "os": {
        "Apache 2.0": {
          "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le modèle, y compris à des fins commerciales. Outre la liberté d'utilisation, elle garantit la protection juridique en incluant une clause de concession de brevets qui fonctionne comme une assurance : si vous utilisez ce modèle, les contributeurs s'engagent à ne pas vous poursuivre pour violation de leurs brevets liés au projet. Cette protection mutuelle évite les conflits juridiques entre utilisateurs et développeurs. Lors de la distribution de versions modifiées, les changements significatifs doivent être signalés par des mentions appropriées, garantissant la transparence pour l'utilisateur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "CC-BY-NC-4.0": {
          "license_desc": "<p>Cette licence permet de partager et adapter le contenu librement à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        },
        "Gemma": {
          "license_desc": "<p>Cette licence est conçue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiées ou améliorées doivent être partagées avec la communauté sous la même licence source, favorisant ainsi la collaboration et la transparence dans le développement logiciel.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Llama 3.1": {
          "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs"
        },
        "Llama 3.3": {
          "license_desc": "<p>Cette licence <strong>non-exclusive, mondiale et sans redevance</strong> permet d'utiliser, reproduire, modifier et distribuer librement le code et les Matériaux Llama 3.3 avec attribution. Elle autorise notamment la réutilisation pour l'amélioration de modèles dérivés, mais impose des restrictions pour les opérations commerciales de très grande envergure.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs"
        },
        "Llama 4": {
          "license_desc": "<p>Cette licence non-exclusive, mondiale et sans redevance permet d'utiliser, reproduire, modifier et distribuer les Matériaux Llama 4 (modèles et documentation) avec attribution. Cependant, elle impose deux restrictions majeures : (1) les entreprises dépassant 700 millions d'utilisateurs actifs mensuels doivent obtenir une licence spéciale de Meta, et (2) <strong>exclusion totale</strong> des personnes résidant dans l'UE et des entreprises ayant leur siège social dans l'UE pour l'utilisation directe des modèles multimodaux, en raison des incertitudes réglementaires liées à l'AI Act européen. Les utilisateurs finaux européens peuvent néanmoins accéder à des services intégrant Llama 4, à condition qu'ils soient fournis depuis l'extérieur de l'UE.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs\n"
        },
        "MIT": {
          "license_desc": "<p>La licence MIT est une licence de logiciel libre permissive : elle permet à quiconque de réutiliser, modifier et distribuer le modèle, même à des fins commerciales, sous réserve d'inclure la licence d'origine et les mentions de droits d'auteur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI Research License": {
          "license_desc": "<p>Cette licence non-exclusive et sans redevance autorise l'utilisation, la copie, la modification et la distribution des modèles Mistral et de leurs dérivés (incluant les versions modifiées ou affinées). Cependant, elle est strictement limitée aux fins de recherche.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        }
      },
      "proprio": {
        "Alibaba": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Alibaba, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Amazon": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via Amazon Bedrock, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "sauf pour distiller ou entraîner d’autres modèles sur les plateformes d’Amazon.",
          "commercial_use_specificities": ""
        },
        "Anthropic": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic ou des sociétés partenaires, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Cohere": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "DeepSeek": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Google": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Google, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée de Google.",
          "reuse_specificities": "sauf pour entraîner d’autres modèles sur Vertex AI",
          "commercial_use_specificities": ""
        },
        "Meta": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Microsoft": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres hébergeurs, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Moonshot AI": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nous": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nvidia": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "OpenAI": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI ou via les services Microsoft Azure, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Zhipu": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "xAI": {
          "license_desc": "Le modèle est disponible sous licence payante et accessible via X et xAI, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        }
      }
    },
    "models": {
      "Aya Expanse 32B": {
        "desc": "<p>Modèle de taille moyenne multilingue, capable de traiter 23 langues.</p>",
        "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 130 000 jetons, utile pour l’analyse de documents longs.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été conçu pour offrir de bonnes capacités dans chacune des 23 langues de son corpus d’entraînement.</p>"
      },
      "Claude 3.7 Sonnet": {
        "desc": "<p>Très grand modèle multimodal et multilingue, performant pour la génération de code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>",
        "fyi": "<p>Claude 4 Opus est la version la plus avancée de la famille Claude 4. Il est optimisé pour la puissance brute et les tâches complexes nécessitant un raisonnement soutenu sur de longues périodes : il peut par exemple travailler sur des tâches à long terme (Anthropic déclarent qu'il peut travailler jusqu'à sept heures de manière indépendante). En contrepartie, Opus est plus coûteux à utiliser, plus lent à répondre et nécessite davantage de ressources pour fonctionner.</p>\n<p>Le modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été majoritairement entraîné sur des données mathématiques, mais adapté à des cas d’usage réels.</p>"
      },
      "Claude 4 Sonnet": {
        "desc": "<p>Très grand modèle multimodal et multilingue, très puissant en code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
        "size_desc": "<p>La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>",
        "fyi": "<p>Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisée pour la vitesse, l’efficacité et l’accessibilité. Il est un peu moins à l’aise sur les tâches qui demandent un raisonnement complexe en plusieurs étapes. En contrepartie, il est nettement moins coûteux, plus rapide, peut générer de plus longs textes et consomme moins d’énergie que Opus.</p>\n<p>Le modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été surtout entraîné sur des données mathématiques, mais adapté à des cas d’usage réels.</p>"
      },
      "Command A": {
        "desc": "<p>Grand modèle, performant pour la programmation, l’utilisation d’outils externes, la “génération augmentée de récupération” (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 111 milliards de paramètres, ce modèle fait partie des grands modèles. Il nécessite au moins deux cartes graphiques puissantes pour l’hébergement, ce qui entraîne un coût de fonctionnement significatif.</p>\n<p>Sa fenêtre de contexte atteint 256 000 jetons, adaptée à l’analyse de vastes ensembles de documents ou de bases de code.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier <a href=\"https://arxiv.org/abs/1706.03762\">« Attention Is All You Need »</a> paru en 2017 et qui a révolutionné l'IA. L'entreprise se démarque par sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle est conçu pour fonctionner dans plus de 23 langues et pour s’intégrer facilement dans les systèmes d’entreprise.  Il fait partie des rares modèles distribués sous licence <strong>CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.</strong> Ce choix de licence reflète la volonté de Cohere de contribuer à la recherche et la communauté open source, tout en gardant le contrôle sur les usages commerciaux pour protéger son modèle économique... Cela exclut par exemple l’intégration du modèle dans des produits ou services vendus par une entreprise à des clients mais autorise un usage académique, des tests ou des projets internes, restreints à un cadre non-commercial.</p>"
      },
      "Command R": {
        "desc": "<p>Modèle de taille moyenne optimisé pour la synthèse, les questions générales, l’utilisation d’outils et efficace dans les systèmes de génération augmentée de récupération (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 35 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été évalué dans plus de 10 langues. Sa fenêtre de contexte atteint 128 000 jetons, ce qui facilite l’analyse de documents longs. Cette fenêtre a été doublée sur la version suivante du modèle (Command A).</p>"
      },
      "DeepSeek R1": {
        "desc": "<p>Très grand modèle très performant sur les tâches mathématiques, scientifiques et de programmation, qui simule une étape de raisonnement avant de générer sa réponse.</p>",
        "size_desc": "<p>Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 128 000 jetons, ce qui est adapté à l’analyse de longs documents.</p>",
        "fyi": "<p>Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (<em>supervised fine-tuning</em> : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage.</p>"
      },
      "DeepSeek R1 Llama 70B": {
        "desc": "<p>Grand modèle basé sur Meta Llama 3.3 70B, ré-entraîné avec des exemples de raisonnement issus du modèle DeepSeek R1. Il offre de bonnes capacités en mathématiques et code.</p>",
        "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle est classé parmi les modèles de grande taille. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne un coût de fonctionnement élevé. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>La fenêtre de contexte est de 16 000 jetons, ce qui peut être limitant pour l’analyse de très grands documents.</p>",
        "fyi": "<p>Le modèle n’a pas été entraîné depuis zéro. Il s’appuie sur Llama 3.3 70B, ré-entraîné en utilisant des résultats générés par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B d’une capacité à simuler le raisonnement, sans possibilité pour l’utilisateur de choisir d’activer ou non cette fonction.</p>\n<p>Conformément aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du modèle source dans le nom du modèle, soumis au même régime de licence.</p>"
      },
      "DeepSeek V3": {
        "desc": "<p>Très grand modèle conçu pour des tâches complexes : génération de code, utilisation d’outils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particulièrement adapté à l’anglais et au chinois.</p>",
        "size_desc": "<p>DeepSeek V3 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.</p>\n<p>La fenêtre de contexte atteint 163 000 jetons, ce qui est utile pour l’analyse de longs documents.</p>",
        "fyi": "<p>Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code.</p>"
      },
      "GPT 4.1 Nano": {
        "desc": "<p>Plus petite version allégée du modèle GPT 4.1 , conçue pour limiter les coûts tout en restant compétitive sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.  Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>",
        "fyi": "<p>Il s'agit d'une version distillée d’un modèle de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio. Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus de textes ou de dépôts de code très longs.</p>"
      },
      "GPT 5": {
        "desc": "<p>Le GPT-5 n'est pas un modèle unique, mais un système unifié composé de deux modèles distincts : un modèle rapide (<code>gpt-5-main</code>) pour les requêtes courantes et un modèle de raisonnement (<code>gpt-5-thinking</code>) pour les problèmes complexes. Comparé à ses prédécesseurs, OpenAI affirme qu'il est plus utile dans les requêtes du monde réel, avec des améliorations notables dans les domaines de l'écriture, du codage et de la santé. Il réduit également le phénomène des hallucinations. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
        "size_desc": "<p>Le système GPT-5 est composé de modèles de différentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est conçue pour inclure plusieurs modèles, orchestrés par un système de routage interne, qui sélectionne le plus petit modèle adapté à la tâche pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.</p>",
        "fyi": "<p>Les développeurs qui utilisent ce modèle peuvent configurer un paramètre de verbosité pour ajuster la longueur de la phase de raisonnement.</p>\n<p>En matière de sécurité, le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête. Les créateurs du modèle ont aussi utilisé la phase d’entraînement au “raisonnement” pour le rendre plus “résistant” aux tentatives de contournement de leurs règles de sécurité (<em>jailbreaking</em>).</p>"
      },
      "GPT 5 Mini": {
        "desc": "<p>Le GPT-5 Mini est une version allégée du modèle GPT-5 principal. Il est conçu pour être utilisé dans des environnements où il est nécessaire de limiter les coûts, par exemple à grande échelle. Son modèle de raisonnement est presque aussi performant que celui du modèle principal (<code>gpt-5-thinking</code>) malgré sa taille plus petite. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
        "size_desc": "<p>Le modèle Mini est une déclinaison plus compacte (taille moyenne supposée) du système GPT-5. Il est conçu pour fonctionner de manière optimale pour un bon équilibre entre performance et coût, grâce à un système de routage qui le sélectionne pour des tâches spécifiques. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Néanmoins, les modèles sont probablement très grands, nécessitant plusieurs cartes graphiques puissantes pour l’inférence.</p>",
        "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>\n<p>Bien qu'il soit une version plus petite, il se montre très compétitif face au modèle GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine médical.</p>"
      },
      "GPT 5 Nano": {
        "desc": "<p>Le GPT-5 Nano est la plus petite et la plus rapide version du modèle de raisonnement GPT-5. Il est conçu pour des contextes où une latence ou un coût ultra-faible est nécessaire. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
        "size_desc": "<p>Le modèle Nano est le plus compact de la famille GPT-5 (taille petite supposée). Il est sélectionné par le système de routage pour les requêtes nécessitant une latence ultra-faible et des réponses instantanées. Son architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui permet une meilleure efficacité énergétique et des performances élevées, même sur des requêtes nécessitant une réponse rapide.</p>",
        "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>"
      },
      "GPT OSS-120B": {
        "desc": "<p>Le plus grand des deux premiers modèles semi-ouverts d'OpenAI depuis GPT-2. Conçu en réponse à la montée en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modèle de raisonnement performant, notamment sur des tâches complexes et dans des environnements « agentiques ».</p>",
        "size_desc": "<p>L'architecture est basée sur le principe du « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (5,1 milliards par jeton) pour chaque requête. C’est un modèle de raisonnement, donc sa consommation d’énergie est plus élevée car ils génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce modèle peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>"
      },
      "GPT OSS-20B": {
        "desc": "<p>Le plus petit des deux modèles semi-ouverts d'OpenAI. Il a été conçu en réponse à la concurrence de l'open source et est destiné aux cas d'utilisation nécessitant une faible latence ainsi qu'aux déploiements locaux ou spécialisés.</p>",
        "size_desc": "<p>Avec 20 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. L'architecture est basée sur le « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (3,6 milliards par jeton) pour chaque requête. Il s'agit d'un modèle de raisonnement, ce qui se traduit par une consommation d'énergie plus élevée car il génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce modèle peut être exécuté localement sur un ordinateur portable haut de gamme équipé de seulement 16 Go de VRAM (ou de RAM système). Cela en fait une option très accessible pour les développeurs. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>"
      },
      "GPT-4.1 Mini": {
        "desc": "<p>Version allégée de GPT 4.1 mais qui reste tout de même de grande taille, conçue pour limiter les coûts tout en restant compétitif sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>",
        "fyi": "<p>Il s'agit d'une version distillée d’un modèle plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio.  Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus très longs ou de dépôts de code.</p>"
      },
      "Gemini 2.5 Flash": {
        "desc": "<p>Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant plusieurs cartes graphiques puissantes pour le fonctionnement. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>",
        "fyi": "<p>Ce modèle repose sur une architecture de mélange d’experts (MoE, Mixture of Experts) et a été distillé en ne conservant qu'une approximation des prédictions du modèle enseignant - Gemini 2.5 Pro. Il a été entraîné sur une architecture TPUv5p intégrant des avancées comme la possibilité de poursuivre l'entraînement automatiquement même en cas d’erreurs d’entraînement, de corruption de données ou de problèmes de mémoire.</p>\n<p>Gemini 2.5 Flash gère des contextes allant jusqu'à 1 million de jetons, et trois heures de contenu vidéo. L'optimisation du traitement de la vision permet de traiter des vidéos environ trois fois plus longues dans la même fenêtre de contexte: seuls 66 jetons visuels sont nécessaires pour générer une image contre 258 auparavant. Ce modèle permet  également la génération audio native pour les dialogues et la synthèse vocale.</p>"
      },
      "Gemma 3 12B": {
        "desc": "<p>Petit modèle multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
        "size_desc": "<p>Avec 12 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.</p>",
        "fyi": "<p>Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 27B": {
        "desc": "<p>Modèle de taille moyenne multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
        "size_desc": "<p>Avec 27 milliards de paramètres, il appartient à la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur avec une seule carte graphique (GPU). </p>\n<p>Il accepte des contextes jusqu’à 128 000 jetons, ce qui convient pour l’analyse de documents longs.</p>",
        "fyi": "<p>Il peut traiter du texte et des images sur un serveur équipé d’une seule carte graphique puissante. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 4B": {
        "desc": "<p>Très petit modèle multimodal et compact adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
        "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte peut atteindre 128 000 jetons, ce qui permet d’analyser de longs documents.</p>",
        "fyi": "<p>Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3n 4B": {
        "desc": "<p>Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin.</p>",
        "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement sur un ordinateur ou un smartphone pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand.</p>\n<p>Sa fenêtre de contexte va jusqu’à 32 000 jetons.</p>",
        "fyi": "<p>Ce modèle peut traiter du texte, des images et de l’audio. Il repose sur l’architecture MatFormer et un système de cache PLE (per-layer embeddings), qui active uniquement les paramètres utiles selon la tâche, s'adaptant à la capacité des machines sur lesquelles fonctionne le modèle.</p>"
      },
      "Grok 3 Mini": {
        "desc": "<p>Version plus légère du modèle Grok 3, permettant de réduire les coûts tout en conservant de bonnes performances pour de nombreuses tâches. Il peut simuler une phase de raisonnement avant de fournir une réponse finale.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 3 Mini est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>",
        "fyi": "<p>Grok 3 Mini est une version distillée de Grok 3: il s’en approche en termes de capacités, tout en étant plus rapide et moins coûteux.\nLe modèle propose deux modes : un mode réflexion avec raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses immédiates.\nSa fenêtre de contexte atteint 131 000 jetons, ce qui le rend adapté à l’analyse de longs documents.</p>"
      },
      "Hermes 3 405B": {
        "desc": "<p>Très grand modèle réentraîné à partir du Llama 3.1 405B, ajusté pour mieux répondre aux demandes des utilisateurs et faciliter l’utilisation d’outils externes.</p>",
        "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important.</p>",
        "fyi": "<p>Ce modèle est le résultat d’un réentraînement de l’ensemble des paramètres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et système - l’utilisateur dispose ainsi d’un plus grand contrôle sur la “personnalité” et comportement du modèle. Des fonctions de raisonnement spécifiques telles que <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> ont été ajoutées pour simuler un raisonnement sur les tâches complexes. L'entraînement a utilisé un outil appelé AdamW (vitesse d'apprentissage de 3.5×10⁻⁶), qui aide le modèle à apprendre de manière efficace en ajustant progressivement ses paramètres. Ensuite, il a été affiné avec une méthode appelée DPO (direct preference optimisation), qui permet d'améliorer ses réponses en se basant sur des préférences spécifiques. Pour rendre cet entraînement plus léger et rapide, des adaptateurs LoRA ont été utilisés ; ce sont des modules plus petits qui modifient seulement une partie du modèle, ce qui évite de devoir retravailler tous les paramètres en même temps.</p>"
      },
      "Llama 3.1 405B": {
        "desc": "<p>Très grand modèle conçu pour des tâches complexes ou spécialisées. Souvent utilisé en tant que “modèle professeur” pour l’entraînement de modèles plus spécialisés.</p>",
        "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important. Le modèle est doté d’une fenêtre de contexte jusqu’à 128 000 jetons, ce qui le rend intéressant pour des tâches d’analyse de longs documents.</p>",
        "fyi": "<p>Le modèle a été entraîné sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le marché en 2025). L'entraînement a combiné génération de données synthétiques et optimisation par préférences directes (DPO). Ce modèle est lui-même souvent utilisé pour générer des données synthétiques pour entraîner de plus petits modèles. Le modèle utilise par défaut une compression 8-bit pour réduire les besoins en mémoire et permettre l'exécution sur un seul serveur très puissant.</p>"
      },
      "Llama 3.1 8B": {
        "desc": "<p>Petit modèle conçu pour un usage local sur ordinateur portable, tout en offrant de bonnes capacités pour la synthèse de texte et les réponses simples.</p>",
        "size_desc": "<p>Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modèle est une version distillée issue des modèles Llama 3 de plus grande tailles : il a été entraîné grâce à un transfert d’une partie des connaissances des plus grands modèles.</p>"
      },
      "Llama 3.3 70B": {
        "desc": "<p>Grand modèle destiné à un large éventail de tâches et pouvant rivaliser avec des modèles plus volumineux.</p>",
        "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation significatifs. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modèle est une version distillée issue du modèle 405B, auquel il doit une partie de ses connaissances transférées. Il a aussi bénéficié de techniques récentes d’alignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome.  Son entraînement s’appuie sur 15 billions de jetons.</p>"
      },
      "Llama 4 Scout": {
        "desc": "<p>Grand modèle doté d’une très large fenêtre de contexte, adapté par exemple à la synthèse d'un ensemble de documents.</p>",
        "size_desc": "<p>Avec 109 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il peut être hébergé sur un serveur doté d’une seule carte graphique très puissante. Sa fenêtre de contexte va jusqu’à 10 millions de jetons, ce qui permet de traiter des corpus documentaires extrêmement longs.</p>",
        "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives . L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. Afin d'équilibrer performances multimodales, raisonnement et qualité conversationnelle, l'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données (pour ne garder que les plus complexes et intéressantes), fine-tuning ciblé et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons et peut traiter jusqu’à 8 images simultanément. </p>\n<p>Le modèle a été bien reçu à son lancement, notamment pour sa fenêtre de contexte impressionnante, une première dans le domaine, ainsi que pour son rapport qualité-prix sur des tâches comme le résumé, l’appel d’outils et la génération augmentée (RAG). Cela en fait un choix adapté pour les pipelines automatisés.</p>"
      },
      "Llama Maverick": {
        "desc": "<p>Très grand modèle doté d’une très large fenêtre de contexte, adapté par exemple au résumé de plusieurs documents en même temps.</p>",
        "size_desc": "<p>Avec 400 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il nécessite moins de ressources pour fonctionner que les modèles “denses” de cette taille. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>",
        "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Cela permet de transférer ses compétences plus vite et avec moins de calcul.  Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives - il peut traiter jusqu’à 8 images simultanément. L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. L'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données - en gardant seulement les plus complexes et intéressantes, fine-tuning ciblé et apprentissage par renforcement en ligne, pour équilibrer performances multimodales, raisonnement et qualité conversationnelle. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons. </p>\n<p>Le modèle Llama 4 Maverick a été présenté comme la réponse directe de Meta aux modèles DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estimé qu’il ne répondait pas aux attentes, en particulier sur les tâches de programmation et les travaux créatifs.</p>"
      },
      "Magistral Medium": {
        "desc": "<p>Modèle de raisonnement de taille moyenne multimodal et multilingue. Adapté à des tâches de programmation ou autres tâches nécessitant analyse approfondie compréhension de systèmes logiques complexes ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement requièrent plus de capacité de calcul pour produire une réponse, ce qui augmente leur consommation énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. Il a été entraîné avec de l’apprentissage par renforcement sur Mistral Medium 3 et n'a pas été distillé à partir de modèles de raisonnement existants. Ce modèle hérite des capacités multimodales de Mistral Medium 3 même si l'apprentissage par renforcement n'a été réalisé que sur du texte.</p>"
      },
      "Magistral Small": {
        "desc": "<p>Modèle de raisonnement de taille moyenne, multimodal et multilingue. Adapté à des tâches nécessitant une analyse approfondie, compréhension de systèmes logiques ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
        "size_desc": "<p>Avec 24 milliards de paramètres, ce modèle est classé parmi les modèles de taille moyenne. Il nécessite une seule carte graphiques puissante pour fonctionner. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. </p>\n<p>L'entraînement s'est fait en deux phases. La première, dite de raisonnement <em>cold-start</em> par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au modèle d'acquérir des capacités de base en raisonnement à partir de données d'instruction générale (10%). La seconde est une phase d'apprentissage par renforcement (RL, <em>renforcement learning</em>) à haute entropie, où le modèle est encouragé à explorer des solutions diverses et variées plutôt que de converger vers une seule réponse, et à générer des complétions longues (jusqu'à 32 000 jetons), ce qui permet de développer des capacités de raisonnement qui dépassent celles du modèle enseignant.</p>"
      },
      "Ministral": {
        "desc": "<p>Petit modèle multilingue conçu pour fonctionner sur un ordinateur portable sans connexion à un serveur, tout en offrant de bonnes capacités en synthèse de texte, réponses à des questions simples et utilisation d’outils.</p>",
        "size_desc": "<p>Avec ses 8 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles (entre 7 et 20 milliards de paramètres). Il peut être déployé localement sur un ordinateur assez puissant, garantissant la confidentialité des données ou hébergé sur un serveur avec une seule carte graphique pour limiter les coûts d’infrastructure.</p>",
        "fyi": "<p>Ce modèle utilise une méthode d'attention de requête groupée (GQA, grouped query attention) pour limiter le texte analysé à chaque étape de génération et gagner en vitesse et en mémoire: les temps de calculs sont réduits sans incidence sur la qualité. Le mécanisme d'attention est amélioré en appliquant des fenêtres de tailles différentes, ce qui permet de gérer de longs contextes (jusqu’à 128 000 jetons) tout en restant léger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui améliore ses performances sur des tâches multilingues.</p>"
      },
      "Mistral Large 2": {
        "desc": "<p>Grand modèle prévu pour traiter des questions et tâches complexes : par exemple génération de code, utilisation d’outils, analyse de documents longs ou compréhension précise du langage.</p>",
        "size_desc": "<p>Avec 123 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite un serveur équipé d’au moins une carte graphique puissante, ce qui implique un coût de fonctionnement important. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>",
        "fyi": "<p>Ce modèle a été entraîné avec une forte proportion de données en code (plus de 80 langages de programmation) et de mathématiques, ce qui améliore sa capacité à résoudre des problèmes complexes et à utiliser des outils externes.</p>"
      },
      "Mistral Medium 2506": {
        "desc": "<p>Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est particulièrement intéressant pour des tâches de programmation ou des tâches de raisonnement, par exemple les mathématiques.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>",
        "fyi": "<p>Ce modèle a été conçu pour offrir des performances solides à un coût inférieur à celui des modèles propriétaires ou semi-ouverts. Une attention particulière a été portée aux données d’usage professionnel pendant son entraînement. Il est particulièrement bon en comparaison à d’autres modèles de taille similaire à générer du code et réaliser des tâches mathématiques.</p>\n<p>Ce modèle a servi de base pour entraîner Magistral Medium - un modèle de raisonnement.</p>"
      },
      "Mistral Saba": {
        "desc": "<p>Modèle de taille moyenne conçu pour une compréhension linguistique et culturelle fine des langues du Moyen-Orient et d’Asie du Sud, notamment l’arabe, le tamoul et le malayalam.</p>",
        "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. </p>\n<p>Le modèle propose une fenêtre de contexte allant jusqu’à 128 000 jetons, adaptée à l’analyse de longs documents.</p>",
        "fyi": "<p>L’entraînement a porté principalement sur des textes en arabe, tamoul et malayalam. Les corpus régionaux ont été sélectionnés pour refléter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (découpage du texte en unités de base que le modèle peut traiter), une stratégie spécialisée adaptée aux langues à morphologie complexe comme l'arabe a été employée. Des optimisations visent à éviter la fragmentation excessive des mots et à maximiser la couverture du vocabulaire.</p>"
      },
      "Mistral Small 3.2": {
        "desc": "<p>Malgré son nom, c’est un modèle de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se démarque par un respect précis des requêtes et sa capacité à utiliser des outils avancées.</p>",
        "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle est considéré comme un modèle de taille moyenne. Il peut être hébergé sur un serveur disposant d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>",
        "fyi": "<p>La version 3.2 de ce modèle est optimisée pour générer des sorties structurées, notamment en JSON, tout en limitant la répétitivité et les comportements indésirables lors de longues générations. Multimodal, il traite à la fois des entrées textuelles et des images, permettant une analyse conjointe.</p>"
      },
      "Nemotron Llama 3.1 70B": {
        "desc": "<p>Grand modèle entraîné à partir de Llama 3.1 70B. Cette version réentraînée (fine-tune) a tendance à détailler davantage et fournir des réponses plus structurées.</p>",
        "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation notables.</p>",
        "fyi": "<p>Ce modèle est issu d’un réentraînement du Llama 3.1 70B, d'où la présence de son modèle-source dans son nom ! Il introduit des améliorations grâce à l’apprentissage par renforcement avec retour humain (RLHF) et à l’algorithme REINFORCE : le modèle explore différentes réponses, reçoit des retours sous forme de récompenses, puis ajuste ses choix progressivement pour mieux répondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilisé quand on veut que le modèle s’adapte à des préférences humaines ou qu’il optimise ses réponses selon des critères spécifiques.</p>"
      },
      "Phi-4": {
        "desc": "<p>Petit modèle multilingue, capable d’utiliser des outils et performant sur des tâches complexes comme la logique, les mathématiques et le code, tout en restant compact.</p>",
        "size_desc": "<p>Avec 14 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles. Il peut être déployé localement sur un ordinateur suffisamment puissant, ou hébergé sur un serveur avec une seule carte graphique, ce qui réduit les coûts d’infrastructure. La fenêtre de contexte, de 16 000 jetons, peut être limitante pour l’analyse de documents très longs.</p>",
        "fyi": "<p>Ce modèle utilise tiktoken pour la tokenisation, ce qui améliore ses capacités en contexte multilingue. Il a été entraîné sur un total de 9,8 <strong>billions</strong> de jetons, dont 400 milliards proviennent spécifiquement de données synthétiques de haute qualité, le reste étant constitué de données organiques filtrées. L'entraînement s'est déroulé sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-évaluation – pendant laquelle le modèle critique et réécrit ses réponses – ainsi que l'inversion des instructions ont été utilisées pour renforcer sa compréhension des consignes et ses capacités de raisonnement.</p>"
      },
      "Qwen 2.5 Coder 32B": {
        "desc": "<p>Modèle de taille moyenne spécialisé en programmation et dans l’usage d’outils externes (recherches web, interactions avec des logiciels…).</p>",
        "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modèle a été entraîné sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code spécialisés comme Haskell ou Racket. </p>\n<p>Grâce à ses performances en code, il est  capable de bien gérer les appels à des outils externes, ce qui est utile pour des usages agentiques.</p>"
      },
      "Qwen 2.5 max 0125": {
        "desc": "<p>Très grand modèle de raisonnement spécialisé et très performant en mathématiques, code et résolution de problèmes logiques.</p>",
        "size_desc": "<p>Ce modèle propriétaire basé sur une <strong>architecture MoE à grande échelle a été</strong>entraîné sur <strong>plus de 20 billions de jetons</strong>. Il est conçu pour des tâches nécessitant plusieurs étapes de réflexion. </p>\n<p>La fenêtre de contexte va jusqu’à 32 000 jetons.</p>",
        "fyi": "<p>La taille exacte du modèle n’est pas connue, mais c’est très probablement un très grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
      },
      "Qwen 3 30B A3B": {
        "desc": "<p>Modèle de taille moyenne multilingue.</p>",
        "size_desc": "<p>Avec 30 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. De plus l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.</p>",
        "fyi": "<p>Ce modèle MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activés par jeton, ce qui permet une inférence plus rapide et plus efficace. Il utilise un système appelé <em>global-batch</em> pour optimiser la répartition du travail entre les experts, afin qu'ils soient tous utilisés de manière équilibrée.</p>\n<p>Contrairement à d'autres modèles comme Qwen 2.5-MoE qui recyclent les mêmes experts à travers plusieurs couches du réseau, Qwen 3 30B A2B attribue des experts uniques à chaque couche. Concrètement, cela signifie que les experts de la première couche ne sont jamais réutilisés dans les couches suivantes - chaque niveau du modèle dispose de son propre ensemble d'experts spécialisés. Cette architecture permet à chaque expert de se concentrer exclusivement sur les tâches spécifiques à sa position dans le réseau neuronal, résultant en une spécialisation plus fine et des performances optimisées pour chaque étape du traitement de l'information.</p>"
      },
      "Qwen 3 32B": {
        "desc": "<p>Modèle de taille moyenne multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
        "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modèle a été entraîné sur un très grand volume de données : 36 billions de jetons, en 119 langues. L'entraînement s’est fait en trois étapes. Le modèle a d'abord appris à partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont été ajoutés pour renforcer ses connaissances factuelles. Enfin, il a été exposé à un corpus spécifique pour l’aider à mieux gérer les très longs textes. Résultat : il dispose en fin d'entrainement d'une fenêtre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents.</p>"
      },
      "o4 mini": {
        "desc": "<p>Très grand modèle de raisonnement, adapté pour des tâches et questions scientifiques et technologiques complexes.</p>",
        "size_desc": "<p>Malgré son nom et le fait que la taille exacte n’est pas connue, o4 mini est très probablement un grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Les modèles de raisonnement comme o4 mini nécessitent plus de temps pour répondre, car une phase de raisonnement précède la génération du résultat final, ce qui accroit leur consommation énergétique. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres pour générer chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>",
        "fyi": "<p>Ce modèle est très performant pour l’analyse d’images et de graphiques. Il a aussi été entraîné pour interagir avec d’autres systèmes via des appels de fonctions, ce qui rend possible son utilisation pour des cas d’usage agentiques. En tant que modèle très puissant de raisonnement, il peut notamment être utilisé pour répartir des tâches entre plusieurs modèles plus petits et/ou plus spécialisés.  Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, ce qui facilite l’analyse de longs documents.</p>"
      },
      "qwq 32B": {
        "desc": "<p>Modèle de raisonnement de taille moyenne spécialisé et très performant en mathématiques, génération de code, et résolution de problèmes logiques.</p>",
        "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Néanmoins, les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse car une phase de raisonnement précède la génération du résultat final, ce qui augmente la consommation énergétique.</p>",
        "fyi": "<p>Ce modèle a été entraîné avec une méthode d’apprentissage par renforcement (RL) pour optimiser la gestion des problèmes de mathématiques et des tâches de programmation. Il utilise plusieurs techniques récentes pour améliorer la qualité des réponses. Par exemple, la méthode RoPE (Rotary Position Embedding) lui permet de mieux comprendre l’ordre des mots dans un texte. La fonction d'activation SwiGLU est une manière plus efficace de gérer les calculs au sein du réseau de neurones qui aide le modèle à produire des réponses plus fiables. La méthode d'ajustement QKV (Query Key Value-biais) améliore la manière dont le modèle repère et sélectionne les informations importantes. Enfin, grâce à la méthode YaRN (Yet another RoPE extensioN method), il peut traiter de très longs textes allant jusqu’à 130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou très détaillés.</p>"
      }
    }
  }
}
