{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "accessData": "Accéder aux données",
        "contact": "Contactez-nous",
        "contactUs": "Nous contacter",
        "copyLink": {
            "do": "Copier le lien",
            "done": "Lien copié dans le presse-papiers"
        },
        "copyMessage": {
            "do": "Copier le message",
            "done": "Message copié"
        },
        "downloadData": "Télécharger les données",
        "home": "Page d'accueil",
        "returnHome": "Revenir à l'accueil",
        "scrollLeft": "Defiler vers la gauche",
        "scrollRight": "Défiler vers la droite",
        "searchModel": "Rechercher un modèle",
        "seeMore": "Voir plus",
        "selectLanguage": "Sélectionner une langue",
        "vote": "Donner mon avis"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 modèles",
            "help": "Si vous n’en choisissez qu’un, le second sera sélectionné de manière aléatoire",
            "question": "Quels modèles voulez-vous comparer ?"
        },
        "modelSelection": "Sélection des modèles",
        "prompt": {
            "label": "Écrivez votre premier message",
            "placeholder": "Écrivez votre premier message ici"
        },
        "selectModels": {
            "help": "Sélectionnez le mode de comparaison qui vous convient",
            "question": "Quels modèles voulez-vous comparer ?"
        },
        "suggestions": {
            "generateAnother": "Générer un autre message",
            "title": "Suggestions de prompts"
        },
        "title": "Comment puis-je vous aider aujourd'hui ?"
    },
    "chatbot": {
        "continuePrompt": "Continuer à discuter avec les modèles",
        "conversation": "Conversation",
        "errors": {
            "other": {
                "message": "Une erreur temporaire est survenue.",
                "retry": "Vous pouvez tenter de réessayer de solliciter les modèles.",
                "title": "Oups, erreur temporaire",
                "vote": "Ou bien conclure votre expérience en donnant votre avis sur les modèles."
            },
            "tooLong": {
                "message": "Chaque modèle est limité dans la taille des conversations qu'il est capable de traiter.",
                "retry": "Vous pouvez recommencer une conversation avec deux nouveaux modèles.",
                "title": "Oups, la conversation est trop longue pour un des modèles.",
                "vote": "Vous pouvez tout de même donner votre avis sur ces modèles ou recommencer une conversation avec deux nouveaux."
            }
        },
        "loading": "Chargement des réponses",
        "reasoning": {
            "finished": "Raisonnement terminé",
            "inProgress": "Raisonnement en cours…"
        },
        "revealButton": "Passer à la révélation des modèles"
    },
    "closeModal": "Fermer la fenêtre modale",
    "components": {
        "pagination": {
            "first": "Première page",
            "label": "Pagination",
            "last": "Dernière page",
            "next": "Page suivante",
            "nth": "Page {count}",
            "previous": "Page précédente"
        },
        "table": {
            "linePerPage": "Nombre de lignes par page",
            "pageCount": "{count} lignes par page",
            "triage": "Trier"
        },
        "theme": {
            "legend": "Choisissez un thème pour personnaliser l’apparence du site.",
            "options": {
                "dark": "Thème sombre",
                "light": "Thème clair",
                "system": "Système",
                "systemSub": "Utilise les paramètres système"
            },
            "title": "Paramètres d'affichage"
        }
    },
    "datasets": {
        "access": {
            "catch": "Editeurs de modèles, chercheurs, chercheuses, entreprises, à vous de jouer !",
            "desc": "Les questions et préférences posées sur la plateforme sont majoritairement en français et reflètent des usages réels et non contraints. Ces jeux de données sont accessibles sur <a {linkProps}>data.gouv</a> et Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "Ensemble des réponses et des questions posées",
                    "title": "/conversations"
                },
                "reactions": {
                    "desc": "Ensemble des réactions exprimées",
                    "title": "/réactions"
                },
                "votes": {
                    "desc": "Ensemble des préférences exprimées",
                    "title": "/votes"
                }
            },
            "share": "Partagez-nous vos réutilisations",
            "title": "Accédez aux jeux de données compar:IA"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "desc": "Analyse des conversations des utilisateurs avec détection des tâches (création, recherche d'informations...), des sujets (arts et culture, éducation...), des émotions complexes (curiosité, enthousiasme...), des types de langage (formel, professionnel...)",
                    "title": "Accéder à l’analyse par indicateur"
                },
                "conversations": {
                    "desc": "Visualisation interactive des conversations où chaque point représente un cluster de discussions évoqué par les utilisateurs (comme l’éducation, la santé, l’environnement, ou encore la philosophie).",
                    "title": "Explorer la visualisation de données"
                },
                "desc": "L'équipe Bunka.ai a mené une étude approfondie sur les interactions entre les utilisateurs de la plateforme Compar:IA et les modèles d'IA, examinant les thématiques privilégiées, les tâches principales et déterminant si ces modèles fonctionnent avant tout comme des outils d'automatisation ou d'augmentation des capacités humaines. Cette analyse repose sur un large échantillon de 25 000 conversations.",
                "method": "En savoir plus sur la méthodologie"
            },
            "desc": "Exemples de réutilisation des jeux de données compar:IA",
            "title": "Comment ces données sont-elles utilisées ?"
        }
    },
    "errors": {
        "404": {
            "desc": "Si vous avez tapé l'adresse web dans le navigateur, vérifiez qu'elle est correcte. La page n’est peut-être plus disponible.<br />Dans ce cas, pour continuer votre visite vous pouvez consulter notre page d’accueil.<br />Sinon contactez-nous pour que l’on puisse vous rediriger vers la bonne information.",
            "error": "Erreur 404",
            "sorry": "La page que vous cherchez est introuvable. Excusez-nous pour la gène occasionnée.",
            "title": "Page non trouvée"
        },
        "unexpected": {
            "desc": "Essayez de rafraîchir la page ou bien ressayez plus tard.",
            "error": "Erreur {code}",
            "sorry": "Désolé, le service rencontre un problème, nous travaillons pour le résoudre le plus rapidement possible.",
            "title": "Erreur inattendue"
        },
        "unknown": "Une erreur est survenue"
    },
    "faq": {
        "datasets": {
            "questions": {
                "1": {
                    "desc": "<p>Les données de préférence servent à améliorer les modèles lors d'entraînements futurs.</p><p>En comparant à l'aveugle les réponses de deux modèles, les utilisateurs de compar:IA expriment leurs préférences, indiquant ainsi quelles réponses sont les plus pertinentes. Ces données de préférence peuvent être utilisées pour affiner l'alignement des modèles, c'est-à-dire pour les entraîner à générer des réponses plus conformes aux attentes et aux préférences des utilisateurs.</p><p>Il s'agit d'un processus itératif, où le modèle apprend progressivement à générer de meilleures réponses en fonction des retours formulés par les humains sur la qualité des réponses. En étant exposés à des données de préférence, les modèles apprennent à les intégrer dans leur processus de génération de réponses.</p>",
                    "title": "Les données de préférence ont-elles un effet immédiat pour améliorer les modèles?"
                },
                "2": {
                    "desc": "<p>La spécificité des données collectées sur la plateforme compar:IA est qu’elles sont en français et qu’elles correspondent à des tâches réelles des utilisateurs. Ces données reflètent des préférences humaines dans un contexte linguistique et culturel précis. Elles permettent dans un second temps d'ajuster les modèles pour qu’ils soient plus pertinents, précis et adaptés aux usages des utilisateurs, tout en comblant les éventuels biais ou lacunes des modèles actuels.</p>",
                    "title": "Pourquoi les données de préférence collectées sur compar:IA ont-elles de la valeur ?"
                },
                "3": {
                    "desc": "<p>compar:IA se positionne comme un outil d'évaluation et d'alignement spécifique au français, axé sur la qualité des réponses et la collecte de données de préférence, se distinguant ainsi de l'approche de classement global de <a href='https://lmarena.ai/' target='_blank'>chatbot arena</a> développé par <a href='http://lmsys.org' target='_blank'>lmsys.org</a> et de l'alignement éthique des modèles d’IA de <a href='https://hannahkirk.github.io/prism-alignment/' target='_blank'>Prism Alignment Project</a>.</p>",
                    "title": "Quelle est la spécificité de compar:IA par rapport à d’autres initiatives similaires ?"
                }
            },
            "title": "Jeu de données"
        },
        "ecology": {
            "questions": {
                "1": {
                    "desc": "<p>compar:IA utilise la méthodologie développée par <a target='_blank' href='https://ecologits.ai/latest/'><strong>Ecologits</strong> (GenAI Impact)</a> pour fournir un bilan énergétique qui permet aux utilisateurs de comparer l'impact environnemental de différents modèles d'IA pour une même requête. Cette transparence est essentielle pour encourager le développement et l'adoption de modèles d'IA plus éco-responsables.</p><p>Ecologits applique les principes de l'analyse du cycle de vie (ACV) conformément à la norme ISO 14044 en se concentrant pour le moment sur l'impact de <strong>l'inférence</strong> (c'est-à-dire l'utilisation des modèles pour répondre aux requêtes) et de la <strong>fabrication des cartes graphiques</strong> (extraction des ressources, fabrication et transport).</p><p>La consommation électrique du modèle est estimée en tenant compte de divers paramètres tels que la taille du modèle d'IA utilisé, la localisation des serveurs où sont déployés les modèles et le nombre de tokens de sortie. Le calcul de l’indicateur de potentiel de réchauffement climatique exprimé en équivalent CO2 est dérivé de la mesure de consommation électrique du modèle.</p><p>Il est important de noter que les méthodologies d'évaluation de l'impact environnemental de l'IA sont encore en développement.</p>",
                    "title": "Comment les indicateurs écologiques sont-ils calculés ?"
                },
                "2": {
                    "desc": "<p>La localisation des centres de données joue un rôle dans l'empreinte carbone de l'IA. Si un modèle est entraîné ou utilisé dans un pays fortement dépendant des énergies fossiles, son impact environnemental sera plus important que s'il est hébergé dans un pays utilisant majoritairement des énergies renouvelables.</p><p>La méthode d'analyse de l'impact environnemental de l'IA développée par <a target='_blank' href='https://ecologits.ai/latest/'>Ecologits (de GenAI Impact)</a>, intègre des données sur le mix énergétique des différents pays où se situent les serveurs. Cela permet d'obtenir une estimation plus précise et nuancée de l'empreinte carbone réelle de l’inférence sur les différents modèles d’IA générative.</p>",
                    "title": "Les indicateurs écologiques tiennent-ils compte du mix énergétique des différents pays ?"
                },
                "3": {
                    "desc": "<p>Les indicateurs d'impact écologique actuels se focalisent principalement sur l'impact de <strong>l'inférence</strong>, c'est-à-dire l'utilisation des modèles d'IA pour répondre aux requêtes. Cette approche peut donner l'illusion que l'inférence est moins énergivore que l'entraînement des modèles. Cependant, <strong>la réalité est plus complexe.</strong> Prenons l'analogie de la voiture :</p><ul><li>Construire une voiture (l'entraînement) est un processus ponctuel et gourmand en ressources.</li><li>Chaque trajet en voiture (l'inférence) consomme moins d'énergie, mais ces trajets sont répétés quotidiennement, et leur nombre est potentiellement immense.</li></ul><p>De la même manière, <strong>l'impact cumulé de l'inférence, à l'échelle de millions d'utilisateurs effectuant des requêtes quotidiennement, peut s'avérer supérieur à l'impact de l'entraînement initial.</strong> C'est pourquoi il est crucial que les outils d'évaluation de l'empreinte carbone de l'IA prennent en compte <strong>l'ensemble du cycle de vie</strong> des modèles, de l'entraînement à l'utilisation en production</p>",
                    "title": "Les indicateurs d’impact écologique tiennent-ils compte des ressources utilisées pour entraîner les modèles ?"
                }
            },
            "title": "Indicateurs écologiques"
        },
        "i18n": {
            "questions": {
                "1": {
                    "desc": "<p>Oui, l'internationalisation de compar:IA est en cours. Nous commençons par un élargissement à trois pays pilotes : la Lituanie, la Suède et le Danemark. Cette première phase permet de tester l’approche et d'adapter l’interface à différents contextes linguistiques et culturels européens. À terme, le cercle pourra s'étendre à davantage de langues européennes selon les retours d'expérience de ces pays pilotes. L'objectif est de construire progressivement un véritable commun numérique européen pour l'évaluation humaine des IA conversationelles, avec une gouvernance collaborative qui reste encore à définir entre les différents pays participants.</p>",
                    "title": "compar:IA s’est d’abord concentré sur le français : y a-t-il des plans pour d'autres langues européennes ?"
                },
                "2": {
                    "desc": "<p>Le développement d’un plateforme européenne de comparaison des modèles d’IA conversationnelle offre plusieurs avantages concrets. Elle permet de collecter des données de préférence reflétant les besoins réels des utilisateurs européens, améliorant ainsi la pertinence des modèles pour ce public. Elle garantit ainsi une meilleure représentation des langues et cultures européennes, souvent sous-représentées dans les évaluations globales dominées par l'anglais. Elle assure aussi une conformité avec les réglementations européennes (RGPD, AI Act) et intègre des critères d'évaluation alignés sur les priorités européennes comme la durabilité environnementale et la transparence algorithmique. Enfin, elle favorise l'émergence d'un écosystème d'IA européen compétitif et autonome.</p>",
                    "title": "Quels sont les avantages d'une plateforme de collecte de préférences spécifiquement européenne ?"
                }
            },
            "title": "Internationalisation"
        },
        "models": {
            "questions": {
                "1": {
                    "desc": "<p>Nous choisissons les modèles en fonction de leur popularité, de leur diversité et de la pertinence pour les utilisateurs. Nous veillons particulièrement à rendre accessibles des modèles dits <em>open weights</em> (semi-ouverts) et de taille différentes.</p>",
                    "title": "Comment choisissez-vous les modèles présents dans le comparateur ?"
                },
                "2": {
                    "desc": "<p>L’inférence, c’est-à-dire le fait de pouvoir interroger les modèles, est prise en charge par le projet. Pour la plupart des modèles, nous passons par les API d’Open Router et de Hugging Face et nous réglons les coûts à l’usage, au jeton.</p>",
                    "title": "Comment parvenez-vous à rendre ce service gratuit ?"
                },
                "3": {
                    "desc": "<p>Les modèles quantisés sont optimisés pour consommer moins de ressources en simplifiant certains calculs tout en visant la meilleure qualité de réponse.</p><p>La quantisation est une technique d'optimisation qui consiste à réduire la précision des nombres utilisés pour représenter les paramètres d'un modèle d'IA. Cela permet de <strong>diminuer la taille du modèle</strong> et <strong>d'accélérer les calculs</strong>, ce qui est particulièrement avantageux pour l'inférence sur des machines limitées en ressources.</p>",
                    "title": "« modèle quantisé », quésaco ?"
                },
                "4": {
                    "desc": "<p><strong>La capacité d'un modèle à parler plusieurs langues est liée à la diversité linguistique de ses données d'entraînement et non au pays</strong>. Les <strong>LLM utilisent d'énormes corpus dans de nombreuses langues</strong>, mais la répartition des langues dans les données d'entraînement n'est pas uniforme. Une surreprésentation de l'anglais peut entraîner des limitations dans d'autres langues. Ces limitations se traduisent par exemple par des <strong>anglicismes ou une incapacité à générer des contenus dans certaines langues classées « en danger » par l'UNESCO</strong>.</p><p><strong>L'exactitude et la richesse du vocabulaire d'un modèle dépendent des données utilisées pour son apprentissage</strong>.</p>",
                    "title": "Y a-t-il un lien entre la nationalité de l’entreprise ou du laboratoire à l’origine du modèle et sa capacité à parler plusieurs langues ?"
                },
                "5": {
                    "desc": "<p>Rares sont les acteurs à être “transparents” sur les sources de données utilisées dans les corpus d’entraînement. Ces informations sont souvent confidentielles pour des raisons légales et commerciales.</p>",
                    "title": "Peut-on connaître les données d’entraînement des modèles ?"
                }
            },
            "title": "Modèles"
        },
        "title": "Foire aux questions",
        "usage": {
            "questions": {
                "1": {
                    "desc": "<p>Les modèles de langage conversationnels actuels sont <strong>incapables de citer les sources</strong> qu'ils ont utilisées pour générer une réponse. Ils fonctionnent en prédisant le mot suivant le plus probable en fonction de la distribution statistique des données d'entraînement. Bien qu'ils puissent synthétiser des informations provenant de diverses sources, ils ne conservent pas la trace de l'origine de ces informations.</p><p>Cependant, il existe des techniques comme la <strong>Génération Augmentée par Récupération (RAG)</strong> qui visent à pallier cette limitation. Le RAG permet aux modèles d'accéder à des bases de connaissances externes et de <strong>fournir des informations contextualisées en citant les sources</strong>. Cette approche est essentielle pour améliorer la transparence et la fiabilité des réponses générées par les modèles.</p>",
                    "title": "Les modèles peuvent-ils citer leurs sources ?"
                },
                "2": {
                    "desc": "<p>Vous avez posé la question suivante “explique-moi la motion de censure à l'œuvre actuellement en France à l'Assemblée nationale et cite-moi tes sources” et avez été déçu·e des réponses ? C’est normal…</p><p><strong>Les modèles d'IA conversationnels “bruts” ne peuvent pas répondre aux questions sur l'actualité la plus récente.</strong> Ils sont entraînés sur des ensembles de données statiques et ne peuvent pas interagir avec le web ou ouvrir des liens. Ils n'ont pas la capacité de se mettre à jour en temps réel avec les événements qui se déroulent dans le monde. Les informations auxquelles le modèle a accès sont limitées à la date de son dernier entraînement.</p><p>Par conséquent, si vous posez une question sur un fait d’actualité récent, le modèle s'appuiera sur des informations potentiellement obsolètes, risquant de générer des réponses inexactes.</p><p>Dans le cas de Perplexity, Copilot ou ChatGPT, les modèles d’IA conversationnelle dits “bruts” sont associés à d’autres briques technologiques qui permettent de se connecter à internet pour accéder à des informations en temps réel. On parle alors “d’agents conversationnels”.</p>",
                    "title": "Si je pose une question sur l’actualité la plus récente, le modèle peut-il répondre ?"
                },
                "3": {
                    "desc": "<p>Si vous intégrez une URL dans une requête, le modèle conversationnel ne peut pas y accéder directement. Les modèles de langage traitent le texte de la requête mais n'ont pas la capacité d'interagir avec le web ou d'ouvrir des liens. Ils sont entraînés sur un ensemble de données textuelles fixes et leurs réponses reposent sur ces données d’entraînement. Lorsqu'une question est posées, les modèles utilisent cet entraînement pour générer une réponse mais ne peuvent pas accéder à de nouvelles informations en ligne.</p><p>Par analogie, imaginez un étudiant passant un examen sans accès à internet. Il peut utiliser ses connaissances acquises pour répondre aux questions, mais ne peut pas consulter de sites web pour obtenir des informations supplémentaires.</p>",
                    "title": "Si j’intègre un lien d’URL dans une requête, le modèle peut-il y accéder ?"
                },
                "4": {
                    "desc": "<p>Il arrive que les modèles perdent le fil d'une conversation en raison de leur <strong>fenêtre de contexte limitée.</strong> Cette « fenêtre » représente la quantité d'informations précédentes que le modèle peut retenir, agissant comme une mémoire à court terme. Plus la fenêtre est petite, plus le modèle est susceptible d'oublier des éléments clés de la conversation, conduisant à des réponses incohérentes. Les conversations longues ou complexes peuvent rapidement saturer la fenêtre de contexte, augmentant le risque d'incohérence.</p><p>Par analogie, imaginez une personne qui ne se souvient que des cinq dernières phrases d'une conversation. Si la conversation est courte, la personne peut suivre. Mais si la conversation devient longue, la personne oubliera des informations cruciales, ce qui rendra ses réponses incohérentes. De même, un modèle d'IA avec une petite fenêtre de contexte peut « perdre le fil » d'une conversation lorsque trop d'informations sont échangées, oubliant des éléments clés et produisant des réponses qui n'ont plus de sens.</p>",
                    "title": "Pourquoi certains modèles perdent-ils rapidement le fil de la conversation ?"
                },
                "5": {
                    "desc": "<p>La formulation des questions, ou « prompts », influence la cohérence de la conversation. Pour obtenir les meilleurs résultats d'un modèle de langage, il est essentiel de maîtriser l'art du <em>prompting</em>, c'est-à-dire la formulation des requêtes ou instructions. <strong>La clarté est primordiale</strong>:</p><ul><li>Utilisez un langage simple et direct, en évitant les questions trop longues ou complexes. Décomposez les requêtes en plusieurs questions plus simples pour des réponses plus précises.</li><li><strong>Précisez si besoin des contraintes de formats spécifiques</strong> : Si vous avez besoin d’une réponse dans un certain format (liste, tableau, résumé, etc.), précisez-le dans le prompt. Vous pouvez également préciser les étapes à suivre et les critères de qualité souhaités.</li><li><strong>Spécifiez le rôle du modèle</strong> : Par exemple, commencez par “Agis comme un expert en…” ou “Imagine que tu es un enseignant…” pour orienter le ton et la perspective de la réponse.</li><li><strong>Contextualisez vos questions</strong> : si nécessaire, fournissez des exemples pertinents pour guider le modèle.</li><li><strong>Encouragez le raisonnement</strong>: utilisez l’incitation au raisonnement pas à pas (<em>Chain-of-Thought Prompting</em>) pour demander au modèle d'expliciter son raisonnement, ce qui rend les réponses plus robustes.</li></ul><p>Les modèles conversationnels sont sensibles aux variations de formulation: un langage simple, des questions courtes et une reformulation si nécessaire peuvent aider à guider le modèle vers des réponses pertinentes. Testez et affinez vos prompts pour trouver la formulation la plus efficace !</p>",
                    "title": "Quelles sont les bonnes pratiques pour prompter ?"
                },
                "6": {
                    "desc": "<p>L'IA conversationnelle répond directement en formulant des phrases à partir d’un grand ensemble de données sur lesquelles le modèle a été entraîné, tandis qu’un moteur de recherche propose des liens et des ressources pour que l’internaute les explore lui-même.</p>",
                    "title": "Quelle est la différence entre poser une question à un modèle d’IA conversationnelle et faire une recherche sur Google ?"
                }
            },
            "title": "Usage"
        }
    },
    "footer": {
        "backHome": "Retour à l'accueil du site - compar:IA",
        "dpg": "Le service est reconnu comme un bien public numérique par l’Alliance Digital Public Goods",
        "helpUs": "Aidez-nous à améliorer ce service !",
        "license": {
            "linkTitle": "Licence etalab - nouvelle fenêtre",
            "mention": "Sauf mention explicite de propriété intellectuelle détenue par des tiers, les contenus de ce site sont proposés sous <a {linkProps}>licence etalab-2.0</a>"
        },
        "links": {
            "accessibility": "Accessibilité : non conforme",
            "legal": "Mentions légales",
            "privacy": "Politique de confidentialité",
            "rgesn": "Écoconception",
            "sources": "Code source",
            "tos": "Modalités d'utilisation"
        },
        "writeUs": "Si vous rencontrez un problème ou si vous avez un commentaire sur le comparateur, n'hésitez pas à nous écrire <a {formLinkProps}>via ce formulaire</a>, nous lisons tous vos messages.<br />Merci !"
    },
    "general": {
        "a11y": {
            "desc": "Cette déclaration d’accessibilité s’applique au site <strong>comparia.beta.gouv.fr</strong>.",
            "disclaimer": "<strong>compar:IA</strong> s’engage à rendre ses services numériques accessibles, conformément à l’article 47 de la loi n° 2005-102 du 11 février 2005.",
            "improveAdress": "Adresse : DINUM, 20 avenue de Ségur 75007 Paris",
            "improveDelay": "Nous essayons de répondre dans les 2 jours ouvrés.",
            "improveDesc": "Si vous n’arrivez pas à accéder à un contenu ou à un service, vous pouvez contacter le responsable de beta.gouv.fr pour être orienté·e vers une alternative accessible ou obtenir le contenu sous une autre forme.",
            "improveMail": "Courriel : <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Amélioration et contact",
            "remedyAdvocate": "Écrire un message au <a {linkProps}>Défenseur des droits</a>",
            "remedyAdvocateAdress": "Envoyer un courrier par la poste (gratuit, ne pas mettre de timbre) : Défenseur des droits - Libre réponse 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Contacter le délégué du <a {linkProps}>Défenseur des droits dans votre région</a>",
            "remedyDesc": "Cette procédure est à utiliser dans le cas suivant : vous avez signalé au responsable du site internet un défaut d’accessibilité qui vous empêche d’accéder à un contenu ou à un des services du portail et vous n’avez pas obtenu de réponse satisfaisante.",
            "remedyList": "Vous pouvez :",
            "remedyTitle": "Voie de recours",
            "stateDesc": "Le site comparia.beta.gouv.fr est non conforme avec le RGAA 4.1. Le site n’a <strong>pas encore été audité</strong>. Il a cependant été conçu pour être accessible au plus grand nombre. Vous devriez donc pouvoir :",
            "stateNavigate": "naviguer sur toutes les pages du site en utilisant un clavier",
            "statePrefs": "adapter le site à votre préférences (taille de la police, zoom écran, changement de typographie…) sans perte de contenu",
            "stateScreenReader": "consulter le site web avec un lecteur d’écran.",
            "stateTitle": "État de conformité",
            "title": "Déclaration d’accessibilité"
        },
        "legal": {
            "a11yDesc": "La conformité aux normes d’accessibilité numérique est un objectif ultérieur mais nous tâchons de rendre ce site accessible à toutes et à tous.",
            "a11yTitle": "Accessibilité",
            "directorDesc": "Monsieur Romain Delassus, chef du service du numérique du Ministère de la Culture",
            "directorTitle": "Directeur de la publication",
            "editorDesc": "Ce site est édité par le Ministère de la culture, 182, rue Saint-Honoré 75001 Paris",
            "editorTitle": "Éditeur",
            "hostingDesc": "Ce site est hébergé par OVH SAS (<a {linkProps}>https://www.ovh.com</a>) dont le siège social est situé au 2 rue Kellermann - 59100 Roubaix - France.",
            "hostingTitle": "Hébergement du site",
            "reportA11y": "Si vous rencontrez un défaut d’accessibilité vous empêchant d’accéder à un contenu ou une fonctionnalité du site, merci de nous en faire part.",
            "reportA11yDesc": "Pour en savoir plus sur la politique d’accessibilité numérique de l’État : <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "reportDesc": "Si vous n’obtenez pas de réponse rapide de notre part, vous êtes en droit de faire parvenir vos doléances ou une demande de saisine au Défenseur des droits.",
            "reportTitle": "Signaler un dysfonctionnement",
            "securityCertif": "Le site est protégé par un certificat électronique, matérialisé pour la grande majorité des navigateurs par un cadenas. Cette protection participe à la confidentialité des échanges.",
            "securityNoMail": "En aucun cas les services associés à la plateforme ne seront à l’origine d’envoi de courriels pour demander la saisie d’informations personnelles.",
            "securityTitle": "Sécurité",
            "sources": "Sauf mention contraire, tous les textes de ce site sont sous <a {etalabLinkProps}>licence Etalab Open 2.0</a>. Le code source de cette application est librement réutilisable et accessible sur <a {githubLinkProps}>GitHub</a>.",
            "title": "Mentions légales"
        },
        "privacy": {
            "cookiesBannerDesc": "C’est vrai, vous n’avez pas eu à cliquer sur un bloc qui recouvre la moitié de la page pour dire que vous êtes d’accord avec le dépôt de cookies — même si vous ne savez pas ce que ça veut dire !",
            "cookiesBannerNoNeed": "Rien d’exceptionnel, pas de passe-droit lié à un .gouv.fr. Nous respectons simplement la loi, qui dit que certains outils de suivi d’audience, correctement configurés pour respecter la vie privée, sont exemptés d’autorisation préalable.",
            "cookiesBannerTitle": "Ce site n’affiche pas de bannière de consentement aux cookies, pourquoi ?",
            "cookiesBannerTools": "Nous utilisons pour cela <a {matomoLinkProps}>Matomo</a>, un outil <a {libreLinkProps}>libre</a>, paramétré pour être en conformité avec la <a {cnilLinkProps}>recommandation « Cookies »</a> de la CNIL. Cela signifie que votre adresse IP, par exemple, est anonymisée avant d’être enregistrée. Il est donc impossible d’associer vos visites sur ce site à votre personne.",
            "cookiesDesc": "Ce site dépose un petit fichier texte (un « cookie ») sur votre ordinateur lorsque vous le consultez. Cela nous permet de mesurer le nombre de visites et de comprendre quelles sont les pages les plus consultées.",
            "cookiesDescMore": "Vous pouvez vous opposer au suivi de votre navigation sur ce site web. Cela protégera votre vie privée, mais empêchera également le propriétaire d'apprendre de vos actions et de créer une meilleure expérience pour vous et les autres utilisateurs.",
            "cookiesTitle": "Cookies déposés et consentement",
            "dataAccessDatasets": "Les données de dialogue et de préférence de l’utilisateur sont distribuées sous la Licence Ouverte 2.0 d'Etalab sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "dataAccessDesc": "Bien sûr ! Les statistiques d’usage du site sont disponibles en accès libre sur <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessTitle": "Je contribue à enrichir vos données, puis-je y accéder ?",
            "dataExtraCountry": "Pays destinataire : France",
            "dataExtraHost": "Sous-traitant : OVH",
            "dataExtraTitle": "Qui nous aide à traiter les données ?",
            "dataExtraWarranty": "Garanties : <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>",
            "dataExtraWhat": "Traitement réalisé : Hébergement",
            "dataRespDesc": "Le service du numérique du ministère de la Culture est le responsable du traitement de vos données à caractère personnel.",
            "dataRespTitle": "Qui est responsable du traitement des données ?",
            "dataTimeDesc": "Les données relatives aux utilisateurs et à leurs conversations avec les modèles sont conservées à compter de l’enregistrement du vote de préférence.",
            "dataTimeTitle": "Pendant combien de temps conservons-nous ces données ?",
            "dataUseDesc": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s’assurer de l’anonymisation des données de dialogue avant leur mise à disposition publique.",
            "dataUseTitle": "Quels sont les traitements réalisés sur les données de conversation ?",
            "desc": "Le service est édité par le service du numérique du ministère de la Culture.",
            "privacyData": "Les données collectées sur le site sont les suivantes :",
            "privacyDataArena": "Données relatives aux conversations des utilisateurs avec les modèles : questions posées par les utilisateurs, réponses des modèles et préférence exprimée par l’utilisateur sur les deux modèles",
            "privacyDataForm": "Données relatives au questionnaire « Nous aider à améliorer compar:IA ».",
            "privacyDesc": "Le service ne traite pas de données à caractère personnel telles que définies par la CNIL, à savoir toute information relative à une personne physique susceptible d'être identifiée, directement ou indirectement.",
            "privacyResp": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme. En acceptant les <a {linkProps}>modalités d’utilisation</a>, l’utilisateur ou l’utilisatrice s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
            "privacyTitle": "Traitons-nous des données à caractère personnel ?",
            "title": "Politique de confidentialité"
        },
        "rgesn": {
            "1": {
                "1": "Le site compar:IA permet de donner accès simplement et gratuitement à une très grande diversité de modèles d'IA conversationnelle et ainsi de sensibiliser les citoyens aux divers enjeux des IA génératives : diversité, biais, impact environnemental. Par ailleurs, les jeux de données de questions et de préférences constitués sont partagés en open source sous licence MIT/Etalab 2.0.",
                "2": "Les cibles utilisatrices du service Compar:IA ont été identifiées en procédant à des entretiens utilisateurs et une phase préalable d'investigation des besoins.",
                "3": "Ainsi, tous les citoyens sont concernés comme utilisateurs potentiels, mais particulièrement les professeurs, élèves, étudiants, médiateurs numériques, formateurs IA, ainsi que chercheurs (computer sciences, IA, SHS) et entreprises intéressées par l'analyse ou l’utilisation des jeux de données partagés en open source.<br> En cohérence, le service numérique répond à leurs besoins grâce aux fonctionnalités suivantes : tester et comparer gratuitement les modèles, utiliser les données partagées en open source.",
                "4": "Le service compar:IA s’inscrit dans une démarche d’écoconception visant à réduire les impacts environnementaux. À cette fin, la déclaration a été rédigée dans le cadre de la mise en œuvre du référentiel général de l’écoconception des services numériques (RGESN, version 2024).<br> Le RGESN, réalisé par l’Arcep et l’Arcom, en collaboration avec l’ADEME, la DINUM, la CNIL et l’Inria, est disponible sur le <a {linkProps}>site web de l’Arcep</a>.",
                "title": "Raison d’être de ce service"
            },
            "2": {
                "1": "L'écoconception de services numériques est une démarche visant à réduire et limiter les impacts environnementaux d’un service dès sa phase de conception et tout au long de son cycle de vie.",
                "2": {
                    "1": "Concevoir des services numériques plus durables permettant d’allonger la durée de vie des terminaux ;",
                    "2": "Promouvoir une démarche de sobriété environnementale face aux stratégies de captation de l’attention de l’utilisateur pour des usages en ligne avec les objectifs environnementaux internationaux ;",
                    "3": "Diminuer les ressources informatiques mobilisées, optimiser le trafic de données et la sollicitation des infrastructures numériques ;",
                    "4": "Accroître le niveau de transparence sur l’empreinte environnementale du service numérique.",
                    "title": "Sa mise en œuvre poursuit quatre objectifs principaux :"
                },
                "3": {
                    "1": "Limiter le poids d’une page",
                    "2": "Limiter la complexité d’une page",
                    "3": "Limiter le nombre de requêtes serveurs",
                    "4": "Limiter la consommation d’énergie permettant d’héberger le service",
                    "title": "Concrètement, l’écoconception se traduit par un ensemble de bonnes pratiques à respecter avant, pendant et après la création ou l’amélioration d’un service, permettant notamment de :"
                },
                "4": "Il s’agit d’une démarche d'amélioration continue et collective.",
                "title": "Qu’est ce que l’écoconception ?"
            },
            "3": {
                "1": {
                    "1": "Simplification de l’expérience utilisateur : les parcours de navigation sont optimisés et centrés autour des fonctionnalités essentielles du service Compar:IA (arène de modèles à tester, jeux de données, classement) avec des tests utilisateurs réguliers.",
                    "2": "Sensibilisation auprès des utilisateurs sur l’empreinte écologique du service : mise en avant d’indicateurs d’impact énergétique sur les questions posées aux IA ; valorisation des usages « frugaux », avec un mode dédié à l’interrogation de « petits » modèles d’IA conversationnelles dont l’impact environnemental est moindre.",
                    "3": "Utilisation des composants d’interfaces du Système de Design de l’État (DSFR) : ils sont accessibles et leur empreinte est minimale.",
                    "4": "Navigation la plus intuitive possible pour réduire le temps passé à trouver une information.",
                    "5": "Conception « mobile first » : tous les contenus sont consultables a minima sur mobile. Le site s’adapte à toutes les tailles d’écran.",
                    "6": "Exclusion de l’intégration des vidéos sur le site.",
                    "title": "L’enjeu principal consiste à ce que le service compar:IA fonctionne sur les terminaux utilisateurs les plus anciens possibles, dans des conditions de connectivité variées. Plusieurs mesures ont été mise en œuvre lors de la conception, parmi lesquelles :"
                },
                "title": "Stratégie mise en œuvre et objectifs en matière de réduction ou de limitation des impacts environnementaux"
            },
            "4": {
                "1": "L’auto-diagnostic, mené entre mai et octobre 2025 par l’équipe produit (deux personnes ont été en charge de la relecture et la validation de tous les critères), a porté sur l’ensemble des pages du site. Les outils utilisés pour évaluer certains indicateurs techniques sont l’inspecteur « réseau » des outils de développement du navigateur, l’extension <a {greenitLinkProps}>Green IT Analysis</a> et l’outil <a {ecoindexLinkProps}>EcoIndex</a>.",
                "title": "Diagnostic RGESN"
            },
            "5": {
                "1": {
                    "1": "Score d’avancement dans la mise en œuvre du RGESN, le 10/12/2025 : 89%",
                    "title": "L’auto-diagnostic donne un taux de conformité au <a {linkProps}>RGESN</a> (référentiel général d’écoconception) de 89%."
                },
                "2": "Le service numérique compar:IA vise une amélioration de ce score d’avancement pour atteindre 95% en 2027.",
                "3": {
                    "1": "Stratégie : 100 %",
                    "2": "Spécifications : 78 %",
                    "3": "Architecture : 100 %",
                    "4": "Expérience et interface utilisateur (UX/UI) : 100 %",
                    "5": "Contenus : 67 %",
                    "6": "Frontend : 82 %",
                    "7": "Backend : 64 %",
                    "8": "Hébergement : 90 %",
                    "9": "Algorithmie : 100 %",
                    "title": "Score par thématique :"
                },
                "4": {
                    "date": "10 décembre 2025",
                    "title": "Tableau d’audit RGESN"
                },
                "title": "Score dans la mise en oeuvre du référentiel"
            },
            "6": {
                "1": "Le site est conçu pour être accessible pour tout équipement mobile datant de 2017 minimum.",
                "2": "Connexion minimum pour un accès et une utilisation confortable du service : 3G en mobile et 512 Kbs en connexion fixe.",
                "3": "Le site est conçu pour les différentes tailles d’écran (minimum 320 pixels de large).",
                "4": "Le site n’est pas accessible via le navigateur Internet Explorer. ",
                "title": "Configuration minimale pour accéder au site"
            },
            "7": {
                "1": "Concevoir le service avec une revue de conception et une revue de code, dont l’un des objectifs de réduire les impacts environnementaux de chaque fonctionnalité (critère 2.1)",
                "2": "Mettre en place des durées de conservation sur les données en vue de leur suppression ou archivage : suppression des logs régulières dans le serveur de fichiers « S3 » et la base de données (Postgres) notamment.",
                "3": {
                    "1": "Nombre de requêtes serveurs maximum astreinte par écran : 40",
                    "2": "Poids des ressources maximum astreint par écran : 600 Ko",
                    "title": "S’astreindre à un poids maximum et une limite de requêtes par écran (critère 6.1) :"
                },
                "4": "Convertir les illustrations en WebP, AVIF, JPEG XL ou un format d’image plus performant pour les images matricielles (critère 5.1).",
                "5": "Laisser la possibilité d'interrompre l'inférence (chargement des réponses) lors de la comparaison des modèles.",
                "6": "Les scores d’analyses EcoIndex démontrent un décalage important dans les mesures d’impact avec des notes allant jusqu’à E ou F pour certaines pages (poids, complexité et nombre de requêtes des pages relativement importants) ; il s’agit désormais de comprendre plus finement ces résultats au cas par cas, malgré les bonnes pratiques mises en œuvre, pour identifier de nouvelles améliorations.",
                "desc": "Plusieurs améliorations ont été ou sont toujours en cours d’identification, parmi lesquelles :",
                "title": "Améliorations identifiées"
            },
            "desc": "publiée le 11 décembre 2025",
            "title": "Déclaration d’écoconception"
        },
        "tos": {
            "contactDesc": "Pour toute question sur le service, vous pouvez écrire à <a {linkProps}>{contactLink}</a>.",
            "contactTitle": "9. Contact",
            "defsEditor": "« Éditeur » désigne le Service du numérique du Ministère de la Culture.",
            "defsModels": "« Modèles » désigne les grands modèles de langage (LLM) réutilisés dans le cadre de leur licence d’utilisation par la plateforme pour répondre à ses finalités.",
            "defsPlatform": "« Plateforme » désigne le site web qui rend les services accessibles.",
            "defsServices": "« Services » désigne les fonctionnalités offertes par la plateforme pour répondre à ses finalités.",
            "defsTitle": "2. Définitions",
            "defsUser": "« Utilisateur » désigne toute personne physique consultant la plateforme et qui bénéficie de ses services.",
            "descDatasets": "Ces jeux de données seront rendus accessibles sous licence ouverte, notamment pour favoriser des usages de recherche.",
            "descEditor": "Édité par le Service du numérique du Ministère de la Culture, le comparateur est une plateforme de comparaison des modèles conversationnels adressée au grand public dans le but (1) de sensibiliser les citoyens aux grands modèles de langage (LLMs), (2) de collecter les préférences des utilisateurs pour constituer des jeux de données d’alignement.",
            "descTitle": "3. Description de la plateforme",
            "descUse": "L’utilisateur ou l’utilisatrice pose une question en français et obtient des réponses de deux grands modèles de langages (LLM) anonymes. Il ou elle vote pour le modèle qui fournit la réponse qu’il préfère et se voit alors révélée l’identité des modèles. Ce dispositif de production participative inspiré de la plateforme <a {linkProps}>« chatbot arena » (LMSYS)</a> permet de constituer des jeux de données de préférences humaines sur des tâches réelles, en français, utilisables pour l’alignement des modèles.",
            "dispoDesc": "La plateforme est accessible, sauf cas de force majeure ou d’évènement hors de contrôle de son éditeur.",
            "dispoResp": "A ce titre, l’éditeur ne saurait être tenu responsable des pertes ou préjudices, de quelque nature qu’ils soient, qui pourraient être causés à la suite d’un dysfonctionnement ou une indisponibilité du service. De telles situations n'ouvriront droit à aucune compensation financière.",
            "dispoRight": "L’éditeur se réserve le droit de suspendre, d'interrompre ou de limiter, sans avis préalable, l'accès à tout ou partie des services, notamment pour des opérations de maintenance et de mises à jour nécessaires au bon fonctionnement du service et des matériels afférents, ou pour toute autre raison, notamment technique.",
            "dispoTitle": "7. Disponibilité des services",
            "dispoWarranty": "Il n’est pas garanti que le service soit exempt d’anomalies ou d'erreurs. Le service est donc mis à disposition sans garantie sur sa disponibilité et ses performances.",
            "evoDesc": "Les modalités d’utilisation peuvent être modifiées ou complétées à tout moment, sans préavis, en fonction des modifications apportées aux services, de l’évolution de la législation ou pour tout autre motif jugé nécessaire.",
            "evoDescMore": "Ces modifications et mises à jour s’imposent à l’utilisateur ou l’utilisatrice qui doit, en conséquence, se référer régulièrement à cette rubrique pour vérifier les modalités générales en vigueur.",
            "evoTitle": "8. Évolution des modalités d'utilisation",
            "featuresDatasets": "Le service collecte les données de dialogue (questions, réponses des deux modèles) et les préférences des utilisateurs (vote, métadonnées associées). Ces données servent à la fois à constituer des jeux de données partagés et à établir un classement des modèles d'IA affiché sur la plateforme, basé sur les votes exprimés.",
            "featuresDatasetsMore": "L’éditeur se réserve le droit de distribuer sous licence ouverte 2.0 les données de dialogue et de préférence de l’utilisateur. Le jeu de données est diffusé sur la plateforme Hugging Face à travers le compte du ministère de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "featuresDesc": "Afin de répondre au double objectif de sensibiliser les citoyens aux grands modèles de langage et collecter les préférences des utilisateurs et utilisatrices, les services rendus par la plateforme sans restriction d’accès sont les suivants :",
            "featuresDescMore": "Une interface humain-machine permettant de dialoguer simultanément avec deux modèles conversationnels et de voter pour la réponse préférée.",
            "featuresModels": "Les modèles intégrés à la plateforme sont déployés sur les serveurs d’inférence des différents hébergeurs et éditeurs de modèles. Les requêtes sont gérées notamment par le service Open Router. Les conditions de standardisation d’inférence sont renseignées sur la plateforme pour garantir la transparence d’utilisation des modèles.",
            "featuresModelsMore": "Une interface de comparaison des modèles.",
            "featuresTitle": "4. Fonctionnalités",
            "featuresVote": "L’utilisateur peut consulter la liste des modèles intégrés au comparateur et accéder à une liste d’informations sur ces modèles.",
            "featuresVoteMore": "Partage, mise à disposition et valorisation des jeux de données issus de la collecte des préférences des utilisateurs.",
            "licenceCode": "Le code source de la plateforme est libre et disponible ici : <a {linkProps}>https://github.com/betagouv/languia</a>",
            "licenceLLM": "Les LLM utilisés pour alimenter les services sont régis par les licences suivantes :",
            "licenceLLMEvolution": "La liste des modèles de langage intégrés à la plateforme est susceptible d’évoluer au cours du temps et est mise à jour à chaque modification.",
            "licenceLLMLicence": "Licence",
            "licenceLLMModel": "Modèle d’IA conversationnelle",
            "licenceLLMNoticeLink": "Lien vers la notice des modèles",
            "licenceLLMUnavailable": "Non disponible",
            "licenceTitle": "6. Code et licences",
            "respEditor": "De manière générale, l’éditeur se dégage de toute responsabilité en cas d’utilisationnon-conforme aux modalités d’utilisation.",
            "respLegal": "La plateforme n’a pas vocation à être utilisée pour générer des contenus illicites ou contraires à l’ordre public et plus généralement toute génération de contenu contrevenant au cadre juridique en vigueur.",
            "respLegalMore": "A cet égard, l’utilisateur ne saisit pas dans l’invite des contenus ou informations contraires aux dispositions légales et réglementaires en vigueur.",
            "respPrivacy": "Les données saisies par l’utilisateur sur la plateforme ayant vocation à être mises à disposition, il ou elle s’engage à ne pas transmettre d’informations permettant de l’identifier ou d’identifier un tiers.",
            "respPrivacyMore": "En tout état de cause, l’éditeur s’engage à mettre en œuvre les moyens permettant de s'assurer de l'anonymisation des données de dialogue avant leur mise à disposition. Si, malgré les efforts de l'éditeur, des données sensibles venaient à être publiées dans les jeux de données, vous pouvez le signaler immédiatement via ce formulaire : [https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ](https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ).",
            "respTitle": "5. Responsabilités",
            "respUser": "L’utilisateur est responsable des données ou contenus qu'il ou elle saisit dans l’invite offert par la plateforme.",
            "scopeDesc": "L’accès à la plateforme est gratuit, sans inscription et entraîne l’application de conditions spécifiques, listées dans les présentes modalités d’utilisation.",
            "scopeTitle": "1. Champ d’application",
            "title": "Modalités d’utilisation"
        }
    },
    "generated": {
        "archs": {
            "dense": {
                "desc": "L’architecture dense désigne un type de réseau de neurones dans lequel chaque neurone d’une couche est connecté à tous les neurones de la couche suivante. Cela permet à tous les paramètres de la couche de contribuer au calcul de la sortie.",
                "name": "Dense",
                "title": "Architecture Dense"
            },
            "matformer": {
                "desc": "Imaginez des poupées russes (matryoshkas → matryoshka transformer → Matformer) : chaque bloc contient plusieurs sous-modèles imbriqués de tailles croissantes, partageant les mêmes paramètres. Cela permet, à chaque requête, de sélectionner un modèle de capacité adaptée, selon la mémoire ou la latence disponibles, sans avoir besoin de ré-entraîner différents modèles.",
                "name": "Matformer",
                "title": "Architecture Matformer"
            },
            "moe": {
                "desc": "L’architecture Mixture of Experts (MoE) utilise un mécanisme de routage pour n’activer, en fonction de l’entrée, que certains sous-ensembles spécialisés (“experts”) du réseau de neurones. Cela permet de construire des modèles très grands tout en gardant un coût de calcul réduit, car seule une partie du réseau est utilisée à chaque étape.",
                "name": "MoE",
                "title": "Architecture MoE"
            },
            "na": {
                "desc": "L’éditeur n’a pas rendu les informations sur l’architecture du modèle publiques.",
                "name": "Propriétaire",
                "title": "Architecture N/A"
            }
        },
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le modèle, y compris à des fins commerciales. Outre la liberté d'utilisation, elle garantit la protection juridique en incluant une clause de concession de brevets qui fonctionne comme une assurance : si vous utilisez ce modèle, les contributeurs s'engagent à ne pas vous poursuivre pour violation de leurs brevets liés au projet. Cette protection mutuelle évite les conflits juridiques entre utilisateurs et développeurs. Lors de la distribution de versions modifiées, les changements significatifs doivent être signalés par des mentions appropriées, garantissant la transparence pour l'utilisateur.</p>",
                    "reuse_specificities": ""
                },
                "CC-BY-NC-4.0": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence permet de partager et adapter le contenu librement à condition de créditer l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilité pour les usages non commerciaux tout en protégeant les droits de l'auteur.</p>",
                    "reuse_specificities": "mais que pour des usages non-commerciaux"
                },
                "Gemma": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence est conçue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiées ou améliorées doivent être partagées avec la communauté sous la même licence source, favorisant ainsi la collaboration et la transparence dans le développement logiciel.</p>",
                    "reuse_specificities": ""
                },
                "Jamba Open Model": {
                    "commercial_use_specificities": "en dessous des 50 millions de dollars de revenus annuels",
                    "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les organismes dépassant 50 millions de dollars de revenus annuels.</p>",
                    "reuse_specificities": ""
                },
                "LFM 1.0": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence mondiale, gratuite et non-exclusive autorise l’usage, la modification et la redistribution du modèle et de ses dérivés, y compris la réutilisation des sorties pour entraîner d’autres modèles. Elle reste permissive, mais limite tout usage commercial aux organisations dont les revenus annuels sont inférieurs à 10 millions de dollars, au-delà desquels l’usage n’est plus couvert.</p>",
                    "reuse_specificities": ""
                },
                "Llama 3 Community": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs",
                    "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels et interdit la réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles concurrents, protégeant ainsi les investissements technologiques et la marque de Meta.</p>",
                    "reuse_specificities": ""
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs",
                    "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opérations dépassant 700 millions d'utilisateurs mensuels. La réutilisation du code ou des contenus générés pour l’entraînement ou l'amélioration de modèles dérivés est autorisée à condition d’afficher “built with llama” et d’inclure “Llama” dans leur nom pour toute distribution.</p>",
                    "reuse_specificities": ""
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs",
                    "license_desc": "<p>Cette licence <strong>non-exclusive, mondiale et sans redevance</strong> permet d'utiliser, reproduire, modifier et distribuer librement le code et les Matériaux Llama 3.3 avec attribution. Elle autorise notamment la réutilisation pour l'amélioration de modèles dérivés, mais impose des restrictions pour les opérations commerciales de très grande envergure.</p>",
                    "reuse_specificities": ""
                },
                "Llama 4": {
                    "commercial_use_specificities": "en dessous des 700 millions d’utilisateurs\n",
                    "license_desc": "<p>Cette licence non-exclusive, mondiale et sans redevance permet d'utiliser, reproduire, modifier et distribuer les Matériaux Llama 4 (modèles et documentation) avec attribution. Cependant, elle impose deux restrictions majeures : (1) les entreprises dépassant 700 millions d'utilisateurs actifs mensuels doivent obtenir une licence spéciale de Meta, et (2) <strong>exclusion totale</strong> des personnes résidant dans l'UE et des entreprises ayant leur siège social dans l'UE pour l'utilisation directe des modèles multimodaux, en raison des incertitudes réglementaires liées à l'AI Act européen. Les utilisateurs finaux européens peuvent néanmoins accéder à des services intégrant Llama 4, à condition qu'ils soient fournis depuis l'extérieur de l'UE.</p>",
                    "reuse_specificities": ""
                },
                "Mistral AI Research License": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>Cette licence non-exclusive et sans redevance autorise l'utilisation, la copie, la modification et la distribution des modèles Mistral et de leurs dérivés (incluant les versions modifiées ou affinées). Cependant, elle est strictement limitée aux fins de recherche.</p>",
                    "reuse_specificities": "mais que pour des usages non-commerciaux"
                },
                "MIT": {
                    "commercial_use_specificities": "",
                    "license_desc": "<p>La licence MIT est une licence de logiciel libre permissive : elle permet à quiconque de réutiliser, modifier et distribuer le modèle, même à des fins commerciales, sous réserve d'inclure la licence d'origine et les mentions de droits d'auteur.</p>",
                    "reuse_specificities": ""
                }
            },
            "proprio": {
                "01-ai": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Ai2": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "AI21": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Alibaba": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Alibaba, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Amazon": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via Amazon Bedrock, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": "sauf pour distiller ou entraîner d’autres modèles sur les plateformes d’Amazon."
                },
                "Anthropic": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Anthropic ou des sociétés partenaires, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Cohere": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "DeepSeek": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "EuroLLM": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Google": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Google, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée de Google.",
                    "reuse_specificities": "sauf pour entraîner d’autres modèles sur Vertex AI"
                },
                "jpacifico": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Liquid": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société Liquid AI, nécessitant un paiement à l'utilisation basé sur le nombre de tokens traités.",
                    "reuse_specificities": ""
                },
                "Meta": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Microsoft": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "MiniMax": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Mistral AI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres hébergeurs, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Moonshot AI": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Nous": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "Nvidia": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "OpenAI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via API sur les plateformes de la société OpenAI ou via les services Microsoft Azure, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Swiss AI": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                },
                "xAI": {
                    "commercial_use_specificities": "",
                    "license_desc": "Le modèle est disponible sous licence payante et accessible via X et xAI, nécessitant un paiement à l'utilisation basé sur le nombre de jetons traités ou sur l’infrastructure réservée.",
                    "reuse_specificities": ""
                },
                "Zhipu": {
                    "commercial_use_specificities": "",
                    "license_desc": "",
                    "reuse_specificities": ""
                }
            }
        },
        "models": {
            "Apertus 70B Instruct": {
                "desc": "<p>Modèle semi-ouvert et reproductible de taille moyenne, développé par un consortium d’institutions suisses. Ses poids et son code d’entraînement sont publiés sous licence permissive. Il a été entraîné sur plus de 1 800 langues sur plus de 15 000 milliards de jetons. L’entraînement a eu lieu sur le supercalculateur Alps du CSCS à Lugano, alimenté par une énergie hydroélectrique neutre en carbone.</p>",
                "fyi": "<p>Le modèle a été entraîné sur le supercalculateur Alps à Lugano, utilisant plus de 10 millions d’heures GPU alimentées par une énergie hydroélectrique neutre en carbone. Le modèle a été préentraîné sur 15 000 milliards jetons couvrant plus de 1 800 langues, dont une part importante de langues peu représentées.</p>\n<p>Apertus s’appuie sur un tokenizer byte-level BPE de 131 000 entrées, dérivé du tokenizer « tekken » de Mistral AI, optimisé pour le multilinguisme, le code et les expressions mathématiques. L’architecture combine plusieurs innovations : Rotary Positional Embeddings (RoPE) avec base étendue et ajustement NTK-aware pour les longs contextes, Grouped Query Attention (GQA) pour une meilleure efficacité mémoire, normalisation QK-Norm pour stabiliser l’entraînement, et une fonction d’activation xIELU (extended Integrated ELU) améliorant la performance des MLP.</p>\n<p>L’affinage final du modèle repose sur un algorithme d’alignement appelé QRPO (Quantile Reward-Preferring Optimization), une alternative au RLHF classique, qui utilise des signaux de récompense absolus pour un apprentissage plus stable et mieux aligné sur les préférences humaines. Bien qu’il ne rivalise pas directement avec les modèles propriétaires les plus avancés, Apertus se distingue par son niveau de transparence.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 65 536 jetons ce qui permet de traiter d'assez longs documents.</p>"
            },
            "Apertus 8B Instruct": {
                "desc": "<p>Petit modèle semi-ouvert et reproductible, développé par un consortium d’institutions suisses. Ses poids et son code d’entraînement sont publiés sous licence permissive. Il a été entraîné sur plus de 1 800 langues sur plus de 15 000 milliards de jetons. L’entraînement a eu lieu sur le supercalculateur Alps du CSCS à Lugano, alimenté par une énergie hydroélectrique neutre en carbone.</p>",
                "fyi": "<p>Le modèle a été entraîné sur le supercalculateur Alps à Lugano, utilisant plus de 10 millions d’heures GPU alimentées par une énergie hydroélectrique neutre en carbone. Le modèle a été préentraîné sur 15 000 milliards jetons couvrant plus de 1 800 langues, dont une part importante de langues peu représentées.</p>\n<p>Apertus s’appuie sur un tokenizer byte-level BPE de 131 000 entrées, dérivé du tokenizer « tekken » de Mistral AI, optimisé pour le multilinguisme, le code et les expressions mathématiques. L’architecture combine plusieurs innovations : Rotary Positional Embeddings (RoPE) avec base étendue et ajustement NTK-aware pour les longs contextes, Grouped Query Attention (GQA) pour une meilleure efficacité mémoire, normalisation QK-Norm pour stabiliser l’entraînement, et une fonction d’activation xIELU (extended Integrated ELU) améliorant la performance des MLP.</p>\n<p>L’affinage final du modèle repose sur un algorithme d’alignement appelé QRPO (Quantile Reward-Preferring Optimization), une alternative au RLHF classique, qui utilise des signaux de récompense absolus pour un apprentissage plus stable et mieux aligné sur les préférences humaines. Bien qu’il ne rivalise pas directement avec les modèles propriétaires les plus avancés, Apertus se distingue par son niveau de transparence.</p>",
                "size_desc": "<p>Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 65 536 jetons ce qui permet de traiter d'assez longs documents.</p>"
            },
            "Aya 23 8B": {
                "desc": "<p>Petit modèle multilingue, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.</p>",
                "fyi": "<p>Aya 23 8B de Cohere est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Aya Expanse 32B": {
                "desc": "<p>Modèle de taille moyenne multilingue, capable de traiter 23 langues.</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été conçu pour offrir de bonnes capacités dans chacune des 23 langues de son corpus d’entraînement.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 130 000 jetons, utile pour l’analyse de documents longs.</p>"
            },
            "Aya Expanse 8B": {
                "desc": "<p>Petit modèle multilingue, sencode itération de la série Aya, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.</p>",
                "fyi": "<p>Aya Expanse 8B de Cohere, entreprise canadienne, est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Aya23-35B": {
                "desc": "<p>Modèle de taille moyenne multilingue, entraîné spécifiquement en grande proportion sur des langues généralement sous-représentées.</p>",
                "fyi": "<p>Aya 23 35B de Cohere est un modèle de taille moyenne de la famille Command R qui a spécialement été entraîné sur un corpus multilingue.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Chocolatine 14B": {
                "desc": "<p>Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française.</p>",
                "fyi": "<p>Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Chocolatine 2 14B": {
                "desc": "<p>Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française.</p>",
                "fyi": "<p>Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Claude 3.5 Sonnet v2": {
                "desc": "<p>Modele tres performant en code, fait apres une amelioration de post-training par rapport au claude 3</p>",
                "fyi": "<p>Meilleur modèle de la famille Claude 3.5, ce modèle est spécialisé dans la génération de textes littéraires et un ton plus naturel. La version v2 est sortie en octobre 2024.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p>Très grand modèle multimodal et multilingue, performant pour la génération de code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Claude 4 Opus est la version la plus avancée de la famille Claude 4. Il est optimisé pour la puissance brute et les tâches complexes nécessitant un raisonnement soutenu sur de longues périodes : il peut par exemple travailler sur des tâches à long terme (Anthropic déclarent qu'il peut travailler jusqu'à sept heures de manière indépendante). En contrepartie, Opus est plus coûteux à utiliser, plus lent à répondre et nécessite davantage de ressources pour fonctionner.</p>\n<p>Le modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été majoritairement entraîné sur des données mathématiques, mais adapté à des cas d’usage réels.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p>Très grand modèle multimodal et multilingue très puissant en code. L'utilisateur ou le développeur qui utilise ce modèle peut choisir entre plusieurs niveaux de raisonnement.</p>",
                "fyi": "<p>Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisée pour la vitesse, l’efficacité et l’accessibilité. Il est un peu moins bon sur les tâches qui demandent un raisonnement complexe en plusieurs étapes. Néanmoins, il est nettement moins coûteux, plus rapide et consomme moins d’énergie que Opus 4.</p>\n<p>Le modèle offre la possibilité de choisir l'intensité de \"raisonnement\". À la différence d’autres modèles, le mode raisonnement n’a pas été surtout entraîné sur des données mathématiques, mais surtout sur des cas d’usage réels.</p>",
                "size_desc": "<p>La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 1 000 000 jetons, adaptée à l’analyse de très longs documents ou de dépôts de code.</p>"
            },
            "Claude 4.5 Sonnet": {
                "desc": "<p>Très grand modèle multimodal et multilingue, extrêmement performant en code, en raisonnement et en mathématiques. L'utilisateur ou le développeur qui utilise ce modèle peut choisir entre plusieurs niveaux de raisonnement.</p>",
                "fyi": "<p>Claude Sonnet 4.5 est une évolution directe de Sonnet 4. Le « .5 » désigne les changements majeurs introduits lors du post-entraînement, qui se traduisent par des gains significatifs en raisonnement, en mathématiques et surtout dans l’utilisation concrète des ordinateurs. Au moment de sa sortie, il est considéré comme le meilleur modèle au monde pour le code et excelle dans la résolution de tâches multi-étapes longues et complexes. Ses performances sur des benchmarks comme SWE-bench Verified et OSWorld marquent un net progrès par rapport aux versions précédentes, avec une capacité à maintenir sa \"concentration\" pendant plus de trente heures sur un même problème.</p>",
                "size_desc": "<p>La taille exacte n’est pas connue. Tout indique qu’il s’agit d’un très grand modèle, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour le faire fonctionner. Les estimations de taille et de consommation énergétique reposent sur des indices indirects comme les coûts d’inférence et la latence observée. Claude Sonnet 4.5 dispose d’une fenêtre de contexte allant jusqu’à 1 000 000 jetons, adaptée à l’analyse de dépôts de code entiers ou de documents très volumineux.</p>"
            },
            "Command A": {
                "desc": "<p>Grand modèle, performant pour la programmation, l’utilisation d’outils externes, la “génération augmentée de récupération” (RAG, retrieval augmented generation).</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier <a href=\"https://arxiv.org/abs/1706.03762\">« Attention Is All You Need »</a> paru en 2017 et qui a révolutionné l'IA. L'entreprise se démarque par sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle est conçu pour fonctionner dans plus de 23 langues et pour s’intégrer facilement dans les systèmes d’entreprise.  Il fait partie des rares modèles distribués sous licence <strong>CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.</strong> Ce choix de licence reflète la volonté de Cohere de contribuer à la recherche et la communauté open source, tout en gardant le contrôle sur les usages commerciaux pour protéger son modèle économique... Cela exclut par exemple l’intégration du modèle dans des produits ou services vendus par une entreprise à des clients mais autorise un usage académique, des tests ou des projets internes, restreints à un cadre non-commercial.</p>",
                "size_desc": "<p>Avec 111 milliards de paramètres, ce modèle fait partie des grands modèles. Il nécessite au moins deux cartes graphiques puissantes pour l’hébergement, ce qui entraîne un coût de fonctionnement significatif.</p>\n<p>Sa fenêtre de contexte atteint 256 000 jetons, adaptée à l’analyse de vastes ensembles de documents ou de bases de code.</p>"
            },
            "Command R": {
                "desc": "<p>Modèle de taille moyenne optimisé pour la synthèse, les questions générales, l’utilisation d’outils et efficace dans les systèmes de génération augmentée de récupération (RAG, retrieval augmented generation).</p>",
                "fyi": "<p>Cohere, l'entreprise canadienne derrière ce modèle, a été fondée en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier « Attention Is All You Need » qui a révolutionné l'IA. Sa spécificité principale réside dans sa focalisation exclusive sur l'IA générative pour les entreprises, particulièrement les secteurs réglementés comme la finance, la santé, la manufacture et l'énergie, ainsi que le secteur public. L'entreprise est également pionnière dans les approches multilingues et maintient un laboratoire de recherche à but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modèle a été évalué dans plus de 10 langues. Sa fenêtre de contexte atteint 128 000 jetons, ce qui facilite l’analyse de documents longs. Cette fenêtre a été doublée sur la version suivante du modèle (Command A).</p>",
                "size_desc": "<p>Avec 35 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut être hébergé sur un serveur équipé d’une seule carte graphique puissante, ce qui contribue à limiter les coûts d’infrastructure.</p>"
            },
            "Command R+": {
                "desc": "<p>Modèle multilingue spécialisé sur 10 langues, spécialisé pour des cas d'usage business.</p>",
                "fyi": "<p>Grand frère de la famille Command R de Cohere, ce modèle de langage est orienté pour l'usage professionnel et conçu spécifiquement pour les tâches de recherche et d'extraction d'informations.</p>",
                "size_desc": "<p>Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.</p>"
            },
            "DeepSeek R1": {
                "desc": "<p>Très grand modèle très performant sur les tâches mathématiques, scientifiques et de programmation, qui simule une étape de raisonnement avant de générer sa réponse.</p>",
                "fyi": "<p>Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (<em>supervised fine-tuning</em> : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage.</p>",
                "size_desc": "<p>Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 163 840 jetons, ce qui est adapté à l’analyse de longs documents.</p>"
            },
            "DeepSeek R1 0528": {
                "desc": "<p>Modèle de très grande taille, spécialisé dans les tâches mathématiques, scientifiques et de programmation. Il simule une étape de raisonnement avant de générer sa réponse et avec la mise à jour de mai 2025 a gagné en profondeur d’analyse et en précision grâce à une optimisation du post-entraînement.</p>",
                "fyi": "<p>Ce modèle s’appuie sur une architecture de mélange d’experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramètres, dont 37 milliards sont activés par jeton. L'entraînement a fait appel à un apprentissage par renforcement à grande échelle, avec plusieurs étapes d'ajustement SFT (<em>supervised fine-tuning</em> : un affinage supervisé où le modèle apprend à partir d'exemples de réponses correctes) et de données de démarrage. Sa dernière version (DeepSeek-R1-0528) améliore sensiblement ses capacités de raisonnement, réduisant le taux d’hallucination et renforçant l’efficacité en programmation, logique et appel de fonctions. Sur le test AIME 2025, son score est passé de 70 % à 87,5 %, se rapprochant ainsi des modèles comme o3 et Gemini 2.5 Pro.</p>",
                "size_desc": "<p>Avec 671 milliards de paramètres DeepSeek R1 est un modèle de très grande taille qui nécessite plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse, ce qui augmente la consommation énergétique. Cependant, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. La fenêtre de contexte atteint 163 840 jetons, ce qui est adapté à l’analyse de longs documents.</p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p>Grand modèle basé sur Meta Llama 3.3 70B, ré-entraîné avec des exemples de raisonnement issus du modèle DeepSeek R1. Il offre de bonnes capacités en mathématiques et code.</p>",
                "fyi": "<p>Le modèle n’a pas été entraîné depuis zéro. Il s’appuie sur Llama 3.3 70B, ré-entraîné en utilisant des résultats générés par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B d’une capacité à simuler le raisonnement, sans possibilité pour l’utilisateur de choisir d’activer ou non cette fonction.</p>\n<p>Conformément aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du modèle source dans le nom du modèle, soumis au même régime de licence.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle est classé parmi les modèles de grande taille. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne un coût de fonctionnement élevé. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>La fenêtre de contexte est de 16 000 jetons, ce qui peut être limitant pour l’analyse de très grands documents.</p>"
            },
            "DeepSeek V3": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes : génération de code, utilisation d’outils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particulièrement adapté à l’anglais et au chinois.</p>",
                "fyi": "<p>Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code.</p>",
                "size_desc": "<p>DeepSeek V3 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.</p>\n<p>La fenêtre de contexte atteint 128 000 jetons, ce qui est utile pour l’analyse de longs documents.</p>"
            },
            "DeepSeek v3": {
                "desc": "<p>Sorti en décembre 2024, le modèle DeepSeek V3 possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence.</p>",
                "fyi": "<p>Sorti en décembre 2024, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "DeepSeek v3.1": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes : génération de code, analyse de documents longs. Cette version est particulièrement forte en  utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.</p>",
                "fyi": "<p>Ce modèle est basé sur une architecture de mélange d’experts (MoE, Mixture of Experts), comptant 671 milliards de paramètres mais n’en activant que 37 milliards par jeton généré. Il est efficace pour les appels d’outils, la génération de sorties structurées (JSON) et la génération de code. L’entraînement a recours au FP8 microscaling, ce qui réduit les coûts de calcul et de mémoire tout en maintenant la précision. Le modèle a été formé en deux phases : d’abord sur des séquences de 32 000 jetons, puis étendu à 163 000 jetons, permettant une meilleure stabilité et une performance accrue sur les contextes très longs.</p>",
                "size_desc": "<p>DeepSeek V3.1 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille.</p>\n<p>La fenêtre de contexte atteint désormais 163 000 jetons, contre 128 000 dans la version précédente, ce qui améliore l’analyse de très longs documents.</p>"
            },
            "DeepSeek V3.2": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes : orchestration agentique, génération de code, analyse de documents longs. Cette version est particulièrement forte en utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.</p>",
                "fyi": "<p>Pour cette version, le coût de l’API et les besoins en calcul sont réduits d’environ 50 % ou plus pour les contextes longs. Cette amélioration repose sur le \"DeepSeek Sparse Attention (DSA)\", un mécanisme d’attention clairsemée à granularité fine qui calcule l’attention de façon sélective afin de diminuer la complexité sur les longues séquences tout en préservant l’essentiel du contexte.</p>\n<p>Le modèle a été entraîné en donnant la priorité aux capacités de raisonnement et aux usages agentiques.</p>",
                "size_desc": "<p>DeepSeek V3.2 est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une partie des paramètres, ce qui réduit l’empreinte par rapport à un modèle dense de même taille. La fenêtre de contexte atteint 163 000 jetons, ce qui est utile pour l’analyse de très longs documents ou des bases de code.</p>"
            },
            "EuroLLM 22B Instruct": {
                "desc": "<p>Modèle multilingue conçu spécifiquement pour la diversité linguistique européenne, avec de fortes capacités en traduction et compréhension multilingue.</p>",
                "fyi": "<p>EuroLLM-22B est un modèle créé par un consortium composé de Sorbonne Université, Université Paris-Saclay, Artefact Research Center, Instituto Superior Técnico – University of Lisbon, Instituto de Telecomunicações, University of Edinburgh, Aveni, Unbabel, University of Amsterdam et Naver Labs. Il a été entraîné avec pour objectif principal la couverture équilibrée des langues européennes. Il couvre les 24 langues officielles de l’Union européenne ainsi que 11 langues supplémentaires. L’entraînement a été réalisé sur environ 4 000 milliards de jetons, en utilisant 400 GPU Nvidia H100 sur le supercalculateur MareNostrum 5, opéré par le Barcelona Supercomputing Center. Le projet a bénéficié d’un soutien institutionnel européen via Horizon Europe et EuroHPC, dans le cadre d’une allocation de calcul « extreme-scale ».</p>\n<p>Les données d’entraînement combinent des données issues du web, des données parallèles multilingues (en–xx et xx–en) et des jeux de données de haute qualité soigneusement sélectionnés, avec un accent fort mis sur l’équilibre entre les langues européennes. Le modèle est particulièrement performant pour les tâches de traduction multilingue.</p>",
                "size_desc": "<p>Avec 22 milliards de paramètres, ce modèle se situe dans la catégorie des modèles de taille intermédiaire. Son utilisation nécessite un ordinateur personnel très puissant ou, plus généralement, un serveur disposant d’au moins une carte graphique performante. La fenêtre de contexte atteint 32 000 jetons.</p>"
            },
            "Gemini 1.5 Pro": {
                "desc": "<p>Sorti en septembre 2024, ce modèle multimodal s'applique à la génération de textes et d'images, l'analyse de vidéos et la transcription d'audio.</p>",
                "fyi": "<p>Sorti en février 2024 et amélioré en septembre 2024, ce modèle multilingue et multimodal est capable de traiter un très grand volume de données d’entrées qu’il s’agisse de données textuelles, d’image, de son (jusqu’à 11h d’audio) ou de vidéo (jusqu’à une heure). C’est le modèle LLM qui alimente le chatbot Gemini de Google.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "Gemini 2.0 Flash": {
                "desc": "<p>Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés.</p>",
                "fyi": "<p>Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés que les modèles Gemini Pro.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p>Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale.</p>",
                "fyi": "<p>Ce modèle repose sur une architecture de mélange d’experts (MoE, Mixture of Experts) et a été distillé en ne conservant qu'une approximation des prédictions du modèle enseignant - Gemini 2.5 Pro. Il a été entraîné sur une architecture TPUv5p intégrant des avancées comme la possibilité de poursuivre l'entraînement automatiquement même en cas d’erreurs d’entraînement, de corruption de données ou de problèmes de mémoire.</p>\n<p>Gemini 2.5 Flash gère des contextes allant jusqu'à 1 million de jetons, et trois heures de contenu vidéo. L'optimisation du traitement de la vision permet de traiter des vidéos environ trois fois plus longues dans la même fenêtre de contexte: seuls 66 jetons visuels sont nécessaires pour générer une image contre 258 auparavant. Ce modèle permet  également la génération audio native pour les dialogues et la synthèse vocale.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant plusieurs cartes graphiques puissantes pour le fonctionnement. Néanmoins, l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>"
            },
            "Gemini 3 Flash": {
                "desc": "<p>Grand modèle multimodal natif et multilingue, distillé depuis Gemini 3 Pro. Il intègre une capacité de raisonnement avancée (\"Deep Think\") activable à la demande, avec un niveau de raisonnement paramétrable par l'utilisateur ou le développeur. Le modèle prend en charge nativement le texte, le code, l'audio, l'image, la vidéo et les PDF.</p>",
                "fyi": "<p>Le modèle a été entraîné sur une infrastructure uniquement composée de TPU (Tensor Processing Units). Gemini 3 Flash peut traiter des volumes importants de contenu multimodal par prompt : jusqu'à 900 images, 900 PDFs de 900 pages chacun, des vidéos de 45 minutes à 1 heure, et jusqu'à 8,4 heures d'audio. Au moment de sa sortie, Gemini 3 Flash se montre très performant tout en étant environ neuf fois moins coûteux que Gemini 3 Pro. </p>",
                "size_desc": "<p>La taille exacte du modèle n'est pas communiquée, mais il repose sur une architecture de type mélange d'experts (MoE). Cette architecture active uniquement un sous-ensemble de paramètres pour chaque jeton, ce qui réduit la puissance de calcul nécessaire. Sa fenêtre de contexte s'étend jusqu'à 1 million de jetons. Le modèle est environ 9 fois moins coûteux que Gemini 3 Pro. L'impact environnemental varie selon le niveau de raisonnement choisi : un raisonnement approfondi génère davantage de jetons et consomme donc plus de ressources.</p>"
            },
            "Gemini 3 Pro": {
                "desc": "<p>Grand modèle multimodal natif et multilingue. Il intègre une capacité de raisonnement avancée (\"Deep Think\") activable à la demande pour les tâches complexes (mathématiques, logique, codage), distincte de sa capacité de génération standard plus rapide. Le modèle prend en charge nativement le texte, le code, l'audio, l'image, la vidéo et la 3D.</p>",
                "fyi": "<p>Le modèle a été entraîné sur une infrastructure uniquement composée de TPU (Tensor Processing Units). Au moment de sa sortie, le modèle marque un très grand pas en avant au niveau des évaluations - par exemple sur Humanity's Last Exam, ARC-AGI-2 et MathArena Apex. Le modèle est optimisé pour l'usage \"agentique\" : il a été entraîné pour simuler des étapes de planification, utilisation d'outils, exécution du code et auto-correction au sein d'environnements de code. Sa compréhension multimodale a été affinée pour réduire significativement le nombre de jetons nécessaires à l'encodage vidéo et audio.</p>",
                "size_desc": "<p>La taille exacte du modèle n'est pas communiquée, mais il repose sur une architecture de type mélange d'experts (MoE). Cette architecture active uniquement un sous-ensemble de paramètres pour chaque jeton d'entrée, ce qui nécessite moins de puissance de calcul pour générer. Sa fenêtre de contexte s'étend jusqu'à 1 million de jetons, adaptée à l'analyse de vastes ensembles de documents, y compris des dépôts de code entiers et des fichiers vidéo.</p>"
            },
            "Gemma 2 27B": {
                "desc": "<p>Modèle performant avec une taille correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision.</p>",
                "fyi": "<p>Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions. Le modèle sollicité ici est la version quantisée (q8).</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Gemma 2 2B": {
                "desc": "<p>Très petit modèle qui offrait des performances très compétitives pour sa taille et la plupart des tâches.</p>",
                "fyi": "<p>Petit frère de la famille Gemma 2, ce très petit modèle sorti en juillet 2024 arrive à rivaliser avec des modèles bien plus gros.</p>",
                "size_desc": "<p>Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.</p>"
            },
            "Gemma 2 9B": {
                "desc": "<p>Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives.</p>",
                "fyi": "<p>Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Gemma 3 12B": {
                "desc": "<p>Petit modèle multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 12 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.</p>"
            },
            "Gemma 3 27B": {
                "desc": "<p>Modèle de taille moyenne multimodal adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il peut traiter du texte et des images sur un serveur équipé d’une seule carte graphique puissante. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 27 milliards de paramètres, il appartient à la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur avec une seule carte graphique (GPU). </p>\n<p>Il accepte des contextes jusqu’à 128 000 jetons, ce qui convient pour l’analyse de documents longs.</p>"
            },
            "Gemma 3 4B": {
                "desc": "<p>Très petit modèle multimodal et compact adapté aux tâches courantes comme les questions-réponses, les résumés ou l’interprétation d’images.</p>",
                "fyi": "<p>Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a été entraîné pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>",
                "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte peut atteindre 128 000 jetons, ce qui permet d’analyser de longs documents.</p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p>Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin.</p>",
                "fyi": "<p>Ce modèle peut traiter du texte, des images et de l’audio. Il repose sur l’architecture MatFormer et un système de cache PLE (per-layer embeddings), qui active uniquement les paramètres utiles selon la tâche, s'adaptant à la capacité des machines sur lesquelles fonctionne le modèle.</p>",
                "size_desc": "<p>Avec 4 milliards de paramètres, il fait partie des modèles de très petite taille. Il peut être utilisé localement sur un ordinateur ou un smartphone pour préserver la confidentialité des données, ou sur serveur pour limiter les coûts par rapport à un modèle plus grand.</p>\n<p>Sa fenêtre de contexte va jusqu’à 32 000 jetons.</p>"
            },
            "GLM 4.5": {
                "desc": "<p>Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent.  Le modèle a deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications web complètes et de générer des artefacts, qui sont des programmes d’un seul fichier utilisable à l’intérieur même des interfaces des agents conversationnels. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin.</p>",
                "size_desc": "<p>Avec 355 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.</p>"
            },
            "GLM 4.6": {
                "desc": "<p>Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent. Cette mise à jour augmente la taille de la fenêtre de contexte, améliore sa performance en code, s'aligne plus avec les préférences humaines et est plus capable en cas d'usages agentiques/utilisation d'outils.</p>",
                "fyi": "<p>Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications web complètes et de générer des artefacts, qui sont des programmes d’un seul fichier utilisable à l’intérieur même des interfaces des agents conversationnels. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin.</p>",
                "size_desc": "<p>Avec 357 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 200 000 jetons, ce qui permet de traiter de très longs documents.</p>"
            },
            "GLM 4.7": {
                "desc": "<p>Grand modèle spécialisé en code créé par Zhipu AI, un éditeur de modèles d’IA Chinois créé en 2019 par des professeurs de l’université de Tsinghua et soutenu par des grands acteurs comme Alibaba et Tencent. Cette mise à jour améliore sa performance en code (notamment pour les interfaces web), interragi mieux avec des environnements de code assisté par IA et plus généralement performe mieux dans des contextes agentiques.</p>",
                "fyi": "<p>Ce modèle a de bonnes capacités agentiques, lui permettant d'effectuer des appels de fonctions avec une grande fiabilité. Ses performances en codage sont élevées et le modèle a une bonne capacité de créer des applications et des interfaces. Pour l'entraînement, une infrastructure d'apprentissage par renforcement spécifique, nommée slime, a été conçue pour optimiser les performances sur des tâches complexes et agentiques en gérant de manière efficiente les flux de travail longs - le modèle est capable de traiter des tâches complexes et de longue durée, comme la création d'une application de A à Z, en utilisant au mieux ses outils et en restant cohérent du début à la fin.</p>",
                "size_desc": "<p>Avec 357 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 200 000 jetons, ce qui permet de traiter de très longs documents.</p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p>Plus petite version allégée du modèle GPT 4.1 , conçue pour limiter les coûts tout en restant compétitive sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
                "fyi": "<p>Il s'agit d'une version distillée d’un modèle de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio. Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus de textes ou de dépôts de code très longs.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.  Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT 5": {
                "desc": "<p>Le GPT-5 n'est pas un modèle unique, mais un système unifié composé de deux modèles distincts : un modèle rapide (<code>gpt-5-main</code>) pour les requêtes courantes et un modèle de raisonnement (<code>gpt-5-thinking</code>) pour les problèmes complexes. Comparé à ses prédécesseurs, OpenAI affirme qu'il est plus utile dans les requêtes du monde réel, avec des améliorations notables dans les domaines de l'écriture, du codage et de la santé. Il réduit également le phénomène des hallucinations. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Les développeurs qui utilisent ce modèle peuvent configurer un paramètre de verbosité pour ajuster la longueur de la phase de raisonnement.</p>\n<p>En matière de sécurité, le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête. Les créateurs du modèle ont aussi utilisé la phase d’entraînement au “raisonnement” pour le rendre plus “résistant” aux tentatives de contournement de leurs règles de sécurité (<em>jailbreaking</em>).</p>",
                "size_desc": "<p>Le système GPT-5 est composé de modèles de différentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est conçue pour inclure plusieurs modèles, orchestrés par un système de routage interne, qui sélectionne le plus petit modèle adapté à la tâche pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT 5 Mini": {
                "desc": "<p>Le GPT-5 Mini est une version allégée du modèle GPT-5 principal. Il est conçu pour être utilisé dans des environnements où il est nécessaire de limiter les coûts, par exemple à grande échelle. Son modèle de raisonnement est presque aussi performant que celui du modèle principal (<code>gpt-5-thinking</code>) malgré sa taille plus petite. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>\n<p>Bien qu'il soit une version plus petite, il se montre très compétitif face au modèle GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine médical.</p>",
                "size_desc": "<p>Le modèle Mini est une déclinaison plus compacte (taille moyenne supposée) du système GPT-5. Il est conçu pour fonctionner de manière optimale pour un bon équilibre entre performance et coût, grâce à un système de routage qui le sélectionne pour des tâches spécifiques. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Néanmoins, les modèles sont probablement très grands, nécessitant plusieurs cartes graphiques puissantes pour l’inférence.</p>"
            },
            "GPT 5 Nano": {
                "desc": "<p>Le GPT-5 Nano est la plus petite et la plus rapide version du modèle de raisonnement GPT-5. Il est conçu pour des contextes où une latence ou un coût ultra-faible est nécessaire. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois.</p>",
                "fyi": "<p>Le système utilise une nouvelle approche de sécurité appelée « safe-completions » pour prévenir le contenu non autorisé au moment de la réponse plutôt qu’au moment de la requête.</p>",
                "size_desc": "<p>Le modèle Nano est le plus compact de la famille GPT-5 (taille petite supposée). Il est sélectionné par le système de routage pour les requêtes nécessitant une latence ultra-faible et des réponses instantanées. Son architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui permet une meilleure efficacité énergétique et des performances élevées, même sur des requêtes nécessitant une réponse rapide.</p>"
            },
            "GPT 5.1": {
                "desc": "<p>Deuxième itération de GPT 5, avec un style jugé (par l'éditeur) plus naturel, avec de meilleurs résultats en code et en tâches d’agents. Le modèle a la spécificité d'ajuster son temps de raisonnement selon la difficulté.</p>",
                "fyi": "<p>L’arrivée de cette nouvelle itération apporte un routage automatique entre deux modes. Pour les demandes simples, le modèle choisit une réponse rapide. Pour les tâches plus complexes, il bascule vers un mode avec raisonnement avant de répondre. Cette logique évite de perdre du temps sur les requêtes faciles tout en gardant une vraie profondeur quand il le faut.</p>\n<p>La première sortie de la famille GPT 5 avait provoqué beaucoup de réactions négatives vis-à-vis de son style d'écriture. Beaucoup d’utilisateurs trouvaient le modèle moins chaleureux que GPT 4o. Cette itération vise à revenir vers un style plus chaleureux.</p>\n<p>Cette nouvelle version marque aussi des progrès en code et en mathématiques qui sont confirmés par des benchmarks comme SWE Bench pour le code et AIME pour les compétences mathématiques.</p>",
                "size_desc": "<p>Les tailles des modèles derrière le système GPT-5.1 ne sont pas connues. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT 5.2": {
                "desc": "<p>Troisième itération de GPT 5, avec une attention spéciale portée à son utilité sur des tâches professionelles.</p>",
                "fyi": "<p>Le modèle a été présenté comme atteignant, voire dépassant, le niveau de performance d’experts humains sur le benchmark GDPval, qui évalue des tâches professionnelles numériques bien définies. Il propose différents niveaux d’effort de raisonnement configurables par l’utilisateur ou le développeur, permettant d’arbitrer entre coût, latence et qualité des réponses selon la complexité de la tâche. GPT 5.2 se distingue également par sa capacité à retrouver des informations précises au sein de contextes très volumineux, y compris lorsque celles-ci sont rares et dispersées dans de longs corpus (needle in the haystack).</p>",
                "size_desc": "<p>Les tailles des modèles derrière le système GPT-5.2 ne sont pas connues. L'architecture est probablement basée sur un « mélange d'experts » (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramètres est activée pour chaque requête. Cela permet une meilleure efficacité énergétique et des performances élevées. Les estimations disponibles sur la taille des modèles s'appuient sur des informations publiques et des indices indirects tels que les coûts d'inférence et la latence de réponse. GPT 5.2 prend en charge une fenêtre de contexte allant jusqu’à 400 000 jetons, ce qui le rend adapté à l’analyse de corpus volumineux ou de bases de code étendues.</p>"
            },
            "GPT OSS-120B": {
                "desc": "<p>Le plus grand des deux premiers modèles semi-ouverts d'OpenAI depuis GPT-2. Conçu en réponse à la montée en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modèle de raisonnement performant, notamment sur des tâches complexes et dans des environnements « agentiques ».</p>",
                "fyi": "<p>Ce modèle peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>",
                "size_desc": "<p>L'architecture est basée sur le principe du « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (5,1 milliards par jeton) pour chaque requête. C’est un modèle de raisonnement, donc sa consommation d’énergie est plus élevée car ils génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>"
            },
            "GPT OSS-20B": {
                "desc": "<p>Le plus petit des deux modèles semi-ouverts d'OpenAI. Il a été conçu en réponse à la concurrence de l'open source et est destiné aux cas d'utilisation nécessitant une faible latence ainsi qu'aux déploiements locaux ou spécialisés.</p>",
                "fyi": "<p>Ce modèle peut être exécuté localement sur un ordinateur portable haut de gamme équipé de seulement 16 Go de VRAM (ou de RAM système). Cela en fait une option très accessible pour les développeurs. </p>\n<p>Dans les configurations du modèle, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui déterminent la verbosité du modèle.</p>",
                "size_desc": "<p>Avec 20 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. L'architecture est basée sur le « mélange d'experts » (MoE), ce qui permet une meilleure efficacité énergétique en n'activant qu'une partie des paramètres (3,6 milliards par jeton) pour chaque requête. Il s'agit d'un modèle de raisonnement, ce qui se traduit par une consommation d'énergie plus élevée car il génère une chaîne de pensée interne avant de fournir la réponse finale. Il dispose d'une fenêtre de contexte de 131 000 jetons, ce qui le rend idéal pour l'analyse de documents volumineux.</p>"
            },
            "GPT-3.5": {
                "desc": "<p>Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel.</p>",
                "fyi": "<p>Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel.</p>",
                "size_desc": "<p>Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.</p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p>Version allégée de GPT 4.1 mais qui reste tout de même de grande taille, conçue pour limiter les coûts tout en restant compétitif sur la plupart des tâches. Le modèle accepte de très longues requêtes, ce qui permet de l’utiliser par exemple pour l’analyse de corpus de documents.</p>",
                "fyi": "<p>Il s'agit d'une version distillée d’un modèle plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l’audio.  Sa fenêtre de contexte peut atteindre jusqu’à 1 million de jetons, ce qui le rend particulièrement adapté à l’analyse de corpus très longs ou de dépôts de code.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant une carte graphique puissante pour l’exécution. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "GPT-4o": {
                "desc": "<p>Le plus grand des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en août 2024.</p>",
                "fyi": "<p>Modèle lancé en août 2024 et successeur de GPT-4, GPT-4o est une version améliorée de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "GPT-4o mini": {
                "desc": "<p>Le plus petit des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en juillet 2024.</p>",
                "fyi": "<p>Modèle lancé en juillet 2024 et remplaçant GPT-3.5, GPT-4o mini est une version plus petite de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Grok 3 Mini": {
                "desc": "<p>Version plus légère du modèle Grok 3, permettant de réduire les coûts tout en conservant de bonnes performances pour de nombreuses tâches. Il peut simuler une phase de raisonnement avant de fournir une réponse finale.</p>",
                "fyi": "<p>Grok 3 Mini est une version distillée de Grok 3: il s’en approche en termes de capacités, tout en étant plus rapide et moins coûteux.\nLe modèle propose deux modes : un mode réflexion avec raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses immédiates.\nSa fenêtre de contexte atteint 131 000 jetons, ce qui le rend adapté à l’analyse de longs documents.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 3 Mini est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "Grok 4 Fast": {
                "desc": "<p>Grok 4 Fast est un modèle axé sur l'équilibre entre performance, vitesse et coût, notamment pour des tâches de recherche d'information et d'autres actions \"agentiques\".</p>",
                "fyi": "<p>La taille exacte du modèle n’est pas connue. Malgré son nom, Grok 4 Fast est sans doute un très grand modèle, nécessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une génération plus longue et donc une consommation énergétique plus élevée. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Entraîné par renforcement, Grok 4 Fast atteint des scores proches du très grand modèle - Grok 4 tout en restant plus économe. Il a été entraîné pour être performant sur la navigation web et spécifiquement de la plateforme X, ainsi que des capacités d’appel d’outils et de l’exécution de code.</p>",
                "size_desc": "<p>Doté d’une fenêtre de contexte de 2 millions de jetons, Grok 4 Fast combine un mode raisonnement et un mode réponse directe dans un même modèle. Il utilise environ 40 % de jetons de raisonnement en moins que Grok 4, permettant une réduction sensible du coût d’exécution et de latence.</p>"
            },
            "Grok 4.1 Fast": {
                "desc": "<p>Grok 4.1 Fast est une nouvelle itération de la quatrième génération de Grok. Basé sur le même modèle de base, selon l’éditeur il bénéficie d'un processus de post-entraînement amélioré qui lui confère un meilleur style, un comportement agentique supérieur et une meilleure cohérence des réponses, en particulier lors de discussions prolongées.</p>",
                "fyi": "<p>Les évaluations disponibles tendent à indiquer un taux d'hallucinations plus faible pour cette version du modèle. L'entraînement du Grok 4.1 aurait intégré une modélisation des récompenses (reward modeling) élargie, incluant non seulement des évaluations humaines, mais aussi des retours des modèles agissant comme des évaluateurs IA.</p>",
                "size_desc": "<p>Grok 4 Fast intègre un mode de raisonnement et un mode de réponse directe. L'utilisateur ou le développeur peut contrôler le choix d'activer ou non le raisonnement. Le modèle est doté d'une fenêtre de contexte de 2 millions de jetons, ce qui lui permet d'analyser des corpus de documents très volumineux ou des bases de code massives.</p>"
            },
            "Hermes 3 405B": {
                "desc": "<p>Très grand modèle réentraîné à partir du Llama 3.1 405B, ajusté pour mieux répondre aux demandes des utilisateurs et faciliter l’utilisation d’outils externes.</p>",
                "fyi": "<p>Ce modèle est le résultat d’un réentraînement de l’ensemble des paramètres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et système - l’utilisateur dispose ainsi d’un plus grand contrôle sur la “personnalité” et comportement du modèle. Des fonctions de raisonnement spécifiques telles que <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> ont été ajoutées pour simuler un raisonnement sur les tâches complexes. L'entraînement a utilisé un outil appelé AdamW (vitesse d'apprentissage de 3.5×10⁻⁶), qui aide le modèle à apprendre de manière efficace en ajustant progressivement ses paramètres. Ensuite, il a été affiné avec une méthode appelée DPO (direct preference optimisation), qui permet d'améliorer ses réponses en se basant sur des préférences spécifiques. Pour rendre cet entraînement plus léger et rapide, des adaptateurs LoRA ont été utilisés ; ce sont des modules plus petits qui modifient seulement une partie du modèle, ce qui évite de devoir retravailler tous les paramètres en même temps.</p>",
                "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important.</p>"
            },
            "Hermes 4 70B": {
                "desc": "<p>Grand modèle réentraîné à partir du Llama 3.1 70B, ajusté pour mieux répondre aux demandes et instructions stylistiques des utilisateurs.</p>",
                "fyi": "<p>Hermes 4-70B a été entraîné sur 56 milliards de tokens en combinant Fully Sharded Data Parallel (FSDP) et Tensor Parallelism pour gérer sa taille. Le modèle repose sur la base de Llama 3.1 70B, adaptée avec TorchTitan et enrichie par environ 19 milliards de tokens synthétiques centrés sur le raisonnement. Son entraînement suit une approche multi-phase, avec un supervised fine-tuning sur des chaînes de raisonnement pouvant dépasser les 30 000 jetons. Il exploite également l’environnement Atropos, utilisé pour générer et vérifier des trajectoires complexes (code, JSON, tâches agentiques), grâce à un rejection sampling massif qui garantit la qualité des données).</p>",
                "size_desc": "<p>Hermes 4-70B est un modèle de très grande taille, nécessitant au moins une carte graphique puissante.</p>\n<p>La fenêtre de contexte atteint 40 960 jetons en raisonnement et 32 768 jetons pour d’autres tâches, avec des mécanismes de fine-tuning qui ont été utilisés pour lui apprendre à “fermer” la séquence de réflexion vers 30k jetons.</p>"
            },
            "Jamba 1.5 Large": {
                "desc": "<p>Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.</p>",
                "fyi": "<p>Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' (State Space Models) et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "Kimi K2": {
                "desc": "<p>Développé par Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), une société basée à Pékin, Kimi K2 est un très grand modèle orienté code et usages agentiques. Il est reconnu pour les tâches de développement dans des contextes agentiques (par ex. dans Cursor ou Windsurf) notamment pour son rôle en tant qu’orchestrateur. Il n’expose pas de “mode raisonnement” explicite, mais pour les grandes tâches il sous-divise sa réponse en étapes et alterne entre actions (appels d’outils) et rédaction de texte.</p>",
                "fyi": "<p>Pour stabiliser l’entraînement à très grande échelle, Moonshot AI a introduit MuonClip, un “limiteur de vitesse” pour l’entraînement qui permet d’entraîner un modèle de cette taille et sur un corpus de 15,5 trillions de jetons sans dérailler dans l’apprentissage.</p>\n<p>Côté données, K2 a beaucoup pratiqué en “simulateur” avec de vrais outils (navigateur, terminal, exécuteurs de code, API…). Comme un pilote sur simulateur, il apprend à planifier, essayer, rater puis réessayer, et à enchaîner plusieurs actions pour atteindre un objectif. Résultat: il est particulièrement bon pour orchestrer des outils et réussir des tâches en plusieurs étapes.</p>",
                "size_desc": "<p>Avec 1 billion de paramètres, ce modèle est un des plus grands modèles qui existe. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que certains autres modèles de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter des documents assez longs.</p>"
            },
            "Kimi K2 Thinking": {
                "desc": "<p>Cette version de Kimi K2 intègre une phase de raisonnement plus avancée, améliorant ses performances par rapport à l'itération originale. Il a été développé par Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), une société basée à Pékin.</p>",
                "fyi": "<p>Le modèle semble démontrer des performances accrues sur les raisonnements longs et les tâches nécessitant des étapes d'utilisation d'outils. Ces caractéristiques sont supposées favoriser une « autonomie » prolongée dans l'exécution des tâches.</p>",
                "size_desc": "<p>Avec 1 billion de paramètres (dont 32 milliards actifs par jeton), ce modèle figure parmi les modèles de très grande taille. Son architecture de Mélange d'Experts (MoE) permet néanmoins une meilleure efficacité par rapport à certains modèles de taille comparable. De plus, il a été quantifié nativement au niveau INT4. Cette quantification, qui consiste à « arrondir » les poids du modèle à moins de chiffres pour simplifier les calculs, est présentée par Moonshot AI comme permettant de réduire le temps d'inférence d'un facteur 2. La fenêtre de contexte atteint 256 000 jetons, offrant la capacité de traiter des documents très longs ou des bases de code importantes.</p>"
            },
            "LFM 2 8B A1B": {
                "desc": "<p>Un modèle spécifiquement conçu pour l'inférence efficace sur appareils locaux (on-device deployment). Son architecture vise à offrir une qualité de sortie compétitive avec celle de modèles denses de plus grande taille, tout en minimisant la latence et les exigences en ressources de calcul.</p>",
                "fyi": "<p>LFM2-8B-A1B est basé sur une architecture hybride intégrant des blocs de convolution ainsi que des mécanismes d'Attention à Requêtes Groupées (GQA). L'implémentation de mélange d'experts (MoE) est composée de 32 experts par bloc de transformation. Le \"routeur\" sélectionne et active les 4 meilleurs experts (top-4 gating) pour le traitement de chaque jeton. Cette conception vise à établir une référence pour les modèles de mélange d'experts optimisés pour l'inférence locale.</p>",
                "size_desc": "<p>Le modèle possède un total de 8,3 milliards de paramètres. Grâce à une architecture de mélange d'experts (Mixture of Experts, MoE), l'activation est limitée à environ 1,5 milliard de paramètres par jeton lors de l'inférence. Cette configuration lui permet d'atteindre une performance généralement associée aux modèles denses de la classe des 3 à 4 milliards de paramètres, avec une vitesse d'exécution comparable à celle des modèles de 1,5 milliard de paramètres. Il est proposé dans des variantes quantifiées optimisées pour les environnements locaux (comme par exemple pour un smartphone). La fenêtre de contexte est limitée à 32 000 jetons.</p>"
            },
            "LFM 40B": {
                "desc": "<p>Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.</p>",
                "fyi": "<p>Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts (MoE), dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Llama 3 70B": {
                "desc": "<p>Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens mais supporte un contexte relativement restreint de 8000 tokens.</p>",
                "fyi": "<p>Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens.</p>",
                "size_desc": "<p>Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.</p>"
            },
            "Llama 3 8B": {
                "desc": "<p>Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité.</p>",
                "fyi": "<p>Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p>Très grand modèle conçu pour des tâches complexes ou spécialisées. Souvent utilisé en tant que “modèle professeur” pour l’entraînement de modèles plus spécialisés.</p>",
                "fyi": "<p>Le modèle a été entraîné sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le marché en 2025). L'entraînement a combiné génération de données synthétiques et optimisation par préférences directes (DPO). Ce modèle est lui-même souvent utilisé pour générer des données synthétiques pour entraîner de plus petits modèles. Le modèle utilise par défaut une compression 8-bit pour réduire les besoins en mémoire et permettre l'exécution sur un seul serveur très puissant.</p>",
                "size_desc": "<p>Avec 405 milliards de paramètres, ce modèle fait partie des modèles de très grande taille. Il nécessite un serveur équipé de plusieurs cartes graphiques puissantes, ce qui entraîne un coût de fonctionnement important. Le modèle est doté d’une fenêtre de contexte jusqu’à 128 000 jetons, ce qui le rend intéressant pour des tâches d’analyse de longs documents.</p>"
            },
            "Llama 3.1 70B": {
                "desc": "<p>Doté de 70 milliards de paramètres et sorti en avril 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues.</p>",
                "fyi": "<p>Comme les autres modèles de la famille Llama 3.1, ce modèle sorti en avril 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. </p>",
                "size_desc": "<p>Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.</p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p>Petit modèle conçu pour un usage local sur ordinateur portable, tout en offrant de bonnes capacités pour la synthèse de texte et les réponses simples.</p>",
                "fyi": "<p>Ce modèle est une version distillée issue des modèles Llama 3 de plus grande tailles : il a été entraîné grâce à un transfert d’une partie des connaissances des plus grands modèles.</p>",
                "size_desc": "<p>Avec 8 milliards de paramètres, ce modèle fait partie des petits modèles. Il peut être utilisé localement sur un ordinateur puissant, garantissant la confidentialité des données, ou hébergé sur un serveur équipé d’une seule carte graphique, ce qui limite les coûts d’infrastructure. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p>Grand modèle destiné à un large éventail de tâches et pouvant rivaliser avec des modèles plus volumineux.</p>",
                "fyi": "<p>Ce modèle est une version distillée issue du modèle 405B, auquel il doit une partie de ses connaissances transférées. Il a aussi bénéficié de techniques récentes d’alignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome.  Son entraînement s’appuie sur 15 billions de jetons.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation significatifs. Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Llama 4 Maverick": {
                "desc": "<p>Très grand modèle doté d’une très large fenêtre de contexte, adapté par exemple au résumé de plusieurs documents en même temps.</p>",
                "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Cela permet de transférer ses compétences plus vite et avec moins de calcul.  Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives - il peut traiter jusqu’à 8 images simultanément. L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. L'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données - en gardant seulement les plus complexes et intéressantes, fine-tuning ciblé et apprentissage par renforcement en ligne, pour équilibrer performances multimodales, raisonnement et qualité conversationnelle. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons. </p>\n<p>Le modèle Llama 4 Maverick a été présenté comme la réponse directe de Meta aux modèles DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estimé qu’il ne répondait pas aux attentes, en particulier sur les tâches de programmation et les travaux créatifs.</p>",
                "size_desc": "<p>Avec 400 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il nécessite moins de ressources pour fonctionner que les modèles “denses” de cette taille. Sa fenêtre de contexte va jusqu’à 1 millions de jetons, ce qui permet de traiter de très grands corpus documentaires.</p>"
            },
            "Llama 4 Scout": {
                "desc": "<p>Grand modèle doté d’une très large fenêtre de contexte, adapté par exemple à la synthèse d'un ensemble de documents.</p>",
                "fyi": "<p>Ce modèle a été codistillé avec Behemoth, ce qui veut dire qu’il a appris en même temps que le modèle géant, et non après comme dans une distillation classique. Il a été entraîné sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacités multimodales natives . L’architecture repose sur un système de mix d’experts (MoE - Mixture of Experts), avec 17 milliards de paramètres actifs, 16 experts et 109 milliards de paramètres totaux. Afin d'équilibrer performances multimodales, raisonnement et qualité conversationnelle, l'équipe Meta a développé une stratégie de post-entraînement progressive, combinant filtrage adaptatif des données (pour ne garder que les plus complexes et intéressantes), fine-tuning ciblé et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modèle apprenait donc en essayant de réaliser des tâches en ligne de manière autonome. Grâce à l’architecture iRoPE (version optimisée de l’encodage positionnel), il peut gérer des fenêtres de contexte très longues, jusqu’à 10 millions de jetons et peut traiter jusqu’à 8 images simultanément. </p>\n<p>Le modèle a été bien reçu à son lancement, notamment pour sa fenêtre de contexte impressionnante, une première dans le domaine, ainsi que pour son rapport qualité-prix sur des tâches comme le résumé, l’appel d’outils et la génération augmentée (RAG). Cela en fait un choix adapté pour les pipelines automatisés.</p>",
                "size_desc": "<p>Avec 109 milliards de paramètres, ce modèle se place dans la catégorie des grands modèles. Néanmoins, grâce à une architecture de mélange d’experts (MoE, Mixture of Experts), il peut être hébergé sur un serveur doté d’une seule carte graphique très puissante. Sa fenêtre de contexte va jusqu’à 10 millions de jetons, ce qui permet de traiter des corpus documentaires extrêmement longs.</p>"
            },
            "Magistral Medium": {
                "desc": "<p>Modèle de raisonnement de taille moyenne multimodal et multilingue. Adapté à des tâches de programmation ou autres tâches nécessitant analyse approfondie compréhension de systèmes logiques complexes ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
                "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. Il a été entraîné avec de l’apprentissage par renforcement sur Mistral Medium 3 et n'a pas été distillé à partir de modèles de raisonnement existants. Ce modèle hérite des capacités multimodales de Mistral Medium 3 même si l'apprentissage par renforcement n'a été réalisé que sur du texte.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les modèles de raisonnement requièrent plus de capacité de calcul pour produire une réponse, ce qui augmente leur consommation énergétique. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>"
            },
            "Magistral Small": {
                "desc": "<p>Modèle de raisonnement de taille moyenne, multimodal et multilingue. Adapté à des tâches nécessitant une analyse approfondie, compréhension de systèmes logiques ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes.</p>",
                "fyi": "<p>Ce modèle fait partie de la première génération des modèles de raisonnement de Mistral AI (été 2025). Contrairement à la plupart des autres modèles de raisonnement, ce modèle peut raisonner en plusieurs langues incluant l'anglais, le français, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifié. </p>\n<p>L'entraînement s'est fait en deux phases. La première, dite de raisonnement <em>cold-start</em> par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au modèle d'acquérir des capacités de base en raisonnement à partir de données d'instruction générale (10%). La seconde est une phase d'apprentissage par renforcement (RL, <em>renforcement learning</em>) à haute entropie, où le modèle est encouragé à explorer des solutions diverses et variées plutôt que de converger vers une seule réponse, et à générer des complétions longues (jusqu'à 32 000 jetons), ce qui permet de développer des capacités de raisonnement qui dépassent celles du modèle enseignant.</p>",
                "size_desc": "<p>Avec 24 milliards de paramètres, ce modèle est classé parmi les modèles de taille moyenne. Il nécessite une seule carte graphiques puissante pour fonctionner. Les modèles de raisonnement fonctionnent également plus longtemps pour produire une réponse, ce qui augmente leur consommation énergétique.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 40 000 jetons, utile pour l’analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>"
            },
            "MiniMax M2": {
                "desc": "<p>Modèle spécialisé dans le code avec un rapport qualité/rapidité/prix très compétitif. Il a été conçu par MiniMax, entreprise basée à Shanghai en Chine.</p>",
                "fyi": "<p>Modèle conçu spécifiquement pour des tâches agentiques (notamment du code), avec un entraînement pour respecter des protocoles stricts de contrôle d'agent (planification, appels d'outils, vérification).</p>",
                "size_desc": "<p>Modèle d'architecture mélange d'experts avec 230 milliards de paramètres (dont 10 milliards actifs par génération de jeton). La fenêtre de contexte s'élève à 200 000 jetons, permettant de traîter des bases de code entières et des documents longs.</p>"
            },
            "Ministral": {
                "desc": "<p>Petit modèle multilingue conçu pour fonctionner sur un ordinateur portable sans connexion à un serveur, tout en offrant de bonnes capacités en synthèse de texte, réponses à des questions simples et utilisation d’outils.</p>",
                "fyi": "<p>Ce modèle utilise une méthode d'attention de requête groupée (GQA, grouped query attention) pour limiter le texte analysé à chaque étape de génération et gagner en vitesse et en mémoire: les temps de calculs sont réduits sans incidence sur la qualité. Le mécanisme d'attention est amélioré en appliquant des fenêtres de tailles différentes, ce qui permet de gérer de longs contextes (jusqu’à 128 000 jetons) tout en restant léger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui améliore ses performances sur des tâches multilingues.</p>",
                "size_desc": "<p>Avec ses 8 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles (entre 7 et 20 milliards de paramètres). Il peut être déployé localement sur un ordinateur assez puissant, garantissant la confidentialité des données ou hébergé sur un serveur avec une seule carte graphique pour limiter les coûts d’infrastructure.</p>"
            },
            "Mistral 3 Large": {
                "desc": "<p>Très grand modèle multimodal semi-ouvert performant en code et contextes multilingues.</p>",
                "fyi": "<p>Ce modèle a été entraîné sur 3 000 GPU Nvidia H200. Il se positionne en concurrence directe avec les modèles semi-ouverts chinois. À sa sortie, il est compétitif avec DeepSeek V3.1, Kimi K2, GLM 4.6 et d’autres références en termes de code et cas d'usages généralistes.</p>",
                "size_desc": "<p>Avec 675 milliards de paramètres, ce modèle se place dans la catégorie des très grands modèles. Grâce à une architecture de mélange d’experts (MoE, Mixture of Experts) il est plus efficace que des modèles denses de taille similaire, mais il nécessite quand-même un serveur doté de plusieurs cartes graphiques très puissantes pour être hébergé. Ce modèle active 41 milliards de paramètres par génération de jeton. Sa fenêtre de contexte va jusqu’à 256 000 jetons, ce qui permet de traiter de très longs documents.</p>"
            },
            "Mistral Large 2": {
                "desc": "<p>Grand modèle prévu pour traiter des questions et tâches complexes : par exemple génération de code, utilisation d’outils, analyse de documents longs ou compréhension précise du langage.</p>",
                "fyi": "<p>Ce modèle a été entraîné avec une forte proportion de données en code (plus de 80 langages de programmation) et de mathématiques, ce qui améliore sa capacité à résoudre des problèmes complexes et à utiliser des outils externes.</p>",
                "size_desc": "<p>Avec 123 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite un serveur équipé d’au moins une carte graphique puissante, ce qui implique un coût de fonctionnement important. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Medium 3.1": {
                "desc": "<p>Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est devenu particulièrement intéressant après une mise à jour en août 2025 avec des améliorations importantes de performance générale, un ton \"amélioré\" et une meilleure capacité de chercher des informations sur Internet.</p>",
                "fyi": "<p>Ce modèle a été conçu pour offrir des performances solides à un coût inférieur à celui des modèles propriétaires ou semi-ouverts. Une attention particulière a été portée aux données d’usage professionnel pendant son entraînement. Il est particulièrement bon en comparaison à d’autres modèles de taille similaire à générer du code et réaliser des tâches mathématiques.</p>\n<p>Ce modèle a servi de base pour entraîner Magistral Medium - un modèle de raisonnement.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de grande taille, nécessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>\n<p>Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Nemo": {
                "desc": "<p>Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter un contexte de 128k tokens en plus de 100 langues. Sorti en juillet 2024.</p>",
                "fyi": "<p>Sorti en juillet 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes allant jusqu’à 128 000 tokens en plus de 100 langues.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Mistral Saba": {
                "desc": "<p>Modèle de taille moyenne conçu pour une compréhension linguistique et culturelle fine des langues du Moyen-Orient et d’Asie du Sud, notamment l’arabe, le tamoul et le malayalam.</p>",
                "fyi": "<p>L’entraînement a porté principalement sur des textes en arabe, tamoul et malayalam. Les corpus régionaux ont été sélectionnés pour refléter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (découpage du texte en unités de base que le modèle peut traiter), une stratégie spécialisée adaptée aux langues à morphologie complexe comme l'arabe a été employée. Des optimisations visent à éviter la fragmentation excessive des mots et à maximiser la couverture du vocabulaire.</p>",
                "size_desc": "<p>La taille exacte du modèle n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de taille moyenne, nécessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. </p>\n<p>Le modèle propose une fenêtre de contexte allant jusqu’à 128 000 jetons, adaptée à l’analyse de longs documents.</p>"
            },
            "Mistral Small 3": {
                "desc": "<p>Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme et possède des capacités de raisonnement avancées.</p>",
                "fyi": "<p>Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme, possède un mode d'appel de fonction et un contexte de 32 000 tokens.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Mistral Small 3.1 24B": {
                "desc": "<p>Mistral Small 3.1 24B Instruct est une variante améliorée du Mistral Small 3 (janvier 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées.</p>",
                "fyi": "<p>Mistral Small 3.1 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p>Malgré son nom, c’est un modèle de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se démarque par un respect précis des requêtes et sa capacité à utiliser des outils avancées.</p>",
                "fyi": "<p>La version 3.2 de ce modèle est optimisée pour générer des sorties structurées, notamment en JSON, tout en limitant la répétitivité et les comportements indésirables lors de longues générations. Multimodal, il traite à la fois des entrées textuelles et des images, permettant une analyse conjointe.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle est considéré comme un modèle de taille moyenne. Il peut être hébergé sur un serveur disposant d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Il dispose d’une fenêtre de contexte allant jusqu’à 128 000 jetons, utile pour l’analyse de longs documents.</p>"
            },
            "Mistral Small 3.2 24B": {
                "desc": "<p>Mistral Small 3.2 24B Instruct est une variante améliorée du Mistral Small 3.1 (mars 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées.</p>",
                "fyi": "<p>Mistral Small 3.2 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Mixtral 8x22B": {
                "desc": "<p>Ce modèle multilingue sorti en avril 2024 a particulièrement été entraîné en anglais, français, allemand, italien et espagnol ainsi que sur des tâches de mathématiques, programmation et raisonnement.</p>",
                "fyi": "<p>L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents. Sorti en avril 2024.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "Mixtral-8x7B": {
                "desc": "<p>Ce modèle entraîné sur un corpus multilingue est efficace pour des tâches variées et peu complexes.</p>",
                "fyi": "<p>Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p>Grand modèle entraîné à partir de Llama 3.1 70B. Cette version réentraînée (fine-tune) a tendance à détailler davantage et fournir des réponses plus structurées.</p>",
                "fyi": "<p>Ce modèle est issu d’un réentraînement du Llama 3.1 70B, d'où la présence de son modèle-source dans son nom ! Il introduit des améliorations grâce à l’apprentissage par renforcement avec retour humain (RLHF) et à l’algorithme REINFORCE : le modèle explore différentes réponses, reçoit des retours sous forme de récompenses, puis ajuste ses choix progressivement pour mieux répondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilisé quand on veut que le modèle s’adapte à des préférences humaines ou qu’il optimise ses réponses selon des critères spécifiques.</p>",
                "size_desc": "<p>Avec 70 milliards de paramètres, ce modèle appartient à la catégorie des grands modèles. Il nécessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraîne des coûts d’exploitation notables.</p>"
            },
            "o3-mini": {
                "desc": "<p>o3-mini est fait pour le raisonnement et le code. Il offre un bon équilibre entre performance, coût et latence, tout en étant plus petit que d'autres modèles de chez OpenAI.</p>",
                "fyi": "<p>Modèle optimisé pour les tâches de raisonnement STEM (science, technologie, ingénierie, mathématiques) et l'écriture de code. Il se démarque dans les domaines scientifiques, mathématiques et de la programmation.</p>",
                "size_desc": "<p>Ces modèles dotés de plusieurs centaines de milliards de paramètres sont les plus complexes et avancés en termes de performance et de précision. Les ressources de calcul et de mémoire nécessaires pour déployer ces modèles sont telles qu’ils sont destinés aux applications les plus avancées et aux environnements hautement spécialisés.</p>"
            },
            "o4 mini": {
                "desc": "<p>Très grand modèle de raisonnement, adapté pour des tâches et questions scientifiques et technologiques complexes.</p>",
                "fyi": "<p>Ce modèle est très performant pour l’analyse d’images et de graphiques. Il a aussi été entraîné pour interagir avec d’autres systèmes via des appels de fonctions, ce qui rend possible son utilisation pour des cas d’usage agentiques. En tant que modèle très puissant de raisonnement, il peut notamment être utilisé pour répartir des tâches entre plusieurs modèles plus petits et/ou plus spécialisés.  Il dispose d’une fenêtre de contexte allant jusqu’à 200 000 jetons, ce qui facilite l’analyse de longs documents.</p>",
                "size_desc": "<p>Malgré son nom et le fait que la taille exacte n’est pas connue, o4 mini est très probablement un grand modèle nécessitant des serveurs équipés de plusieurs cartes graphiques. Les modèles de raisonnement comme o4 mini nécessitent plus de temps pour répondre, car une phase de raisonnement précède la génération du résultat final, ce qui accroit leur consommation énergétique. Néanmoins, l'architecture supposée de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres pour générer chaque jeton, limitant ainsi son empreinte énergétique. Les estimations de taille s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse.</p>"
            },
            "Olmo 3 32B Think": {
                "desc": "<p>Modèle de raisonnement dont le code et les données sont complètement ouverts. Il a été entraîné par AI2, un institut de recherche à but non lucratif.</p>",
                "fyi": "<p>Ce modèle est un exemple rare de transparence totale. Le code, les données (6 billions de jetons), les “checkpoints” d'entraînement et les recettes d'évaluation sont tous publics. Bien que la reproduction complète de l'entraînement soit coûteuse en infrastructure et probablement rare, cette transparence permet aux développeurs de mieux réentrainer (fine-tune) le modèle pour des tâches spécifiques, notamment en mélangeant les données de comportement souhaitées avec le jeu de données existant, garantissant ainsi un entraînement plus stable.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, il est classé dans la catégorie des modèles de taille moyenne. Il peut être déployé sur un serveur équipé d'une seule carte graphique (GPU). Sa fenêtre de contexte atteint 65 000 jetons, ce qui le rend adapté à l'analyse d'assez longs documents.</p>"
            },
            "OLMo-2 32B": {
                "desc": "<p>OLMo 2 32B est un modèle entièrement open source (corpus et code d'entraînement inclus) créé par l'Allen AI Institute (Ai2), publié en mars 2025. </p>",
                "fyi": "<p>OLMo 2 32B est un modèle entièrement open source : le corpus et le code d'entraînement sont entièrement accessibles. Cette famille de modèle OLMo a été conçue par l'Allen Institute for AI (Ai2).</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Phi-3-Mini": {
                "desc": "<p>Performant pour des tâches de génération de code, et de résumé, ce modèle compact supporte un contexte restreint de 4000 tokens.</p>",
                "fyi": "<p>Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.</p>",
                "size_desc": "<p>Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.</p>"
            },
            "Phi-3-small-8k-Instruct": {
                "desc": "<p>Optimisé pour le raisonnement logique, ce petit modèle supporte un contexte de 8000 tokens, adapté pour la génération de code et tâches complexes.</p>",
                "fyi": "<p>Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Phi-3.5-mini": {
                "desc": "<p>Performant pour des tâches de génération de code et de résumé, ce modèle gère un grand contexte de 128k tokens.</p>",
                "fyi": "<p>Petit modèle de la famille Phi, remplaçant Phi-3-mini, ce modèle supporte un grand contexte de 128 000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.</p>",
                "size_desc": "<p>Les modèles très petits, avec moins de 7 milliards de paramètres, sont les moins complexes et les plus économiques en termes de ressources, offrant des performances suffisantes pour des tâches simples comme la classification de texte.</p>"
            },
            "Phi-4": {
                "desc": "<p>Petit modèle multilingue, capable d’utiliser des outils et performant sur des tâches complexes comme la logique, les mathématiques et le code, tout en restant compact.</p>",
                "fyi": "<p>Ce modèle utilise tiktoken pour la tokenisation, ce qui améliore ses capacités en contexte multilingue. Il a été entraîné sur un total de 9,8 <strong>billions</strong> de jetons, dont 400 milliards proviennent spécifiquement de données synthétiques de haute qualité, le reste étant constitué de données organiques filtrées. L'entraînement s'est déroulé sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-évaluation – pendant laquelle le modèle critique et réécrit ses réponses – ainsi que l'inversion des instructions ont été utilisées pour renforcer sa compréhension des consignes et ses capacités de raisonnement.</p>",
                "size_desc": "<p>Avec 14 milliards de paramètres, ce modèle appartient à la catégorie des petits modèles. Il peut être déployé localement sur un ordinateur suffisamment puissant, ou hébergé sur un serveur avec une seule carte graphique, ce qui réduit les coûts d’infrastructure. La fenêtre de contexte, de 16 000 jetons, peut être limitante pour l’analyse de documents très longs.</p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p>Modèle de taille moyenne spécialisé en programmation et dans l’usage d’outils externes (recherches web, interactions avec des logiciels…).</p>",
                "fyi": "<p>Ce modèle a été entraîné sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code spécialisés comme Haskell ou Racket. </p>\n<p>Grâce à ses performances en code, il est  capable de bien gérer les appels à des outils externes, ce qui est utile pour des usages agentiques.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p>Modèle de taille moyenne multilingue.</p>",
                "fyi": "<p>Ce modèle MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activés par jeton, ce qui permet une inférence plus rapide et plus efficace. Il utilise un système appelé <em>global-batch</em> pour optimiser la répartition du travail entre les experts, afin qu'ils soient tous utilisés de manière équilibrée.</p>\n<p>Contrairement à d'autres modèles comme Qwen 2.5-MoE qui recyclent les mêmes experts à travers plusieurs couches du réseau, Qwen 3 30B A2B attribue des experts uniques à chaque couche. Concrètement, cela signifie que les experts de la première couche ne sont jamais réutilisés dans les couches suivantes - chaque niveau du modèle dispose de son propre ensemble d'experts spécialisés. Cette architecture permet à chaque expert de se concentrer exclusivement sur les tâches spécifiques à sa position dans le réseau neuronal, résultant en une spécialisation plus fine et des performances optimisées pour chaque étape du traitement de l'information.</p>",
                "size_desc": "<p>Avec 30 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. De plus l'architecture de mélange d’experts (MoE, Mixture of Experts) n'active qu'une partie des paramètres à chaque jeton, limitant ainsi son empreinte énergétique.</p>"
            },
            "Qwen 3 32B": {
                "desc": "<p>Modèle de taille moyenne multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale.</p>",
                "fyi": "<p>Ce modèle a été entraîné sur un très grand volume de données : 36 billions de jetons, en 119 langues. L'entraînement s’est fait en trois étapes. Le modèle a d'abord appris à partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont été ajoutés pour renforcer ses connaissances factuelles. Enfin, il a été exposé à un corpus spécifique pour l’aider à mieux gérer les très longs textes. Résultat : il dispose en fin d'entrainement d'une fenêtre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure.</p>\n<p>Sa fenêtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>"
            },
            "Qwen 3 8B": {
                "desc": "<p>Petit modèle multilingue dense de la famille Qwen 3, offrant un mode “raisonnement” pour des tâches complexes (mathématiques, code) et un mode “réponse directe” pour des réponses plus rapides.</p>",
                "fyi": "<p>Qwen 3 8B a été entraîné sur le même corpus que les modèles plus grands de la famille Qwen : 36 billions de jetons couvrant 119 langues. Son apprentissage suit trois étapes : pré-entraînement sur 30 billions de jetons avec une fenêtre de 4 000, enrichissement factuel avec 5 billions de jetons, puis une phase spécialisée pour les contextes longs.</p>",
                "size_desc": "<p>Avec 8 milliards de paramètres, il fait partie des modèles de petite taille. Il peut être utilisé localement sur un poste pour préserver la confidentialité des données, ou sur serveur peu coûteux pour limiter les coûts par rapport à un modèle plus grand. </p>\n<p>Sa fenêtre de contexte va jusqu’à 128 000 jetons, ce qui permet de traiter de longs documents.</p>"
            },
            "Qwen 3 Max": {
                "desc": "<p>Parmi les rares modèles propriétaires de Qwen, celui-ci est le plus grand et le plus puissant de la troisième génération. Il a été entraîné avec une attention particulière à l’usage en entreprise et aux cas d’utilisation agentiques.</p>",
                "fyi": "<p>Ce modèle a été entraîné sur 36 billions de jetons, soit presque le double de Qwen 2.5, et il est officiellement capable de répondre dans 100 langues.</p>",
                "size_desc": "<p>La taille exacte n’est pas connue. Des indices laissent penser qu’il s’agit d’un modèle de très grande taille, nécessitant des serveurs équipés de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s’appuient sur des indices indirects comme les coûts d'inférence et la latence de réponse. Le modèle dispose d’une fenêtre de contexte allant jusqu’à 256 000 jetons, adaptée à l’analyse de longs documents ou de dépôts de code.</p>"
            },
            "Qwen1.5-32B": {
                "desc": "<p>Modèle de taille moyenne avec un processus d'entraînement très ciblé sur l'alignement aux préférences des utilisateurs.</p>",
                "fyi": "<p>Le modèle a traversé une phase d'alignement avec les préférences des utilisateurs via des techniques telles que le DPO (Direct Preference Optimization) et PPO (Proximal policy optimization). En plus de ces techniques, très innovantes au moment de la conception du modèle, les équipes d'Alibaba ont aussi optimisé les données d'entraînement pour les rendre très multilingues, notamment sur des langues européennes, d'Asie de l'Est et Sud-Est.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Qwen2-57B-A14B-Instruct": {
                "desc": "<p>Modèle de taille moyenne avec une architecture de mix d'experts, performant en code, mathématiques et tâches multilingues.</p>",
                "fyi": "<p>Cette itération des modèles Qwen a des fenêtres de contexte plus longues.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Qwen2-72b-instruct": {
                "desc": "<p>Grand modèle performant en code, mathématiques et tâches multilingues.</p>",
                "fyi": "<p>Cette itération des modèles Qwen a des fenêtres de contexte plus longues.</p>",
                "size_desc": "<p>Les grands modèles nécessitent des ressources significatives, mais offrent les meilleures performances pour des tâches avancées comme la rédaction créative, la modélisation de dialogues et les applications nécessitant une compréhension fine du contexte.</p>"
            },
            "Qwen2-7B": {
                "desc": "<p>Supportant un contexte de 130k tokens, ce petit modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.</p>",
                "fyi": "<p>Petit frère de la famille Qwen2 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Qwen2.5-32B": {
                "desc": "<p>Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.</p>",
                "fyi": "<p>Modèle intermédiaire de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes.</p>",
                "size_desc": "<p>Les modèles moyens offrent un bon équilibre entre complexité, coût et performance : ils sont beaucoup moins consommateurs de ressources que les grands modèles tout en étant capables de gérer des tâches complexes telles que l'analyse de sentiment ou le raisonnement.</p>"
            },
            "Qwen2.5-7B": {
                "desc": "<p>Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement.</p>",
                "fyi": "<p>Petit modèle de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            },
            "Qwen3 Coder 480B A35B": {
                "desc": "<p>Très grand modèle spécialisé dans la génération de code, analyse de dépôts entiers et résolution de problèmes multi-étapes. Cette version est particulièrement forte en utilisation d’outils et peut simuler une phase de raisonnement avant de fournir la réponse finale.</p>",
                "fyi": "<p>Ce modèle a été pré-entraîné sur 7,5 trillions de jetons (dont 70 % de code), et utilise un processus de post-entraînement avancée - Code RL (Hard to Solve, Easy to Verify) pour renforcer l’exécution correcte du code et Agent RL (long-horizon reinforcement learning) pour optimiser la résolution de tâches logicielles multi-tours, avec un environnement massivement parallèle (20 000 simulations en parallèle sur Alibaba Cloud).</p>",
                "size_desc": "<p>Qwen3-Coder-480B-A35B-Instruct est un modèle de très grande taille, nécessitant plusieurs cartes graphiques pour fonctionner. L’architecture de mélange d’experts (MoE, Mixture of Experts) permet néanmoins de n’activer qu’une fraction des paramètres (35 B sur 480 B), ce qui réduit considérablement l'impact environnemental et le coût par rapport à un modèle dense équivalent.\nLa fenêtre de contexte atteint nativement 256 000 jetons et peut être étendue jusqu’à 1 million grâce à des techniques d’extrapolation (YaRN), ce qui est idéal pour l’analyse de bases de code volumineuses.</p>"
            },
            "qwq 32B": {
                "desc": "<p>Modèle de raisonnement de taille moyenne spécialisé et très performant en mathématiques, génération de code, et résolution de problèmes logiques.</p>",
                "fyi": "<p>Ce modèle a été entraîné avec une méthode d’apprentissage par renforcement (RL) pour optimiser la gestion des problèmes de mathématiques et des tâches de programmation. Il utilise plusieurs techniques récentes pour améliorer la qualité des réponses. Par exemple, la méthode RoPE (Rotary Position Embedding) lui permet de mieux comprendre l’ordre des mots dans un texte. La fonction d'activation SwiGLU est une manière plus efficace de gérer les calculs au sein du réseau de neurones qui aide le modèle à produire des réponses plus fiables. La méthode d'ajustement QKV (Query Key Value-biais) améliore la manière dont le modèle repère et sélectionne les informations importantes. Enfin, grâce à la méthode YaRN (Yet another RoPE extensioN method), il peut traiter de très longs textes allant jusqu’à 130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou très détaillés.</p>",
                "size_desc": "<p>Avec 32 milliards de paramètres, ce modèle appartient à la catégorie des modèles de taille moyenne. Il peut fonctionner sur un serveur équipé d’une seule carte graphique puissante, ce qui limite les coûts d’infrastructure. Néanmoins, les modèles de raisonnement de ce type fonctionnent plus longtemps pour produire une réponse car une phase de raisonnement précède la génération du résultat final, ce qui augmente la consommation énergétique.</p>"
            },
            "Yi-1.5 9B": {
                "desc": "<p>Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage.</p>",
                "fyi": "<p>Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage.</p>",
                "size_desc": "<p>Un modèle de petit gabarit est moins complexe et coûteux en ressources par rapport aux modèles plus grands, tout en offrant une performance suffisante pour diverses tâches (résumé, traduction, classification de texte...)</p>"
            }
        }
    },
    "header": {
        "banner": "🏆 Le classement des modèles est maintenant disponible ! Découvrir le classement.",
        "chatbot": {
            "newDiscussion": "Nouvelle discussion",
            "step": "Étape",
            "stepOne": {
                "description": "Prêtez attention au fond et à la forme puis évaluez chaque réponse",
                "title": "Que pensez-vous des réponses ?"
            },
            "stepTwo": {
                "description": "Découvrez l’impact environnemental de vos discussions avec chaque modèle",
                "title": "Les modèles sont démasqués !"
            }
        },
        "help": {
            "link": {
                "content": "Améliorer le comparateur",
                "title": "Donner mon avis sur le comparateur - ouvre une nouvelle fenêtre"
            }
        },
        "homeTitle": "Accueil - compar:IA",
        "logoAlt": "République Française",
        "menu": "Menu",
        "startDiscussion": "Commencer à discuter",
        "subtitle": "Le comparateur d’IA conversationnelles",
        "title": "compar:IA",
        "votes": {
            "count": "{count} votes",
            "legend": "Légende",
            "objective": "Obj : {count}",
            "tooltip": "Discutez, votez et aidez-nous à atteindre cet objectif !<br /><strong>Vos votes sont importants</strong> : ils alimentent le jeu de données compar:IA mis à disposition librement pour affiner les prochains modèles sur le français.<br />Ce commun numérique contribue au meilleur <strong>respect de la diversité linguistique et culturelle des futurs modèles de langue.</strong>"
        }
    },
    "home": {
        "europe": {
            "desc": "La Lituanie, la Suède et le Danemark rejoignent la France en adoptant le comparateur dans le but d’affiner les futurs modèles d’IA dans leurs langues nationales.",
            "languages": {
                "da": "en danois",
                "fr": "en français",
                "lt": "en lituanien",
                "sv": "en suédois"
            },
            "question": "Vous souhaitez également disposer du comparateur dans votre langue ?",
            "title": "Le comparateur <span {props}>devient européen !</span>"
        },
        "faq": {
            "discover": "Découvrir les autres questions",
            "title": "Vos questions les plus courantes"
        },
        "intro": {
            "desc": "Discutez avec deux IA à l’aveugle et évaluez leurs réponses",
            "steps": {
                "a11yDesc": "1. Je discute avec deux IA anonymes : échangez aussi longtemps que vous le souhaitez. 2. Je donne mon avis : vous contribuez ainsi à l'amélioration des modèles d’IA. 3. Les modèles sont démasqués : apprenez-en plus sur les modèles d’IA et leurs caractéristiques.",
                "one": {
                    "desc": "Échangez aussi longtemps que vous le souhaitez",
                    "title": "Je discute avec deux IA anonymes"
                },
                "three": {
                    "desc": "Apprenez en plus sur les modèles d’IA et leurs caractéristiques",
                    "title": "Les modèles sont démasqués !"
                },
                "title": "Comment ça marche",
                "two": {
                    "desc": "Vous contribuez ainsi à l'amélioration des modèles d’IA",
                    "title": "Je donne mon avis"
                }
            },
            "title": "Ne vous fiez pas aux réponses <span {props}>d’une seule IA</span>",
            "tos": {
                "accept": "J'accepte les <a {linkProps}>conditions générales d’utilisation</a>",
                "error": "Vous devez accepter les modalités d'utilisation pour continuer",
                "help": "Vos données sont partagées à des fins de recherche"
            }
        },
        "origin": {
            "project": {
                "desc": "Le comparateur a été conçu et développé dans le cadre d’une start-up d’Etat portée par le ministère de la Culture et intégrée au programme <a {linkProps}>Beta.gouv.fr</a> de la Direction interministérielle du numérique (DINUM) qui aide les administrations publiques françaises à construire des services numériques utiles, simples et faciles à utiliser.",
                "title": "Qui est à l’origine du projet ?"
            },
            "team": {
                "desc": "Le comparateur est porté au sein du Ministère de la Culture par une équipe pluridisciplinaire réunissant expert en Intelligence artificielle, développeurs, chargé de déploiement, designer, avec pour mission de rendre les IA conversationnelles plus transparentes et accessibles à toutes et tous.",
                "title": "Qui sommes-nous ?"
            }
        },
        "usage": {
            "desc": "L’outil s’adresse également aux experts IA et aux formateurs pour des usages plus spécifiques",
            "educate": {
                "desc": "Utilisez le comparateur comme un support pédagogique de sensibilisation à l’IA auprès de votre public",
                "title": "Former et sensibiliser"
            },
            "explore": {
                "desc": "Consultez au même endroit toutes les caractéristiques et conditions d’utilisation des modèles",
                "title": "Explorer les modèles"
            },
            "title": "Les usages spécifiques de compar:IA",
            "use": {
                "desc": "Développeurs, chercheurs, éditeurs de modèles… accédez aux jeux de données compar:IA pour améliorer les modèles",
                "title": "Exploiter les données"
            }
        },
        "use": {
            "compare": {
                "alt": "Comparer",
                "desc": "Discutez et développez votre esprit critique en donnant votre préférence",
                "title": "Comparer les réponses de différents modèles d’IA"
            },
            "desc": "compar:IA est un outil gratuit qui permet de sensibiliser les citoyens à l’IA générative et à ses enjeux.",
            "measure": {
                "alt": "Mesurer",
                "desc": "Découvrez l’impact environnemental de vos discussions avec chaque modèle",
                "title": "Mesurer l’empreinte écologique des questions posées aux IA"
            },
            "test": {
                "alt": "Tester",
                "desc": "Testez différents modèles, propriétaires ou non, de petites et grandes tailles...",
                "title": "Tester au même endroit les dernières IA de l’écosystème"
            },
            "title": "À quoi sert compar:IA ?"
        },
        "vote": {
            "datasetAccess": "Accéder aux jeux de données",
            "desc": "L’outil s’adresse également aux experts IA et aux formateurs pour des usages plus spécifiques",
            "steps": {
                "datasets": {
                    "desc": "Toutes les questions posées et les votes sont compilés dans des jeux de données et publiés librement après anonymisation.",
                    "title": "Les jeux de données par langue"
                },
                "finetune": {
                    "desc": "À terme, les entreprises et les acteurs universitaires peuvent exploiter les jeux de données pour entrainer de nouveaux modèles plus respectueux de la diversité linguistique et culturelle.",
                    "title": "Des modèles affinés sur la langue spécifique"
                },
                "prefs": {
                    "desc": "Après discussion avec les IA, vous indiquez votre préférence pour un modèle selon des critères donnés, tels que la pertinence ou l’utilité des réponses.",
                    "title": "Vos préférences"
                }
            },
            "title": "Pourquoi votre vote est-il important ?"
        }
    },
    "models": {
        "arch": {
            "title": "Le saviez-vous ?"
        },
        "conditions": {
            "commercialUse": {
                "question": "Les usages commerciaux du modèle sont-ils autorisés ?",
                "title": "Usage commercial"
            },
            "reuse": {
                "question": "Puis-je utiliser les sorties du modèle pour en entrainer de nouveaux ?",
                "subTitle": "Vous ne pouvez pas les réutiliser pour entraîner d’autres modèles",
                "title": "Réutilisation des résultats générés"
            },
            "title": "Conditions d'utilisation",
            "types": {
                "allowed": "Autorisé",
                "conditions": "Sous conditions",
                "forbidden": "Interdit"
            }
        },
        "conso": {
            "count": {
                "L": "> 100 Wh",
                "M": "de 10 à 100 Wh",
                "S": "< 10 Wh"
            },
            "filterLegend": "Filtre par consommation énergétique moyenne pour 1000 tokens"
        },
        "extra": {
            "experts": {
                "api-only": "Pour les expert·es, consultez la <a {linkProps}>site officiel du modèle</a>",
                "open-weights": "Pour les expert·es, consultez la <a {linkProps}>fiche du modèle sur Hugging Face</a>"
            },
            "impacts": "Les calculs d’impacts environnementaux reposent sur les projets <a {linkProps1}>EcoLogits</a> et <a {linkProps2}>Impact CO<sub>2</sub></a>.",
            "title": "Pour aller plus loin"
        },
        "licenses": {
            "type": {
                "openSource": "Open source",
                "proprietary": "Propriétaire",
                "semiOpen": "Semi-ouvert"
            }
        },
        "list": {
            "filters": {
                "archived": {
                    "checkedLabel": "Visibles",
                    "help": "Par défaut, seuls les modèles actifs dans le comparateur sont visibles.",
                    "label": "Modèles archivés",
                    "uncheckedLabel": "Non visibles"
                },
                "display": "Afficher les filtres",
                "editor": {
                    "legend": "Éditeur"
                },
                "license": {
                    "legend": "Licence d'utilisation"
                },
                "reset": "Effacer tous les filtres",
                "size": {
                    "labels": {
                        "L": "de 70 à 150 milliards",
                        "M": "de 20 à 70 milliards",
                        "S": "de 7 à 20 milliards",
                        "XL": "> 150 milliards",
                        "XS": "< à 7 milliards"
                    },
                    "legend": "Taille (paramètres)"
                }
            },
            "intro": "Explorez les différents modèles d'IA conversationnels disponibles, leurs caractéristiques et leurs licences.",
            "model": "modèle",
            "models": "modèles",
            "noresults": "Aucun modèle ne correspond à vos critères de recherche.",
            "title": "Découvrez les modèles",
            "triage": {
                "label": "Trier par",
                "options": {
                    "date-desc": "Date de sortie (du plus au moins récent)",
                    "name-asc": "Nom du modèle (A à Z)",
                    "org-asc": "Éditeur (A à Z)",
                    "params-asc": "Taille (du plus petit au plus grand)"
                }
            }
        },
        "names": {
            "a": "Modèle A",
            "b": "Modèle B"
        },
        "openWeight": {
            "tooltips": {
                "copyleft": "Une fois modifié, le modèle doit être redistribué sous la même licence que celle du modèle source.",
                "free": "Une fois modifié, le modèle peut être redistribué sous une licence différente du modèle source.",
                "openSource": "Le corpus, le code d'entraînement, et les poids de ce modèle (c’est-à-dire les paramètres appris pendant son entraînement) sont entièrement téléchargeables et modifiables par le public, lui permettant de faire fonctionner et modifier le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant qu'« open weights », notamment à cause de la nécessité de transparence du corpus d'entraînement, et rares sont les modèles qui sont considérés « open source ».",
                "openWeight": "Modèle dit « open weights » dont les poids, c’est-à-dire les paramètres appris pendant son entraînement, sont téléchargeables par le public, lui permettant de faire fonctionner le modèle sur son propre matériel. Qu'un modèle soit « open source » est plus contraignant (principalement par rapport à la transparence du corpus d'entraînement), et rares sont les modèles qui sont considérés « open source ».",
                "params": "Les paramètres ou les poids, comptés en milliards, sont les variables, apprises par un modèle au cours de son entrainement, qui déterminent ses réponses. Plus le nombre de paramètres est important, plus il a besoin de ressources pour fonctionner.",
                "ram": "La mémoire vive stocke les données traitées par un LLM en temps réel. Plus le modèle est grand, plus il a besoin de mémoire vive pour fonctionner."
            }
        },
        "parameters": "{number} mds de paramètres",
        "ram": "{min} à {max} Go",
        "release": "Sortie {date}",
        "size": {
            "count": {
                "L": "100 à 400 mds",
                "M": "60 à 100 mds",
                "S": "15 à 60 mds",
                "XL": ">400 mds",
                "XS": "<15 mds"
            },
            "estimated": "Taille estimée ({size})",
            "title": "Taille"
        }
    },
    "modes": {
        "big-vs-small": {
            "altLabel": "David contre Goliath",
            "description": "Un petit modèle contre un grand, les deux tirés au hasard",
            "label": "David contre Goliath",
            "title": "Mode David contre Goliath"
        },
        "custom": {
            "altLabel": "Sélection manuelle",
            "description": "Reconnaîtrez-vous les deux modèles que vous avez choisis ?",
            "label": "Sélection manuelle",
            "title": "Mode Sélection"
        },
        "random": {
            "altLabel": "Modèles aléatoires",
            "description": "Deux modèles choisis au hasard parmi toute la liste",
            "label": "Aléatoire",
            "title": "Mode Aléatoire"
        },
        "reasoning": {
            "altLabel": "Modèles avec raisonnement",
            "description": "Deux modèles tirés au hasard parmi ceux optimisés pour des tâches complexes",
            "label": "Raisonnement",
            "title": "Mode Raisonnement"
        },
        "small-models": {
            "altLabel": "Modèles frugaux",
            "description": "Deux modèles tirés au hasard parmi ceux de plus petite taille",
            "label": "Frugal",
            "title": "Mode Frugal"
        }
    },
    "product": {
        "community": {
            "countries": {
                "da": "Danemark",
                "fr": "France"
            },
            "tabLabel": "Communauté",
            "teams": {
                "fr": {
                    "people": {
                        "aurelien": {
                            "date": "Depuis Juin 2024",
                            "job": "Designer produit UX/UI"
                        },
                        "elie": {
                            "date": "Depuis Novembre 2025",
                            "job": "Développeur Full stack"
                        },
                        "hadrien": {
                            "date": "Juin 2024 - Novembre 2025",
                            "job": "Développeur Full stack"
                        },
                        "lucie": {
                            "date": "Janvier 2024 - Décembre 2025",
                            "job": "Fondatrice - cheffe de produit"
                        },
                        "mathilde": {
                            "date": "Depuis Septembre 2024",
                            "job": "Responsable de l'Atelier numérique / Product Ops"
                        },
                        "nicolas": {
                            "date": "Depuis Juin 2025",
                            "job": "Développeur Full stack"
                        },
                        "simonas": {
                            "date": "Depuis Décembre 2024",
                            "job": "Chef de produit - ex chargé de déploiement"
                        }
                    },
                    "title": "L’équipe compar:IA - France"
                }
            },
            "title": "Les partenaires"
        },
        "comparator": {
            "challenges": {
                "bias": {
                    "desc": "Mettre en avant les biais de l'IA liés à la sous-représentation des données non anglophones dans les modèles et sensibiliser à leurs conséquences.",
                    "title": "Biais culturels et linguistiques"
                },
                "impacts": {
                    "desc": "Révéler les effets écologiques de l'IA générative, encore largement méconnus du grand public.",
                    "title": "Impact environnemental"
                },
                "pluralism": {
                    "desc": "Assurer aux citoyens l'accès à une diversité de modèles d'IA afin qu'ils puissent faire des choix éclairés et développer un regard critique sur ces technologies.",
                    "title": "Pluralisme des modèles"
                },
                "thinking": {
                    "desc": "Inciter au questionnement critique sur la place de l’IA générative dans les pratiques personnelles et professionnelles (éducation, travail).",
                    "title": "Esprit critique et questions sociétales"
                },
                "title": "L’application développée répond à plusieurs enjeux"
            },
            "cta": "Accéder au comparateur",
            "europe": {
                "adventure": "A partir de l’automne, la Lituanie, la Suède et le Danemark rejoignent l’aventure !",
                "catch": "Vous souhaitez disposer du comparateur dans votre langue ?",
                "desc": "Le comparateur sera mis à disposition de leurs citoyens dans leurs langues nationales. L’objectif est de créer des jeux de données de préférence afin d’améliorer les futurs modèles d’IA dans ces langues européennes.",
                "title": "Le comparateur <span {props}>devient européen</span> !"
            },
            "screenshotAlt": "Capture d'écran de l'arène compar:ia, avec la question initiale, les deux réponses des modèles et les boutons de vote.",
            "tabLabel": "Le comparateur",
            "title": "Le comparateur permet de créer des <span {props}>jeux de données</span> de préférence centrés sur des <span {props}>usages réels</span> exprimés dans les <span {props}>langues européennes</span>."
        },
        "faq": {
            "tabLabel": "FAQ"
        },
        "history": {
            "steps": {
                "acceleration": {
                    "items": {
                        "1": {
                            "date": "Mars 2025",
                            "desc": "Création et mise à disposition du premier jeu de données compar:IA en français regroupant les questions et préférences des utilisateurs de la plateforme.",
                            "title": "50 000 votes sur le comparateur !"
                        },
                        "2": {
                            "date": "Mai 2025",
                            "desc": "Et première réutilisation du jeu de données avec Bunka.ai qui a mené une <a {linkProps}>étude approfondie</a> sur les interactions entre les utilisateurs de la plateforme.",
                            "title": "Objectif des 100 000 votes atteint !"
                        },
                        "3": {
                            "date": "Juin 2025",
                            "desc": "Mise à disposition de ces trois jeux de données, conversations, réactions, votes, sur <a {hgLinkProps}>HuggingFace</a> et <a {dataLinkProps}>Data.gouv.fr</a>.",
                            "title": "Publication des trois jeux de données compar:IA"
                        }
                    },
                    "tag": "Accélération"
                },
                "construction": {
                    "items": {
                        "1": {
                            "date": "Juin-Sept 2024",
                            "desc": "Développement des premières fonctionnalités du comparateur et intégrations des retours des premiers bêta-testeurs.",
                            "title": "Conception du produit minimum viable"
                        },
                        "2": {
                            "date": "Octobre 2024",
                            "desc": "Présentation officielle et premiers déploiements de l’outil.",
                            "title": "Lancement officiel lors du Sommet de la francophonie à Villers Cotterêts !"
                        },
                        "3": {
                            "date": "Janvier 2025",
                            "desc": "Lancement de la nouvelle fonctionnalité de choix du mode de sélection des modèles.",
                            "title": "compar:IA v2"
                        },
                        "4": {
                            "date": "Février 2025",
                            "desc": "Plus de 300 personnes dans le cadre de conférences et d’ateliers dédiés aux enjeux éthiques, culturels et environnementaux des systèmes d'IA conversationnelle.",
                            "title": "<a {linkProps}>Journée comparIA à la BnF</a> pendant le sommet pour l’action sur l’IA"
                        }
                    },
                    "tag": "Construction"
                },
                "i18n": {
                    "items": {
                        "1": {
                            "date": "Été 2025",
                            "desc": "Partenariat avec le Danemark, la Suède et la Lituanie pour ouvrir le service dans leur langue.",
                            "title": "Extension à l’échelle européenne"
                        },
                        "2": {
                            "date": "Septembre 2025",
                            "desc": "Création de ce nouveau format d’atelier ouvert à tout public, pour découvrir les coulisses des IA génératives et réfléchir à leur impact environnemental, et publication du kit de facilitation associé à l’extension « Duels de l’IA ».",
                            "title": "Lancement des <a {linkProps}>« Duels de l’IA »</a>"
                        },
                        "3": {
                            "desc": "Construit en partenariat avec le Pôle d'Expertise de la Régulation Numérique (<a {linkProps}>PEReN</a>), le classement compar:IA repose sur l’ensemble des votes et réactions collectés depuis l’ouverture du service au public en octobre 2024.",
                            "title": "Publication du <a {linkProps}>classement compar:IA</a>"
                        },
                        "4": {
                            "desc": "Le Danemark rejoint l’aventure en proposant le comparateur dans sa langue national avec un nom de domaine dédié.",
                            "title": "Ouverture de compar:IA <a {linkProps}>en danois</a>"
                        },
                        "5": {
                            "desc": "Le service est reconnu comme un bien public numérique par l’Alliance Digital Public Goods.",
                            "title": "compar:IA reconnu comme <a {linkProps}>Bien Commun numérique</a>"
                        },
                        "6": {
                            "date": "Novembre 2025",
                            "desc": "Avec plus de 500 000 conversations uniques depuis le lancement de compar:IA.",
                            "title": "Objectif des 200 000 votes atteint !"
                        }
                    },
                    "tag": "Internationalisation"
                },
                "investigation": {
                    "items": {
                        "1": {
                            "date": "Janv-Mars 2024",
                            "desc": "Entretiens avec des acteurs de l’écosystème et premières hypothèses de solution sur la problématique : « Comment faciliter l’accès à des données en français pour l’entrainement des modèles de langue ? ».",
                            "title": "Phase d’investigation"
                        }
                    },
                    "tag": "Investigation"
                }
            },
            "tabLabel": "Dates clés",
            "title": "compar:IA en quelques dates"
        },
        "partners": {
            "academy": {
                "catch": "Vous menez un projet de recherche et avez des suggestions ou besoin de précision sur la démarche et/ou les jeux de données produits ?",
                "desc": "Nous avons à coeur que les jeux de données générés alimentent des travaux de recherche multidisciplinaires mêlant sciences humaines et sociales et data science.",
                "title": "Partenaires académiques"
            },
            "diffusion": {
                "catch": "Vous souhaitez utiliser le comparateur pour répondre à un besoin métier ?",
                "cta": "Dites nous en plus",
                "desc": "Nous créons un réseau de partenaires intégrant le comparateur dans leur offre de services et de formation.",
                "title": "Partenaires de diffusion"
            },
            "institution": {
                "title": "Partenaires institutionnels"
            },
            "services": {
                "desc": "Les calculs d’impacts environnementaux reposent sur les produits ci dessus.",
                "title": "Services mis à contribution"
            },
            "tabLabel": "Partenaires"
        },
        "problem": {
            "alignment": {
                "alignment": {
                    "a": "L'alignement intervient après l'étape de pré-entraînement d'un modèle de langage, comme une étape de « finition » ou de « polissage ». Lors de son pré-entrainement, le modèle apprend à prédire le mot suivant et devient capable de générer du texte cohérent.",
                    "b": "L’étape d’alignement consiste à apprendre au modèle à mieux répondre aux besoins humains, c’est à dire à le rendre plus <strong>pertinent</strong> (le modèle répond « mieux » aux questions), <strong>honnête</strong> (capacité à assumer « qu’il ne sait pas répondre » quand il n’y a pas suffisamment de données), et <strong>inoffensif</strong> (éviter de générer des contenus dangereux ou inappropriés).",
                    "c": "<strong>Sans alignement, un LLM pourrait être techniquement compétent mais difficile à utiliser en pratique, car il ne comprendrait pas vraiment ce qu'on attend de lui dans une conversation.</strong>",
                    "title": "L’alignement, étape décisive d’instruction du modèle"
                },
                "datasets": {
                    "a": "L'alignement utilise des données très spécifiques, spécialement créées pour enseigner au modèle comment « bien » se comporter.",
                    "b": "Les <strong>données de préférence</strong> constituent un type particulier de données d’alignement, aux côtés des <strong>données de démonstration</strong> (exemples de conversations entre humains et assistants IA, rédigées par des annotateurs experts selon des consignes précises de ton et de style), des <strong>données de sécurité</strong> (exemples spécifiques enseignant au modèle à éviter les contenus dangereux en montrant comment refuser les demandes problématiques) ou des <strong>données spécialisées</strong> couvrant des domaines spécifiques (médecine, droit, éducation…).",
                    "c": "Les données de préférence présentent plusieurs réponses possibles à une même question, classées par ordre de qualité par des évaluateurs humains : les utilisateurs indiquent quelle réponse est la meilleure selon des critères donnés, telles que la pertinence, l’utilité, la nocivité. Une fois constitués, ces jeux de données sont utilisés pour entraîner les modèles en les ajustant selon les préférences exprimées par les utilisateurs.",
                    "title": "Des jeux de données spécifiques"
                },
                "desc": "L'alignement : une technique de réduction des biais qui repose sur la collecte des préférences d’utilisateurs",
                "diversity": {
                    "a": "Pour refléter la diversité des cultures et des langues dans les résultats générés par les modèles, <strong>les jeux de données d’alignement doivent inclure une variété de langues</strong>, de contextes et d’exemples issus de tâches courantes des utilisateurs. La diversification des données d'alignement permet d’améliorer à terme les performances d’un modèle à double titre :",
                    "b": "D'une part, elle <strong>réduit les biais culturels</strong> en évitant qu'une seule perspective - souvent anglo-saxonne - domine les réponses de l'IA. Le modèle apprend ainsi à reconnaître qu'il existe plusieurs façons valides d'aborder une même question selon le contexte culturel.",
                    "c": "D'autre part, cette exposition à la diversité de langues et de cultures favorise l’adaptation des réponses à des contextes spécifiques : un utilisateur français recevra des conseils adaptés au système français, tandis qu'un utilisateur danois obtiendra des informations correspondant à son contexte national.",
                    "d": "Le résultat est un modèle d’IA conversationnelle plus inclusif, capable de tenir compte des différentes cultures.",
                    "title": "Diversifier les données pour réduire les biais"
                },
                "english": {
                    "a": "Les données de préférence sont couteuses à produire car <strong>elles nécessitent du travail humain qualifié pour chaque exemple</strong>. Des plateformes telles que https://chat.lmsys.org/ permettent de constituer ces jeux de données de préférence mais peu d’utilisateurs s’en servent dans leur langue d’origine.",
                    "b": "Les jeux de données de préférence sont rares, voire inexistants dans les langues européennes. La part des questions posées en français dans le jeu de données de LMSYS est par exemple inférieure à 1%.",
                    "c": "comparIA est un exemple de dispositif permettant de collecter des conversations dans de multiples langues, incluant des références culturelles spécifiques à chaque région ou pays : tâches courantes, traditions culinaires locales, systèmes éducatifs, références historiques ou littéraires, etc.",
                    "title": "Peu de données de préférence en langues européennes"
                },
                "title": "Comment réduire les biais culturels et linguistiques de ces modèles ?"
            },
            "diversity": {
                "diversity": {
                    "desc": "Ces biais peuvent aussi se traduire par des réponses partielles voire incorrectes négligeant la diversité des langues et des cultures, notamment européennes.",
                    "title": "Diversités culturelles et linguistiques négligées"
                },
                "english": {
                    "desc": "Les IA conversationnelles reposent sur des grands modèles de langage (LLM) entraînés principalement sur des données en anglais, ce qui crée des biais linguistiques et culturels dans les résultats qu'ils produisent.",
                    "title": "Données d’entrainement majoritairement en anglais"
                },
                "stereotypes": {
                    "desc": "Les systèmes d’IA conversationnelle donnent l’impression de parler toutes les langues mais les résultats qu’ils génèrent sont parfois stéréotypés ou discriminants.",
                    "title": "Réponses stéréotypées"
                }
            },
            "tabLabel": "Le problème initial",
            "title": "Les modèles d’IA conversationnelles respectent-ils la <span {props}>diversité</span> des langues européennes ?"
        },
        "title": "Tout savoir sur le comparateur"
    },
    "ranking": {
        "energy": {
            "desc": "Ce graphique représente pour chaque modèle le score de satisfaction (score Bradley Terry) en fonction de l’estimation de la consommation énergétique moyenne pour 1000 tokens. La consommation énergétique est estimée à partir de la méthodologie <a {linkProps}>Ecologits</a> et repose sur la prise en compte de deux paramètres: la <strong>taille</strong> des modèles (nombre de paramètres) et leur <strong>architecture</strong>. Pour les <strong>modèles propriétaires</strong>, ces informations ne sont pas ou que partiellement communiquées. C’est pourquoi ils sont exclus du graphique ci-dessous.",
            "tabLabel": "Focus énergie",
            "title": "Les modèles les plus appréciés sont-ils économes en énergie ?",
            "views": {
                "graph": {
                    "desc": "Sélectionnez un modèle pour connaitre son score Bradley-Terry (BT) et sa consommation énergétique",
                    "legends": {
                        "arch": "Architecture du modèle",
                        "size": "Filtre par taille",
                        "sizeSub": "(milliards de paramètres)"
                    },
                    "tabLabel": "Vue graphique",
                    "title": "Score de satisfaction Bradley-Terry (BT) VS Consommation moyenne pour 1000 tokens",
                    "tooltip": {
                        "active_params": "Paramètres actifs (mds)",
                        "arch": "Architecture",
                        "consumption_wh": "Conso. moyenne (Wh)",
                        "elo": "Score BT",
                        "params": "Paramètres (mds)"
                    },
                    "xLabel": "Consommation moyenne pour 1000 tokens (Wh)",
                    "yLabel": "Score Bradley-Terry (BT)"
                },
                "methodo": {
                    "1": {
                        "list": {
                            "1": "<strong>Plus un modèle est situé en haut du graphique</strong>, plus son score de satisfaction Bradley-Terry est élevé. <strong>Plus un modèle est situé sur la gauche du graphique</strong>, moins il consomme d’énergie par rapport aux autres modèles.",
                            "2": "<strong>En haut à gauche</strong> se trouvent les modèles qui plaisent et qui consomment peu d’énergie par rapport aux autres modèles.",
                            "3": "Au-delà de la taille, <strong>l’architecture</strong> a un impact sur la consommation énergétique moyenne des modèles: par exemple, à gabarit similaire, le modèle Llama 3 405B (architecture dense, 405 milliards de paramètres) consomme 10 fois plus d’énergie en moyenne que le modèle GLM 4.5 (architecture MOE, 355 milliards de paramètres et 32 milliards de paramètres actifs)."
                        },
                        "subTitle": "Exemples de lecture du graphique",
                        "title": "Comment trouver le bon équilibre entre performance perçue et sobriété énergétique ?"
                    },
                    "2": {
                        "descs": {
                            "1": "L’estimation de la consommation énergétique pour l’inférence des modèles repose sur la méthodologie Ecologits qui prend en compte la taille et l’architecture des modèles. Or ces informations ne sont pas rendues publiques par les éditeurs de modèles pour les modèles dits « propriétaires ».",
                            "2": "Nous prenons ainsi le parti de ne pas intégrer les modèles propriétaires au graphique tant que les informations contribuant au calcul de consommation énergétique ne sont pas transparentes."
                        },
                        "title": "Pourquoi les modèles propriétaires ne sont-ils pas affichés sur le graphique ?"
                    },
                    "3": {
                        "descs": {
                            "1": "compar:IA utilise la méthodologie développée par <a {ecoLinkProps}><strong>Ecologits</strong></a> (<a {genaiLinkProps}><strong>GenAI Impact</strong></a>) pour fournir une estimation du bilan énergétique lié à l’inférence des modèles d’IA générative conversationnelle. Cette estimation permet aux utilisateurs de comparer l'impact environnemental de différents modèles d'IA pour une même requête. Cette transparence est essentielle pour encourager le développement et l'adoption de modèles d'IA plus éco-responsables.",
                            "2": "Ecologits applique les principes de l'analyse du cycle de vie (ACV) conformément à la norme ISO 14044 en se concentrant pour le moment sur l'impact de l'inférence (c'est-à-dire l'utilisation des modèles pour répondre aux requêtes) et de la <strong>fabrication des cartes graphiques</strong> (extraction des ressources, fabrication et transport).",
                            "3": "La consommation électrique du modèle est estimée en tenant compte de divers paramètres tels que la taille et l’architecture du modèle d'IA utilisé, la localisation des serveurs où sont déployés les modèles et le nombre de tokens de sortie. Le calcul de l’indicateur de potentiel de réchauffement climatique exprimé en équivalent CO2 est dérivé de la mesure de consommation électrique du modèle.",
                            "4": "Il est important de noter que les méthodologies d'évaluation de l'impact environnemental de l'IA sont encore en développement."
                        },
                        "title": "Comment est calculé l’impact énergétique des modèles ?"
                    }
                },
                "table": {
                    "title": "Données du graphique en tableau"
                }
            }
        },
        "methodo": {
            "desc": {
                "1": "Depuis 2024, des milliers d’utilisateurs ont utilisé compar:IA pour comparer les réponses de différents modèles, générant ainsi des centaines de milliers de votes. Compter simplement le nombre de victoires ne suffit pas pour établir un classement. Un système équitable doit être statistiquement robuste, s’ajuster après chaque confrontation, et refléter réellement la valeur des performances obtenues.",
                "2": "C’est dans cette perspective qu’a été mis en place un <strong>classement basé sur le modèle Bradley-Terry</strong>, élaboré en collaboration avec les <strong>équipes du Pôle d’Expertise de la Régulation numérique (<a {perenLinkProps}>PEReN</a>)</strong>, à partir de l’ensemble des votes et réactions collectés sur la plateforme. Pour aller plus loin, consultez notre <a {notebookLinkProps}>carnet méthodologique</a>."
            },
            "impacts": {
                "elo": {
                    "desc": {
                        "1": "Le modèle <strong>Bradley-Terry</strong> transforme un ensemble de comparaisons locales et potentiellement incomplètes en un système de classement global cohérent et statistiquement robuste, là où le taux de victoire empirique reste limité aux observations directes."
                    },
                    "title": "10 premiers modèles du classement selon de taux de victoire estimé avec le modèle Bradley-Terry"
                },
                "title": "Impact du choix de la méthode sur le classement des modèles",
                "winrate": {
                    "desc": {
                        "1": "En se basant uniquement sur le <strong>taux de victoire moyen</strong>, on peut obtenir un classement global, mais ce calcul suppose que chaque modèle ait joué contre tous les autres.",
                        "2": "Cette méthode n'est pas idéale car elle nécessite les données de toutes les combinaisons de modèles et ès qu’on augmente le nombre de modèles, cela devient vite coûteux et lourd à maintenir."
                    },
                    "title": "10 premiers modèles du classement selon de taux de victoire « empirique »"
                }
            },
            "methods": {
                "cons": "Problèmes principaux",
                "def": "",
                "elo": {
                    "def": "<strong>Définition</strong> : Système de classement où le gain ou la perte de points dépend du résultat (victoire/défaite/nul <strong>et</strong> du niveau estimé de l’adversaire : si un modèle plus faible bat un modèle plus fort, sa progression dans le classement est plus importante.",
                    "list": {
                        "1": "<strong>Modèle probabiliste</strong> : on peut estimer le résultat probable de n'importe quelle confrontation, même entre des modèles n'ayant jamais été directement comparés.",
                        "2": "<strong>Prise en compte de la difficulté des matchs</strong> : les scores estimés à partir du modèle Bradley Terry tiennent compte du niveau des adversaires rencontrés, permettant une comparaison équitable entre modèles.",
                        "3": "<strong>Meilleure gestion de l’incertitude</strong> : l'intervalle de confiance intègre l'ensemble du réseau de comparaisons. Cela permet une estimation plus précise de l'incertitude, surtout pour les modèles ayant peu de confrontations directes mais beaucoup d'adversaires communs."
                    },
                    "title": "Classement Bradley-Terry (BT)"
                },
                "pros": "Avantages",
                "title": "Deux manières de classer les modèles",
                "winrate": {
                    "def": "<strong>Définition</strong> : Système de classement empirique des modèles reposant sur le pourcentage de parties gagnées par un modèle contre tous les autres modèles.",
                    "list": {
                        "1": "<strong>Biais du nombre de parties</strong> : un modèle ayant remporté trois victoires sur trois “matchs” affiche un taux de victoire de 100 %, mais ce score est peu significatif étant basé sur très peu de données.",
                        "2": "<strong>Aucune prise en compte de la difficulté des matchs</strong> : battre un modèle “débutant” ou un “expert” compte pareil. Les taux de victoire ne sont pas équitables puisqu’ils ne tiennent pas compte de la difficulté des matchs.",
                        "3": "<strong>Stagnation</strong> : à long terme, beaucoup de bons modèles finissent autour de 50 % de taux de victoire car ils affrontent des modèles de leur niveau, ce qui rend le classement peu discriminant."
                    },
                    "title": "Classement par taux de victoire"
                }
            },
            "tabLabel": "Méthodologie",
            "title": "Comment choisir la méthode de classement des modèles ?"
        },
        "preferences": {
            "desc": "Quand vous votez, vous pouvez qualifier votre préférence selon différentes catégories positives et négatives. Comparez leur répartition d’un modèle à l’autre.",
            "modal": {
                "cta": "Qu’est-ce qu’une préférence ?",
                "title": "Préférences positives et négatives"
            },
            "tabLabel": "Focus préférences",
            "table": {
                "cols": {
                    "clear_formatting": "Mise en forme<br> claire",
                    "complete": "Complète",
                    "creative": "Créative",
                    "incorrect": "Incorrecte",
                    "instructions_not_followed": "Instructions<br> non suivies",
                    "n_match": "Total matchs",
                    "name": "Modèle",
                    "positive_prefs_ratio": "Répartition des préférences",
                    "superficial": "Superficielle",
                    "total_negative_prefs": "Total préférences<br> Négatives",
                    "total_positive_prefs": "Total préférences<br> Positives",
                    "useful": "Utile"
                },
                "percentLabel": "Préférences en pourcentage",
                "tooltips": {
                    "positive_prefs_ratio": "Lors du vote, des badges permettaient de préciser les raisons de votre préférence. Cette colonne présente la répartition en pourcentage de ces badges (positifs ou négatifs) pour l'ensemble des votes."
                }
            },
            "title": "Comment se répartissent les préférences des utilisateurs ?"
        },
        "ranking": {
            "desc": "Merci pour vos contributions !<br> Le classement <strong>compar:IA</strong> repose sur l’ensemble des votes et réactions issus de la <strong>comparaison à l'aveugle </strong> des modèles et collectés depuis l’ouverture du service au public en octobre 2024.<br> Construit en partenariat avec le Pôle d'Expertise de la Régulation Numérique (<a {linkProps}>PEReN</a>), le classement des modèles est établi en fonction du <strong>score de satisfaction</strong> calculé à partir du modèle statistique <strong>Bradley Terry</strong>, méthode largement répandue pour convertir des votes binaires en classement probabiliste.<br> Le classement compar:IA n’a pas vocation à constituer une recommandation officielle ni à évaluer la performance technique des modèles. Il reflète les <strong>préférences subjectives</strong> des utilisateurs de la plateforme et non la factualité ou la véracité des réponses.",
            "tabLabel": "Classement"
        },
        "table": {
            "data": {
                "billions": "{count} Mds",
                "cols": {
                    "arch": "Architecture",
                    "consumption_wh": "Conso. moyenne<br>(1000 tokens)",
                    "elo": "Score de<br> satisfaction BT",
                    "license": "Licence",
                    "n_match": "Total votes",
                    "name": "Modèle",
                    "organisation": "Organisation",
                    "rank": "Rang",
                    "release": "Date sortie",
                    "size": "Taille<br>(paramètres)",
                    "trust_range": "Confiance (±)"
                },
                "estimation": "(estimation)",
                "tooltips": {
                    "arch": "L'architecture d'un modèle LLM désigne les principes de conception qui définissent comment les composants d'un réseau de neurones sont agencés et interagissent pour transformer les données d'entrée en sorties prédictives, incluant le mode d'activation des paramètres (dense vs. sparse), la spécialisation des composants et les mécanismes de traitement de l'information (transformers, réseaux convolutifs, architectures hybrides).",
                    "consumption_wh": "Mesurée en wattheures, l’énergie consommée représente l'électricité utilisée par le modèle pour traiter une requête et générer la réponse correspondante. La consommation énergétique des modèles dépend de leur taille et de leur architecture. Nous prenons le parti d’afficher en grisé non analysés (N/A) les modèles propriétaires pour lesquels nous ne disposons pas d’information transparente sur la taille et l’architecture.",
                    "elo": "Score statistique estimé selon le modèle Bradley-Terry reflétant la probabilité qu'un modèle soit préféré à un autre. Ce score est calculé à partir de l'ensemble des votes et réactions des utilisateurs. Pour en savoir plus, rendez-vous sur l’onglet méthodologie.",
                    "rank": "Rang de classement attribué selon le score de satisfaction Bradley-Terry",
                    "size": "Taille du modèle en milliards de paramètres, catégorisée selon cinq classes. Pour les modèles propriétaires, cette taille n’est pas communiquée.",
                    "trust_range": "Intervalle indiquant la fiabilité du rang de classement : plus l'intervalle est étroit, plus l'estimation du rang est fiable. Il y a 95% de chances que le vrai rang du modèle soit dans cette plage."
                }
            },
            "lastUpdate": "Mise à jour le {date}",
            "totalModels": "Total modèles :",
            "totalVotes": "Total votes :"
        },
        "title": "Des votes… au classement des modèles"
    },
    "reveal": {
        "equivalent": {
            "co2": {
                "label": "CO<sub>2</sub> émis",
                "tooltip": "Le CO<sub>2</sub> émis équivaut aux émissions de dioxyde de carbone produites par l’énergie utilisée pour faire fonctionner le modèle. Elle traduit l'impact environnemental lié à la consommation énergétique. Le calcul d’équivalence Wattheures/CO<sub>2</sub> diffère selon le mix énergétique de chaque pays. Or, les serveurs utilisés pour l’inférence des modèles ne sont pas tous localisés en France. Ainsi, le calcul d’équivalence repose sur la moyenne mondiale du taux d’émissions de CO<sub>2</sub> par énergie consommée."
            },
            "lightbulb": {
                "label": "ampoule LED",
                "tooltip": "Donnée calculée sur la base de consommation d’une ampoule LED standard de 5 W (E14)"
            },
            "streaming": {
                "label": "vidéos en ligne",
                "tooltip": "Donnée calculée selon l’impact carbone d’une heure de vidéo en ligne en haute définition, sur une télévision, en connexion wifi (source <a {linkProps}>ADEME</a>)"
            },
            "title": "Ce qui correspond à :"
        },
        "feedback": {
            "description": "Faites découvrir compar:IA en partageant les modèles d’IA avec lesquels vous avez échangé ! Seuls les noms et l’impact énergétique de la discussion seront visibles via ce lien, sans accès aux messages échangés.",
            "example": "Exemple de partage de résultat",
            "moreOnVotes": "En savoir plus sur les votes",
            "shareResult": "Partager votre résultat"
        },
        "impacts": {
            "energy": {
                "label": "énergie conso.",
                "tooltip": "Mesurée en wattheures, l’énergie consommée représente l'électricité utilisée par le modèle pour traiter une requête et générer la réponse correspondante. Plus un modèle est grand (en milliards de paramètres), plus il faut d'énergie pour produire un token."
            },
            "size": {
                "count": "milliards param.",
                "estimated": "(est.)",
                "label": "taille du modèle",
                "quantized": "(quantisé)"
            },
            "title": "Impact énergétique estimé de la discussion",
            "tokens": {
                "label": "taille du texte",
                "tokens": "jetons",
                "tooltip": "L’IA analyse et génère des phrases à partir de mots ou de parties de mots d’à peu près quatre lettres, cette unité de texte est appelée token (« jeton »). Plus un texte est long, plus le nombre de tokens est grand."
            },
            "tooltip": "Estimation basée sur la taille du modèle, son architecture et la longueur de la réponse générée."
        },
        "thanks": {
            "cta": "Accéder au classement",
            "desc": "Découvrez comment votre préférence se situe par rapport au classement général des modèles dans compar:IA.",
            "graphAlt": "Aperçu du graphique représentant le score et la consommation d'électricité des modèles",
            "rankingAlt": "Aperçu du classement sous forme de tableau",
            "title": "Merci pour votre contribution !"
        }
    },
    "seo": {
        "desc": "compar:IA est un outil permettant de comparer à l’aveugle différents modèles d'IA conversationnelle pour sensibiliser aux enjeux de l'IA générative (biais, impact environmental) et constituer des jeux de données de préférence en français.",
        "title": "compar:IA, le comparateur d'IA conversationnelles",
        "titles": {
            "accessibilite": "Déclaration d’accessibilité",
            "arene": "Discussion",
            "community": "Communauté",
            "comparator": "Le comparateur",
            "datasets": "Jeux de données",
            "donnees-personnelles": "Politique de confidentialité",
            "duel": "Atelier Duel de l’IA",
            "faq": "FAQ",
            "history": "Dates clés",
            "home": "Accueil",
            "mentions-legales": "Mentions légales",
            "modalites": "Modalités d’utilisation",
            "modeles": "Liste des modèles",
            "news": "Actualités et ressources",
            "partners": "Partenaires",
            "problem": "Le problème initial",
            "product": "À propos",
            "ranking": "Classement",
            "rgesn": "Déclaration d’écoconception",
            "share": "Mon bilan"
        }
    },
    "vote": {
        "bothEqual": "Les deux se valent",
        "choices": {
            "altText": "{choice} pour le modèle {model}",
            "negative": {
                "incorrect": "Incorrecte",
                "instructions_not_followed": "Instructions non suivies",
                "question": "Pourquoi la réponse ne convient-elle pas ?",
                "superficial": "Superficielle"
            },
            "other": "Autre…",
            "positive": {
                "clear_formatting": "Mise en forme claire",
                "complete": "Complète",
                "creative": "Créative",
                "question": "Qu'avez-vous apprécié dans la réponse ?",
                "useful": "Utile"
            }
        },
        "comment": {
            "add": "Ajouter des commentaires",
            "placeholder": "Vous pouvez ajouter des précisions sur cette réponse du modèle {model}"
        },
        "dislike": {
            "label": "je n'apprécie pas",
            "selectedLabel": "je n'apprécie pas (sélectionné)"
        },
        "introA": "Avant de découvrir l’identité des modèles, nous avons besoin de votre préférence.",
        "introB": "Elle permet d'enrichir les jeux de données compar:IA dont l’objectif est d’affiner les futurs modèles d’IA sur le français",
        "like": {
            "label": "j'apprécie",
            "selectedLabel": "j'apprécie (sélectionné)"
        },
        "qualify": {
            "addDetails": "Ajouter des détails",
            "placeholder": "Les réponses du modèle {model} sont...",
            "question": "Comment qualifiez-vous ses réponses ?"
        },
        "title": "Quel modèle d’IA préférez-vous ?",
        "yours": "Votre vote"
    },
    "welcome": {
        "errors": "Les IA peuvent faire des erreurs : nous vous encourageons à vérifier les informations communiquées.",
        "go": "C'est parti",
        "privacy": "Ne communiquez pas d’informations personnelles comme votre nom, prénom ou adresse.",
        "title": "Bienvenue sur compar:IA !",
        "tos": {
            "desc": "Les conversations et les préférences que vous exprimez sur compar:IA sont utilisées de manière anonyme pour constituer des jeux de données représentatifs des langues et usages européens, afin de réduire les biais culturels et proposer de futurs modèles d’IA plus inclusifs.",
            "moreInfos": "En savoir plus sur le projet."
        },
        "use": "N’utilisez pas l’arène à des fins illégales ou nuisibles."
    },
    "words": {
        "activated": "Activé",
        "archived": "Archivé",
        "back": "Retour",
        "close": "Fermer",
        "deactivated": "Désactivé",
        "loading": "Chargement",
        "NA": "N/A",
        "new": "Nouveau",
        "random": "Aléatoire",
        "regenerate": "Regénérer",
        "reset": "Réinitialiser",
        "restart": "Recommencer",
        "retry": "Recommencer",
        "search": "Rechercher",
        "send": "Envoyer",
        "tooltip": "Infobulle",
        "validate": "Valider"
    }
}
