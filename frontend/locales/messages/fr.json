{
  "a11y": {
    "externalLink": "{text}"
  },
  "seo": {
    "title": "compar:IA, le comparateur d'IA conversationnelles",
    "desc": "compar:IA est un outil permettant de comparer √† l‚Äôaveugle diff√©rents mod√®les d'IA conversationnelle pour sensibiliser aux enjeux de l'IA g√©n√©rative (biais, impact environmental) et constituer des jeux de donn√©es de pr√©f√©rence en fran√ßais.",
    "titles": {
      "home": "Accueil",
      "product": "Produit et partenaires",
      "ranking": "Classement",
      "modeles": "Liste des mod√®les",
      "datasets": "Jeux de donn√©es",
      "comparator": "Le comparateur",
      "problem": "Le probleÃÄme initial",
      "history": "Historique du projet",
      "faq": "FAQ",
      "partners": "Partenaires",
      "news": "Nouveaut√©s",
      "mentions-legales": "Mentions l√©gales",
      "modalites": "Modalit√©s d‚Äôutilisation",
      "donnees-personnelles": "Politique de confidentialit√©",
      "accessibilite": "D√©claration d‚Äôaccessibilit√©",
      "arene": "Discussion",
      "share": "Mon bilan"
    }
  },
  "header": {
    "title": {
      "compar": "compar",
      "ia": "IA"
    },
    "subtitle": "Le comparateur d‚ÄôIA¬†conversationnelles",
    "homeTitle": "Accueil - compar:IA",
    "logoAlt": "R√©publique Fran√ßaise",
    "startDiscussion": "Commencer √† discuter",
    "help": {
      "link": {
        "title": "Donner mon avis sur le comparateur - ouvre une nouvelle fen√™tre",
        "content": "Nous aider √† am√©liorer compar:IA"
      }
    },
    "banner": "Le comparateur est d√©sormais disponible en lituanien üá±üáπ en su√©dois üá∏üá™ et en danois üá©üá∞ !",
    "votes": {
      "count": "{count} votes",
      "objective": "Obj‚ÄØ: {count}",
      "legend": "L√©gende",
      "tooltip": "Discutez, votez et aidez-nous √† atteindre cet objectif !<br /><strong>Vos votes sont importants</strong> : ils alimentent le jeu de donn√©es compar:IA mis √† disposition librement pour affiner les prochains mod√®les sur le fran√ßais.<br />Ce commun num√©rique contribue au meilleur <strong>respect de la diversit√© linguistique et culturelle des futurs mod√®les de langue.</strong>"
    },
    "chatbot": {
      "step": "√âtape",
      "stepOne": {
        "title": "Que pensez-vous des r√©ponses ?",
        "description": "Pr√™tez attention au fond et √† la forme puis √©valuez chaque r√©ponse"
      },
      "stepTwo": {
        "title": "Les mod√®les sont d√©masqu√©s !",
        "description": "D√©couvrez l‚Äôimpact environnemental de vos discussions avec chaque mod√®le"
      },
      "newDiscussion": "Nouvelle discussion"
    },
    "menu": "Menu"
  },
  "footer": {
    "backHome": "Retour √† l'accueil du site - compar:IA",
    "helpUs": "Aidez-nous √† am√©liorer ce service !",
    "writeUs": "Si vous rencontrez un probl√®me ou si vous avez un commentaire sur le comparateur, n'h√©sitez pas √† nous √©crire <a {linkProps}>via ce formulaire</a>, nous lisons tous vos messages.<br />Merci !",
    "links": {
      "legal": "Mentions l√©gales",
      "tos": "Modalit√©s d'utilisation",
      "privacy": "Politique de confidentialit√©",
      "accessibility": "Accessibilit√© : non conforme",
      "sources": "Code source"
    },
    "license": {
      "mention": "Sauf mention explicite de propri√©t√© intellectuelle d√©tenue par des tiers, les contenus de ce site sont propos√©s sous <a {linkProps}>licence etalab-2.0</a>",
      "linkTitle": "Licence etalab - nouvelle fen√™tre"
    }
  },
  "general": {
    "legal": {
      "title": "Mentions l√©gales",
      "editorTitle": "√âditeur",
      "editorDesc": "Ce site est √©dit√© par le Minist√®re de la culture, 182, rue Saint-Honor√© 75001 Paris",
      "directorTitle": "Directeur de la publication",
      "directorDesc": "Monsieur Romain Delassus, chef du service du num√©rique du Minist√®re de la Culture",
      "hostingTitle": "H√©bergement du site",
      "hostingDesc": "Ce site est h√©berg√© par OVH SAS (<a {linkProps}>https://www.ovh.com</a>) dont le si√®ge social est situ√© au 2 rue Kellermann - 59100 Roubaix - France.",
      "a11yTitle": "Accessibilit√©",
      "a11yDesc": "La conformit√© aux normes d‚Äôaccessibilit√© num√©rique est un objectif ult√©rieur mais nous t√¢chons de rendre ce site accessible √† toutes et √† tous.",
      "reportTitle": "Signaler un dysfonctionnement",
      "reportA11y": "Si vous rencontrez un d√©faut d‚Äôaccessibilit√© vous emp√™chant d‚Äôacc√©der √† un contenu ou une fonctionnalit√© du site, merci de nous en faire part.",
      "reportDesc": "Si vous n‚Äôobtenez pas de r√©ponse rapide de notre part, vous √™tes en droit de faire parvenir vos dol√©ances ou une demande de saisine au D√©fenseur des droits.",
      "reportA11yDesc": "Pour en savoir plus sur la politique d‚Äôaccessibilit√© num√©rique de l‚Äô√âtat : <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
      "securityTitle": "S√©curit√©",
      "securityCertif": "Le site est prot√©g√© par un certificat √©lectronique, mat√©rialis√© pour la grande majorit√© des navigateurs par un cadenas. Cette protection participe √† la confidentialit√© des √©changes.",
      "securityNoMail": "En aucun cas les services associ√©s √† la plateforme ne seront √† l‚Äôorigine d‚Äôenvoi de courriels pour demander la saisie d‚Äôinformations personnelles.",
      "sources": "Sauf mention contraire, tous les textes de ce site sont sous <a {etalabLinkProps}>licence Etalab Open 2.0</a>. Le code source de cette application est librement r√©utilisable et accessible sur <a {githubLinkProps}>GitHub</a>."
    },
    "tos": {
      "title": "Modalit√©s d‚Äôutilisation",
      "scopeTitle": "1. Champ d‚Äôapplication",
      "scopeDesc": "L‚Äôacc√®s √† la plateforme est gratuit, sans inscription et entra√Æne l‚Äôapplication de conditions sp√©cifiques, list√©es dans les pr√©sentes modalit√©s d‚Äôutilisation.",
      "defsTitle": "2. D√©finitions",
      "defsUser": "¬´ Utilisateur ¬ª d√©signe toute personne physique consultant la plateforme et qui b√©n√©ficie de ses services.",
      "defsEditor": "¬´ √âditeur ¬ª d√©signe le Service du num√©rique du Minist√®re de la Culture.",
      "defsPlatform": "¬´ Plateforme ¬ª d√©signe le site web qui rend les services accessibles.",
      "defsModels": "¬´ Mod√®les ¬ª d√©signe les grands mod√®les de langages (LLM) r√©utilis√©s dans le cadre de leur licence d‚Äôutilisation par la plateforme pour r√©pondre √† ses finalit√©s.",
      "defsServices": "¬´ Services ¬ª d√©signe les fonctionnalit√©s offertes par la plateforme pour r√©pondre √† ses finalit√©s.",
      "descTitle": "3. Description de la plateforme",
      "descEditor": "√âdit√© par le Service du num√©rique du Minist√®re de la Culture, le comparateur est une plateforme de comparaison des mod√®les conversationnels adress√©e au grand public dans le but (1) de sensibiliser les citoyens aux grands mod√®les de langage (LLMs), (2) de collecter les pr√©f√©rences des utilisateurs pour constituer des jeux de donn√©es d‚Äôalignement.",
      "descUse": "L‚Äôutilisateur ou l‚Äôutilisatrice pose une question en fran√ßais et obtient des r√©ponses de deux grands mod√®les de langages (LLM) anonymes. Il ou elle vote pour le mod√®le qui fournit la r√©ponse qu‚Äôil pr√©f√®re et se voit alors r√©v√©l√©e l‚Äôidentit√© des mod√®les. Ce dispositif de production participative inspir√© de la plateforme <a {linkProps}>¬´ chatbot arena ¬ª (LMSYS)</a> permet de constituer des jeux de donn√©es de pr√©f√©rences humaines sur des t√¢ches r√©elles, en fran√ßais, utilisables pour l‚Äôalignement des mod√®les.",
      "descDatasets": "Ces jeux de donn√©es seront rendus accessibles sous licence ouverte, notamment pour favoriser des usages de recherche.",
      "featuresTitle": "4. Fonctionnalit√©s",
      "featuresDesc": "Afin de r√©pondre au double objectif de sensibiliser les citoyens aux grands mod√®les de langage et collecter les pr√©f√©rences des utilisateurs et utilisatrices, les services rendus par la plateforme sans restriction d‚Äôacc√®s sont les suivants¬†:",
      "featuresDescMore": "Une interface humain-machine permettant de dialoguer simultan√©ment avec deux mod√®les conversationnels et de voter pour la r√©ponse pr√©f√©r√©e.",
      "featuresModels": "Les mod√®les int√©gr√©s √† la plateforme sont d√©ploy√©s sur les serveurs d‚Äôinf√©rence des diff√©rents partenaires (Scaleway, OVH, Hugging Face, Google Cloud, Mistral Ai). Les conditions de standardisation d‚Äôinf√©rence sont renseign√©es sur la plateforme pour garantir la transparence d‚Äôutilisation des mod√®les.",
      "featuresModelsMore": "Une interface de comparaison des mod√®les.",
      "featuresVote": "A l‚Äôissue du parcours de vote, l‚Äôutilisateur peut consulter la liste des mod√®les int√©gr√©s au comparateur et acc√©der √† une liste d‚Äôinformations sur ces mod√®les. Les informations documentant les mod√®les sont sourc√©es.",
      "featuresVoteMore": "Partage et mise √† disposition des jeux de donn√©es issus de la collecte des pr√©f√©rences des utilisateurs.",
      "featuresDatasets": "Le service recueille les donn√©es de dialogue et de pr√©f√©rence des utilisateurs. Les jeux de donn√©es partag√©s comprendront les questions de l‚Äôutilisateur, les r√©ponses des deux mod√®les, le vote et les pr√©f√©rences de l‚Äôutilisateur.",
      "featuresDatasetsMore": "L‚Äô√©diteur se r√©serve le droit de distribuer sous licence ouverte 2.0 les donn√©es de dialogue et de pr√©f√©rence de l‚Äôutilisateur. Le jeu de donn√©es est diffus√© sur la plateforme Hugging Face √† travers le compte du minist√®re de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "respTitle": "5. Responsabilit√©s",
      "respUser": "L‚Äôutilisateur est responsable des donn√©es ou contenus qu'il ou elle saisit dans l‚Äôinvite offert par la plateforme.",
      "respLegal": "La plateforme n‚Äôa pas vocation √† √™tre utilis√©e pour g√©n√©rer des contenus illicites ou contraires √† l‚Äôordre public et plus g√©n√©ralement toute g√©n√©ration contrevenant au cadre juridique en vigueur.",
      "respLegalMore": "A cet √©gard, l‚Äôutilisateur ne saisit pas dans l‚Äôinvite des contenus ou informations contraires aux dispositions l√©gales et r√©glementaires en vigueur.",
      "respPrivacy": "Les donn√©es saisies par l‚Äôutilisateur sur la plateforme ayant vocation √† √™tre mis √† disposition, il ou elle s‚Äôengage √† ne pas transmettre d‚Äôinformations permettant de l‚Äôidentifier ou d‚Äôidentifier un tiers.",
      "respPrivacyMore": "En tout √©tat de cause, l‚Äô√©diteur s‚Äôengage √† mettre en ≈ìuvre les moyens permettant de s‚Äôassurerde l‚Äôanonymisation les donn√©es de dialogue avant leur mise √† disposition.",
      "respEditor": "De mani√®re g√©n√©rale, l‚Äô√©diteur se d√©gage de toute responsabilit√© en cas d‚Äôutilisationnon-conforme aux modalit√©s d‚Äôutilisation.",
      "licenceTitle": "6. Code et licences",
      "licenceCode": "Le code source de la plateforme est libre et disponible ici : <a {linkProps}>https://github.com/betagouv/languia</a>",
      "licenceLLM": "Les LLM utilis√©s pour alimenter les services sont r√©gis par les licences suivantes :",
      "licenceLLMModel": "Mod√®le d‚ÄôIA conversationnelle",
      "licenceLLMNoticeLink": "Lien vers la notice des mod√®les",
      "licenceLLMLicence": "Licence",
      "licenceLLMUnavailable": "Non disponible",
      "licenceLLMEvolution": "La liste des mod√®les de langage int√©gr√©s √† la plateforme est susceptible d‚Äô√©voluer au cours du temps et est mise √† jour √† chaque modification.",
      "dispoTitle": "7. Disponibilit√© des services",
      "dispoDesc": "La plateforme est accessible, sauf cas de force majeure ou d‚Äô√©v√®nement hors de contr√¥le de son √©diteur.",
      "dispoRight": "L‚Äô√©diteur se r√©serve le droit de suspendre, d&#39;interrompre ou de limiter, sans avis pr√©alable, l&#39;acc√®s √† tout ou partie des services, notamment pour des op√©rations de maintenance et de mises √† jour n√©cessaires au bon fonctionnement du service et des mat√©riels aff√©rents, ou pour toute autre raison, notamment technique.",
      "dispoWarranty": "Il n‚Äôest pas garanti que le service soit exempt d‚Äôanomalies ou erreurs. Le service est donc mis √† disposition sans garantie sur sa disponibilit√© et ses performances.",
      "dispoResp": "A ce titre, l‚Äô√©diteur ne saurait √™tre tenu responsable des pertes ou pr√©judices, de quelque nature qu‚Äôils soient, qui pourraient √™tre caus√©s √† la suite d‚Äôun dysfonctionnement ou une indisponibilit√© du service. De telles situations n&#39;ouvriront droit √† aucune compensation financi√®re.",
      "evoTitle": "8. √âvolution des modalit√©s d'utilisation",
      "evoDesc": "Les modalit√©s d‚Äôutilisation peuvent √™tre modifi√©es ou compl√©t√©es √† tout moment, sans pr√©avis, en fonction des modifications apport√©es aux services, de l‚Äô√©volution de la l√©gislation ou pour tout autre motif jug√© n√©cessaire.",
      "evoDescMore": "Ces modifications et mises √† jour s‚Äôimposent √† l‚Äôutilisateur ou l‚Äôutilisatrice qui doit, en cons√©quence, se r√©f√©rer r√©guli√®rement √† cette rubrique pour v√©rifier les modalit√©s g√©n√©rales en vigueur.",
      "contactTitle": "9. Contact",
      "contactDesc": "Pour toute question sur le service, vous pouvez √©crire √† <a {linkProps}>contact@comparia.beta.gouv.fr</a>."
    },
    "privacy": {
      "title": "Politique de confidentialit√©",
      "desc": "Le service est √©dit√© par le service du num√©rique du minist√®re de la Culture.",
      "cookiesTitle": "Cookies d√©pos√©s et consentement",
      "cookiesDesc": "Ce site d√©pose un petit fichier texte (un ¬´‚ÄØcookie‚ÄØ¬ª) sur votre ordinateur lorsque vous le consultez. Cela nous permet de mesurer le nombre de visites et de comprendre quelles sont les pages les plus consult√©es.",
      "cookiesDescMore": "Vous pouvez vous opposer au suivi de votre navigation sur ce site web. Cela prot√©gera votre vie priv√©e, mais emp√™chera √©galement le propri√©taire d'apprendre de vos actions et de cr√©er une meilleure exp√©rience pour vous et les autres utilisateurs.",
      "cookiesBannerTitle": "Ce site n‚Äôaffiche pas de banni√®re de consentement aux cookies, pourquoi‚ÄØ?",
      "cookiesBannerDesc": "C‚Äôest vrai, vous n‚Äôavez pas eu √† cliquer sur un bloc qui recouvre la moiti√© de la page pour dire que vous √™tes d‚Äôaccord avec le d√©p√¥t de cookies ‚Äî‚ÄØm√™me si vous ne savez pas ce que √ßa veut dire‚ÄØ!",
      "cookiesBannerNoNeed": "Rien d‚Äôexceptionnel, pas de passe-droit li√© √† un‚ÄØ.gouv.fr. Nous respectons simplement la loi, qui dit que certains outils de suivi d‚Äôaudience, correctement configur√©s pour respecter la vie priv√©e, sont exempt√©s d‚Äôautorisation pr√©alable.",
      "cookiesBannerTools": "Nous utilisons pour cela <a {matomoLinkProps}>Matomo</a>, un outil <a {libreLinkProps}>libre</a>, param√©tr√© pour √™tre en conformit√© avec la <a {cnilLinkProps}>recommandation ¬´‚ÄØCookies‚ÄØ¬ª</a> de la CNIL. Cela signifie que votre adresse IP, par exemple, est anonymis√©e avant d‚Äô√™tre enregistr√©e. Il est donc impossible d‚Äôassocier vos visites sur ce site √† votre personne.",
      "dataAccessTitle": "Je contribue √† enrichir vos donn√©es, puis-je y acc√©der‚ÄØ?",
      "dataAccessDesc": "Bien s√ªr‚ÄØ! Les statistiques d‚Äôusage du site sont disponibles en acc√®s libre sur <a {linkProps}>stats.beta.gouv.fr</a>.",
      "dataAccessDatasets": "Les donn√©es de dialogue et de pr√©f√©rence de l‚Äôutilisateur sont distribu√©es sous la Licence Ouverte 2.0 d'Etalab sur la plateforme Hugging Face √† travers le compte du minist√®re de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "privacyTitle": "Traitons-nous des donn√©es √† caract√®re personnel‚ÄØ?",
      "privacyDesc": "Le service ne traite pas de donn√©es √† caract√®re personnel telles que d√©finies par la CNIL, √† savoir toute information relative √† une personne physique susceptible d'√™tre identifi√©e, directement ou indirectement.",
      "privacyData": "Les donn√©es collect√©es sur le site sont les suivantes‚ÄØ:",
      "privacyDataArena": "Donn√©es relatives aux conversations des utilisateurs avec les mod√®les‚ÄØ: questions pos√©es par les utilisateurs, r√©ponses des mod√®les et pr√©f√©rence exprim√©e par l‚Äôutilisateur sur les deux mod√®les",
      "privacyDataForm": "Donn√©es relatives au questionnaire ¬´‚ÄØNous aider √† am√©liorer compar:IA‚ÄØ¬ª.",
      "privacyResp": "L‚Äôutilisateur est responsable des donn√©es ou contenus qu'il ou elle saisit dans l‚Äôinvite offert par la plateforme. En acceptant les <a {linkProps}>modalit√©s d‚Äôutilisation</a>, l‚Äôutilisateur ou l‚Äôutilisatrice s‚Äôengage √† ne pas transmettre d‚Äôinformations permettant de l‚Äôidentifier ou d‚Äôidentifier un tiers.",
      "dataUseTitle": "Quels sont les traitements r√©alis√©s sur les donn√©es de conversation‚ÄØ?",
      "dataUseDesc": "En tout √©tat de cause, l‚Äô√©diteur s‚Äôengage √† mettre en ≈ìuvre les moyens permettant de s‚Äôassurer de l‚Äôanonymisation des donn√©es de dialogue avant leur mise √† disposition publique.",
      "dataTimeTitle": "Pendant combien de temps conservons-nous ces donn√©es‚ÄØ?",
      "dataTimeDesc": "Les donn√©es relatives aux utilisateurs et √† leurs conversations avec les mod√®les sont conserv√©es √† compter de l‚Äôenregistrement du vote de pr√©f√©rence.",
      "dataRespTitle": "Qui est responsable du traitement des donn√©es‚ÄØ?",
      "dataRespDesc": "Le service du num√©rique du minist√®re de la Culture est le responsable du traitement de vos donn√©es √† caract√®re personnel.",
      "dataExtraTitle": "Qui nous aide √† traiter les donn√©es‚ÄØ?",
      "dataExtraHost": "Sous-traitant‚ÄØ: OVH",
      "dataExtraCountry": "Pays destinataire‚ÄØ: France",
      "dataExtraWhat": "Traitement r√©alis√©‚ÄØ: H√©bergement",
      "dataExtraWarranty": "Garanties‚ÄØ: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>"
    },
    "a11y": {
      "disclaimer": "<strong>compar:IA</strong> s‚Äôengage √† rendre ses services num√©riques accessibles, conform√©ment √† l‚Äôarticle 47 de la loi n¬∞ 2005-102 du 11 f√©vrier 2005.",
      "title": "D√©claration d‚Äôaccessibilit√©",
      "desc": "Cette d√©claration d‚Äôaccessibilit√© s‚Äôapplique au site <strong>comparia.beta.gouv.fr</strong>.",
      "stateTitle": "√âtat de conformit√©",
      "stateDesc": "Le site comparia.beta.gouv.fr est non conforme avec le RGAA 4.1. Le site n‚Äôa <strong>pas encore √©t√© audit√©</strong>. Il a cependant √©t√© con√ßu pour √™tre accessible au plus grand nombre. Vous devriez donc pouvoir‚ÄØ:",
      "stateNavigate": "naviguer sur toutes les pages du site en utilisant un clavier",
      "stateScreenReader": "consulter le site web avec un lecteur d‚Äô√©cran.",
      "statePrefs": "adapter le site √† votre pr√©f√©rences (taille de la police, zoom √©cran, changement de typographie‚Ä¶) sans perte de contenu",
      "improveTitle": "Am√©lioration et contact",
      "improveDesc": "Si vous n‚Äôarrivez pas √† acc√©der √† un contenu ou √† un service, vous pouvez contacter le responsable de beta.gouv.fr pour √™tre orient√©¬∑e vers une alternative accessible ou obtenir le contenu sous une autre forme.",
      "improveMail": "E-mail‚ÄØ: <a {linkProps}>contact@beta.gouv.fr</a>",
      "improveAdress": "Adresse‚ÄØ: DINUM, 20 avenue de S√©gur 75007 Paris",
      "improveDelay": "Nous essayons de r√©pondre dans les 2 jours ouvr√©s.",
      "remedyTitle": "Voie de recours",
      "remedyDesc": "Cette proc√©dure est √† utiliser dans le cas suivant‚ÄØ: vous avez signal√© au responsable du site internet un d√©faut d‚Äôaccessibilit√© qui vous emp√™che d‚Äôacc√©der √† un contenu ou √† un des services du portail et vous n‚Äôavez pas obtenu de r√©ponse satisfaisante.",
      "remedyList": "Vous pouvez‚ÄØ:",
      "remedyAdvocate": "√âcrire un message au <a {linkProps}>D√©fenseur des droits</a>",
      "remedyDelegateAdvocate": "Contacter le d√©l√©gu√© du <a {linkProps}>D√©fenseur des droits dans votre r√©gion</a>",
      "remedyAdvocateAdress": "Envoyer un courrier par la poste (gratuit, ne pas mettre de timbre)‚ÄØ: D√©fenseur des droits - Libre r√©ponse 71120 75342 Paris CEDEX 07"
    }
  },
  "welcome": {
    "title": "Bienvenue dans compar:IA !",
    "goodPractices": "Voici quelques bonnes pratiques‚ÄØ:",
    "errors": "Les IA peuvent faire des erreurs‚ÄØ: nous vous encourageons √† v√©rifier les informations communiqu√©es",
    "privacy": "Ne communiquez pas d'informations personnelles comme votre nom, pr√©nom ou adresse",
    "use": "N'utilisez pas le comparateur √† des fins ill√©gales ou nuisibles",
    "go": "C'est parti"
  },
  "home": {
    "intro": {
      "title": "Ne vous fiez pas aux r√©ponses <span {props}>d‚Äôune seule IA</span>",
      "desc": "Discutez avec deux IA √† l‚Äôaveugle et √©valuez leurs r√©ponses",
      "tos": {
        "accept": "J'accepte les <a {linkProps}>conditions g√©n√©rales d‚Äôutilisation</a>",
        "help": "Les donn√©es sont partag√©es √† des fins de recherche",
        "error": "Vous devez accepter les modalit√©s d'utilisation pour continuer"
      },
      "steps": {
        "title": "Comment √ßa marche",
        "a11yDesc": "1. Je discute avec deux IA anonymes : √©changez aussi longtemps que vous le souhaitez. 2. Je donne mon avis : vous contribuez ainsi √† l'am√©lioration des mod√®les d‚ÄôIA. 3. Les mod√®les sont d√©masqu√©s : apprenez-en plus sur les mod√®les d‚ÄôIA et leurs caract√©ristiques.",
        "one": {
          "title": "Je discute avec deux IA anonymes",
          "a": "√âchangez aussi longtemps que",
          "b": "vous le souhaitez"
        },
        "two": {
          "title": "Je donne mon avis",
          "a": "Vous contribuez ainsi √†",
          "b": "l'am√©lioration des mod√®les d‚ÄôIA"
        },
        "three": {
          "title": "Les mod√®les sont d√©masqu√©s‚ÄØ!",
          "a": "Apprenez en plus sur les mod√®les",
          "b": "d‚ÄôIA et leurs caract√©ristiques"
        }
      }
    },
    "use": {
      "title": "√Ä quoi sert compar:IA‚ÄØ?",
      "desc": "compar:IA est un outil gratuit qui permet de sensibiliser les citoyens √† l‚ÄôIA g√©n√©rative et √† ses enjeux",
      "compare": {
        "title": "Comparer les r√©ponses de diff√©rents mod√®les d‚ÄôIA",
        "desc": "Discutez et d√©veloppez votre esprit critique en donnant votre pr√©f√©rence",
        "alt": "Comparer"
      },
      "test": {
        "title": "Tester au m√™me endroit les derni√®res IA de l‚Äô√©cosyst√®me",
        "desc": "Testez diff√©rents mod√®les, propri√©taires ou non, de petites et grandes tailles",
        "alt": "Tester"
      },
      "measure": {
        "title": "Mesurer l‚Äôempreinte √©cologique des questions pos√©es aux IA",
        "desc": "D√©couvrez l‚Äôimpact environnemental de vos discussions avec chaque mod√®le",
        "alt": "Mesurer"
      }
    },
    "europe": {
      "title": "Le comparateur <span {props}>devient europ√©en‚ÄØ!</span>",
      "desc": "La Lituanie, la Su√®de et le Danemark rejoignent la France en adoptant le comparateur dans le but d‚Äôaffiner les futurs mod√®les d‚ÄôIA dans leurs langues nationales.",
      "question": "Vous souhaitez √©galement disposer du comparateur dans votre langue‚ÄØ?",
      "languages": {
        "da": "en danois",
        "fr": "en fran√ßais",
        "lt": "en lituanien",
        "sv": "en su√©dois"
      }
    },
    "vote": {
      "title": "Pourquoi votre vote est-il important‚ÄØ?",
      "desc": "Votre pr√©f√©rence enrichit les jeux de donn√©es compar:IA dont l‚Äôobjectif est d‚Äôaffiner les futurs mod√®les d‚ÄôIA sur le fran√ßais, le su√©dois, le lituanien et le danois",
      "steps": {
        "prefs": {
          "title": "Vos pr√©f√©rences",
          "desc": "Apr√®s discussion avec les IA, vous indiquez votre pr√©f√©rence pour un mod√®le selon des crit√®res donn√©s, tels que la pertinence ou l‚Äôutilit√© des r√©ponses."
        },
        "datasets": {
          "title": "Les jeux de donn√©es par langue",
          "desc": "Toutes les questions pos√©es et les votes sont compil√©es dans des jeux de donn√©es et publi√©s librement apr√®s anonmymisation."
        },
        "finetune": {
          "title": "Des mod√®les affin√©s sur la langue sp√©cifique",
          "desc": "A terme, les acteurs industriels et acad√©miques peuvent exploiter les jeux de donn√©es pour entrainer de nouveaux mod√®les plus respectueux de la diversit√© linguistique et culturelle."
        }
      },
      "datasetAccess": "Acc√©der aux jeux de donn√©es"
    },
    "usage": {
      "title": "Les usages sp√©cifiques de compar:IA",
      "desc": "L‚Äôoutil s‚Äôadresse √©galement aux experts IA et aux formateurs pour des usages plus sp√©cifiques",
      "use": {
        "title": "Exploiter les donn√©es",
        "desc": "D√©veloppeurs, chercheurs, √©diteurs de mod√®les‚Ä¶ acc√©dez aux jeux de donn√©es compar:IA pour am√©liorer les mod√®les"
      },
      "explore": {
        "title": "Explorer les mod√®les",
        "desc": "Consultez au m√™me endroit toutes les caract√©ristiques et conditions d‚Äôutilisation des mod√®les"
      },
      "educate": {
        "title": "Former et sensibiliser",
        "desc": "Utilisez le comparateur comme un support p√©dagogique de sensibilisation √† l‚ÄôIA aupr√®s de votre public"
      }
    },
    "origin": {
      "team": {
        "title": "Qui sommes-nous‚ÄØ?",
        "desc": "Le comparateur est port√© au sein du Minist√®re de la Culture par une √©quipe pluridisciplinaire r√©unissant expert en Intelligence artificielle, d√©veloppeurs, charg√© de d√©ploiement, designer, avec pour mission de rendre les IA conversationnelles plus transparentes et accessibles √† toutes et tous."
      },
      "project": {
        "title": "Qui est √† l‚Äôorigine du projet‚ÄØ?",
        "desc": "Le comparateur a √©t√© con√ßu et d√©velopp√© dans le cadre d‚Äôune start-up d‚ÄôEtat port√©e par le minist√®re de la Culture et int√©gr√©e au programme <a {linkProps}>Beta.gouv.fr</a> de la Direction interminist√©rielle du num√©rique (DINUM) qui aide les administrations publiques fran√ßaises √† construire des services num√©riques utiles, simples et faciles √† utiliser."
      }
    },
    "faq": {
      "title": "Vos questions les plus courantes",
      "discover": "D√©couvrir les autres questions"
    }
  },
  "ranking": {
    "title": "Classement des mod√®les",
    "desc": "D√©couvrez comment les meilleurs mod√®les d‚ÄôIA se positionnent √† travers leur <strong>score de satisfaction</strong> issus des votes de la communaut√© compar:IA. Pour en savoir plus, consultez <a {linkProps}>notre m√©thodologie de classement</a>.",
    "table": {
      "search": "Rechercher un mod√®le",
      "lastUpdate": "Mise √† jour le {date}",
      "totalModels": "Total mod√®les‚ÄØ:",
      "totalVotes": "Total votes‚ÄØ:",
      "data": {
        "cols": {
          "rank": "Rang",
          "name": "Mod√®le",
          "elo": "Score satisfaction",
          "trust_range": "Confiance (¬±)",
          "total_votes": "Total votes",
          "consumption_wh": "√ânergie<br>(1000 tokens)",
          "size": "Taille<br>(param√®tres actifs)",
          "release": "Date sortie",
          "organisation": "Organisation",
          "license": "Licence"
        }
      }
    }
  },
  "product": {
    "title": "Tout savoir sur le comparateur",
    "comparator": {
      "title": "Le comparateur permet de cr√©er des <span {props}>jeux de donn√©es</span> de pr√©f√©rence centr√©s sur des <span {props}>usages r√©els</span> exprim√©s dans les <span {props}>langues europ√©ennes</span>.",
      "cta": "Acc√©der au comparateur",
      "challenges": {
        "title": "L‚Äôapplication d√©velopp√©e r√©pond √† plusieurs enjeux",
        "bias": {
          "title": "Biais culturels et linguistiques",
          "desc": "Mettre en avant les biais de l'IA li√©s √† la sous-repr√©sentation des donn√©es non anglophones dans les mod√®les et sensibiliser √† leurs cons√©quences."
        },
        "impacts": {
          "title": "Impact environnemental",
          "desc": "R√©v√©ler les effets √©cologiques de l'IA g√©n√©rative, encore largement m√©connus du grand public."
        },
        "pluralism": {
          "title": "Pluralisme des mod√®les",
          "desc": "Assurer aux citoyens l'acc√®s √† une diversit√© de mod√®les d'IA afin qu'ils puissent faire des choix √©clair√©s et d√©velopper un regard critique sur ces technologies."
        },
        "thinking": {
          "title": "Esprit critique et questions soci√©tales",
          "desc": "Inciter au questionnement critique sur la place de l‚ÄôIA g√©n√©rative dans les pratiques personnelles et professionnelles (√©ducation, travail)."
        }
      },
      "europe": {
        "title": "Le comparateur <span {props}>devient europ√©en</span>‚ÄØ!",
        "adventure": "Depuis l‚Äô√©t√© 2025, la Lituanie, la Su√®de et le Danemark rejoignent l‚Äôaventure‚ÄØ!",
        "desc": "Le comparateur est mis √† disposition de leurs citoyens dans leurs langues nationales. L‚Äôobjectif est de cr√©er des jeux de donn√©es de pr√©f√©rence afin d‚Äôam√©liorer les futurs mod√®les d‚ÄôIA dans ces langues europ√©ennes.",
        "catch": "Vous souhaitez disposer du comparateur dans votre langue‚ÄØ?"
      }
    },
    "problem": {
      "title": "Les mod√®les d‚ÄôIA conversationnelles respectent-ils la <span {props}>diversit√©</span> des langues europ√©ennes‚ÄØ?",
      "diversity": {
        "stereotypes": {
          "title": "R√©ponses st√©r√©otyp√©es",
          "desc": "Les syst√®mes d‚ÄôIA conversationnelle donnent l‚Äôimpression de parler toutes les langues mais les r√©sultats qu‚Äôils g√©n√®rent sont parfois st√©r√©otyp√©s ou discriminants."
        },
        "english": {
          "title": "Donn√©es d‚Äôentrainement majoritairement en anglais",
          "desc": "Les IA conversationnelles reposent sur des grands mod√®les de langage (LLM) entra√Æn√©s principalement sur des donn√©es en anglais, ce qui cr√©e des biais linguistiques et culturels dans les r√©sultats qu'ils produisent."
        },
        "diversity": {
          "title": "Diversit√©s culturelles et linguistiques n√©glig√©es",
          "desc": "Ces biais peuvent aussi se traduire par des r√©ponses partielles voire incorrectes n√©gligeant la diversit√© des langues et des cultures, notamment europ√©ennes."
        }
      },
      "alignment": {
        "title": "Comment r√©duire les biais culturels et linguistiques de ces mod√®les‚ÄØ?",
        "desc": "L'alignement‚ÄØ: une technique de r√©duction des biais qui repose sur la collecte des pr√©f√©rences d‚Äôutilisateurs",
        "alignment": {
          "title": "L‚Äôalignement, √©tape d√©cisive d‚Äôinstruction du mod√®le",
          "a": "L'alignement intervient apr√®s l'√©tape de pr√©-entra√Ænement d'un mod√®le de langage, comme une √©tape de ¬´¬†finition‚ÄØ¬ª ou de ¬´¬†polissage‚ÄØ¬ª. Lors de son pr√©-entrainement, le mod√®le apprend √† pr√©dire le mot suivant et devient capable de g√©n√©rer du texte coh√©rent.",
          "b": "L‚Äô√©tape d‚Äôalignement consiste √† apprendre au mod√®le √† mieux r√©pondre aux besoins humains, c‚Äôest √† dire √† le rendre plus <strong>pertinent</strong> (le mod√®le r√©pond ¬´‚ÄØmieux‚ÄØ¬ª aux questions), <strong>honn√™te</strong> (capacit√© √† assumer ¬´¬†qu‚Äôil ne sait pas r√©pondre‚ÄØ¬ª quand il n‚Äôy a pas suffisamment de donn√©es), et <strong>inoffensif</strong> (√©viter de g√©n√©rer des contenus dangereux ou inappropri√©s).",
          "c": "<strong>Sans alignement, un LLM pourrait √™tre techniquement comp√©tent mais difficile √† utiliser en pratique, car il ne comprendrait pas vraiment ce qu'on attend de lui dans une conversation.</strong>"
        },
        "datasets": {
          "title": "Des jeux de donn√©es sp√©cifiques",
          "a": "L'alignement utilise des donn√©es tr√®s sp√©cifiques, sp√©cialement cr√©√©es pour enseigner au mod√®le comment ¬´‚ÄØbien‚ÄØ¬ª se comporter.",
          "b": "Les <strong>donn√©es de pr√©f√©rence</strong> constituent un type particulier de donn√©es d‚Äôalignement, aux c√¥t√©s des <strong>donn√©es de d√©monstration</strong> (exemples de conversations entre humains et assistants IA, r√©dig√©es par des annotateurs experts selon des consignes pr√©cises de ton et de style), des <strong>donn√©es de s√©curit√©</strong> (exemples sp√©cifiques enseignant au mod√®le √† √©viter les contenus dangereux en montrant comment refuser les demandes probl√©matiques) ou des <strong>donn√©es sp√©cialis√©es</strong> couvrant des domaines sp√©cifiques (m√©decine, droit, √©ducation‚Ä¶).",
          "c": "Les donn√©es de pr√©f√©rence pr√©sentent plusieurs r√©ponses possibles √† une m√™me question, class√©es par ordre de qualit√© par des √©valuateurs humains: les utilisateurs indiquent quelle r√©ponse est la meilleure selon des crit√®res donn√©s, telles que la pertinence, l‚Äôutilit√©, la nocivit√©. Une fois constitu√©s, ces jeux de donn√©es sont utilis√©s pour entrainer les mod√®les en les ajustant selon les pr√©f√©rences exprim√©es par les utilisateurs."
        },
        "english": {
          "title": "Peu de donn√©es de pr√©f√©rence en langues europ√©ennes",
          "a": "Les donn√©es de pr√©f√©rence sont couteuses √† produire car <strong>elles n√©cessitent du travail humain qualifi√© pour chaque exemple</strong>. Des plateformes telles que https://chat.lmsys.org/ permettent de constituer ces jeux de donn√©es de pr√©f√©rence mais peu d‚Äôutilisateurs s‚Äôen servent dans leur langue d‚Äôorigine.",
          "b": "Les jeux de donn√©es de pr√©f√©rence sont rares, voire inexistants dans les langues europ√©ennes. La part des questions pos√©es en fran√ßais dans le jeu de donn√©es de LMSYS est par exemple inf√©rieure √† 1%.",
          "c": "comparIA est un exemple de dispositif permettant de collecter des conversations dans de multiples langues, incluant des r√©f√©rences culturelles sp√©cifiques √† chaque r√©gion ou pays‚ÄØ: t√¢ches courantes, traditions culinaires locales, syst√®mes √©ducatifs, r√©f√©rences historiques ou litt√©raires, etc."
        },
        "diversity": {
          "title": "Diversifier les donn√©es pour r√©duire les biais",
          "a": "Pour refl√©ter la diversit√© des cultures et des langues dans les r√©sultats g√©n√©r√©s par les mod√®les, <strong>les jeux de donn√©es d‚Äôalignement doivent inclure une vari√©t√© de langues</strong>, de contextes et d‚Äôexemples issus de t√¢ches courantes des utilisateurs. La diversification des donn√©es d'alignement permet d‚Äôam√©iorer √† terme les performances d‚Äôun mod√®le √† double titre‚ÄØ:",
          "b": "D'une part, elle <strong>r√©duit les biais culturels</strong> en √©vitant qu'une seule perspective - souvent anglo-saxonne - domine les r√©ponses de l'IA. Le mod√®le apprend ainsi √† reconna√Ætre qu'il existe plusieurs fa√ßons valides d'aborder une m√™me question selon le contexte culturel.",
          "c": "D'autre part, cette exposition √† la diversit√© de langues et de cultures favorise l‚Äôadaptation des r√©ponses √† des contextes sp√©cifiques: un utilisateur fran√ßais recevra des conseils adapt√©s au syst√®me fran√ßais, tandis qu'un utilisateur danois obtiendra des informations correspondant √† son contexte national.",
          "d": "Le r√©sultat est un mod√®le d‚ÄôIA conversationnelle plus inclusif, capable de tenir compte des diff√©rentes cultures."
        }
      }
    },
    "partners": {
      "institution": {
        "title": "Partenaires institutionnels"
      },
      "diffusion": {
        "title": "Partenaires de diffusion",
        "desc": "Nous cr√©ons un r√©seau de partenaires int√©grant le comparateur dans leur offre de services et de formation.",
        "catch": "Vous souhaitez utiliser le comparateur pour r√©pondre √† un besoin m√©tier‚ÄØ?",
        "cta": "Dites nous en plus"
      },
      "academy": {
        "title": "Partenaires acad√©miques",
        "desc": "Nous avons √† coeur que les jeux de donn√©es g√©n√©r√©s alimentent des travaux de recherche multidisciplinaires m√™lant sciences humaines et sociales et data science.",
        "catch": "Vous menez un projet de recherche et avez des suggestions ou besoin de pr√©cision sur la d√©marche et/ou les jeux de donn√©es produits‚ÄØ?"
      },
      "services": {
        "title": "Services mis √† contribution",
        "desc": "Les calculs d‚Äôimpacts environnementaux reposent sur les produits ci dessus."
      }
    }
  },
  "datasets": {
    "access": {
      "title": "Acc√©dez aux jeux de donn√©es compar:IA",
      "desc": "Les questions et pr√©f√©rences pos√©es sur la plateforme sont majoritairement en fran√ßais et refl√®tent des usages r√©els et non contraints. Ces jeux de donn√©es sont accessibles sur <a {linkProps}>data.gouv</a> et Hugging Face.",
      "catch": "Editeurs de mod√®les, chercheurs, chercheuses, entreprises, √† vous de jouer !",
      "share": "Partagez-nous vos r√©utilisations",
      "repos": {
        "conversations": {
          "title": "/conversations",
          "desc": "Ensemble des r√©ponses et des questions pos√©es"
        },
        "reactions": {
          "title": "/r√©actions",
          "desc": "Ensemble des r√©actions exprim√©es"
        },
        "votes": {
          "title": "/votes",
          "desc": "Ensemble des pr√©f√©rences exprim√©es"
        }
      }
    },
    "reuse": {
      "title": "Comment ces donn√©es sont-elles utilis√©es ?",
      "desc": "Exemples de r√©utilisation des jeux de donn√©es compar:IA",
      "bunka": {
        "desc": "L'√©quipe Bunka.ai a men√© une √©tude approfondie sur les interactions entre les utilisateurs de la plateforme Compar:IA et les mod√®les d'IA, examinant les th√©matiques privil√©gi√©es, les t√¢ches principales et d√©terminant si ces mod√®les fonctionnent avant tout comme des outils d'automatisation ou d'augmentation des capacit√©s humaines. Cette analyse repose sur un large √©chantillon de 25 000 conversations.",
        "conversations": {
          "title": "Explorer la visualisation de donn√©es",
          "desc": "Visualisation interactive des conversations o√π chaque point repr√©sente un cluster de discussions √©voqu√© par les utilisateurs (comme l‚Äô√©ducation, la sant√©, l‚Äôenvironnement, ou encore la philosophie)."
        },
        "analyze": {
          "title": "Acc√©der √† l‚Äôanalyse par indicateur",
          "desc": "Analyse des conversations des utilisateurs avec d√©tection des t√¢ches (cr√©ation, recherche d'informations...), des sujets (arts et culture, √©ducation...), des √©motions complexes (curiosit√©, enthousiasme...), des types de langage (formel, professionnel...)"
        },
        "method": "En savoir plus sur la m√©thodologie"
      }
    }
  },
  "arenaHome": {
    "title": "Comment puis-je vous aider aujourd'hui ?",
    "modelSelection": "S√©lection des mod√®les",
    "prompt": {
      "label": "√âcrivez votre premier message",
      "placeholder": "√âcrivez votre premier message ici"
    },
    "selectModels": {
      "question": "Quels mod√®les voulez-vous comparer ?",
      "help": "S√©lectionnez le mode de comparaison qui vous convient"
    },
    "compareModels": {
      "question": "Quels mod√®les voulez-vous comparer ?",
      "count": "{count}/2 mod√®les",
      "help": "Si vous n‚Äôen choisissez qu‚Äôun, le second sera s√©lectionn√© de mani√®re al√©atoire"
    },
    "suggestions": {
      "title": "Suggestions de prompts",
      "generateAnother": "G√©n√©rer un autre message",
      "choices": {
        "iasummit": {
          "iconAlt": "Sommet pour l'action sur l'IA",
          "title": "Prompts issus de la consultation citoyenne sur l‚ÄôIA¬†",
          "tooltip": "Ces questions sont issues de la consultation citoyenne sur l‚ÄôIA qui a lieu du 16/09/2024 au 08/11/2024. Elle visait √† associer largement les citoyens et la soci√©t√© civile au Sommet international pour l‚Äôaction sur l‚ÄôIA, en collectant leurs id√©es pour faire de l‚Äôintelligence artificielle une opportunit√© pour toutes et tous, mais aussi de nous pr√©munir ensemble contre tout usage inappropri√© ou abusif de ces technologies."
        },
        "ideas": {
          "iconAlt": "Id√©es",
          "title": "G√©n√©rer de nouvelles id√©es"
        },
        "explanations": {
          "iconAlt": "Explications",
          "title": "Expliquer simplement un concept"
        },
        "languages": {
          "iconAlt": "Traduction",
          "title": "M‚Äôexprimer dans une autre langue"
        },
        "administrative": {
          "iconAlt": "Administratif",
          "title": "R√©diger un document administratif"
        },
        "recipes": {
          "iconAlt": "Recettes",
          "title": "D√©couvrir une nouvelle recette de cuisine"
        },
        "coach": {
          "iconAlt": "Conseils",
          "title": "Obtenir des conseils sur l‚Äôalimentation et le sport"
        },
        "stories": {
          "iconAlt": "Histoires",
          "title": "Raconter une histoire"
        },
        "recommendations": {
          "iconAlt": "Recommandations",
          "title": "Proposer des id√©es de films, livres, musiques"
        }
      }
    }
  },
  "chatbot": {
    "continuePrompt": "Continuer √† discuter avec les deux mod√®les d'IA",
    "revealButton": "Passer √† la r√©v√©lation des mod√®les",
    "conversation": "Conversation",
    "errors": {
      "tooLong": {
        "title": "Oups, la conversation est trop longue pour un des mod√®les.",
        "message": "Chaque mod√®le est limit√© dans la taille des conversations qu'il est capable de traiter.",
        "vote": "Vous pouvez tout de m√™me donner votre avis sur ces mod√®les ou recommencer une conversation avec deux nouveaux.",
        "retry": "Vous pouvez recommencer une conversation avec deux nouveaux mod√®les."
      },
      "other": {
        "title": "Oups, erreur temporaire",
        "message": "Une erreur temporaire est survenue.",
        "vote": "Ou bien conclure votre exp√©rience en donnant votre avis sur les mod√®les.",
        "retry": "Vous pouvez tenter de r√©essayer de solliciter les mod√®les."
      }
    },
    "loading": "Chargement des r√©ponses"
  },
  "closeModal": "Fermer la fen√™tre modale",
  "models": {
    "conditions": "Conditions d'utilisation",
    "licenses": {
      "type": {
        "proprietary": "Propri√©taire",
        "openSource": "Open source",
        "semiOpen": "Semi-ouvert"
      },
      "name": "Licence {licence}",
      "commercial": "Licence commerciale",
      "noDesc": "Les informations de licence n'ont pas √©t√© remplies pour ce mod√®le.",
      "descriptions": {
        "MIT": "La licence MIT est une licence de logiciel libre permissive : elle permet √† quiconque de r√©utiliser, modifier et distribuer le mod√®le, m√™me √† des fins commerciales, sous r√©serve d'inclure la licence d'origine et les mentions de droits d'auteur.",
        "Apache 2.0": "Cette licence permet d'utiliser, modifier et distribuer librement, m√™me √† des fins commerciales. Outre la libert√© d‚Äôutilisation, elle garantit la protection juridique en incluant une clause de non-atteinte aux brevets et la transparence : toutes les modifications doivent √™tre document√©es et sont donc tra√ßables.",
        "Gemma": "Cette licence est con√ßue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifi√©es ou am√©lior√©es doivent √™tre partag√©e avec la communaut√© sous la m√™me licence, favorisant ainsi la collaboration et la transparence dans le d√©veloppement logiciel.",
        "Llama 3 Community": "Cette licence permet d'utiliser, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les op√©rations d√©passant 700 millions d'utilisateurs mensuels et interdit la r√©utilisation du code ou des contenus g√©n√©r√©s pour l‚Äôentra√Ænement ou l'am√©lioration de mod√®les concurrents, prot√©geant ainsi les investissements technologiques et la marque de Meta.",
        "Llama 3.1": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les op√©rations d√©passant 700 millions d'utilisateurs mensuels. La r√©utilisation du code ou des contenus g√©n√©r√©s pour l‚Äôentra√Ænement ou l'am√©lioration de mod√®les d√©riv√©s est autoris√©e √† condition d‚Äôafficher ‚Äúbuilt with llama‚Äù et d‚Äôinclure ‚ÄúLlama‚Äù dans leur nom pour toute distribution.",
        "Llama 3.3": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les op√©rations d√©passant 700 millions d'utilisateurs mensuels. La r√©utilisation du code ou des contenus g√©n√©r√©s pour l‚Äôentra√Ænement ou l'am√©lioration de mod√®les d√©riv√©s est autoris√©e √† condition d‚Äôafficher ‚Äúbuilt with llama‚Äù et d‚Äôinclure ‚ÄúLlama‚Äù dans leur nom pour toute distribution.",
        "Llama 4": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les op√©rations d√©passant 700 millions d'utilisateurs mensuels. La r√©utilisation du code ou des contenus g√©n√©r√©s pour l‚Äôentra√Ænement ou l'am√©lioration de mod√®les d√©riv√©s est autoris√©e √† condition d‚Äôafficher ‚Äúbuilt with llama‚Äù et d‚Äôinclure ‚ÄúLlama‚Äù dans leur nom pour toute distribution.",
        "Jamba Open Model": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les organismes d√©passant 50 millions de dollars de revenus annuels.",
        "CC-BY-NC-4.0": "Cette licence permet de partager et adapter le contenu √† condition de cr√©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilit√© pour les usages non commerciaux tout en prot√©geant les droits de l'auteur.",
        "propri√©taire Gemini": "Le mod√®le est disponible sous licence payante et accessible via l'API Gemini disponible sur les plateformes Google AI Studio et Vertex AI, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s ou selon les termes de l'entreprise.",
        "propri√©taire Mistral": "Le mod√®le est disponible sous licence payante et accessible via l'API Mistral, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s.",
        "propri√©taire xAI": "Le mod√®le est accessible via API xAI, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s ou selon les termes de l'entreprise.",
        "propri√©taire Liquid": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© Liquid AI, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s.",
        "propri√©taire OpenAI": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© OpenAI, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s ou selon les termes de l'entreprise.",
        "propri√©taire Anthropic": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© Anthropic, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de tokens trait√©s ou selon les termes de l'entreprise.",
        "Mistral AI Non-Production": "Cette licence permet de partager et adapter le contenu √† condition de cr√©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilit√© pour les usages non commerciaux tout en prot√©geant les droits de l'auteur."
      }
    },
    "release": "Sortie¬†{date}",
    "size": {
      "title": "Taille",
      "estimated": "Taille¬†estim√©e‚ÄØ({size})",
      "descriptions": {
        "XS": "Les mod√®les tr√®s petits, avec moins de 7 milliards de param√®tres, sont les moins complexes et les plus √©conomiques en termes de ressources, offrant des performances suffisantes pour des t√¢ches simples comme la classification de texte.",
        "S": "Un mod√®le de petit gabarit est moins complexe et co√ªteux en ressources par rapport aux mod√®les plus grands, tout en offrant une performance suffisante pour diverses t√¢ches (r√©sum√©, traduction, classification de texte...)",
        "M": "Les mod√®les moyens offrent un bon √©quilibre entre complexit√©, co√ªt et performance : ils sont beaucoup moins consommateurs de ressources que les grands mod√®les tout en √©tant capables de g√©rer des t√¢ches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "L": "Les grands mod√®les n√©cessitent des ressources significatives, mais offrent les meilleures performances pour des t√¢ches avanc√©es comme la r√©daction cr√©ative, la mod√©lisation de dialogues et les applications n√©cessitant une compr√©hension fine du contexte.",
        "XL": "Ces mod√®les dot√©s de plusieurs centaines de milliards de param√®tres sont les plus complexes et avanc√©s en termes de performance et de pr√©cision. Les ressources de calcul et de m√©moire n√©cessaires pour d√©ployer ces mod√®les sont telles qu‚Äôils sont destin√©s aux applications les plus avanc√©es et aux environnements hautement sp√©cialis√©s."
      }
    },
    "openWeight": {
      "conditions": {
        "free": "Permissive",
        "copyleft": "Copyleft",
        "restricted": "Sous conditions"
      },
      "tooltips": {
        "openSource": "Le corpus, le code d'entra√Ænement, et les poids de ce mod√®le (c‚Äôest-√†-dire les param√®tres appris pendant son entra√Ænement) sont enti√®rement t√©l√©chargeables et modifiables par le public, lui permettant de faire fonctionner et modifier le mod√®le sur son propre mat√©riel. Qu'un mod√®le soit ¬´‚ÄØopen source‚ÄØ¬ª est plus contraignant qu'¬´‚ÄØopen weights‚ÄØ¬ª, notamment √† cause de la n√©cessit√© de transparence du corpus d'entra√Ænement, et rares sont les mod√®les qui sont consid√©r√©s ¬´‚ÄØopen source‚ÄØ¬ª.",
        "openWeight": "Mod√®le dit ¬´‚ÄØopen weights‚ÄØ¬ª dont les poids, c‚Äôest-√†-dire les param√®tres appris pendant son entra√Ænement, sont t√©l√©chargeables par le public, lui permettant de faire fonctionner le mod√®le sur son propre mat√©riel. Qu'un mod√®le soit ¬´‚ÄØopen source‚ÄØ¬ª est plus contraignant (principalement par rapport √† la transparence du corpus d'entra√Ænement), et rares sont les mod√®les qui sont consid√©r√©s ¬´‚ÄØopen source‚ÄØ¬ª.",
        "params": "Les param√®tres ou les poids, compt√©s en milliards, sont les variables, apprises par un mod√®le au cours de son entrainement, qui d√©terminent ses r√©ponses. Plus le nombre de param√®tres est important, plus il est capable d‚Äôeffectuer des t√¢ches complexes.",
        "free": "Une fois modifi√©, le mod√®le peut √™tre redistribu√© sous une licence diff√©rente du mod√®le source.",
        "copyleft": "Une fois modifi√©, le mod√®le doit √™tre redistribu√© sous la m√™me licence que celle du mod√®le source.",
        "ram": "La RAM (m√©moire vive) stocke les donn√©es trait√©es par un LLM en temps r√©el. Plus le mod√®le est grand, plus il a besoin de RAM pour fonctionner."
      },
      "descriptions": {
        "XS": "Dot√© de {paramsCount} milliards de param√®tres, ce mod√®le fait partie de la classe des mod√®les tr√®s petits (moins de 7 milliards de param√®tres).",
        "S": "Dot√© de {paramsCount} milliards de param√®tres, ce mod√®le fait partie de la classe des petits mod√®les (entre 7 et 20 milliards de param√®tres).",
        "M": "Dot√© de {paramsCount} milliards de param√®tres, ce mod√®le fait partie de la classe des moyens mod√®les (entre 20 et 70 milliards de param√®tres).",
        "L": "Dot√© de {paramsCount} milliards de param√®tres, ce mod√®le fait partie de la classe des grands mod√®les (entre 70 et 100 milliards de param√®tres).",
        "XL": "Dot√© de {paramsCount} milliards de param√®tres, ce mod√®le fait partie de la classe des tr√®s grands mod√®les."
      },
      "use": {
        "commercial": "Utilisation commerciale",
        "modification": "Modification autoris√©e",
        "attribution": "Attribution requise",
        "licenseType": "Type de licence",
        "requiredRam": "RAM n√©cessaire"
      }
    },
    "parameters": "{number}¬†mds¬†de¬†param√®tres",
    "ram": "{min} √† {max} Go",
    "names": {
      "a": "Mod√®le A",
      "b": "Mod√®le B"
    },
    "extra": {
      "title": "Pour aller plus loin",
      "experts": {
        "open-weights": "Pour les expert¬∑es, consultez la <a {linkProps}>fiche du mod√®le sur Hugging Face</a>",
        "api-only": "Pour les expert¬∑es, consultez la <a {linkProps}>site officiel du mod√®le</a>"
      },
      "impacts": "Les calculs d‚Äôimpacts environnementaux reposent sur les projets <a {linkProps1}>EcoLogits</a> et <a {linkProps2}>Impact CO<sub>2</sub></a>."
    },
    "list": {
      "title": "D√©couvrez les mod√®les",
      "intro": "Explorez les diff√©rents mod√®les d'IA conversationnels disponibles, leurs caract√©ristiques et leurs licences.",
      "filters": {
        "editor": {
          "legend": "√âditeur"
        },
        "size": {
          "legend": "Taille (en milliards de param√®tres)",
          "labels": {
            "XS": "< √† 7 milliards",
            "S": "de 7 √† 20 milliards",
            "M": "de 20 √† 70 milliards",
            "L": "de 70 √† 150 milliards",
            "XL": "> 150 milliards"
          }
        },
        "license": {
          "legend": "Licence d'utilisation"
        },
        "display": "Afficher les filtres"
      },
      "triage": {
        "label": "Trier par",
        "options": {
          "name-asc": "Nom du mod√®le (A √† Z)",
          "date-desc": "Date de sortie (du plus au moins r√©cent)",
          "params-asc": "Taille (du plus petit au plus grand)",
          "org-asc": "√âditeur (A √† Z)"
        }
      },
      "model": "mod√®le",
      "models": "mod√®les",
      "noresults": "Aucun mod√®le ne correspond √† vos crit√®res de recherche."
    }
  },
  "modes": {
    "random": {
      "title": "Mode Al√©atoire",
      "label": "Al√©atoire",
      "altLabel": "Mod√®les al√©atoires",
      "description": "Deux mod√®les choisis au hasard parmi toute la liste"
    },
    "custom": {
      "title": "Mode S√©lection",
      "label": "S√©lection manuelle",
      "altLabel": "S√©lection manuelle",
      "description": "Reconna√Ætrez-vous les deux mod√®les que vous avez choisis ?"
    },
    "small-models": {
      "title": "Mode Frugal",
      "label": "Frugal",
      "altLabel": "Mod√®les frugaux",
      "description": "Deux mod√®les tir√©s au hasard parmi ceux de plus petite taille"
    },
    "big-vs-small": {
      "title": "Mode David contre Goliath",
      "label": "David contre Goliath",
      "altLabel": "David contre Goliath",
      "description": "Un petit mod√®le contre un grand, les deux tir√©s au hasard"
    },
    "reasoning": {
      "title": "Mode Raisonnement",
      "label": "Raisonnement",
      "altLabel": "Mod√®les avec raisonnement",
      "description": "Deux mod√®les tir√©s au hasard parmi ceux optimis√©s pour des t√¢ches complexes"
    }
  },
  "vote": {
    "title": "Quel mod√®le d‚ÄôIA pr√©f√©rez-vous ?",
    "introA": "Avant de d√©couvrir l‚Äôidentit√© des mod√®les, nous avons besoin de votre pr√©f√©rence.",
    "introB": "Elle permet d'enrichir les jeux de donn√©es compar:IA dont l‚Äôobjectif est d‚Äôaffiner les futurs mod√®les d‚ÄôIA sur le fran√ßais",
    "bothEqual": "Les deux se valent",
    "comment": {
      "add": "Ajouter des commentaires",
      "placeholder": "Vous pouvez ajouter des pr√©cisions sur cette r√©ponse du mod√®le {model}"
    },
    "choices": {
      "positive": {
        "question": "Qu'avez-vous appr√©ci√© dans la r√©ponse ?",
        "useful": "Utile",
        "complete": "Compl√®te",
        "creative": "Cr√©ative",
        "clear-formatting": "Mise en forme claire"
      },
      "negative": {
        "question": "Pourquoi la r√©ponse ne convient-elle pas ?",
        "incorrect": "Incorrecte",
        "superficial": "Superficielle",
        "instructions-not-followed": "Instructions non suivies"
      },
      "altText": "{choice} pour le mod√®le {model}"
    },
    "qualify": {
      "question": "Comment qualifiez-vous ses r√©ponses ?",
      "placeholder": "Les r√©ponses du mod√®le {model} sont...",
      "addDetails": "Ajouter des d√©tails"
    },
    "like": {
      "label": "j'appr√©cie",
      "selectedLabel": "j'appr√©cie (s√©lectionn√©)"
    },
    "dislike": {
      "label": "je n'appr√©cie pas",
      "selectedLabel": "je n'appr√©cie pas (s√©lectionn√©)"
    },
    "yours": "Votre vote"
  },
  "reveal": {
    "impacts": {
      "title": "Impact √©nerg√©tique de la discussion",
      "size": {
        "label": "taille du mod√®le",
        "count": "milliards param.",
        "estimated": "(est.)",
        "quantized": "(quantis√©)"
      },
      "tokens": {
        "label": "taille du texte",
        "tooltip": "L‚ÄôIA analyse et g√©n√®re des phrases √† partir de mots ou de parties de mots d‚Äô√† peu pr√®s quatre lettres, cette unit√© de texte est appel√©e token (¬´‚ÄØjeton‚ÄØ¬ª). Plus un texte est long, plus le nombre de tokens est grand.",
        "tokens": "tokens"
      },
      "energy": {
        "label": "√©nergie conso.",
        "tooltip": "Mesur√©e en wattheures, l‚Äô√©nergie consomm√©e repr√©sente l'√©lectricit√© utilis√©e par le mod√®le pour traiter une requ√™te et g√©n√©rer la r√©ponse correspondante. Plus un mod√®le est grand (en milliards de param√®tres), plus il faut d'√©nergie pour produire un token."
      }
    },
    "equivalent": {
      "title": "Ce qui correspond √† :",
      "co2": {
        "label": "CO<sub>2</sub> √©mis",
        "tooltip": "Le CO<sub>2</sub> √©mis √©quivaut aux √©missions de dioxyde de carbone produites par l‚Äô√©nergie utilis√©e pour faire fonctionner le mod√®le. Elle traduit l'impact environnemental li√© √† la consommation √©nerg√©tique. Le calcul d‚Äô√©quivalence Wattheures/CO<sub>2</sub> diff√®re selon le mix √©nerg√©tique de chaque pays. Or, les serveurs utilis√©s pour l‚Äôinf√©rence des mod√®les ne sont pas tous localis√©s en France. Ainsi, le calcul d‚Äô√©quivalence repose sur la moyenne mondiale du taux d‚Äô√©missions de CO<sub>2</sub> par √©nergie consomm√©e."
      },
      "lightbulb": {
        "label": "ampoule LED",
        "tooltip": "Donn√©e calcul√©e sur la base de consommation d‚Äôune ampoule LED standard de 5W (E14)"
      },
      "streaming": {
        "label": "vid√©os en ligne",
        "tooltip": "Donn√©e calcul√©e selon l‚Äôimpact carbone d‚Äôune heure de vid√©o en ligne en haute d√©finition, sur une t√©l√©vision, en connexion wifi (source <a {linkProps}>ADEME</a>)"
      }
    },
    "feedback": {
      "shareResult": "Partager votre r√©sultat",
      "moreOnVotes": "En savoir plus sur les votes",
      "description": "Faites d√©couvrir compar:IA en partageant les mod√®les d‚ÄôIA avec lesquels vous avez √©chang√© ! Seuls les noms et l‚Äôimpact √©nerg√©tique de la discussion seront visibles via ce lien, sans acc√®s aux messages √©chang√©s.",
      "example": "Exemple de partage de r√©sultat"
    }
  },
  "errors": {
    "unknown": "Une erreur est survenue",
    "404": {
      "title": "Page non trouv√©e",
      "error": "Erreur 404",
      "sorry": "La page que vous cherchez est introuvable. Excusez-nous pour la g√®ne occasionn√©e.",
      "desc": "Si vous avez tap√© l'adresse web dans le navigateur, v√©rifiez qu'elle est correcte. La page n‚Äôest peut-√™tre plus disponible.<br />Dans ce cas, pour continuer votre visite vous pouvez consulter notre page d‚Äôaccueil.<br />Sinon contactez-nous pour que l‚Äôon puisse vous rediriger vers la bonne information."
    },
    "unexpected": {
      "title": "Erreur inattendue",
      "error": "Erreur {code}",
      "sorry": "D√©sol√©, le service rencontre un probleÃÄme, nous travaillons pour le reÃÅsoudre le plus rapidement possible.",
      "desc": "Essayez de rafra√Æchir la page ou bien ressayez plus tard."
    }
  },
  "actions": {
    "copyMessage": {
      "do": "Copier le message",
      "done": "Message copi√©"
    },
    "copyLink": {
      "do": "Copier le lien",
      "done": "Lien copi√© dans le presse-papiers"
    },
    "contact": "Contactez-nous",
    "contactUs": "Nous contacter",
    "home": "Page d'accueil",
    "returnHome": "Revenir √† l'accueil",
    "seeMore": "Voir plus",
    "selectLanguage": "S√©lectionner une langue"
  },
  "words": {
    "back": "Retour",
    "close": "Fermer",
    "NA": "N/A",
    "random": "Al√©atoire",
    "regenerate": "Reg√©n√©rer",
    "reset": "R√©initialiser",
    "restart": "Recommencer",
    "retry": "Recommencer",
    "search": "Rechercher",
    "send": "Envoyer",
    "tooltip": "Infobulle",
    "validate": "Valider"
  },
  "generated": {
    "licenses": {
      "os": {
        "Apache 2.0": {
          "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le mod√®le, y compris √† des fins commerciales. Outre la libert√© d'utilisation, elle garantit la protection juridique en incluant une clause de concession de brevets qui fonctionne comme une assurance : si vous utilisez ce mod√®le, les contributeurs s'engagent √† ne pas vous poursuivre pour violation de leurs brevets li√©s au projet. Cette protection mutuelle √©vite les conflits juridiques entre utilisateurs et d√©veloppeurs. Lors de la distribution de versions modifi√©es, les changements significatifs doivent √™tre signal√©s par des mentions appropri√©es, garantissant la transparence pour l'utilisateur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "CC-BY-NC-4.0": {
          "license_desc": "<p>Cette licence permet de partager et adapter le contenu librement √† condition de cr√©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilit√© pour les usages non commerciaux tout en prot√©geant les droits de l'auteur.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        },
        "Gemma": {
          "license_desc": "<p>Cette licence est con√ßue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifi√©es ou am√©lior√©es doivent √™tre partag√©es avec la communaut√© sous la m√™me licence source, favorisant ainsi la collaboration et la transparence dans le d√©veloppement logiciel.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Llama 3.1": {
          "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les op√©rations d√©passant 700 millions d'utilisateurs mensuels. La r√©utilisation du code ou des contenus g√©n√©r√©s pour l‚Äôentra√Ænement ou l'am√©lioration de mod√®les d√©riv√©s est autoris√©e √† condition d‚Äôafficher ‚Äúbuilt with llama‚Äù et d‚Äôinclure ‚ÄúLlama‚Äù dans leur nom pour toute distribution.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d‚Äôutilisateurs"
        },
        "Llama 3.3": {
          "license_desc": "<p>Cette licence <strong>non-exclusive, mondiale et sans redevance</strong> permet d'utiliser, reproduire, modifier et distribuer librement le code et les Mat√©riaux Llama 3.3 avec attribution. Elle autorise notamment la r√©utilisation pour l'am√©lioration de mod√®les d√©riv√©s, mais impose des restrictions pour les op√©rations commerciales de tr√®s grande envergure.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d‚Äôutilisateurs"
        },
        "Llama 4": {
          "license_desc": "<p>Cette licence non-exclusive, mondiale et sans redevance permet d'utiliser, reproduire, modifier et distribuer les Mat√©riaux Llama 4 (mod√®les et documentation) avec attribution. Cependant, elle impose deux restrictions majeures : (1) les entreprises d√©passant 700 millions d'utilisateurs actifs mensuels doivent obtenir une licence sp√©ciale de Meta, et (2) <strong>exclusion totale</strong> des personnes r√©sidant dans l'UE et des entreprises ayant leur si√®ge social dans l'UE pour l'utilisation directe des mod√®les multimodaux, en raison des incertitudes r√©glementaires li√©es √† l'AI Act europ√©en. Les utilisateurs finaux europ√©ens peuvent n√©anmoins acc√©der √† des services int√©grant Llama 4, √† condition qu'ils soient fournis depuis l'ext√©rieur de l'UE.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions d‚Äôutilisateurs\n"
        },
        "MIT": {
          "license_desc": "<p>La licence MIT est une licence de logiciel libre permissive : elle permet √† quiconque de r√©utiliser, modifier et distribuer le mod√®le, m√™me √† des fins commerciales, sous r√©serve d'inclure la licence d'origine et les mentions de droits d'auteur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI Research License": {
          "license_desc": "<p>Cette licence non-exclusive et sans redevance autorise l'utilisation, la copie, la modification et la distribution des mod√®les Mistral et de leurs d√©riv√©s (incluant les versions modifi√©es ou affin√©es). Cependant, elle est strictement limit√©e aux fins de recherche.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        }
      },
      "proprio": {
        "Alibaba": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© Alibaba, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Amazon": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via Amazon Bedrock, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "sauf pour distiller ou entra√Æner d‚Äôautres mod√®les sur les plateformes d‚ÄôAmazon.",
          "commercial_use_specificities": ""
        },
        "Anthropic": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© Anthropic ou des soci√©t√©s partenaires, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Cohere": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "DeepSeek": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Google": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© Google, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e de Google.",
          "reuse_specificities": "sauf pour entra√Æner d‚Äôautres mod√®les sur Vertex AI",
          "commercial_use_specificities": ""
        },
        "Meta": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Microsoft": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres h√©bergeurs, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Moonshot AI": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nous": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nvidia": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "OpenAI": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via API sur les plateformes de la soci√©t√© OpenAI ou via les services Microsoft Azure, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Zhipu": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "xAI": {
          "license_desc": "Le mod√®le est disponible sous licence payante et accessible via X et xAI, n√©cessitant un paiement √† l'utilisation bas√© sur le nombre de jetons trait√©s ou sur l‚Äôinfrastructure r√©serv√©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        }
      }
    },
    "models": {
      "Aya Expanse 32B": {
        "desc": "<p>Mod√®le de taille moyenne multilingue, capable de traiter 23 langues.</p>",
        "size_desc": "<p>Avec 32 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut √™tre h√©berg√© sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui contribue √† limiter les co√ªts d‚Äôinfrastructure.</p>\n<p>Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 130 000 jetons, utile pour l‚Äôanalyse de documents longs.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derri√®re ce mod√®le, a √©t√© fond√©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier ¬´‚ÄØAttention Is All You Need‚ÄØ¬ª qui a r√©volutionn√© l'IA. Sa sp√©cificit√© principale r√©side dans sa focalisation exclusive sur l'IA g√©n√©rative pour les entreprises, particuli√®rement les secteurs r√©glement√©s comme la finance, la sant√©, la manufacture et l'√©nergie, ainsi que le secteur public. L'entreprise est √©galement pionni√®re dans les approches multilingues et maintient un laboratoire de recherche √† but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce mod√®le a √©t√© con√ßu pour offrir de bonnes capacit√©s dans chacune des 23 langues de son corpus d‚Äôentra√Ænement.</p>"
      },
      "Claude 3.7 Sonnet": {
        "desc": "<p>Tr√®s grand mod√®le multimodal et multilingue, performant pour la g√©n√©ration de code, avec deux modalit√©s de r√©ponses: l‚Äôutilisateur peut choisir entre un mode de raisonnement, pour des r√©ponses plus approfondies, ou un mode rapide, pour g√©n√©rer directement la r√©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de tr√®s grande taille, n√©cessitant des serveurs √©quip√©s de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse. Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 200 000 jetons, adapt√©e √† l‚Äôanalyse de longs documents ou de d√©p√¥ts de code.</p>",
        "fyi": "<p>Claude 4 Opus est la version la plus avanc√©e de la famille Claude 4. Il est optimis√© pour la puissance brute et les t√¢ches complexes n√©cessitant un raisonnement soutenu sur de longues p√©riodes‚ÄØ: il peut par exemple travailler sur des t√¢ches √† long terme (Anthropic d√©clarent qu'il peut travailler jusqu'√† sept heures de mani√®re ind√©pendante). En contrepartie, Opus est plus co√ªteux √† utiliser, plus lent √† r√©pondre et n√©cessite davantage de ressources pour fonctionner.</p>\n<p>Le mod√®le offre deux modes d‚Äôutilisation‚ÄØ: un mode r√©flexion avec un raisonnement √©tape par √©tape pour les probl√®mes complexes, et un mode rapide pour les r√©ponses directes. √Ä la diff√©rence d‚Äôautres mod√®les, le mode raisonnement n‚Äôa pas √©t√© majoritairement entra√Æn√© sur des donn√©es math√©matiques, mais adapt√© √† des cas d‚Äôusage r√©els.</p>"
      },
      "Claude 4 Sonnet": {
        "desc": "<p>Tr√®s grand mod√®le multimodal et multilingue, tr√®s puissant en code, avec deux modalit√©s de r√©ponses: l‚Äôutilisateur peut choisir entre un mode de raisonnement, pour des r√©ponses plus approfondies, ou un mode rapide, pour g√©n√©rer directement la r√©ponse finale.</p>",
        "size_desc": "<p>La taille exacte n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de tr√®s grande taille, n√©cessitant des serveurs √©quip√©s de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse. Le mod√®le dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 200 000 jetons, adapt√©e √† l‚Äôanalyse de longs documents ou de d√©p√¥ts de code.</p>",
        "fyi": "<p>Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimis√©e pour la vitesse, l‚Äôefficacit√© et l‚Äôaccessibilit√©. Il est un peu moins √† l‚Äôaise sur les t√¢ches qui demandent un raisonnement complexe en plusieurs √©tapes. En contrepartie, il est nettement moins co√ªteux, plus rapide, peut g√©n√©rer de plus longs textes et consomme moins d‚Äô√©nergie que Opus.</p>\n<p>Le mod√®le offre deux modes d‚Äôutilisation‚ÄØ: un mode r√©flexion avec un raisonnement √©tape par √©tape pour les probl√®mes complexes, et un mode rapide pour les r√©ponses directes. √Ä la diff√©rence d‚Äôautres mod√®les, le mode raisonnement n‚Äôa pas √©t√© surtout entra√Æn√© sur des donn√©es math√©matiques, mais adapt√© √† des cas d‚Äôusage r√©els.</p>"
      },
      "Command A": {
        "desc": "<p>Grand mod√®le, performant pour la programmation, l‚Äôutilisation d‚Äôoutils externes, la ‚Äúg√©n√©ration augment√©e de r√©cup√©ration‚Äù (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 111 milliards de param√®tres, ce mod√®le fait partie des grands mod√®les. Il n√©cessite au moins deux cartes graphiques puissantes pour l‚Äôh√©bergement, ce qui entra√Æne un co√ªt de fonctionnement significatif.</p>\n<p>Sa fen√™tre de contexte atteint 256 000 jetons, adapt√©e √† l‚Äôanalyse de vastes ensembles de documents ou de bases de code.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derri√®re ce mod√®le, a √©t√© fond√©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier <a href=\"https://arxiv.org/abs/1706.03762\">¬´‚ÄØAttention Is All You Need‚ÄØ¬ª</a> paru en 2017 et qui a r√©volutionn√© l'IA. L'entreprise se d√©marque par sa focalisation exclusive sur l'IA g√©n√©rative pour les entreprises, particuli√®rement les secteurs r√©glement√©s comme la finance, la sant√©, la manufacture et l'√©nergie, ainsi que le secteur public. L'entreprise est √©galement pionni√®re dans les approches multilingues et maintient un laboratoire de recherche √† but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce mod√®le est con√ßu pour fonctionner dans plus de 23 langues et pour s‚Äôint√©grer facilement dans les syst√®mes d‚Äôentreprise.  Il fait partie des rares mod√®les distribu√©s sous licence <strong>CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.</strong> Ce choix de licence refl√®te la volont√© de Cohere de contribuer √† la recherche et la communaut√© open source, tout en gardant le contr√¥le sur les usages commerciaux pour prot√©ger son mod√®le √©conomique... Cela exclut par exemple l‚Äôint√©gration du mod√®le dans des produits ou services vendus par une entreprise √† des clients mais autorise un usage acad√©mique, des tests ou des projets internes, restreints √† un cadre non-commercial.</p>"
      },
      "Command R": {
        "desc": "<p>Mod√®le de taille moyenne optimis√© pour la synth√®se, les questions g√©n√©rales, l‚Äôutilisation d‚Äôoutils et efficace dans les syst√®mes de g√©n√©ration augment√©e de r√©cup√©ration (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 35 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut √™tre h√©berg√© sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui contribue √† limiter les co√ªts d‚Äôinfrastructure.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derri√®re ce mod√®le, a √©t√© fond√©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier ¬´‚ÄØAttention Is All You Need‚ÄØ¬ª qui a r√©volutionn√© l'IA. Sa sp√©cificit√© principale r√©side dans sa focalisation exclusive sur l'IA g√©n√©rative pour les entreprises, particuli√®rement les secteurs r√©glement√©s comme la finance, la sant√©, la manufacture et l'√©nergie, ainsi que le secteur public. L'entreprise est √©galement pionni√®re dans les approches multilingues et maintient un laboratoire de recherche √† but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce mod√®le a √©t√© √©valu√© dans plus de 10 langues. Sa fen√™tre de contexte atteint 128 000 jetons, ce qui facilite l‚Äôanalyse de documents longs. Cette fen√™tre a √©t√© doubl√©e sur la version suivante du mod√®le (Command A).</p>"
      },
      "DeepSeek R1": {
        "desc": "<p>Tr√®s grand mod√®le tr√®s performant sur les t√¢ches math√©matiques, scientifiques et de programmation, qui simule une √©tape de raisonnement avant de g√©n√©rer sa r√©ponse.</p>",
        "size_desc": "<p>Avec 671 milliards de param√®tres DeepSeek R1 est un mod√®le de tr√®s grande taille qui n√©cessite plusieurs cartes graphiques puissantes pour fonctionner. Les mod√®les de raisonnement de ce type fonctionnent plus longtemps pour produire une r√©ponse, ce qui augmente la consommation √©nerg√©tique. Cependant, l'architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique. La fen√™tre de contexte atteint 128 000 jetons, ce qui est adapt√© √† l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>Ce mod√®le s‚Äôappuie sur une architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de param√®tres, dont 37 milliards sont activ√©s par jeton. L'entra√Ænement a fait appel √† un apprentissage par renforcement √† grande √©chelle, avec plusieurs √©tapes d'ajustement SFT (<em>supervised fine-tuning</em>‚ÄØ: un affinage supervis√© o√π le mod√®le apprend √† partir d'exemples de r√©ponses correctes) et de donn√©es de d√©marrage.</p>"
      },
      "DeepSeek R1 Llama 70B": {
        "desc": "<p>Grand mod√®le bas√© sur Meta Llama 3.3 70B, r√©-entra√Æn√© avec des exemples de raisonnement issus du mod√®le DeepSeek R1. Il offre de bonnes capacit√©s en math√©matiques et code.</p>",
        "size_desc": "<p>Avec 70 milliards de param√®tres, ce mod√®le est class√© parmi les mod√®les de grande taille. Il n√©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entra√Æne un co√ªt de fonctionnement √©lev√©. Les mod√®les de raisonnement fonctionnent √©galement plus longtemps pour produire une r√©ponse, ce qui augmente leur consommation √©nerg√©tique.</p>\n<p>La fen√™tre de contexte est de 16 000 jetons, ce qui peut √™tre limitant pour l‚Äôanalyse de tr√®s grands documents.</p>",
        "fyi": "<p>Le mod√®le n‚Äôa pas √©t√© entra√Æn√© depuis z√©ro. Il s‚Äôappuie sur Llama 3.3 70B, r√©-entra√Æn√© en utilisant des r√©sultats g√©n√©r√©s par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B d‚Äôune capacit√© √† simuler le raisonnement, sans possibilit√© pour l‚Äôutilisateur de choisir d‚Äôactiver ou non cette fonction.</p>\n<p>Conform√©ment aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du mod√®le source dans le nom du mod√®le, soumis au m√™me r√©gime de licence.</p>"
      },
      "DeepSeek V3": {
        "desc": "<p>Tr√®s grand mod√®le con√ßu pour des t√¢ches complexes‚ÄØ: g√©n√©ration de code, utilisation d‚Äôoutils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particuli√®rement adapt√© √† l‚Äôanglais et au chinois.</p>",
        "size_desc": "<p>DeepSeek V3 est un mod√®le de tr√®s grande taille, n√©cessitant plusieurs cartes graphiques pour fonctionner. L‚Äôarchitecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) permet n√©anmoins de n‚Äôactiver qu‚Äôune partie des param√®tres, ce qui r√©duit l‚Äôempreinte par rapport √† un mod√®le dense de m√™me taille.</p>\n<p>La fen√™tre de contexte atteint 163 000 jetons, ce qui est utile pour l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>Ce mod√®le est bas√© sur une architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts), comptant 671 milliards de param√®tres mais n‚Äôen activant que 37 milliards par jeton g√©n√©r√©. Il est efficace pour les appels d‚Äôoutils, la g√©n√©ration de sorties structur√©es (JSON) et la g√©n√©ration de code.</p>"
      },
      "GPT 4.1 Nano": {
        "desc": "<p>Plus petite version all√©g√©e du mod√®le GPT 4.1 , con√ßue pour limiter les co√ªts tout en restant comp√©titive sur la plupart des t√¢ches. Le mod√®le accepte de tr√®s longues requ√™tes, ce qui permet de l‚Äôutiliser par exemple pour l‚Äôanalyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de taille moyenne, n√©cessitant une carte graphique puissante pour l‚Äôex√©cution. N√©anmoins, l'architecture suppos√©e de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique.  Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>",
        "fyi": "<p>Il s'agit d'une version distill√©e d‚Äôun mod√®le de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l‚Äôaudio. Sa fen√™tre de contexte peut atteindre jusqu‚Äô√† 1 million de jetons, ce qui le rend particuli√®rement adapt√© √† l‚Äôanalyse de corpus de textes ou de d√©p√¥ts de code tr√®s longs.</p>"
      },
      "GPT 5": {
        "desc": "<p>Le GPT-5 n'est pas un mod√®le unique, mais un syst√®me unifi√© compos√© de deux mod√®les distincts‚ÄØ: un mod√®le rapide (<code>gpt-5-main</code>) pour les requ√™tes courantes et un mod√®le de raisonnement (<code>gpt-5-thinking</code>) pour les probl√®mes complexes. Compar√© √† ses pr√©d√©cesseurs, OpenAI affirme qu'il est plus utile dans les requ√™tes du monde r√©el, avec des am√©liorations notables dans les domaines de l'√©criture, du codage et de la sant√©. Il r√©duit √©galement le ph√©nom√®ne des hallucinations. Gr√¢ce √† sa fen√™tre de contexte de 400 000 jetons, il peut accepter de longues requ√™tes, ce rend possible l'analyse de plusieurs documents √† la fois.</p>",
        "size_desc": "<p>Le syst√®me GPT-5 est compos√© de mod√®les de diff√©rentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est con√ßue pour inclure plusieurs mod√®les, orchestr√©s par un syst√®me de routage interne, qui s√©lectionne le plus petit mod√®le adapt√© √† la t√¢che pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement bas√©e sur un ¬´ m√©lange d'experts ¬ª (MoE, Mixture of Experts), ce qui signifie que seule une partie des param√®tres est activ√©e pour chaque requ√™te. Cela permet une meilleure efficacit√© √©nerg√©tique et des performances √©lev√©es. Les estimations disponibles sur la taille des mod√®les s'appuient sur des informations publiques et des indices indirects tels que les co√ªts d'inf√©rence et la latence de r√©ponse.</p>",
        "fyi": "<p>Les d√©veloppeurs qui utilisent ce mod√®le peuvent configurer un param√®tre de verbosit√© pour ajuster la longueur de la phase de raisonnement.</p>\n<p>En mati√®re de s√©curit√©, le syst√®me utilise une nouvelle approche de s√©curit√© appel√©e ¬´ safe-completions ¬ª pour pr√©venir le contenu non autoris√© au moment de la r√©ponse plut√¥t qu‚Äôau moment de la requ√™te. Les cr√©ateurs du mod√®le ont aussi utilis√© la phase d‚Äôentra√Ænement au ‚Äúraisonnement‚Äù pour le rendre plus ‚Äúr√©sistant‚Äù aux tentatives de contournement de leurs r√®gles de s√©curit√© (<em>jailbreaking</em>).</p>"
      },
      "GPT 5 Mini": {
        "desc": "<p>Le GPT-5 Mini est une version all√©g√©e du mod√®le GPT-5 principal. Il est con√ßu pour √™tre utilis√© dans des environnements o√π il est n√©cessaire de limiter les co√ªts, par exemple √† grande √©chelle. Son mod√®le de raisonnement est presque aussi performant que celui du mod√®le principal (<code>gpt-5-thinking</code>) malgr√© sa taille plus petite. Gr√¢ce √† sa fen√™tre de contexte de 400 000 jetons, il peut accepter de longues requ√™tes, ce rend possible l'analyse de plusieurs documents √† la fois.</p>",
        "size_desc": "<p>Le mod√®le Mini est une d√©clinaison plus compacte (taille moyenne suppos√©e) du syst√®me GPT-5. Il est con√ßu pour fonctionner de mani√®re optimale pour un bon √©quilibre entre performance et co√ªt, gr√¢ce √† un syst√®me de routage qui le s√©lectionne pour des t√¢ches sp√©cifiques. L'architecture est probablement bas√©e sur un ¬´ m√©lange d'experts ¬ª (MoE, Mixture of Experts), ce qui signifie que seule une partie des param√®tres est activ√©e pour chaque requ√™te. N√©anmoins, les mod√®les sont probablement tr√®s grands, n√©cessitant plusieurs cartes graphiques puissantes pour l‚Äôinf√©rence.</p>",
        "fyi": "<p>Le syst√®me utilise une nouvelle approche de s√©curit√© appel√©e ¬´ safe-completions ¬ª pour pr√©venir le contenu non autoris√© au moment de la r√©ponse plut√¥t qu‚Äôau moment de la requ√™te.</p>\n<p>Bien qu'il soit une version plus petite, il se montre tr√®s comp√©titif face au mod√®le GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine m√©dical.</p>"
      },
      "GPT 5 Nano": {
        "desc": "<p>Le GPT-5 Nano est la plus petite et la plus rapide version du mod√®le de raisonnement GPT-5. Il est con√ßu pour des contextes o√π une latence ou un co√ªt ultra-faible est n√©cessaire. Gr√¢ce √† sa fen√™tre de contexte de 400 000 jetons, il peut accepter de longues requ√™tes, ce rend possible l'analyse de plusieurs documents √† la fois.</p>",
        "size_desc": "<p>Le mod√®le Nano est le plus compact de la famille GPT-5 (taille petite suppos√©e). Il est s√©lectionn√© par le syst√®me de routage pour les requ√™tes n√©cessitant une latence ultra-faible et des r√©ponses instantan√©es. Son architecture est probablement bas√©e sur un ¬´ m√©lange d'experts ¬ª (MoE, Mixture of Experts), ce qui permet une meilleure efficacit√© √©nerg√©tique et des performances √©lev√©es, m√™me sur des requ√™tes n√©cessitant une r√©ponse rapide.</p>",
        "fyi": "<p>Le syst√®me utilise une nouvelle approche de s√©curit√© appel√©e ¬´ safe-completions ¬ª pour pr√©venir le contenu non autoris√© au moment de la r√©ponse plut√¥t qu‚Äôau moment de la requ√™te.</p>"
      },
      "GPT OSS-120B": {
        "desc": "<p>Le plus grand des deux premiers mod√®les semi-ouverts d'OpenAI depuis GPT-2. Con√ßu en r√©ponse √† la mont√©e en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un mod√®le de raisonnement performant, notamment sur des t√¢ches complexes et dans des environnements ¬´‚ÄØagentiques‚ÄØ¬ª.</p>",
        "size_desc": "<p>L'architecture est bas√©e sur le principe du ¬´ m√©lange d'experts ¬ª (MoE), ce qui permet une meilleure efficacit√© √©nerg√©tique en n'activant qu'une partie des param√®tres (5,1 milliards par jeton) pour chaque requ√™te. C‚Äôest un mod√®le de raisonnement, donc sa consommation d‚Äô√©nergie est plus √©lev√©e car ils g√©n√®re une cha√Æne de pens√©e interne avant de fournir la r√©ponse finale. Il dispose d'une fen√™tre de contexte de 131 000 jetons, ce qui le rend id√©al pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce mod√®le peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fen√™tre de contexte de 131 000 jetons, ce qui le rend id√©al pour l'analyse de documents volumineux. </p>\n<p>Dans les configurations du mod√®le, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui d√©terminent la verbosit√© du mod√®le.</p>"
      },
      "GPT OSS-20B": {
        "desc": "<p>Le plus petit des deux mod√®les semi-ouverts d'OpenAI. Il a √©t√© con√ßu en r√©ponse √† la concurrence de l'open source et est destin√© aux cas d'utilisation n√©cessitant une faible latence ainsi qu'aux d√©ploiements locaux ou sp√©cialis√©s.</p>",
        "size_desc": "<p>Avec 20 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. L'architecture est bas√©e sur le ¬´ m√©lange d'experts ¬ª (MoE), ce qui permet une meilleure efficacit√© √©nerg√©tique en n'activant qu'une partie des param√®tres (3,6 milliards par jeton) pour chaque requ√™te. Il s'agit d'un mod√®le de raisonnement, ce qui se traduit par une consommation d'√©nergie plus √©lev√©e car il g√©n√®re une cha√Æne de pens√©e interne avant de fournir la r√©ponse finale. Il dispose d'une fen√™tre de contexte de 131 000 jetons, ce qui le rend id√©al pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce mod√®le peut √™tre ex√©cut√© localement sur un ordinateur portable haut de gamme √©quip√© de seulement 16 Go de VRAM (ou de RAM syst√®me). Cela en fait une option tr√®s accessible pour les d√©veloppeurs. </p>\n<p>Dans les configurations du mod√®le, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui d√©terminent la verbosit√© du mod√®le.</p>"
      },
      "GPT-4.1 Mini": {
        "desc": "<p>Version all√©g√©e de GPT 4.1 mais qui reste tout de m√™me de grande taille, con√ßue pour limiter les co√ªts tout en restant comp√©titif sur la plupart des t√¢ches. Le mod√®le accepte de tr√®s longues requ√™tes, ce qui permet de l‚Äôutiliser par exemple pour l‚Äôanalyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de grande taille, n√©cessitant une carte graphique puissante pour l‚Äôex√©cution. N√©anmoins, l'architecture suppos√©e de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>",
        "fyi": "<p>Il s'agit d'une version distill√©e d‚Äôun mod√®le plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de l‚Äôaudio.  Sa fen√™tre de contexte peut atteindre jusqu‚Äô√† 1 million de jetons, ce qui le rend particuli√®rement adapt√© √† l‚Äôanalyse de corpus tr√®s longs ou de d√©p√¥ts de code.</p>"
      },
      "Gemini 2.5 Flash": {
        "desc": "<p>Grand mod√®le multimodal et multilingue avec deux modalit√©s de r√©ponses: l‚Äôutilisateur peut choisir entre un mode de raisonnement, pour des r√©ponses plus approfondies, ou un mode rapide, pour g√©n√©rer directement √† la r√©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de grande taille, n√©cessitant plusieurs cartes graphiques puissantes pour le fonctionnement. N√©anmoins, l'architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse. Sa fen√™tre de contexte va jusqu‚Äô√† 1 millions de jetons, ce qui permet de traiter de tr√®s grands corpus documentaires.</p>",
        "fyi": "<p>Ce mod√®le repose sur une architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) et a √©t√© distill√© en ne conservant qu'une approximation des pr√©dictions du mod√®le enseignant - Gemini 2.5 Pro. Il a √©t√© entra√Æn√© sur une architecture TPUv5p int√©grant des avanc√©es comme la possibilit√© de poursuivre l'entra√Ænement automatiquement m√™me en cas d‚Äôerreurs d‚Äôentra√Ænement, de corruption de donn√©es ou de probl√®mes de m√©moire.</p>\n<p>Gemini 2.5 Flash g√®re des contextes allant jusqu'√† 1 million de jetons, et trois heures de contenu vid√©o. L'optimisation du traitement de la vision permet de traiter des vid√©os environ trois fois plus longues dans la m√™me fen√™tre de contexte: seuls 66 jetons visuels sont n√©cessaires pour g√©n√©rer une image contre 258 auparavant. Ce mod√®le permet  √©galement la g√©n√©ration audio native pour les dialogues et la synth√®se vocale.</p>"
      },
      "Gemma 3 12B": {
        "desc": "<p>Petit mod√®le multimodal adapt√© aux t√¢ches courantes comme les questions-r√©ponses, les r√©sum√©s ou l‚Äôinterpr√©tation d‚Äôimages.</p>",
        "size_desc": "<p>Avec 12 milliards de param√®tres, il fait partie des mod√®les de petite taille. Il peut √™tre utilis√© localement sur un poste pour pr√©server la confidentialit√© des donn√©es, ou sur serveur peu co√ªteux pour limiter les co√ªts par rapport √† un mod√®le plus grand. </p>\n<p>Sa fen√™tre de contexte va jusqu‚Äô√† 128 000 jetons, ce qui permet de traiter de longs documents.</p>",
        "fyi": "<p>Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a √©t√© entra√Æn√© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 27B": {
        "desc": "<p>Mod√®le de taille moyenne multimodal adapt√© aux t√¢ches courantes comme les questions-r√©ponses, les r√©sum√©s ou l‚Äôinterpr√©tation d‚Äôimages.</p>",
        "size_desc": "<p>Avec 27 milliards de param√®tres, il appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut √™tre d√©ploy√© sur un serveur avec une seule carte graphique (GPU). </p>\n<p>Il accepte des contextes jusqu‚Äô√† 128 000 jetons, ce qui convient pour l‚Äôanalyse de documents longs.</p>",
        "fyi": "<p>Il peut traiter du texte et des images sur un serveur √©quip√© d‚Äôune seule carte graphique puissante. Il a √©t√© entra√Æn√© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 4B": {
        "desc": "<p>Tr√®s petit mod√®le multimodal et compact adapt√© aux t√¢ches courantes comme les questions-r√©ponses, les r√©sum√©s ou l‚Äôinterpr√©tation d‚Äôimages.</p>",
        "size_desc": "<p>Avec 4 milliards de param√®tres, il fait partie des mod√®les de tr√®s petite taille. Il peut √™tre utilis√© localement pour pr√©server la confidentialit√© des donn√©es, ou sur serveur pour limiter les co√ªts par rapport √† un mod√®le plus grand. </p>\n<p>Sa fen√™tre de contexte peut atteindre 128 000 jetons, ce qui permet d‚Äôanalyser de longs documents.</p>",
        "fyi": "<p>Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a √©t√© entra√Æn√© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3n 4B": {
        "desc": "<p>Tr√®s petit mod√®le multimodal et compact con√ßu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours √† un serveur - il est capable d‚Äôadapter sa puissance selon la capacit√© de la capacit√© et le besoin.</p>",
        "size_desc": "<p>Avec 4 milliards de param√®tres, il fait partie des mod√®les de tr√®s petite taille. Il peut √™tre utilis√© localement sur un ordinateur ou un smartphone pour pr√©server la confidentialit√© des donn√©es, ou sur serveur pour limiter les co√ªts par rapport √† un mod√®le plus grand.</p>\n<p>Sa fen√™tre de contexte va jusqu‚Äô√† 32 000 jetons.</p>",
        "fyi": "<p>Ce mod√®le peut traiter du texte, des images et de l‚Äôaudio. Il repose sur l‚Äôarchitecture MatFormer et un syst√®me de cache PLE (per-layer embeddings), qui active uniquement les param√®tres utiles selon la t√¢che, s'adaptant √† la capacit√© des machines sur lesquelles fonctionne le mod√®le.</p>"
      },
      "Grok 3 Mini": {
        "desc": "<p>Version plus l√©g√®re du mod√®le Grok 3, permettant de r√©duire les co√ªts tout en conservant de bonnes performances pour de nombreuses t√¢ches. Il peut simuler une phase de raisonnement avant de fournir une r√©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Malgr√© son nom, Grok 3 Mini est sans doute un tr√®s grand mod√®le, n√©cessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une g√©n√©ration plus longue et donc une consommation √©nerg√©tique plus √©lev√©e. N√©anmoins, l'architecture suppos√©e de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>",
        "fyi": "<p>Grok 3 Mini est une version distill√©e de Grok 3: il s‚Äôen approche en termes de capacit√©s, tout en √©tant plus rapide et moins co√ªteux.\nLe mod√®le propose deux modes‚ÄØ: un mode r√©flexion avec raisonnement √©tape par √©tape pour les probl√®mes complexes, et un mode rapide pour les r√©ponses imm√©diates.\nSa fen√™tre de contexte atteint 131 000 jetons, ce qui le rend adapt√© √† l‚Äôanalyse de longs documents.</p>"
      },
      "Hermes 3 405B": {
        "desc": "<p>Tr√®s grand mod√®le r√©entra√Æn√© √† partir du Llama 3.1 405B, ajust√© pour mieux r√©pondre aux demandes des utilisateurs et faciliter l‚Äôutilisation d‚Äôoutils externes.</p>",
        "size_desc": "<p>Avec 405 milliards de param√®tres, ce mod√®le fait partie des mod√®les de tr√®s grande taille. Il n√©cessite un serveur √©quip√© de plusieurs cartes graphiques puissantes, ce qui entra√Æne un co√ªt de fonctionnement important.</p>",
        "fyi": "<p>Ce mod√®le est le r√©sultat d‚Äôun r√©entra√Ænement de l‚Äôensemble des param√®tres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et syst√®me - l‚Äôutilisateur dispose ainsi d‚Äôun plus grand contr√¥le sur la ‚Äúpersonnalit√©‚Äù et comportement du mod√®le. Des fonctions de raisonnement sp√©cifiques telles que <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> ont √©t√© ajout√©es pour simuler un raisonnement sur les t√¢ches complexes. L'entra√Ænement a utilis√© un outil appel√© AdamW (vitesse d'apprentissage de 3.5√ó10‚Åª‚Å∂), qui aide le mod√®le √† apprendre de mani√®re efficace en ajustant progressivement ses param√®tres. Ensuite, il a √©t√© affin√© avec une m√©thode appel√©e DPO (direct preference optimisation), qui permet d'am√©liorer ses r√©ponses en se basant sur des pr√©f√©rences sp√©cifiques. Pour rendre cet entra√Ænement plus l√©ger et rapide, des adaptateurs LoRA ont √©t√© utilis√©s‚ÄØ; ce sont des modules plus petits qui modifient seulement une partie du mod√®le, ce qui √©vite de devoir retravailler tous les param√®tres en m√™me temps.</p>"
      },
      "Llama 3.1 405B": {
        "desc": "<p>Tr√®s grand mod√®le con√ßu pour des t√¢ches complexes ou sp√©cialis√©es. Souvent utilis√© en tant que ‚Äúmod√®le professeur‚Äù pour l‚Äôentra√Ænement de mod√®les plus sp√©cialis√©s.</p>",
        "size_desc": "<p>Avec 405 milliards de param√®tres, ce mod√®le fait partie des mod√®les de tr√®s grande taille. Il n√©cessite un serveur √©quip√© de plusieurs cartes graphiques puissantes, ce qui entra√Æne un co√ªt de fonctionnement important. Le mod√®le est dot√© d‚Äôune fen√™tre de contexte jusqu‚Äô√† 128 000 jetons, ce qui le rend int√©ressant pour des t√¢ches d‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>Le mod√®le a √©t√© entra√Æn√© sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le march√© en 2025). L'entra√Ænement a combin√© g√©n√©ration de donn√©es synth√©tiques et optimisation par pr√©f√©rences directes (DPO). Ce mod√®le est lui-m√™me souvent utilis√© pour g√©n√©rer des donn√©es synth√©tiques pour entra√Æner de plus petits mod√®les. Le mod√®le utilise par d√©faut une compression 8-bit pour r√©duire les besoins en m√©moire et permettre l'ex√©cution sur un seul serveur tr√®s puissant.</p>"
      },
      "Llama 3.1 8B": {
        "desc": "<p>Petit mod√®le con√ßu pour un usage local sur ordinateur portable, tout en offrant de bonnes capacit√©s pour la synth√®se de texte et les r√©ponses simples.</p>",
        "size_desc": "<p>Avec 8 milliards de param√®tres, ce mod√®le fait partie des petits mod√®les. Il peut √™tre utilis√© localement sur un ordinateur puissant, garantissant la confidentialit√© des donn√©es, ou h√©berg√© sur un serveur √©quip√© d‚Äôune seule carte graphique, ce qui limite les co√ªts d‚Äôinfrastructure. Sa fen√™tre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce mod√®le est une version distill√©e issue des mod√®les Llama 3 de plus grande tailles‚ÄØ: il a √©t√© entra√Æn√© gr√¢ce √† un transfert d‚Äôune partie des connaissances des plus grands mod√®les.</p>"
      },
      "Llama 3.3 70B": {
        "desc": "<p>Grand mod√®le destin√© √† un large √©ventail de t√¢ches et pouvant rivaliser avec des mod√®les plus volumineux.</p>",
        "size_desc": "<p>Avec 70 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des grands mod√®les. Il n√©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entra√Æne des co√ªts d‚Äôexploitation significatifs. Sa fen√™tre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce mod√®le est une version distill√©e issue du mod√®le 405B, auquel il doit une partie de ses connaissances transf√©r√©es. Il a aussi b√©n√©fici√© de techniques r√©centes d‚Äôalignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le mod√®le apprenait donc en essayant de r√©aliser des t√¢ches en ligne de mani√®re autonome.  Son entra√Ænement s‚Äôappuie sur 15 billions de jetons.</p>"
      },
      "Llama 4 Scout": {
        "desc": "<p>Grand mod√®le dot√© d‚Äôune tr√®s large fen√™tre de contexte, adapt√© par exemple √† la synth√®se d'un ensemble de documents.</p>",
        "size_desc": "<p>Avec 109 milliards de param√®tres, ce mod√®le se place dans la cat√©gorie des grands mod√®les. N√©anmoins, gr√¢ce √† une architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts), il peut √™tre h√©berg√© sur un serveur dot√© d‚Äôune seule carte graphique tr√®s puissante. Sa fen√™tre de contexte va jusqu‚Äô√† 10 millions de jetons, ce qui permet de traiter des corpus documentaires extr√™mement longs.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© codistill√© avec Behemoth, ce qui veut dire qu‚Äôil a appris en m√™me temps que le mod√®le g√©ant, et non apr√®s comme dans une distillation classique. Il a √©t√© entra√Æn√© sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacit√©s multimodales natives . L‚Äôarchitecture repose sur un syst√®me de mix d‚Äôexperts (MoE - Mixture of Experts), avec 17 milliards de param√®tres actifs, 16 experts et 109 milliards de param√®tres totaux. Afin d'√©quilibrer performances multimodales, raisonnement et qualit√© conversationnelle, l'√©quipe Meta a d√©velopp√© une strat√©gie de post-entra√Ænement progressive, combinant filtrage adaptatif des donn√©es (pour ne garder que les plus complexes et int√©ressantes), fine-tuning cibl√© et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le mod√®le apprenait donc en essayant de r√©aliser des t√¢ches en ligne de mani√®re autonome. Gr√¢ce √† l‚Äôarchitecture iRoPE (version optimis√©e de l‚Äôencodage positionnel), il peut g√©rer des fen√™tres de contexte tr√®s longues, jusqu‚Äô√† 10 millions de jetons et peut traiter jusqu‚Äô√† 8 images simultan√©ment. </p>\n<p>Le mod√®le a √©t√© bien re√ßu √† son lancement, notamment pour sa fen√™tre de contexte impressionnante, une premi√®re dans le domaine, ainsi que pour son rapport qualit√©-prix sur des t√¢ches comme le r√©sum√©, l‚Äôappel d‚Äôoutils et la g√©n√©ration augment√©e (RAG). Cela en fait un choix adapt√© pour les pipelines automatis√©s.</p>"
      },
      "Llama Maverick": {
        "desc": "<p>Tr√®s grand mod√®le dot√© d‚Äôune tr√®s large fen√™tre de contexte, adapt√© par exemple au r√©sum√© de plusieurs documents en m√™me temps.</p>",
        "size_desc": "<p>Avec 400 milliards de param√®tres, ce mod√®le se place dans la cat√©gorie des grands mod√®les. N√©anmoins, gr√¢ce √† une architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts), il n√©cessite moins de ressources pour fonctionner que les mod√®les ‚Äúdenses‚Äù de cette taille. Sa fen√™tre de contexte va jusqu‚Äô√† 1 millions de jetons, ce qui permet de traiter de tr√®s grands corpus documentaires.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© codistill√© avec Behemoth, ce qui veut dire qu‚Äôil a appris en m√™me temps que le mod√®le g√©ant, et non apr√®s comme dans une distillation classique. Cela permet de transf√©rer ses comp√©tences plus vite et avec moins de calcul.  Il a √©t√© entra√Æn√© sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacit√©s multimodales natives - il peut traiter jusqu‚Äô√† 8 images simultan√©ment. L‚Äôarchitecture repose sur un syst√®me de mix d‚Äôexperts (MoE - Mixture of Experts), avec 17 milliards de param√®tres actifs, 16 experts et 109 milliards de param√®tres totaux. L'√©quipe Meta a d√©velopp√© une strat√©gie de post-entra√Ænement progressive, combinant filtrage adaptatif des donn√©es - en gardant seulement les plus complexes et int√©ressantes, fine-tuning cibl√© et apprentissage par renforcement en ligne, pour √©quilibrer performances multimodales, raisonnement et qualit√© conversationnelle. Gr√¢ce √† l‚Äôarchitecture iRoPE (version optimis√©e de l‚Äôencodage positionnel), il peut g√©rer des fen√™tres de contexte tr√®s longues, jusqu‚Äô√† 10 millions de jetons. </p>\n<p>Le mod√®le Llama 4 Maverick a √©t√© pr√©sent√© comme la r√©ponse directe de Meta aux mod√®les DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estim√© qu‚Äôil ne r√©pondait pas aux attentes, en particulier sur les t√¢ches de programmation et les travaux cr√©atifs.</p>"
      },
      "Magistral Medium": {
        "desc": "<p>Mod√®le de raisonnement de taille moyenne multimodal et multilingue. Adapt√© √† des t√¢ches de programmation ou autres t√¢ches n√©cessitant analyse approfondie compr√©hension de syst√®mes logiques complexes ou planification - par exemple pour des cas d‚Äôusages agentiques ou de la r√©daction de longs contenus complexes.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de grande taille, n√©cessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les mod√®les de raisonnement requi√®rent plus de capacit√© de calcul pour produire une r√©ponse, ce qui augmente leur consommation √©nerg√©tique. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>\n<p>Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 40 000 jetons, utile pour l‚Äôanalyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce mod√®le fait partie de la premi√®re g√©n√©ration des mod√®les de raisonnement de Mistral AI (√©t√© 2025). Contrairement √† la plupart des autres mod√®les de raisonnement, ce mod√®le peut raisonner en plusieurs langues incluant l'anglais, le fran√ßais, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifi√©. Il a √©t√© entra√Æn√© avec de l‚Äôapprentissage par renforcement sur Mistral Medium 3 et n'a pas √©t√© distill√© √† partir de mod√®les de raisonnement existants. Ce mod√®le h√©rite des capacit√©s multimodales de Mistral Medium 3 m√™me si l'apprentissage par renforcement n'a √©t√© r√©alis√© que sur du texte.</p>"
      },
      "Magistral Small": {
        "desc": "<p>Mod√®le de raisonnement de taille moyenne, multimodal et multilingue. Adapt√© √† des t√¢ches n√©cessitant une analyse approfondie, compr√©hension de syst√®mes logiques ou planification - par exemple pour des cas d‚Äôusages agentiques ou de la r√©daction de longs contenus complexes.</p>",
        "size_desc": "<p>Avec 24 milliards de param√®tres, ce mod√®le est class√© parmi les mod√®les de taille moyenne. Il n√©cessite une seule carte graphiques puissante pour fonctionner. Les mod√®les de raisonnement fonctionnent √©galement plus longtemps pour produire une r√©ponse, ce qui augmente leur consommation √©nerg√©tique.</p>\n<p>Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 40 000 jetons, utile pour l‚Äôanalyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce mod√®le fait partie de la premi√®re g√©n√©ration des mod√®les de raisonnement de Mistral AI (√©t√© 2025). Contrairement √† la plupart des autres mod√®les de raisonnement, ce mod√®le peut raisonner en plusieurs langues incluant l'anglais, le fran√ßais, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifi√©. </p>\n<p>L'entra√Ænement s'est fait en deux phases. La premi√®re, dite de raisonnement <em>cold-start</em> par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au mod√®le d'acqu√©rir des capacit√©s de base en raisonnement √† partir de donn√©es d'instruction g√©n√©rale (10%). La seconde est une phase d'apprentissage par renforcement (RL, <em>renforcement learning</em>) √† haute entropie, o√π le mod√®le est encourag√© √† explorer des solutions diverses et vari√©es plut√¥t que de converger vers une seule r√©ponse, et √† g√©n√©rer des compl√©tions longues (jusqu'√† 32 000 jetons), ce qui permet de d√©velopper des capacit√©s de raisonnement qui d√©passent celles du mod√®le enseignant.</p>"
      },
      "Ministral": {
        "desc": "<p>Petit mod√®le multilingue con√ßu pour fonctionner sur un ordinateur portable sans connexion √† un serveur, tout en offrant de bonnes capacit√©s en synth√®se de texte, r√©ponses √† des questions simples et utilisation d‚Äôoutils.</p>",
        "size_desc": "<p>Avec ses 8 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des petits mod√®les (entre 7 et 20 milliards de param√®tres). Il peut √™tre d√©ploy√© localement sur un ordinateur assez puissant, garantissant la confidentialit√© des donn√©es ou h√©berg√© sur un serveur avec une seule carte graphique pour limiter les co√ªts d‚Äôinfrastructure.</p>",
        "fyi": "<p>Ce mod√®le utilise une m√©thode d'attention de requ√™te group√©e (GQA, grouped query attention) pour limiter le texte analys√© √† chaque √©tape de g√©n√©ration et gagner en vitesse et en m√©moire: les temps de calculs sont r√©duits sans incidence sur la qualit√©. Le m√©canisme d'attention est am√©lior√© en appliquant des fen√™tres de tailles diff√©rentes, ce qui permet de g√©rer de longs contextes (jusqu‚Äô√† 128 000 jetons) tout en restant l√©ger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui am√©liore ses performances sur des t√¢ches multilingues.</p>"
      },
      "Mistral Large 2": {
        "desc": "<p>Grand mod√®le pr√©vu pour traiter des questions et t√¢ches complexes‚ÄØ: par exemple g√©n√©ration de code, utilisation d‚Äôoutils, analyse de documents longs ou compr√©hension pr√©cise du langage.</p>",
        "size_desc": "<p>Avec 123 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des grands mod√®les. Il n√©cessite un serveur √©quip√© d‚Äôau moins une carte graphique puissante, ce qui implique un co√ªt de fonctionnement important. Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 128 000 jetons, utile pour l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© entra√Æn√© avec une forte proportion de donn√©es en code (plus de 80 langages de programmation) et de math√©matiques, ce qui am√©liore sa capacit√© √† r√©soudre des probl√®mes complexes et √† utiliser des outils externes.</p>"
      },
      "Mistral Medium 2506": {
        "desc": "<p>Mod√®le de taille moyenne multilingue, multimodal et peu couteux par rapport √† d‚Äôautres mod√®les qui offrent des performances similaires. Il est particuli√®rement int√©ressant pour des t√¢ches de programmation ou des t√¢ches de raisonnement, par exemple les math√©matiques.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de grande taille, n√©cessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>\n<p>Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 128 000 jetons, utile pour l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© con√ßu pour offrir des performances solides √† un co√ªt inf√©rieur √† celui des mod√®les propri√©taires ou semi-ouverts. Une attention particuli√®re a √©t√© port√©e aux donn√©es d‚Äôusage professionnel pendant son entra√Ænement. Il est particuli√®rement bon en comparaison √† d‚Äôautres mod√®les de taille similaire √† g√©n√©rer du code et r√©aliser des t√¢ches math√©matiques.</p>\n<p>Ce mod√®le a servi de base pour entra√Æner Magistral Medium - un mod√®le de raisonnement.</p>"
      },
      "Mistral Saba": {
        "desc": "<p>Mod√®le de taille moyenne con√ßu pour une compr√©hension linguistique et culturelle fine des langues du Moyen-Orient et d‚ÄôAsie du Sud, notamment l‚Äôarabe, le tamoul et le malayalam.</p>",
        "size_desc": "<p>La taille exacte du mod√®le n‚Äôest pas connue. Des indices laissent penser qu‚Äôil s‚Äôagit d‚Äôun mod√®le de taille moyenne, n√©cessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse. </p>\n<p>Le mod√®le propose une fen√™tre de contexte allant jusqu‚Äô√† 128 000 jetons, adapt√©e √† l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>L‚Äôentra√Ænement a port√© principalement sur des textes en arabe, tamoul et malayalam. Les corpus r√©gionaux ont √©t√© s√©lectionn√©s pour refl√©ter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (d√©coupage du texte en unit√©s de base que le mod√®le peut traiter), une strat√©gie sp√©cialis√©e adapt√©e aux langues √† morphologie complexe comme l'arabe a √©t√© employ√©e. Des optimisations visent √† √©viter la fragmentation excessive des mots et √† maximiser la couverture du vocabulaire.</p>"
      },
      "Mistral Small 3.2": {
        "desc": "<p>Malgr√© son nom, c‚Äôest un mod√®le de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se d√©marque par un respect pr√©cis des requ√™tes et sa capacit√© √† utiliser des outils avanc√©es.</p>",
        "size_desc": "<p>Avec 32 milliards de param√®tres, ce mod√®le est consid√©r√© comme un mod√®le de taille moyenne. Il peut √™tre h√©berg√© sur un serveur disposant d‚Äôune seule carte graphique puissante, ce qui limite les co√ªts d‚Äôinfrastructure. Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 128 000 jetons, utile pour l‚Äôanalyse de longs documents.</p>",
        "fyi": "<p>La version 3.2 de ce mod√®le est optimis√©e pour g√©n√©rer des sorties structur√©es, notamment en JSON, tout en limitant la r√©p√©titivit√© et les comportements ind√©sirables lors de longues g√©n√©rations. Multimodal, il traite √† la fois des entr√©es textuelles et des images, permettant une analyse conjointe.</p>"
      },
      "Nemotron Llama 3.1 70B": {
        "desc": "<p>Grand mod√®le entra√Æn√© √† partir de Llama 3.1 70B. Cette version r√©entra√Æn√©e (fine-tune) a tendance √† d√©tailler davantage et fournir des r√©ponses plus structur√©es.</p>",
        "size_desc": "<p>Avec 70 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des grands mod√®les. Il n√©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entra√Æne des co√ªts d‚Äôexploitation notables.</p>",
        "fyi": "<p>Ce mod√®le est issu d‚Äôun r√©entra√Ænement du Llama 3.1 70B, d'o√π la pr√©sence de son mod√®le-source dans son nom‚ÄØ! Il introduit des am√©liorations gr√¢ce √† l‚Äôapprentissage par renforcement avec retour humain (RLHF) et √† l‚Äôalgorithme REINFORCE‚ÄØ: le mod√®le explore diff√©rentes r√©ponses, re√ßoit des retours sous forme de r√©compenses, puis ajuste ses choix progressivement pour mieux r√©pondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilis√© quand on veut que le mod√®le s‚Äôadapte √† des pr√©f√©rences humaines ou qu‚Äôil optimise ses r√©ponses selon des crit√®res sp√©cifiques.</p>"
      },
      "Phi-4": {
        "desc": "<p>Petit mod√®le multilingue, capable d‚Äôutiliser des outils et performant sur des t√¢ches complexes comme la logique, les math√©matiques et le code, tout en restant compact.</p>",
        "size_desc": "<p>Avec 14 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des petits mod√®les. Il peut √™tre d√©ploy√© localement sur un ordinateur suffisamment puissant, ou h√©berg√© sur un serveur avec une seule carte graphique, ce qui r√©duit les co√ªts d‚Äôinfrastructure. La fen√™tre de contexte, de 16 000 jetons, peut √™tre limitante pour l‚Äôanalyse de documents tr√®s longs.</p>",
        "fyi": "<p>Ce mod√®le utilise tiktoken pour la tokenisation, ce qui am√©liore ses capacit√©s en contexte multilingue. Il a √©t√© entra√Æn√© sur un total de 9,8 <strong>billions</strong> de jetons, dont 400 milliards proviennent sp√©cifiquement de donn√©es synth√©tiques de haute qualit√©, le reste √©tant constitu√© de donn√©es organiques filtr√©es. L'entra√Ænement s'est d√©roul√© sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-√©valuation ‚Äì pendant laquelle le mod√®le critique et r√©√©crit ses r√©ponses ‚Äì ainsi que l'inversion des instructions ont √©t√© utilis√©es pour renforcer sa compr√©hension des consignes et ses capacit√©s de raisonnement.</p>"
      },
      "Qwen 2.5 Coder 32B": {
        "desc": "<p>Mod√®le de taille moyenne sp√©cialis√© en programmation et dans l‚Äôusage d‚Äôoutils externes (recherches web, interactions avec des logiciels‚Ä¶).</p>",
        "size_desc": "<p>Avec 32 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut fonctionner sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui limite les co√ªts d‚Äôinfrastructure.</p>\n<p>Sa fen√™tre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© entra√Æn√© sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code sp√©cialis√©s comme Haskell ou Racket. </p>\n<p>Gr√¢ce √† ses performances en code, il est  capable de bien g√©rer les appels √† des outils externes, ce qui est utile pour des usages agentiques.</p>"
      },
      "Qwen 2.5 max 0125": {
        "desc": "<p>Tr√®s grand mod√®le de raisonnement sp√©cialis√© et tr√®s performant en math√©matiques, code et r√©solution de probl√®mes logiques.</p>",
        "size_desc": "<p>Ce mod√®le propri√©taire bas√© sur une <strong>architecture MoE √† grande √©chelle a √©t√©</strong>entra√Æn√© sur <strong>plus de 20 billions de jetons</strong>. Il est con√ßu pour des t√¢ches n√©cessitant plusieurs √©tapes de r√©flexion. </p>\n<p>La fen√™tre de contexte va jusqu‚Äô√† 32 000 jetons.</p>",
        "fyi": "<p>La taille exacte du mod√®le n‚Äôest pas connue, mais c‚Äôest tr√®s probablement un tr√®s grand mod√®le n√©cessitant des serveurs √©quip√©s de plusieurs cartes graphiques. N√©anmoins, l'architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique. Les estimations de taille s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>"
      },
      "Qwen 3 30B A3B": {
        "desc": "<p>Mod√®le de taille moyenne multilingue.</p>",
        "size_desc": "<p>Avec 30 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut fonctionner sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui limite les co√ªts d‚Äôinfrastructure. De plus l'architecture de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres √† chaque jeton, limitant ainsi son empreinte √©nerg√©tique.</p>",
        "fyi": "<p>Ce mod√®le MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activ√©s par jeton, ce qui permet une inf√©rence plus rapide et plus efficace. Il utilise un syst√®me appel√© <em>global-batch</em> pour optimiser la r√©partition du travail entre les experts, afin qu'ils soient tous utilis√©s de mani√®re √©quilibr√©e.</p>\n<p>Contrairement √† d'autres mod√®les comme Qwen 2.5-MoE qui recyclent les m√™mes experts √† travers plusieurs couches du r√©seau, Qwen 3 30B A2B attribue des experts uniques √† chaque couche. Concr√®tement, cela signifie que les experts de la premi√®re couche ne sont jamais r√©utilis√©s dans les couches suivantes - chaque niveau du mod√®le dispose de son propre ensemble d'experts sp√©cialis√©s. Cette architecture permet √† chaque expert de se concentrer exclusivement sur les t√¢ches sp√©cifiques √† sa position dans le r√©seau neuronal, r√©sultant en une sp√©cialisation plus fine et des performances optimis√©es pour chaque √©tape du traitement de l'information.</p>"
      },
      "Qwen 3 32B": {
        "desc": "<p>Mod√®le de taille moyenne multilingue avec deux modalit√©s de r√©ponses: l‚Äôutilisateur peut choisir entre un mode de raisonnement, pour des r√©ponses plus approfondies, ou un mode rapide, pour g√©n√©rer directement la r√©ponse finale.</p>",
        "size_desc": "<p>Avec 32 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut fonctionner sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui limite les co√ªts d‚Äôinfrastructure.</p>\n<p>Sa fen√™tre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© entra√Æn√© sur un tr√®s grand volume de donn√©es‚ÄØ: 36 billions de jetons, en 119 langues. L'entra√Ænement s‚Äôest fait en trois √©tapes. Le mod√®le a d'abord appris √† partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont √©t√© ajout√©s pour renforcer ses connaissances factuelles. Enfin, il a √©t√© expos√© √† un corpus sp√©cifique pour l‚Äôaider √† mieux g√©rer les tr√®s longs textes. R√©sultat‚ÄØ: il dispose en fin d'entrainement d'une fen√™tre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents.</p>"
      },
      "o4 mini": {
        "desc": "<p>Tr√®s grand mod√®le de raisonnement, adapt√© pour des t√¢ches et questions scientifiques et technologiques complexes.</p>",
        "size_desc": "<p>Malgr√© son nom et le fait que la taille exacte n‚Äôest pas connue, o4 mini est tr√®s probablement un grand mod√®le n√©cessitant des serveurs √©quip√©s de plusieurs cartes graphiques. Les mod√®les de raisonnement comme o4 mini n√©cessitent plus de temps pour r√©pondre, car une phase de raisonnement pr√©c√®de la g√©n√©ration du r√©sultat final, ce qui accroit leur consommation √©nerg√©tique. N√©anmoins, l'architecture suppos√©e de m√©lange d‚Äôexperts (MoE, Mixture of Experts) n'active qu'une partie des param√®tres pour g√©n√©rer chaque jeton, limitant ainsi son empreinte √©nerg√©tique. Les estimations de taille s‚Äôappuient sur des indices indirects comme les co√ªts d'inf√©rence et la latence de r√©ponse.</p>",
        "fyi": "<p>Ce mod√®le est tr√®s performant pour l‚Äôanalyse d‚Äôimages et de graphiques. Il a aussi √©t√© entra√Æn√© pour interagir avec d‚Äôautres syst√®mes via des appels de fonctions, ce qui rend possible son utilisation pour des cas d‚Äôusage agentiques. En tant que mod√®le tr√®s puissant de raisonnement, il peut notamment √™tre utilis√© pour r√©partir des t√¢ches entre plusieurs mod√®les plus petits et/ou plus sp√©cialis√©s.  Il dispose d‚Äôune fen√™tre de contexte allant jusqu‚Äô√† 200 000 jetons, ce qui facilite l‚Äôanalyse de longs documents.</p>"
      },
      "qwq 32B": {
        "desc": "<p>Mod√®le de raisonnement de taille moyenne sp√©cialis√© et tr√®s performant en math√©matiques, g√©n√©ration de code, et r√©solution de probl√®mes logiques.</p>",
        "size_desc": "<p>Avec 32 milliards de param√®tres, ce mod√®le appartient √† la cat√©gorie des mod√®les de taille moyenne. Il peut fonctionner sur un serveur √©quip√© d‚Äôune seule carte graphique puissante, ce qui limite les co√ªts d‚Äôinfrastructure. N√©anmoins, les mod√®les de raisonnement de ce type fonctionnent plus longtemps pour produire une r√©ponse car une phase de raisonnement pr√©c√®de la g√©n√©ration du r√©sultat final, ce qui augmente la consommation √©nerg√©tique.</p>",
        "fyi": "<p>Ce mod√®le a √©t√© entra√Æn√© avec une m√©thode d‚Äôapprentissage par renforcement (RL) pour optimiser la gestion des probl√®mes de math√©matiques et des t√¢ches de programmation. Il utilise plusieurs techniques r√©centes pour am√©liorer la qualit√© des r√©ponses. Par exemple, la m√©thode RoPE (Rotary Position Embedding) lui permet de mieux comprendre l‚Äôordre des mots dans un texte. La fonction d'activation SwiGLU est une mani√®re plus efficace de g√©rer les calculs au sein du r√©seau de neurones qui aide le mod√®le √† produire des r√©ponses plus fiables. La m√©thode d'ajustement QKV (Query Key Value-biais) am√©liore la mani√®re dont le mod√®le rep√®re et s√©lectionne les informations importantes. Enfin, gr√¢ce √† la m√©thode YaRN (Yet another RoPE extensioN method), il peut traiter de tr√®s longs textes allant jusqu‚Äô√† 130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou tr√®s d√©taill√©s.</p>"
      }
    }
  }
}
