{
  "a11y": {
    "externalLink": "{text}"
  },
  "seo": {
    "title": "compar:IA, le comparateur d'IA conversationnelles",
    "desc": "compar:IA est un outil permettant de comparer Ã  lâ€™aveugle diffÃ©rents modÃ¨les d'IA conversationnelle pour sensibiliser aux enjeux de l'IA gÃ©nÃ©rative (biais, impact environmental) et constituer des jeux de donnÃ©es de prÃ©fÃ©rence en franÃ§ais.",
    "titles": {
      "home": "Accueil",
      "product": "Produit et partenaires",
      "ranking": "Classement",
      "modeles": "Liste des modÃ¨les",
      "datasets": "Jeux de donnÃ©es",
      "comparator": "Le comparateur",
      "problem": "Le probleÌ€me initial",
      "history": "Historique du projet",
      "faq": "FAQ",
      "partners": "Partenaires",
      "news": "NouveautÃ©s",
      "mentions-legales": "Mentions lÃ©gales",
      "modalites": "ModalitÃ©s dâ€™utilisation",
      "donnees-personnelles": "Politique de confidentialitÃ©",
      "accessibilite": "DÃ©claration dâ€™accessibilitÃ©",
      "arene": "Discussion",
      "share": "Mon bilan"
    }
  },
  "header": {
    "title": {
      "compar": "compar",
      "ia": "IA"
    },
    "subtitle": "Le comparateur dâ€™IAÂ conversationnelles",
    "homeTitle": "Accueil - compar:IA",
    "logoAlt": "RÃ©publique FranÃ§aise",
    "startDiscussion": "Commencer Ã  discuter",
    "help": {
      "link": {
        "title": "Donner mon avis sur le comparateur - ouvre une nouvelle fenÃªtre",
        "content": "Nous aider Ã  amÃ©liorer compar:IA"
      }
    },
    "banner": "Le comparateur est dÃ©sormais disponible en lituanien ğŸ‡±ğŸ‡¹ en suÃ©dois ğŸ‡¸ğŸ‡ª et en danois ğŸ‡©ğŸ‡° !",
    "votes": {
      "count": "{count} votes",
      "objective": "Objâ€¯: {count}",
      "legend": "LÃ©gende",
      "tooltip": "Discutez, votez et aidez-nous Ã  atteindre cet objectif !<br /><strong>Vos votes sont importants</strong> : ils alimentent le jeu de donnÃ©es compar:IA mis Ã  disposition librement pour affiner les prochains modÃ¨les sur le franÃ§ais.<br />Ce commun numÃ©rique contribue au meilleur <strong>respect de la diversitÃ© linguistique et culturelle des futurs modÃ¨les de langue.</strong>"
    },
    "chatbot": {
      "step": "Ã‰tape",
      "stepOne": {
        "title": "Que pensez-vous des rÃ©ponses ?",
        "description": "PrÃªtez attention au fond et Ã  la forme puis Ã©valuez chaque rÃ©ponse"
      },
      "stepTwo": {
        "title": "Les modÃ¨les sont dÃ©masquÃ©s !",
        "description": "DÃ©couvrez lâ€™impact environnemental de vos discussions avec chaque modÃ¨le"
      },
      "newDiscussion": "Nouvelle discussion"
    },
    "menu": "Menu"
  },
  "footer": {
    "backHome": "Retour Ã  l'accueil du site - compar:IA",
    "helpUs": "Aidez-nous Ã  amÃ©liorer ce service !",
    "writeUs": "Si vous rencontrez un problÃ¨me ou si vous avez un commentaire sur le comparateur, n'hÃ©sitez pas Ã  nous Ã©crire <a {linkProps}>via ce formulaire</a>, nous lisons tous vos messages.<br />Merci !",
    "links": {
      "legal": "Mentions lÃ©gales",
      "tos": "ModalitÃ©s d'utilisation",
      "privacy": "Politique de confidentialitÃ©",
      "accessibility": "AccessibilitÃ© : non conforme",
      "sources": "Code source"
    },
    "license": {
      "mention": "Sauf mention explicite de propriÃ©tÃ© intellectuelle dÃ©tenue par des tiers, les contenus de ce site sont proposÃ©s sous <a {linkProps}>licence etalab-2.0</a>",
      "linkTitle": "Licence etalab - nouvelle fenÃªtre"
    }
  },
  "general": {
    "legal": {
      "title": "Mentions lÃ©gales",
      "editorTitle": "Ã‰diteur",
      "editorDesc": "Ce site est Ã©ditÃ© par le MinistÃ¨re de la culture, 182, rue Saint-HonorÃ© 75001 Paris",
      "directorTitle": "Directeur de la publication",
      "directorDesc": "Monsieur Romain Delassus, chef du service du numÃ©rique du MinistÃ¨re de la Culture",
      "hostingTitle": "HÃ©bergement du site",
      "hostingDesc": "Ce site est hÃ©bergÃ© par OVH SAS (<a {linkProps}>https://www.ovh.com</a>) dont le siÃ¨ge social est situÃ© au 2 rue Kellermann - 59100 Roubaix - France.",
      "a11yTitle": "AccessibilitÃ©",
      "a11yDesc": "La conformitÃ© aux normes dâ€™accessibilitÃ© numÃ©rique est un objectif ultÃ©rieur mais nous tÃ¢chons de rendre ce site accessible Ã  toutes et Ã  tous.",
      "reportTitle": "Signaler un dysfonctionnement",
      "reportA11y": "Si vous rencontrez un dÃ©faut dâ€™accessibilitÃ© vous empÃªchant dâ€™accÃ©der Ã  un contenu ou une fonctionnalitÃ© du site, merci de nous en faire part.",
      "reportDesc": "Si vous nâ€™obtenez pas de rÃ©ponse rapide de notre part, vous Ãªtes en droit de faire parvenir vos dolÃ©ances ou une demande de saisine au DÃ©fenseur des droits.",
      "reportA11yDesc": "Pour en savoir plus sur la politique dâ€™accessibilitÃ© numÃ©rique de lâ€™Ã‰tat : <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
      "securityTitle": "SÃ©curitÃ©",
      "securityCertif": "Le site est protÃ©gÃ© par un certificat Ã©lectronique, matÃ©rialisÃ© pour la grande majoritÃ© des navigateurs par un cadenas. Cette protection participe Ã  la confidentialitÃ© des Ã©changes.",
      "securityNoMail": "En aucun cas les services associÃ©s Ã  la plateforme ne seront Ã  lâ€™origine dâ€™envoi de courriels pour demander la saisie dâ€™informations personnelles.",
      "sources": "Sauf mention contraire, tous les textes de ce site sont sous <a {etalabLinkProps}>licence Etalab Open 2.0</a>. Le code source de cette application est librement rÃ©utilisable et accessible sur <a {githubLinkProps}>GitHub</a>."
    },
    "tos": {
      "title": "ModalitÃ©s dâ€™utilisation",
      "scopeTitle": "1. Champ dâ€™application",
      "scopeDesc": "Lâ€™accÃ¨s Ã  la plateforme est gratuit, sans inscription et entraÃ®ne lâ€™application de conditions spÃ©cifiques, listÃ©es dans les prÃ©sentes modalitÃ©s dâ€™utilisation.",
      "defsTitle": "2. DÃ©finitions",
      "defsUser": "Â« Utilisateur Â» dÃ©signe toute personne physique consultant la plateforme et qui bÃ©nÃ©ficie de ses services.",
      "defsEditor": "Â« Ã‰diteur Â» dÃ©signe le Service du numÃ©rique du MinistÃ¨re de la Culture.",
      "defsPlatform": "Â« Plateforme Â» dÃ©signe le site web qui rend les services accessibles.",
      "defsModels": "Â« ModÃ¨les Â» dÃ©signe les grands modÃ¨les de langages (LLM) rÃ©utilisÃ©s dans le cadre de leur licence dâ€™utilisation par la plateforme pour rÃ©pondre Ã  ses finalitÃ©s.",
      "defsServices": "Â« Services Â» dÃ©signe les fonctionnalitÃ©s offertes par la plateforme pour rÃ©pondre Ã  ses finalitÃ©s.",
      "descTitle": "3. Description de la plateforme",
      "descEditor": "Ã‰ditÃ© par le Service du numÃ©rique du MinistÃ¨re de la Culture, le comparateur est une plateforme de comparaison des modÃ¨les conversationnels adressÃ©e au grand public dans le but (1) de sensibiliser les citoyens aux grands modÃ¨les de langage (LLMs), (2) de collecter les prÃ©fÃ©rences des utilisateurs pour constituer des jeux de donnÃ©es dâ€™alignement.",
      "descUse": "Lâ€™utilisateur ou lâ€™utilisatrice pose une question en franÃ§ais et obtient des rÃ©ponses de deux grands modÃ¨les de langages (LLM) anonymes. Il ou elle vote pour le modÃ¨le qui fournit la rÃ©ponse quâ€™il prÃ©fÃ¨re et se voit alors rÃ©vÃ©lÃ©e lâ€™identitÃ© des modÃ¨les. Ce dispositif de production participative inspirÃ© de la plateforme <a {linkProps}>Â« chatbot arena Â» (LMSYS)</a> permet de constituer des jeux de donnÃ©es de prÃ©fÃ©rences humaines sur des tÃ¢ches rÃ©elles, en franÃ§ais, utilisables pour lâ€™alignement des modÃ¨les.",
      "descDatasets": "Ces jeux de donnÃ©es seront rendus accessibles sous licence ouverte, notamment pour favoriser des usages de recherche.",
      "featuresTitle": "4. FonctionnalitÃ©s",
      "featuresDesc": "Afin de rÃ©pondre au double objectif de sensibiliser les citoyens aux grands modÃ¨les de langage et collecter les prÃ©fÃ©rences des utilisateurs et utilisatrices, les services rendus par la plateforme sans restriction dâ€™accÃ¨s sont les suivantsÂ :",
      "featuresDescMore": "Une interface humain-machine permettant de dialoguer simultanÃ©ment avec deux modÃ¨les conversationnels et de voter pour la rÃ©ponse prÃ©fÃ©rÃ©e.",
      "featuresModels": "Les modÃ¨les intÃ©grÃ©s Ã  la plateforme sont dÃ©ployÃ©s sur les serveurs dâ€™infÃ©rence des diffÃ©rents partenaires (Scaleway, OVH, Hugging Face, Google Cloud, Mistral Ai). Les conditions de standardisation dâ€™infÃ©rence sont renseignÃ©es sur la plateforme pour garantir la transparence dâ€™utilisation des modÃ¨les.",
      "featuresModelsMore": "Une interface de comparaison des modÃ¨les.",
      "featuresVote": "A lâ€™issue du parcours de vote, lâ€™utilisateur peut consulter la liste des modÃ¨les intÃ©grÃ©s au comparateur et accÃ©der Ã  une liste dâ€™informations sur ces modÃ¨les. Les informations documentant les modÃ¨les sont sourcÃ©es.",
      "featuresVoteMore": "Partage et mise Ã  disposition des jeux de donnÃ©es issus de la collecte des prÃ©fÃ©rences des utilisateurs.",
      "featuresDatasets": "Le service recueille les donnÃ©es de dialogue et de prÃ©fÃ©rence des utilisateurs. Les jeux de donnÃ©es partagÃ©s comprendront les questions de lâ€™utilisateur, les rÃ©ponses des deux modÃ¨les, le vote et les prÃ©fÃ©rences de lâ€™utilisateur.",
      "featuresDatasetsMore": "Lâ€™Ã©diteur se rÃ©serve le droit de distribuer sous licence ouverte 2.0 les donnÃ©es de dialogue et de prÃ©fÃ©rence de lâ€™utilisateur. Le jeu de donnÃ©es est diffusÃ© sur la plateforme Hugging Face Ã  travers le compte du ministÃ¨re de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "respTitle": "5. ResponsabilitÃ©s",
      "respUser": "Lâ€™utilisateur est responsable des donnÃ©es ou contenus qu'il ou elle saisit dans lâ€™invite offert par la plateforme.",
      "respLegal": "La plateforme nâ€™a pas vocation Ã  Ãªtre utilisÃ©e pour gÃ©nÃ©rer des contenus illicites ou contraires Ã  lâ€™ordre public et plus gÃ©nÃ©ralement toute gÃ©nÃ©ration contrevenant au cadre juridique en vigueur.",
      "respLegalMore": "A cet Ã©gard, lâ€™utilisateur ne saisit pas dans lâ€™invite des contenus ou informations contraires aux dispositions lÃ©gales et rÃ©glementaires en vigueur.",
      "respPrivacy": "Les donnÃ©es saisies par lâ€™utilisateur sur la plateforme ayant vocation Ã  Ãªtre mis Ã  disposition, il ou elle sâ€™engage Ã  ne pas transmettre dâ€™informations permettant de lâ€™identifier ou dâ€™identifier un tiers.",
      "respPrivacyMore": "En tout Ã©tat de cause, lâ€™Ã©diteur sâ€™engage Ã  mettre en Å“uvre les moyens permettant de sâ€™assurerde lâ€™anonymisation les donnÃ©es de dialogue avant leur mise Ã  disposition.",
      "respEditor": "De maniÃ¨re gÃ©nÃ©rale, lâ€™Ã©diteur se dÃ©gage de toute responsabilitÃ© en cas dâ€™utilisationnon-conforme aux modalitÃ©s dâ€™utilisation.",
      "licenceTitle": "6. Code et licences",
      "licenceCode": "Le code source de la plateforme est libre et disponible ici : <a {linkProps}>https://github.com/betagouv/languia</a>",
      "licenceLLM": "Les LLM utilisÃ©s pour alimenter les services sont rÃ©gis par les licences suivantes :",
      "licenceLLMModel": "ModÃ¨le dâ€™IA conversationnelle",
      "licenceLLMNoticeLink": "Lien vers la notice des modÃ¨les",
      "licenceLLMLicence": "Licence",
      "licenceLLMUnavailable": "Non disponible",
      "licenceLLMEvolution": "La liste des modÃ¨les de langage intÃ©grÃ©s Ã  la plateforme est susceptible dâ€™Ã©voluer au cours du temps et est mise Ã  jour Ã  chaque modification.",
      "dispoTitle": "7. DisponibilitÃ© des services",
      "dispoDesc": "La plateforme est accessible, sauf cas de force majeure ou dâ€™Ã©vÃ¨nement hors de contrÃ´le de son Ã©diteur.",
      "dispoRight": "Lâ€™Ã©diteur se rÃ©serve le droit de suspendre, d&#39;interrompre ou de limiter, sans avis prÃ©alable, l&#39;accÃ¨s Ã  tout ou partie des services, notamment pour des opÃ©rations de maintenance et de mises Ã  jour nÃ©cessaires au bon fonctionnement du service et des matÃ©riels affÃ©rents, ou pour toute autre raison, notamment technique.",
      "dispoWarranty": "Il nâ€™est pas garanti que le service soit exempt dâ€™anomalies ou erreurs. Le service est donc mis Ã  disposition sans garantie sur sa disponibilitÃ© et ses performances.",
      "dispoResp": "A ce titre, lâ€™Ã©diteur ne saurait Ãªtre tenu responsable des pertes ou prÃ©judices, de quelque nature quâ€™ils soient, qui pourraient Ãªtre causÃ©s Ã  la suite dâ€™un dysfonctionnement ou une indisponibilitÃ© du service. De telles situations n&#39;ouvriront droit Ã  aucune compensation financiÃ¨re.",
      "evoTitle": "8. Ã‰volution des modalitÃ©s d'utilisation",
      "evoDesc": "Les modalitÃ©s dâ€™utilisation peuvent Ãªtre modifiÃ©es ou complÃ©tÃ©es Ã  tout moment, sans prÃ©avis, en fonction des modifications apportÃ©es aux services, de lâ€™Ã©volution de la lÃ©gislation ou pour tout autre motif jugÃ© nÃ©cessaire.",
      "evoDescMore": "Ces modifications et mises Ã  jour sâ€™imposent Ã  lâ€™utilisateur ou lâ€™utilisatrice qui doit, en consÃ©quence, se rÃ©fÃ©rer rÃ©guliÃ¨rement Ã  cette rubrique pour vÃ©rifier les modalitÃ©s gÃ©nÃ©rales en vigueur.",
      "contactTitle": "9. Contact",
      "contactDesc": "Pour toute question sur le service, vous pouvez Ã©crire Ã  <a {linkProps}>contact@comparia.beta.gouv.fr</a>."
    },
    "privacy": {
      "title": "Politique de confidentialitÃ©",
      "desc": "Le service est Ã©ditÃ© par le service du numÃ©rique du ministÃ¨re de la Culture.",
      "cookiesTitle": "Cookies dÃ©posÃ©s et consentement",
      "cookiesDesc": "Ce site dÃ©pose un petit fichier texte (un Â«â€¯cookieâ€¯Â») sur votre ordinateur lorsque vous le consultez. Cela nous permet de mesurer le nombre de visites et de comprendre quelles sont les pages les plus consultÃ©es.",
      "cookiesDescMore": "Vous pouvez vous opposer au suivi de votre navigation sur ce site web. Cela protÃ©gera votre vie privÃ©e, mais empÃªchera Ã©galement le propriÃ©taire d'apprendre de vos actions et de crÃ©er une meilleure expÃ©rience pour vous et les autres utilisateurs.",
      "cookiesBannerTitle": "Ce site nâ€™affiche pas de banniÃ¨re de consentement aux cookies, pourquoiâ€¯?",
      "cookiesBannerDesc": "Câ€™est vrai, vous nâ€™avez pas eu Ã  cliquer sur un bloc qui recouvre la moitiÃ© de la page pour dire que vous Ãªtes dâ€™accord avec le dÃ©pÃ´t de cookies â€”â€¯mÃªme si vous ne savez pas ce que Ã§a veut direâ€¯!",
      "cookiesBannerNoNeed": "Rien dâ€™exceptionnel, pas de passe-droit liÃ© Ã  unâ€¯.gouv.fr. Nous respectons simplement la loi, qui dit que certains outils de suivi dâ€™audience, correctement configurÃ©s pour respecter la vie privÃ©e, sont exemptÃ©s dâ€™autorisation prÃ©alable.",
      "cookiesBannerTools": "Nous utilisons pour cela <a {matomoLinkProps}>Matomo</a>, un outil <a {libreLinkProps}>libre</a>, paramÃ©trÃ© pour Ãªtre en conformitÃ© avec la <a {cnilLinkProps}>recommandation Â«â€¯Cookiesâ€¯Â»</a> de la CNIL. Cela signifie que votre adresse IP, par exemple, est anonymisÃ©e avant dâ€™Ãªtre enregistrÃ©e. Il est donc impossible dâ€™associer vos visites sur ce site Ã  votre personne.",
      "dataAccessTitle": "Je contribue Ã  enrichir vos donnÃ©es, puis-je y accÃ©derâ€¯?",
      "dataAccessDesc": "Bien sÃ»râ€¯! Les statistiques dâ€™usage du site sont disponibles en accÃ¨s libre sur <a {linkProps}>stats.beta.gouv.fr</a>.",
      "dataAccessDatasets": "Les donnÃ©es de dialogue et de prÃ©fÃ©rence de lâ€™utilisateur sont distribuÃ©es sous la Licence Ouverte 2.0 d'Etalab sur la plateforme Hugging Face Ã  travers le compte du ministÃ¨re de la culture (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
      "privacyTitle": "Traitons-nous des donnÃ©es Ã  caractÃ¨re personnelâ€¯?",
      "privacyDesc": "Le service ne traite pas de donnÃ©es Ã  caractÃ¨re personnel telles que dÃ©finies par la CNIL, Ã  savoir toute information relative Ã  une personne physique susceptible d'Ãªtre identifiÃ©e, directement ou indirectement.",
      "privacyData": "Les donnÃ©es collectÃ©es sur le site sont les suivantesâ€¯:",
      "privacyDataArena": "DonnÃ©es relatives aux conversations des utilisateurs avec les modÃ¨lesâ€¯: questions posÃ©es par les utilisateurs, rÃ©ponses des modÃ¨les et prÃ©fÃ©rence exprimÃ©e par lâ€™utilisateur sur les deux modÃ¨les",
      "privacyDataForm": "DonnÃ©es relatives au questionnaire Â«â€¯Nous aider Ã  amÃ©liorer compar:IAâ€¯Â».",
      "privacyResp": "Lâ€™utilisateur est responsable des donnÃ©es ou contenus qu'il ou elle saisit dans lâ€™invite offert par la plateforme. En acceptant les <a {linkProps}>modalitÃ©s dâ€™utilisation</a>, lâ€™utilisateur ou lâ€™utilisatrice sâ€™engage Ã  ne pas transmettre dâ€™informations permettant de lâ€™identifier ou dâ€™identifier un tiers.",
      "dataUseTitle": "Quels sont les traitements rÃ©alisÃ©s sur les donnÃ©es de conversationâ€¯?",
      "dataUseDesc": "En tout Ã©tat de cause, lâ€™Ã©diteur sâ€™engage Ã  mettre en Å“uvre les moyens permettant de sâ€™assurer de lâ€™anonymisation des donnÃ©es de dialogue avant leur mise Ã  disposition publique.",
      "dataTimeTitle": "Pendant combien de temps conservons-nous ces donnÃ©esâ€¯?",
      "dataTimeDesc": "Les donnÃ©es relatives aux utilisateurs et Ã  leurs conversations avec les modÃ¨les sont conservÃ©es Ã  compter de lâ€™enregistrement du vote de prÃ©fÃ©rence.",
      "dataRespTitle": "Qui est responsable du traitement des donnÃ©esâ€¯?",
      "dataRespDesc": "Le service du numÃ©rique du ministÃ¨re de la Culture est le responsable du traitement de vos donnÃ©es Ã  caractÃ¨re personnel.",
      "dataExtraTitle": "Qui nous aide Ã  traiter les donnÃ©esâ€¯?",
      "dataExtraHost": "Sous-traitantâ€¯: OVH",
      "dataExtraCountry": "Pays destinataireâ€¯: France",
      "dataExtraWhat": "Traitement rÃ©alisÃ©â€¯: HÃ©bergement",
      "dataExtraWarranty": "Garantiesâ€¯: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>"
    },
    "a11y": {
      "disclaimer": "<strong>compar:IA</strong> sâ€™engage Ã  rendre ses services numÃ©riques accessibles, conformÃ©ment Ã  lâ€™article 47 de la loi nÂ° 2005-102 du 11 fÃ©vrier 2005.",
      "title": "DÃ©claration dâ€™accessibilitÃ©",
      "desc": "Cette dÃ©claration dâ€™accessibilitÃ© sâ€™applique au site <strong>comparia.beta.gouv.fr</strong>.",
      "stateTitle": "Ã‰tat de conformitÃ©",
      "stateDesc": "Le site comparia.beta.gouv.fr est non conforme avec le RGAA 4.1. Le site nâ€™a <strong>pas encore Ã©tÃ© auditÃ©</strong>. Il a cependant Ã©tÃ© conÃ§u pour Ãªtre accessible au plus grand nombre. Vous devriez donc pouvoirâ€¯:",
      "stateNavigate": "naviguer sur toutes les pages du site en utilisant un clavier",
      "stateScreenReader": "consulter le site web avec un lecteur dâ€™Ã©cran.",
      "statePrefs": "adapter le site Ã  votre prÃ©fÃ©rences (taille de la police, zoom Ã©cran, changement de typographieâ€¦) sans perte de contenu",
      "improveTitle": "AmÃ©lioration et contact",
      "improveDesc": "Si vous nâ€™arrivez pas Ã  accÃ©der Ã  un contenu ou Ã  un service, vous pouvez contacter le responsable de beta.gouv.fr pour Ãªtre orientÃ©Â·e vers une alternative accessible ou obtenir le contenu sous une autre forme.",
      "improveMail": "E-mailâ€¯: <a {linkProps}>contact@beta.gouv.fr</a>",
      "improveAdress": "Adresseâ€¯: DINUM, 20 avenue de SÃ©gur 75007 Paris",
      "improveDelay": "Nous essayons de rÃ©pondre dans les 2 jours ouvrÃ©s.",
      "remedyTitle": "Voie de recours",
      "remedyDesc": "Cette procÃ©dure est Ã  utiliser dans le cas suivantâ€¯: vous avez signalÃ© au responsable du site internet un dÃ©faut dâ€™accessibilitÃ© qui vous empÃªche dâ€™accÃ©der Ã  un contenu ou Ã  un des services du portail et vous nâ€™avez pas obtenu de rÃ©ponse satisfaisante.",
      "remedyList": "Vous pouvezâ€¯:",
      "remedyAdvocate": "Ã‰crire un message au <a {linkProps}>DÃ©fenseur des droits</a>",
      "remedyDelegateAdvocate": "Contacter le dÃ©lÃ©guÃ© du <a {linkProps}>DÃ©fenseur des droits dans votre rÃ©gion</a>",
      "remedyAdvocateAdress": "Envoyer un courrier par la poste (gratuit, ne pas mettre de timbre)â€¯: DÃ©fenseur des droits - Libre rÃ©ponse 71120 75342 Paris CEDEX 07"
    }
  },
  "welcome": {
    "title": "Bienvenue dans compar:IA !",
    "goodPractices": "Voici quelques bonnes pratiquesâ€¯:",
    "errors": "Les IA peuvent faire des erreursâ€¯: nous vous encourageons Ã  vÃ©rifier les informations communiquÃ©es",
    "privacy": "Ne communiquez pas d'informations personnelles comme votre nom, prÃ©nom ou adresse",
    "use": "N'utilisez pas le comparateur Ã  des fins illÃ©gales ou nuisibles",
    "go": "C'est parti"
  },
  "home": {
    "intro": {
      "title": "Ne vous fiez pas aux rÃ©ponses <span {props}>dâ€™une seule IA</span>",
      "desc": "Discutez avec deux IA Ã  lâ€™aveugle et Ã©valuez leurs rÃ©ponses",
      "tos": {
        "accept": "J'accepte les <a {linkProps}>conditions gÃ©nÃ©rales dâ€™utilisation</a>",
        "help": "Les donnÃ©es sont partagÃ©es Ã  des fins de recherche",
        "error": "Vous devez accepter les modalitÃ©s d'utilisation pour continuer"
      },
      "steps": {
        "title": "Comment Ã§a marche",
        "a11yDesc": "1. Je discute avec deux IA anonymes : Ã©changez aussi longtemps que vous le souhaitez. 2. Je donne mon avis : vous contribuez ainsi Ã  l'amÃ©lioration des modÃ¨les dâ€™IA. 3. Les modÃ¨les sont dÃ©masquÃ©s : apprenez-en plus sur les modÃ¨les dâ€™IA et leurs caractÃ©ristiques.",
        "one": {
          "title": "Je discute avec deux IA anonymes",
          "a": "Ã‰changez aussi longtemps que",
          "b": "vous le souhaitez"
        },
        "two": {
          "title": "Je donne mon avis",
          "a": "Vous contribuez ainsi Ã ",
          "b": "l'amÃ©lioration des modÃ¨les dâ€™IA"
        },
        "three": {
          "title": "Les modÃ¨les sont dÃ©masquÃ©sâ€¯!",
          "a": "Apprenez en plus sur les modÃ¨les",
          "b": "dâ€™IA et leurs caractÃ©ristiques"
        }
      }
    },
    "use": {
      "title": "Ã€ quoi sert compar:IAâ€¯?",
      "desc": "compar:IA est un outil gratuit qui permet de sensibiliser les citoyens Ã  lâ€™IA gÃ©nÃ©rative et Ã  ses enjeux",
      "compare": {
        "title": "Comparer les rÃ©ponses de diffÃ©rents modÃ¨les dâ€™IA",
        "desc": "Discutez et dÃ©veloppez votre esprit critique en donnant votre prÃ©fÃ©rence",
        "alt": "Comparer"
      },
      "test": {
        "title": "Tester au mÃªme endroit les derniÃ¨res IA de lâ€™Ã©cosystÃ¨me",
        "desc": "Testez diffÃ©rents modÃ¨les, propriÃ©taires ou non, de petites et grandes tailles",
        "alt": "Tester"
      },
      "measure": {
        "title": "Mesurer lâ€™empreinte Ã©cologique des questions posÃ©es aux IA",
        "desc": "DÃ©couvrez lâ€™impact environnemental de vos discussions avec chaque modÃ¨le",
        "alt": "Mesurer"
      }
    },
    "europe": {
      "title": "Le comparateur <span {props}>devient europÃ©enâ€¯!</span>",
      "desc": "La Lituanie, la SuÃ¨de et le Danemark rejoignent la France en adoptant le comparateur dans le but dâ€™affiner les futurs modÃ¨les dâ€™IA dans leurs langues nationales.",
      "question": "Vous souhaitez Ã©galement disposer du comparateur dans votre langueâ€¯?",
      "languages": {
        "da": "en danois",
        "fr": "en franÃ§ais",
        "lt": "en lituanien",
        "sv": "en suÃ©dois"
      }
    },
    "vote": {
      "title": "Pourquoi votre vote est-il importantâ€¯?",
      "desc": "Votre prÃ©fÃ©rence enrichit les jeux de donnÃ©es compar:IA dont lâ€™objectif est dâ€™affiner les futurs modÃ¨les dâ€™IA sur le franÃ§ais, le suÃ©dois, le lituanien et le danois",
      "steps": {
        "prefs": {
          "title": "Vos prÃ©fÃ©rences",
          "desc": "AprÃ¨s discussion avec les IA, vous indiquez votre prÃ©fÃ©rence pour un modÃ¨le selon des critÃ¨res donnÃ©s, tels que la pertinence ou lâ€™utilitÃ© des rÃ©ponses."
        },
        "datasets": {
          "title": "Les jeux de donnÃ©es par langue",
          "desc": "Toutes les questions posÃ©es et les votes sont compilÃ©es dans des jeux de donnÃ©es et publiÃ©s librement aprÃ¨s anonmymisation."
        },
        "finetune": {
          "title": "Des modÃ¨les affinÃ©s sur la langue spÃ©cifique",
          "desc": "A terme, les acteurs industriels et acadÃ©miques peuvent exploiter les jeux de donnÃ©es pour entrainer de nouveaux modÃ¨les plus respectueux de la diversitÃ© linguistique et culturelle."
        }
      },
      "datasetAccess": "AccÃ©der aux jeux de donnÃ©es"
    },
    "usage": {
      "title": "Les usages spÃ©cifiques de compar:IA",
      "desc": "Lâ€™outil sâ€™adresse Ã©galement aux experts IA et aux formateurs pour des usages plus spÃ©cifiques",
      "use": {
        "title": "Exploiter les donnÃ©es",
        "desc": "DÃ©veloppeurs, chercheurs, Ã©diteurs de modÃ¨lesâ€¦ accÃ©dez aux jeux de donnÃ©es compar:IA pour amÃ©liorer les modÃ¨les"
      },
      "explore": {
        "title": "Explorer les modÃ¨les",
        "desc": "Consultez au mÃªme endroit toutes les caractÃ©ristiques et conditions dâ€™utilisation des modÃ¨les"
      },
      "educate": {
        "title": "Former et sensibiliser",
        "desc": "Utilisez le comparateur comme un support pÃ©dagogique de sensibilisation Ã  lâ€™IA auprÃ¨s de votre public"
      }
    },
    "origin": {
      "team": {
        "title": "Qui sommes-nousâ€¯?",
        "desc": "Le comparateur est portÃ© au sein du MinistÃ¨re de la Culture par une Ã©quipe pluridisciplinaire rÃ©unissant expert en Intelligence artificielle, dÃ©veloppeurs, chargÃ© de dÃ©ploiement, designer, avec pour mission de rendre les IA conversationnelles plus transparentes et accessibles Ã  toutes et tous."
      },
      "project": {
        "title": "Qui est Ã  lâ€™origine du projetâ€¯?",
        "desc": "Le comparateur a Ã©tÃ© conÃ§u et dÃ©veloppÃ© dans le cadre dâ€™une start-up dâ€™Etat portÃ©e par le ministÃ¨re de la Culture et intÃ©grÃ©e au programme <a {linkProps}>Beta.gouv.fr</a> de la Direction interministÃ©rielle du numÃ©rique (DINUM) qui aide les administrations publiques franÃ§aises Ã  construire des services numÃ©riques utiles, simples et faciles Ã  utiliser."
      }
    },
    "faq": {
      "title": "Vos questions les plus courantes",
      "discover": "DÃ©couvrir les autres questions"
    }
  },
  "ranking": {
    "title": "Classement des modÃ¨les",
    "desc": "DÃ©couvrez comment les meilleurs modÃ¨les dâ€™IA se positionnent Ã  travers leur <strong>score de satisfaction</strong> issus des votes de la communautÃ© compar:IA. Pour en savoir plus, consultez <a {linkProps}>notre mÃ©thodologie de classement</a>.",
    "table": {
      "search": "Rechercher un modÃ¨le",
      "lastUpdate": "Mise Ã  jour le {date}",
      "totalModels": "Total modÃ¨lesâ€¯:",
      "totalVotes": "Total votesâ€¯:",
      "data": {
        "cols": {
          "rank": "Rang",
          "name": "ModÃ¨le",
          "elo": "Score satisfaction",
          "trust": "Confiance",
          "votes": "Total votes",
          "consumption": "Ã‰nergie<br>(1000 tokens)",
          "size": "Taille<br>(paramÃ¨tres actifs)",
          "release": "Date sortie",
          "organisation": "Organisation",
          "license": "Licence"
        }
      }
    }
  },
  "product": {
    "title": "Tout savoir sur le comparateur",
    "comparator": {
      "title": "Le comparateur permet de crÃ©er des <span {props}>jeux de donnÃ©es</span> de prÃ©fÃ©rence centrÃ©s sur des <span {props}>usages rÃ©els</span> exprimÃ©s dans les <span {props}>langues europÃ©ennes</span>.",
      "cta": "AccÃ©der au comparateur",
      "challenges": {
        "title": "Lâ€™application dÃ©veloppÃ©e rÃ©pond Ã  plusieurs enjeux",
        "bias": {
          "title": "Biais culturels et linguistiques",
          "desc": "Mettre en avant les biais de l'IA liÃ©s Ã  la sous-reprÃ©sentation des donnÃ©es non anglophones dans les modÃ¨les et sensibiliser Ã  leurs consÃ©quences."
        },
        "impacts": {
          "title": "Impact environnemental",
          "desc": "RÃ©vÃ©ler les effets Ã©cologiques de l'IA gÃ©nÃ©rative, encore largement mÃ©connus du grand public."
        },
        "pluralism": {
          "title": "Pluralisme des modÃ¨les",
          "desc": "Assurer aux citoyens l'accÃ¨s Ã  une diversitÃ© de modÃ¨les d'IA afin qu'ils puissent faire des choix Ã©clairÃ©s et dÃ©velopper un regard critique sur ces technologies."
        },
        "thinking": {
          "title": "Esprit critique et questions sociÃ©tales",
          "desc": "Inciter au questionnement critique sur la place de lâ€™IA gÃ©nÃ©rative dans les pratiques personnelles et professionnelles (Ã©ducation, travail)."
        }
      },
      "europe": {
        "title": "Le comparateur <span {props}>devient europÃ©en</span>â€¯!",
        "adventure": "Depuis lâ€™Ã©tÃ© 2025, la Lituanie, la SuÃ¨de et le Danemark rejoignent lâ€™aventureâ€¯!",
        "desc": "Le comparateur est mis Ã  disposition de leurs citoyens dans leurs langues nationales. Lâ€™objectif est de crÃ©er des jeux de donnÃ©es de prÃ©fÃ©rence afin dâ€™amÃ©liorer les futurs modÃ¨les dâ€™IA dans ces langues europÃ©ennes.",
        "catch": "Vous souhaitez disposer du comparateur dans votre langueâ€¯?"
      }
    },
    "problem": {
      "title": "Les modÃ¨les dâ€™IA conversationnelles respectent-ils la <span {props}>diversitÃ©</span> des langues europÃ©ennesâ€¯?",
      "diversity": {
        "stereotypes": {
          "title": "RÃ©ponses stÃ©rÃ©otypÃ©es",
          "desc": "Les systÃ¨mes dâ€™IA conversationnelle donnent lâ€™impression de parler toutes les langues mais les rÃ©sultats quâ€™ils gÃ©nÃ¨rent sont parfois stÃ©rÃ©otypÃ©s ou discriminants."
        },
        "english": {
          "title": "DonnÃ©es dâ€™entrainement majoritairement en anglais",
          "desc": "Les IA conversationnelles reposent sur des grands modÃ¨les de langage (LLM) entraÃ®nÃ©s principalement sur des donnÃ©es en anglais, ce qui crÃ©e des biais linguistiques et culturels dans les rÃ©sultats qu'ils produisent."
        },
        "diversity": {
          "title": "DiversitÃ©s culturelles et linguistiques nÃ©gligÃ©es",
          "desc": "Ces biais peuvent aussi se traduire par des rÃ©ponses partielles voire incorrectes nÃ©gligeant la diversitÃ© des langues et des cultures, notamment europÃ©ennes."
        }
      },
      "alignment": {
        "title": "Comment rÃ©duire les biais culturels et linguistiques de ces modÃ¨lesâ€¯?",
        "desc": "L'alignementâ€¯: une technique de rÃ©duction des biais qui repose sur la collecte des prÃ©fÃ©rences dâ€™utilisateurs",
        "alignment": {
          "title": "Lâ€™alignement, Ã©tape dÃ©cisive dâ€™instruction du modÃ¨le",
          "a": "L'alignement intervient aprÃ¨s l'Ã©tape de prÃ©-entraÃ®nement d'un modÃ¨le de langage, comme une Ã©tape de Â«Â finitionâ€¯Â» ou de Â«Â polissageâ€¯Â». Lors de son prÃ©-entrainement, le modÃ¨le apprend Ã  prÃ©dire le mot suivant et devient capable de gÃ©nÃ©rer du texte cohÃ©rent.",
          "b": "Lâ€™Ã©tape dâ€™alignement consiste Ã  apprendre au modÃ¨le Ã  mieux rÃ©pondre aux besoins humains, câ€™est Ã  dire Ã  le rendre plus <strong>pertinent</strong> (le modÃ¨le rÃ©pond Â«â€¯mieuxâ€¯Â» aux questions), <strong>honnÃªte</strong> (capacitÃ© Ã  assumer Â«Â quâ€™il ne sait pas rÃ©pondreâ€¯Â» quand il nâ€™y a pas suffisamment de donnÃ©es), et <strong>inoffensif</strong> (Ã©viter de gÃ©nÃ©rer des contenus dangereux ou inappropriÃ©s).",
          "c": "<strong>Sans alignement, un LLM pourrait Ãªtre techniquement compÃ©tent mais difficile Ã  utiliser en pratique, car il ne comprendrait pas vraiment ce qu'on attend de lui dans une conversation.</strong>"
        },
        "datasets": {
          "title": "Des jeux de donnÃ©es spÃ©cifiques",
          "a": "L'alignement utilise des donnÃ©es trÃ¨s spÃ©cifiques, spÃ©cialement crÃ©Ã©es pour enseigner au modÃ¨le comment Â«â€¯bienâ€¯Â» se comporter.",
          "b": "Les <strong>donnÃ©es de prÃ©fÃ©rence</strong> constituent un type particulier de donnÃ©es dâ€™alignement, aux cÃ´tÃ©s des <strong>donnÃ©es de dÃ©monstration</strong> (exemples de conversations entre humains et assistants IA, rÃ©digÃ©es par des annotateurs experts selon des consignes prÃ©cises de ton et de style), des <strong>donnÃ©es de sÃ©curitÃ©</strong> (exemples spÃ©cifiques enseignant au modÃ¨le Ã  Ã©viter les contenus dangereux en montrant comment refuser les demandes problÃ©matiques) ou des <strong>donnÃ©es spÃ©cialisÃ©es</strong> couvrant des domaines spÃ©cifiques (mÃ©decine, droit, Ã©ducationâ€¦).",
          "c": "Les donnÃ©es de prÃ©fÃ©rence prÃ©sentent plusieurs rÃ©ponses possibles Ã  une mÃªme question, classÃ©es par ordre de qualitÃ© par des Ã©valuateurs humains: les utilisateurs indiquent quelle rÃ©ponse est la meilleure selon des critÃ¨res donnÃ©s, telles que la pertinence, lâ€™utilitÃ©, la nocivitÃ©. Une fois constituÃ©s, ces jeux de donnÃ©es sont utilisÃ©s pour entrainer les modÃ¨les en les ajustant selon les prÃ©fÃ©rences exprimÃ©es par les utilisateurs."
        },
        "english": {
          "title": "Peu de donnÃ©es de prÃ©fÃ©rence en langues europÃ©ennes",
          "a": "Les donnÃ©es de prÃ©fÃ©rence sont couteuses Ã  produire car <strong>elles nÃ©cessitent du travail humain qualifiÃ© pour chaque exemple</strong>. Des plateformes telles que https://chat.lmsys.org/ permettent de constituer ces jeux de donnÃ©es de prÃ©fÃ©rence mais peu dâ€™utilisateurs sâ€™en servent dans leur langue dâ€™origine.",
          "b": "Les jeux de donnÃ©es de prÃ©fÃ©rence sont rares, voire inexistants dans les langues europÃ©ennes. La part des questions posÃ©es en franÃ§ais dans le jeu de donnÃ©es de LMSYS est par exemple infÃ©rieure Ã  1%.",
          "c": "comparIA est un exemple de dispositif permettant de collecter des conversations dans de multiples langues, incluant des rÃ©fÃ©rences culturelles spÃ©cifiques Ã  chaque rÃ©gion ou paysâ€¯: tÃ¢ches courantes, traditions culinaires locales, systÃ¨mes Ã©ducatifs, rÃ©fÃ©rences historiques ou littÃ©raires, etc."
        },
        "diversity": {
          "title": "Diversifier les donnÃ©es pour rÃ©duire les biais",
          "a": "Pour reflÃ©ter la diversitÃ© des cultures et des langues dans les rÃ©sultats gÃ©nÃ©rÃ©s par les modÃ¨les, <strong>les jeux de donnÃ©es dâ€™alignement doivent inclure une variÃ©tÃ© de langues</strong>, de contextes et dâ€™exemples issus de tÃ¢ches courantes des utilisateurs. La diversification des donnÃ©es d'alignement permet dâ€™amÃ©iorer Ã  terme les performances dâ€™un modÃ¨le Ã  double titreâ€¯:",
          "b": "D'une part, elle <strong>rÃ©duit les biais culturels</strong> en Ã©vitant qu'une seule perspective - souvent anglo-saxonne - domine les rÃ©ponses de l'IA. Le modÃ¨le apprend ainsi Ã  reconnaÃ®tre qu'il existe plusieurs faÃ§ons valides d'aborder une mÃªme question selon le contexte culturel.",
          "c": "D'autre part, cette exposition Ã  la diversitÃ© de langues et de cultures favorise lâ€™adaptation des rÃ©ponses Ã  des contextes spÃ©cifiques: un utilisateur franÃ§ais recevra des conseils adaptÃ©s au systÃ¨me franÃ§ais, tandis qu'un utilisateur danois obtiendra des informations correspondant Ã  son contexte national.",
          "d": "Le rÃ©sultat est un modÃ¨le dâ€™IA conversationnelle plus inclusif, capable de tenir compte des diffÃ©rentes cultures."
        }
      }
    },
    "partners": {
      "institution": {
        "title": "Partenaires institutionnels"
      },
      "diffusion": {
        "title": "Partenaires de diffusion",
        "desc": "Nous crÃ©ons un rÃ©seau de partenaires intÃ©grant le comparateur dans leur offre de services et de formation.",
        "catch": "Vous souhaitez utiliser le comparateur pour rÃ©pondre Ã  un besoin mÃ©tierâ€¯?",
        "cta": "Dites nous en plus"
      },
      "academy": {
        "title": "Partenaires acadÃ©miques",
        "desc": "Nous avons Ã  coeur que les jeux de donnÃ©es gÃ©nÃ©rÃ©s alimentent des travaux de recherche multidisciplinaires mÃªlant sciences humaines et sociales et data science.",
        "catch": "Vous menez un projet de recherche et avez des suggestions ou besoin de prÃ©cision sur la dÃ©marche et/ou les jeux de donnÃ©es produitsâ€¯?"
      },
      "services": {
        "title": "Services mis Ã  contribution",
        "desc": "Les calculs dâ€™impacts environnementaux reposent sur les produits ci dessus."
      }
    }
  },
  "datasets": {
    "access": {
      "title": "AccÃ©dez aux jeux de donnÃ©es compar:IA",
      "desc": "Les questions et prÃ©fÃ©rences posÃ©es sur la plateforme sont majoritairement en franÃ§ais et reflÃ¨tent des usages rÃ©els et non contraints. Ces jeux de donnÃ©es sont accessibles sur <a {linkProps}>data.gouv</a> et Hugging Face.",
      "catch": "Editeurs de modÃ¨les, chercheurs, chercheuses, entreprises, Ã  vous de jouer !",
      "share": "Partagez-nous vos rÃ©utilisations",
      "repos": {
        "conversations": {
          "title": "/conversations",
          "desc": "Ensemble des rÃ©ponses et des questions posÃ©es"
        },
        "reactions": {
          "title": "/rÃ©actions",
          "desc": "Ensemble des rÃ©actions exprimÃ©es"
        },
        "votes": {
          "title": "/votes",
          "desc": "Ensemble des prÃ©fÃ©rences exprimÃ©es"
        }
      }
    },
    "reuse": {
      "title": "Comment ces donnÃ©es sont-elles utilisÃ©es ?",
      "desc": "Exemples de rÃ©utilisation des jeux de donnÃ©es compar:IA",
      "bunka": {
        "desc": "L'Ã©quipe Bunka.ai a menÃ© une Ã©tude approfondie sur les interactions entre les utilisateurs de la plateforme Compar:IA et les modÃ¨les d'IA, examinant les thÃ©matiques privilÃ©giÃ©es, les tÃ¢ches principales et dÃ©terminant si ces modÃ¨les fonctionnent avant tout comme des outils d'automatisation ou d'augmentation des capacitÃ©s humaines. Cette analyse repose sur un large Ã©chantillon de 25 000 conversations.",
        "conversations": {
          "title": "Explorer la visualisation de donnÃ©es",
          "desc": "Visualisation interactive des conversations oÃ¹ chaque point reprÃ©sente un cluster de discussions Ã©voquÃ© par les utilisateurs (comme lâ€™Ã©ducation, la santÃ©, lâ€™environnement, ou encore la philosophie)."
        },
        "analyze": {
          "title": "AccÃ©der Ã  lâ€™analyse par indicateur",
          "desc": "Analyse des conversations des utilisateurs avec dÃ©tection des tÃ¢ches (crÃ©ation, recherche d'informations...), des sujets (arts et culture, Ã©ducation...), des Ã©motions complexes (curiositÃ©, enthousiasme...), des types de langage (formel, professionnel...)"
        },
        "method": "En savoir plus sur la mÃ©thodologie"
      }
    }
  },
  "arenaHome": {
    "title": "Comment puis-je vous aider aujourd'hui ?",
    "modelSelection": "SÃ©lection des modÃ¨les",
    "prompt": {
      "label": "Ã‰crivez votre premier message",
      "placeholder": "Ã‰crivez votre premier message ici"
    },
    "selectModels": {
      "question": "Quels modÃ¨les voulez-vous comparer ?",
      "help": "SÃ©lectionnez le mode de comparaison qui vous convient"
    },
    "compareModels": {
      "question": "Quels modÃ¨les voulez-vous comparer ?",
      "count": "{count}/2 modÃ¨les",
      "help": "Si vous nâ€™en choisissez quâ€™un, le second sera sÃ©lectionnÃ© de maniÃ¨re alÃ©atoire"
    },
    "suggestions": {
      "title": "Suggestions de prompts",
      "generateAnother": "GÃ©nÃ©rer un autre message",
      "choices": {
        "iasummit": {
          "iconAlt": "Sommet pour l'action sur l'IA",
          "title": "Prompts issus de la consultation citoyenne sur lâ€™IAÂ ",
          "tooltip": "Ces questions sont issues de la consultation citoyenne sur lâ€™IA qui a lieu du 16/09/2024 au 08/11/2024. Elle visait Ã  associer largement les citoyens et la sociÃ©tÃ© civile au Sommet international pour lâ€™action sur lâ€™IA, en collectant leurs idÃ©es pour faire de lâ€™intelligence artificielle une opportunitÃ© pour toutes et tous, mais aussi de nous prÃ©munir ensemble contre tout usage inappropriÃ© ou abusif de ces technologies."
        },
        "ideas": {
          "iconAlt": "IdÃ©es",
          "title": "GÃ©nÃ©rer de nouvelles idÃ©es"
        },
        "explanations": {
          "iconAlt": "Explications",
          "title": "Expliquer simplement un concept"
        },
        "languages": {
          "iconAlt": "Traduction",
          "title": "Mâ€™exprimer dans une autre langue"
        },
        "administrative": {
          "iconAlt": "Administratif",
          "title": "RÃ©diger un document administratif"
        },
        "recipes": {
          "iconAlt": "Recettes",
          "title": "DÃ©couvrir une nouvelle recette de cuisine"
        },
        "coach": {
          "iconAlt": "Conseils",
          "title": "Obtenir des conseils sur lâ€™alimentation et le sport"
        },
        "stories": {
          "iconAlt": "Histoires",
          "title": "Raconter une histoire"
        },
        "recommendations": {
          "iconAlt": "Recommandations",
          "title": "Proposer des idÃ©es de films, livres, musiques"
        }
      }
    }
  },
  "chatbot": {
    "continuePrompt": "Continuer Ã  discuter avec les deux modÃ¨les d'IA",
    "revealButton": "Passer Ã  la rÃ©vÃ©lation des modÃ¨les",
    "conversation": "Conversation",
    "errors": {
      "tooLong": {
        "title": "Oups, la conversation est trop longue pour un des modÃ¨les.",
        "message": "Chaque modÃ¨le est limitÃ© dans la taille des conversations qu'il est capable de traiter.",
        "vote": "Vous pouvez tout de mÃªme donner votre avis sur ces modÃ¨les ou recommencer une conversation avec deux nouveaux.",
        "retry": "Vous pouvez recommencer une conversation avec deux nouveaux modÃ¨les."
      },
      "other": {
        "title": "Oups, erreur temporaire",
        "message": "Une erreur temporaire est survenue.",
        "vote": "Ou bien conclure votre expÃ©rience en donnant votre avis sur les modÃ¨les.",
        "retry": "Vous pouvez tenter de rÃ©essayer de solliciter les modÃ¨les."
      }
    },
    "loading": "Chargement des rÃ©ponses"
  },
  "closeModal": "Fermer la fenÃªtre modale",
  "models": {
    "conditions": "Conditions d'utilisation",
    "licenses": {
      "type": {
        "proprietary": "PropriÃ©taire",
        "openSource": "Open source",
        "semiOpen": "Semi-ouvert"
      },
      "name": "Licence {licence}",
      "commercial": "Licence commerciale",
      "noDesc": "Les informations de licence n'ont pas Ã©tÃ© remplies pour ce modÃ¨le.",
      "descriptions": {
        "MIT": "La licence MIT est une licence de logiciel libre permissive : elle permet Ã  quiconque de rÃ©utiliser, modifier et distribuer le modÃ¨le, mÃªme Ã  des fins commerciales, sous rÃ©serve d'inclure la licence d'origine et les mentions de droits d'auteur.",
        "Apache 2.0": "Cette licence permet d'utiliser, modifier et distribuer librement, mÃªme Ã  des fins commerciales. Outre la libertÃ© dâ€™utilisation, elle garantit la protection juridique en incluant une clause de non-atteinte aux brevets et la transparence : toutes les modifications doivent Ãªtre documentÃ©es et sont donc traÃ§ables.",
        "Gemma": "Cette licence est conÃ§ue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiÃ©es ou amÃ©liorÃ©es doivent Ãªtre partagÃ©e avec la communautÃ© sous la mÃªme licence, favorisant ainsi la collaboration et la transparence dans le dÃ©veloppement logiciel.",
        "Llama 3 Community": "Cette licence permet d'utiliser, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opÃ©rations dÃ©passant 700 millions d'utilisateurs mensuels et interdit la rÃ©utilisation du code ou des contenus gÃ©nÃ©rÃ©s pour lâ€™entraÃ®nement ou l'amÃ©lioration de modÃ¨les concurrents, protÃ©geant ainsi les investissements technologiques et la marque de Meta.",
        "Llama 3.1": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opÃ©rations dÃ©passant 700 millions d'utilisateurs mensuels. La rÃ©utilisation du code ou des contenus gÃ©nÃ©rÃ©s pour lâ€™entraÃ®nement ou l'amÃ©lioration de modÃ¨les dÃ©rivÃ©s est autorisÃ©e Ã  condition dâ€™afficher â€œbuilt with llamaâ€ et dâ€™inclure â€œLlamaâ€ dans leur nom pour toute distribution.",
        "Llama 3.3": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opÃ©rations dÃ©passant 700 millions d'utilisateurs mensuels. La rÃ©utilisation du code ou des contenus gÃ©nÃ©rÃ©s pour lâ€™entraÃ®nement ou l'amÃ©lioration de modÃ¨les dÃ©rivÃ©s est autorisÃ©e Ã  condition dâ€™afficher â€œbuilt with llamaâ€ et dâ€™inclure â€œLlamaâ€ dans leur nom pour toute distribution.",
        "Llama 4": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opÃ©rations dÃ©passant 700 millions d'utilisateurs mensuels. La rÃ©utilisation du code ou des contenus gÃ©nÃ©rÃ©s pour lâ€™entraÃ®nement ou l'amÃ©lioration de modÃ¨les dÃ©rivÃ©s est autorisÃ©e Ã  condition dâ€™afficher â€œbuilt with llamaâ€ et dâ€™inclure â€œLlamaâ€ dans leur nom pour toute distribution.",
        "Jamba Open Model": "Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les organismes dÃ©passant 50 millions de dollars de revenus annuels.",
        "CC-BY-NC-4.0": "Cette licence permet de partager et adapter le contenu Ã  condition de crÃ©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilitÃ© pour les usages non commerciaux tout en protÃ©geant les droits de l'auteur.",
        "propriÃ©taire Gemini": "Le modÃ¨le est disponible sous licence payante et accessible via l'API Gemini disponible sur les plateformes Google AI Studio et Vertex AI, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s ou selon les termes de l'entreprise.",
        "propriÃ©taire Mistral": "Le modÃ¨le est disponible sous licence payante et accessible via l'API Mistral, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s.",
        "propriÃ©taire xAI": "Le modÃ¨le est accessible via API xAI, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s ou selon les termes de l'entreprise.",
        "propriÃ©taire Liquid": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© Liquid AI, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s.",
        "propriÃ©taire OpenAI": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© OpenAI, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s ou selon les termes de l'entreprise.",
        "propriÃ©taire Anthropic": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© Anthropic, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de tokens traitÃ©s ou selon les termes de l'entreprise.",
        "Mistral AI Non-Production": "Cette licence permet de partager et adapter le contenu Ã  condition de crÃ©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilitÃ© pour les usages non commerciaux tout en protÃ©geant les droits de l'auteur."
      }
    },
    "release": "SortieÂ {date}",
    "size": {
      "title": "Taille",
      "estimated": "TailleÂ estimÃ©eâ€¯({size})",
      "descriptions": {
        "XS": "Les modÃ¨les trÃ¨s petits, avec moins de 7 milliards de paramÃ¨tres, sont les moins complexes et les plus Ã©conomiques en termes de ressources, offrant des performances suffisantes pour des tÃ¢ches simples comme la classification de texte.",
        "S": "Un modÃ¨le de petit gabarit est moins complexe et coÃ»teux en ressources par rapport aux modÃ¨les plus grands, tout en offrant une performance suffisante pour diverses tÃ¢ches (rÃ©sumÃ©, traduction, classification de texte...)",
        "M": "Les modÃ¨les moyens offrent un bon Ã©quilibre entre complexitÃ©, coÃ»t et performance : ils sont beaucoup moins consommateurs de ressources que les grands modÃ¨les tout en Ã©tant capables de gÃ©rer des tÃ¢ches complexes telles que l'analyse de sentiment ou le raisonnement.",
        "L": "Les grands modÃ¨les nÃ©cessitent des ressources significatives, mais offrent les meilleures performances pour des tÃ¢ches avancÃ©es comme la rÃ©daction crÃ©ative, la modÃ©lisation de dialogues et les applications nÃ©cessitant une comprÃ©hension fine du contexte.",
        "XL": "Ces modÃ¨les dotÃ©s de plusieurs centaines de milliards de paramÃ¨tres sont les plus complexes et avancÃ©s en termes de performance et de prÃ©cision. Les ressources de calcul et de mÃ©moire nÃ©cessaires pour dÃ©ployer ces modÃ¨les sont telles quâ€™ils sont destinÃ©s aux applications les plus avancÃ©es et aux environnements hautement spÃ©cialisÃ©s."
      }
    },
    "openWeight": {
      "conditions": {
        "free": "Permissive",
        "copyleft": "Copyleft",
        "restricted": "Sous conditions"
      },
      "tooltips": {
        "openSource": "Le corpus, le code d'entraÃ®nement, et les poids de ce modÃ¨le (câ€™est-Ã -dire les paramÃ¨tres appris pendant son entraÃ®nement) sont entiÃ¨rement tÃ©lÃ©chargeables et modifiables par le public, lui permettant de faire fonctionner et modifier le modÃ¨le sur son propre matÃ©riel. Qu'un modÃ¨le soit Â«â€¯open sourceâ€¯Â» est plus contraignant qu'Â«â€¯open weightsâ€¯Â», notamment Ã  cause de la nÃ©cessitÃ© de transparence du corpus d'entraÃ®nement, et rares sont les modÃ¨les qui sont considÃ©rÃ©s Â«â€¯open sourceâ€¯Â».",
        "openWeight": "ModÃ¨le dit Â«â€¯open weightsâ€¯Â» dont les poids, câ€™est-Ã -dire les paramÃ¨tres appris pendant son entraÃ®nement, sont tÃ©lÃ©chargeables par le public, lui permettant de faire fonctionner le modÃ¨le sur son propre matÃ©riel. Qu'un modÃ¨le soit Â«â€¯open sourceâ€¯Â» est plus contraignant (principalement par rapport Ã  la transparence du corpus d'entraÃ®nement), et rares sont les modÃ¨les qui sont considÃ©rÃ©s Â«â€¯open sourceâ€¯Â».",
        "params": "Les paramÃ¨tres ou les poids, comptÃ©s en milliards, sont les variables, apprises par un modÃ¨le au cours de son entrainement, qui dÃ©terminent ses rÃ©ponses. Plus le nombre de paramÃ¨tres est important, plus il est capable dâ€™effectuer des tÃ¢ches complexes.",
        "free": "Une fois modifiÃ©, le modÃ¨le peut Ãªtre redistribuÃ© sous une licence diffÃ©rente du modÃ¨le source.",
        "copyleft": "Une fois modifiÃ©, le modÃ¨le doit Ãªtre redistribuÃ© sous la mÃªme licence que celle du modÃ¨le source.",
        "ram": "La RAM (mÃ©moire vive) stocke les donnÃ©es traitÃ©es par un LLM en temps rÃ©el. Plus le modÃ¨le est grand, plus il a besoin de RAM pour fonctionner."
      },
      "descriptions": {
        "XS": "DotÃ© de {paramsCount} milliards de paramÃ¨tres, ce modÃ¨le fait partie de la classe des modÃ¨les trÃ¨s petits (moins de 7 milliards de paramÃ¨tres).",
        "S": "DotÃ© de {paramsCount} milliards de paramÃ¨tres, ce modÃ¨le fait partie de la classe des petits modÃ¨les (entre 7 et 20 milliards de paramÃ¨tres).",
        "M": "DotÃ© de {paramsCount} milliards de paramÃ¨tres, ce modÃ¨le fait partie de la classe des moyens modÃ¨les (entre 20 et 70 milliards de paramÃ¨tres).",
        "L": "DotÃ© de {paramsCount} milliards de paramÃ¨tres, ce modÃ¨le fait partie de la classe des grands modÃ¨les (entre 70 et 100 milliards de paramÃ¨tres).",
        "XL": "DotÃ© de {paramsCount} milliards de paramÃ¨tres, ce modÃ¨le fait partie de la classe des trÃ¨s grands modÃ¨les."
      },
      "use": {
        "commercial": "Utilisation commerciale",
        "modification": "Modification autorisÃ©e",
        "attribution": "Attribution requise",
        "licenseType": "Type de licence",
        "requiredRam": "RAM nÃ©cessaire"
      }
    },
    "parameters": "{number}Â mdsÂ deÂ paramÃ¨tres",
    "ram": "{min} Ã  {max} Go",
    "names": {
      "a": "ModÃ¨le A",
      "b": "ModÃ¨le B"
    },
    "extra": {
      "title": "Pour aller plus loin",
      "experts": {
        "open-weights": "Pour les expertÂ·es, consultez la <a {linkProps}>fiche du modÃ¨le sur Hugging Face</a>",
        "api-only": "Pour les expertÂ·es, consultez la <a {linkProps}>site officiel du modÃ¨le</a>"
      },
      "impacts": "Les calculs dâ€™impacts environnementaux reposent sur les projets <a {linkProps1}>EcoLogits</a> et <a {linkProps2}>Impact CO<sub>2</sub></a>."
    },
    "list": {
      "title": "DÃ©couvrez les modÃ¨les",
      "intro": "Explorez les diffÃ©rents modÃ¨les d'IA conversationnels disponibles, leurs caractÃ©ristiques et leurs licences.",
      "filters": {
        "editor": {
          "legend": "Ã‰diteur"
        },
        "size": {
          "legend": "Taille (en milliards de paramÃ¨tres)",
          "labels": {
            "XS": "< Ã  7 milliards",
            "S": "de 7 Ã  20 milliards",
            "M": "de 20 Ã  70 milliards",
            "L": "de 70 Ã  150 milliards",
            "XL": "> 150 milliards"
          }
        },
        "license": {
          "legend": "Licence d'utilisation"
        },
        "display": "Afficher les filtres"
      },
      "triage": {
        "label": "Trier par",
        "options": {
          "name-asc": "Nom du modÃ¨le (A Ã  Z)",
          "date-desc": "Date de sortie (du plus au moins rÃ©cent)",
          "params-asc": "Taille (du plus petit au plus grand)",
          "org-asc": "Ã‰diteur (A Ã  Z)"
        }
      },
      "model": "modÃ¨le",
      "models": "modÃ¨les",
      "noresults": "Aucun modÃ¨le ne correspond Ã  vos critÃ¨res de recherche."
    }
  },
  "modes": {
    "random": {
      "title": "Mode AlÃ©atoire",
      "label": "AlÃ©atoire",
      "altLabel": "ModÃ¨les alÃ©atoires",
      "description": "Deux modÃ¨les choisis au hasard parmi toute la liste"
    },
    "custom": {
      "title": "Mode SÃ©lection",
      "label": "SÃ©lection manuelle",
      "altLabel": "SÃ©lection manuelle",
      "description": "ReconnaÃ®trez-vous les deux modÃ¨les que vous avez choisis ?"
    },
    "small-models": {
      "title": "Mode Frugal",
      "label": "Frugal",
      "altLabel": "ModÃ¨les frugaux",
      "description": "Deux modÃ¨les tirÃ©s au hasard parmi ceux de plus petite taille"
    },
    "big-vs-small": {
      "title": "Mode David contre Goliath",
      "label": "David contre Goliath",
      "altLabel": "David contre Goliath",
      "description": "Un petit modÃ¨le contre un grand, les deux tirÃ©s au hasard"
    },
    "reasoning": {
      "title": "Mode Raisonnement",
      "label": "Raisonnement",
      "altLabel": "ModÃ¨les avec raisonnement",
      "description": "Deux modÃ¨les tirÃ©s au hasard parmi ceux optimisÃ©s pour des tÃ¢ches complexes"
    }
  },
  "vote": {
    "title": "Quel modÃ¨le dâ€™IA prÃ©fÃ©rez-vous ?",
    "introA": "Avant de dÃ©couvrir lâ€™identitÃ© des modÃ¨les, nous avons besoin de votre prÃ©fÃ©rence.",
    "introB": "Elle permet d'enrichir les jeux de donnÃ©es compar:IA dont lâ€™objectif est dâ€™affiner les futurs modÃ¨les dâ€™IA sur le franÃ§ais",
    "bothEqual": "Les deux se valent",
    "comment": {
      "add": "Ajouter des commentaires",
      "placeholder": "Vous pouvez ajouter des prÃ©cisions sur cette rÃ©ponse du modÃ¨le {model}"
    },
    "choices": {
      "positive": {
        "question": "Qu'avez-vous apprÃ©ciÃ© dans la rÃ©ponse ?",
        "useful": "Utile",
        "complete": "ComplÃ¨te",
        "creative": "CrÃ©ative",
        "clear-formatting": "Mise en forme claire"
      },
      "negative": {
        "question": "Pourquoi la rÃ©ponse ne convient-elle pas ?",
        "incorrect": "Incorrecte",
        "superficial": "Superficielle",
        "instructions-not-followed": "Instructions non suivies"
      },
      "altText": "{choice} pour le modÃ¨le {model}"
    },
    "qualify": {
      "question": "Comment qualifiez-vous ses rÃ©ponses ?",
      "placeholder": "Les rÃ©ponses du modÃ¨le {model} sont...",
      "addDetails": "Ajouter des dÃ©tails"
    },
    "like": {
      "label": "j'apprÃ©cie",
      "selectedLabel": "j'apprÃ©cie (sÃ©lectionnÃ©)"
    },
    "dislike": {
      "label": "je n'apprÃ©cie pas",
      "selectedLabel": "je n'apprÃ©cie pas (sÃ©lectionnÃ©)"
    },
    "yours": "Votre vote"
  },
  "reveal": {
    "impacts": {
      "title": "Impact Ã©nergÃ©tique de la discussion",
      "size": {
        "label": "taille du modÃ¨le",
        "count": "milliards param.",
        "estimated": "(est.)",
        "quantized": "(quantisÃ©)"
      },
      "tokens": {
        "label": "taille du texte",
        "tooltip": "Lâ€™IA analyse et gÃ©nÃ¨re des phrases Ã  partir de mots ou de parties de mots dâ€™Ã  peu prÃ¨s quatre lettres, cette unitÃ© de texte est appelÃ©e token (Â«â€¯jetonâ€¯Â»). Plus un texte est long, plus le nombre de tokens est grand.",
        "tokens": "tokens"
      },
      "energy": {
        "label": "Ã©nergie conso.",
        "tooltip": "MesurÃ©e en wattheures, lâ€™Ã©nergie consommÃ©e reprÃ©sente l'Ã©lectricitÃ© utilisÃ©e par le modÃ¨le pour traiter une requÃªte et gÃ©nÃ©rer la rÃ©ponse correspondante. Plus un modÃ¨le est grand (en milliards de paramÃ¨tres), plus il faut d'Ã©nergie pour produire un token."
      }
    },
    "equivalent": {
      "title": "Ce qui correspond Ã  :",
      "co2": {
        "label": "CO<sub>2</sub> Ã©mis",
        "tooltip": "Le CO<sub>2</sub> Ã©mis Ã©quivaut aux Ã©missions de dioxyde de carbone produites par lâ€™Ã©nergie utilisÃ©e pour faire fonctionner le modÃ¨le. Elle traduit l'impact environnemental liÃ© Ã  la consommation Ã©nergÃ©tique. Le calcul dâ€™Ã©quivalence Wattheures/CO<sub>2</sub> diffÃ¨re selon le mix Ã©nergÃ©tique de chaque pays. Or, les serveurs utilisÃ©s pour lâ€™infÃ©rence des modÃ¨les ne sont pas tous localisÃ©s en France. Ainsi, le calcul dâ€™Ã©quivalence repose sur la moyenne mondiale du taux dâ€™Ã©missions de CO<sub>2</sub> par Ã©nergie consommÃ©e."
      },
      "lightbulb": {
        "label": "ampoule LED",
        "tooltip": "DonnÃ©e calculÃ©e sur la base de consommation dâ€™une ampoule LED standard de 5W (E14)"
      },
      "streaming": {
        "label": "vidÃ©os en ligne",
        "tooltip": "DonnÃ©e calculÃ©e selon lâ€™impact carbone dâ€™une heure de vidÃ©o en ligne en haute dÃ©finition, sur une tÃ©lÃ©vision, en connexion wifi (source <a {linkProps}>ADEME</a>)"
      }
    },
    "feedback": {
      "shareResult": "Partager votre rÃ©sultat",
      "moreOnVotes": "En savoir plus sur les votes",
      "description": "Faites dÃ©couvrir compar:IA en partageant les modÃ¨les dâ€™IA avec lesquels vous avez Ã©changÃ© ! Seuls les noms et lâ€™impact Ã©nergÃ©tique de la discussion seront visibles via ce lien, sans accÃ¨s aux messages Ã©changÃ©s.",
      "example": "Exemple de partage de rÃ©sultat"
    }
  },
  "errors": {
    "unknown": "Une erreur est survenue",
    "404": {
      "title": "Page non trouvÃ©e",
      "error": "Erreur 404",
      "sorry": "La page que vous cherchez est introuvable. Excusez-nous pour la gÃ¨ne occasionnÃ©e.",
      "desc": "Si vous avez tapÃ© l'adresse web dans le navigateur, vÃ©rifiez qu'elle est correcte. La page nâ€™est peut-Ãªtre plus disponible.<br />Dans ce cas, pour continuer votre visite vous pouvez consulter notre page dâ€™accueil.<br />Sinon contactez-nous pour que lâ€™on puisse vous rediriger vers la bonne information."
    },
    "unexpected": {
      "title": "Erreur inattendue",
      "error": "Erreur {code}",
      "sorry": "DÃ©solÃ©, le service rencontre un probleÌ€me, nous travaillons pour le reÌsoudre le plus rapidement possible.",
      "desc": "Essayez de rafraÃ®chir la page ou bien ressayez plus tard."
    }
  },
  "actions": {
    "copyMessage": {
      "do": "Copier le message",
      "done": "Message copiÃ©"
    },
    "copyLink": {
      "do": "Copier le lien",
      "done": "Lien copiÃ© dans le presse-papiers"
    },
    "contact": "Contactez-nous",
    "contactUs": "Nous contacter",
    "home": "Page d'accueil",
    "returnHome": "Revenir Ã  l'accueil",
    "seeMore": "Voir plus",
    "selectLanguage": "SÃ©lectionner une langue"
  },
  "words": {
    "back": "Retour",
    "close": "Fermer",
    "random": "AlÃ©atoire",
    "regenerate": "RegÃ©nÃ©rer",
    "reset": "RÃ©initialiser",
    "restart": "Recommencer",
    "retry": "Recommencer",
    "search": "Rechercher",
    "send": "Envoyer",
    "tooltip": "Infobulle",
    "validate": "Valider"
  },
  "generated": {
    "licenses": {
      "os": {
        "Apache 2.0": {
          "license_desc": "<p>Cette licence permet d'utiliser, modifier et distribuer librement le modÃ¨le, y compris Ã  des fins commerciales. Outre la libertÃ© d'utilisation, elle garantit la protection juridique en incluant une clause de concession de brevets qui fonctionne comme une assurance : si vous utilisez ce modÃ¨le, les contributeurs s'engagent Ã  ne pas vous poursuivre pour violation de leurs brevets liÃ©s au projet. Cette protection mutuelle Ã©vite les conflits juridiques entre utilisateurs et dÃ©veloppeurs. Lors de la distribution de versions modifiÃ©es, les changements significatifs doivent Ãªtre signalÃ©s par des mentions appropriÃ©es, garantissant la transparence pour l'utilisateur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "CC-BY-NC-4.0": {
          "license_desc": "<p>Cette licence permet de partager et adapter le contenu librement Ã  condition de crÃ©diter l'auteur, mais interdit toute utilisation commerciale. Elle offre une flexibilitÃ© pour les usages non commerciaux tout en protÃ©geant les droits de l'auteur.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        },
        "Gemma": {
          "license_desc": "<p>Cette licence est conÃ§ue pour encourager l'utilisation, la modification et la redistribution des logiciels mais inclut une clause stipulant que toutes les versions modifiÃ©es ou amÃ©liorÃ©es doivent Ãªtre partagÃ©es avec la communautÃ© sous la mÃªme licence source, favorisant ainsi la collaboration et la transparence dans le dÃ©veloppement logiciel.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Llama 3.1": {
          "license_desc": "<p>Cette licence permet d'utiliser, reproduire, modifier et distribuer librement le code avec attribution, mais impose des restrictions pour les opÃ©rations dÃ©passant 700 millions d'utilisateurs mensuels. La rÃ©utilisation du code ou des contenus gÃ©nÃ©rÃ©s pour lâ€™entraÃ®nement ou l'amÃ©lioration de modÃ¨les dÃ©rivÃ©s est autorisÃ©e Ã  condition dâ€™afficher â€œbuilt with llamaâ€ et dâ€™inclure â€œLlamaâ€ dans leur nom pour toute distribution.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions dâ€™utilisateurs"
        },
        "Llama 3.3": {
          "license_desc": "<p>Cette licence <strong>non-exclusive, mondiale et sans redevance</strong> permet d'utiliser, reproduire, modifier et distribuer librement le code et les MatÃ©riaux Llama 3.3 avec attribution. Elle autorise notamment la rÃ©utilisation pour l'amÃ©lioration de modÃ¨les dÃ©rivÃ©s, mais impose des restrictions pour les opÃ©rations commerciales de trÃ¨s grande envergure.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions dâ€™utilisateurs"
        },
        "Llama 4": {
          "license_desc": "<p>Cette licence non-exclusive, mondiale et sans redevance permet d'utiliser, reproduire, modifier et distribuer les MatÃ©riaux Llama 4 (modÃ¨les et documentation) avec attribution. Cependant, elle impose deux restrictions majeures : (1) les entreprises dÃ©passant 700 millions d'utilisateurs actifs mensuels doivent obtenir une licence spÃ©ciale de Meta, et (2) <strong>exclusion totale</strong> des personnes rÃ©sidant dans l'UE et des entreprises ayant leur siÃ¨ge social dans l'UE pour l'utilisation directe des modÃ¨les multimodaux, en raison des incertitudes rÃ©glementaires liÃ©es Ã  l'AI Act europÃ©en. Les utilisateurs finaux europÃ©ens peuvent nÃ©anmoins accÃ©der Ã  des services intÃ©grant Llama 4, Ã  condition qu'ils soient fournis depuis l'extÃ©rieur de l'UE.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": "en dessous des 700 millions dâ€™utilisateurs\n"
        },
        "MIT": {
          "license_desc": "<p>La licence MIT est une licence de logiciel libre permissive : elle permet Ã  quiconque de rÃ©utiliser, modifier et distribuer le modÃ¨le, mÃªme Ã  des fins commerciales, sous rÃ©serve d'inclure la licence d'origine et les mentions de droits d'auteur.</p>",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI Research License": {
          "license_desc": "<p>Cette licence non-exclusive et sans redevance autorise l'utilisation, la copie, la modification et la distribution des modÃ¨les Mistral et de leurs dÃ©rivÃ©s (incluant les versions modifiÃ©es ou affinÃ©es). Cependant, elle est strictement limitÃ©e aux fins de recherche.</p>",
          "reuse_specificities": "mais que pour des usages non-commerciaux",
          "commercial_use_specificities": ""
        }
      },
      "proprio": {
        "Alibaba": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© Alibaba, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Amazon": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via Amazon Bedrock, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "sauf pour distiller ou entraÃ®ner dâ€™autres modÃ¨les sur les plateformes dâ€™Amazon.",
          "commercial_use_specificities": ""
        },
        "Anthropic": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© Anthropic ou des sociÃ©tÃ©s partenaires, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Cohere": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "DeepSeek": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Google": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© Google, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e de Google.",
          "reuse_specificities": "sauf pour entraÃ®ner dâ€™autres modÃ¨les sur Vertex AI",
          "commercial_use_specificities": ""
        },
        "Meta": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Microsoft": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Mistral AI": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via l'API Mistral, Amazon Sagemaker et plusieurs autres hÃ©bergeurs, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Moonshot AI": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nous": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Nvidia": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "OpenAI": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via API sur les plateformes de la sociÃ©tÃ© OpenAI ou via les services Microsoft Azure, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "Zhipu": {
          "license_desc": "",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        },
        "xAI": {
          "license_desc": "Le modÃ¨le est disponible sous licence payante et accessible via X et xAI, nÃ©cessitant un paiement Ã  l'utilisation basÃ© sur le nombre de jetons traitÃ©s ou sur lâ€™infrastructure rÃ©servÃ©e.",
          "reuse_specificities": "",
          "commercial_use_specificities": ""
        }
      }
    },
    "models": {
      "Aya Expanse 32B": {
        "desc": "<p>ModÃ¨le de taille moyenne multilingue, capable de traiter 23 langues.</p>",
        "size_desc": "<p>Avec 32 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut Ãªtre hÃ©bergÃ© sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui contribue Ã  limiter les coÃ»ts dâ€™infrastructure.</p>\n<p>Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  130 000 jetons, utile pour lâ€™analyse de documents longs.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derriÃ¨re ce modÃ¨le, a Ã©tÃ© fondÃ©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier Â«â€¯Attention Is All You Needâ€¯Â» qui a rÃ©volutionnÃ© l'IA. Sa spÃ©cificitÃ© principale rÃ©side dans sa focalisation exclusive sur l'IA gÃ©nÃ©rative pour les entreprises, particuliÃ¨rement les secteurs rÃ©glementÃ©s comme la finance, la santÃ©, la manufacture et l'Ã©nergie, ainsi que le secteur public. L'entreprise est Ã©galement pionniÃ¨re dans les approches multilingues et maintient un laboratoire de recherche Ã  but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modÃ¨le a Ã©tÃ© conÃ§u pour offrir de bonnes capacitÃ©s dans chacune des 23 langues de son corpus dâ€™entraÃ®nement.</p>"
      },
      "Claude 3.7 Sonnet": {
        "desc": "<p>TrÃ¨s grand modÃ¨le multimodal et multilingue, performant pour la gÃ©nÃ©ration de code, avec deux modalitÃ©s de rÃ©ponses: lâ€™utilisateur peut choisir entre un mode de raisonnement, pour des rÃ©ponses plus approfondies, ou un mode rapide, pour gÃ©nÃ©rer directement la rÃ©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de trÃ¨s grande taille, nÃ©cessitant des serveurs Ã©quipÃ©s de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse. Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  200 000 jetons, adaptÃ©e Ã  lâ€™analyse de longs documents ou de dÃ©pÃ´ts de code.</p>",
        "fyi": "<p>Claude 4 Opus est la version la plus avancÃ©e de la famille Claude 4. Il est optimisÃ© pour la puissance brute et les tÃ¢ches complexes nÃ©cessitant un raisonnement soutenu sur de longues pÃ©riodesâ€¯: il peut par exemple travailler sur des tÃ¢ches Ã  long terme (Anthropic dÃ©clarent qu'il peut travailler jusqu'Ã  sept heures de maniÃ¨re indÃ©pendante). En contrepartie, Opus est plus coÃ»teux Ã  utiliser, plus lent Ã  rÃ©pondre et nÃ©cessite davantage de ressources pour fonctionner.</p>\n<p>Le modÃ¨le offre deux modes dâ€™utilisationâ€¯: un mode rÃ©flexion avec un raisonnement Ã©tape par Ã©tape pour les problÃ¨mes complexes, et un mode rapide pour les rÃ©ponses directes. Ã€ la diffÃ©rence dâ€™autres modÃ¨les, le mode raisonnement nâ€™a pas Ã©tÃ© majoritairement entraÃ®nÃ© sur des donnÃ©es mathÃ©matiques, mais adaptÃ© Ã  des cas dâ€™usage rÃ©els.</p>"
      },
      "Claude 4 Sonnet": {
        "desc": "<p>TrÃ¨s grand modÃ¨le multimodal et multilingue, trÃ¨s puissant en code, avec deux modalitÃ©s de rÃ©ponses: lâ€™utilisateur peut choisir entre un mode de raisonnement, pour des rÃ©ponses plus approfondies, ou un mode rapide, pour gÃ©nÃ©rer directement la rÃ©ponse finale.</p>",
        "size_desc": "<p>La taille exacte nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de trÃ¨s grande taille, nÃ©cessitant des serveurs Ã©quipÃ©s de plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse. Le modÃ¨le dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  200 000 jetons, adaptÃ©e Ã  lâ€™analyse de longs documents ou de dÃ©pÃ´ts de code.</p>",
        "fyi": "<p>Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisÃ©e pour la vitesse, lâ€™efficacitÃ© et lâ€™accessibilitÃ©. Il est un peu moins Ã  lâ€™aise sur les tÃ¢ches qui demandent un raisonnement complexe en plusieurs Ã©tapes. En contrepartie, il est nettement moins coÃ»teux, plus rapide, peut gÃ©nÃ©rer de plus longs textes et consomme moins dâ€™Ã©nergie que Opus.</p>\n<p>Le modÃ¨le offre deux modes dâ€™utilisationâ€¯: un mode rÃ©flexion avec un raisonnement Ã©tape par Ã©tape pour les problÃ¨mes complexes, et un mode rapide pour les rÃ©ponses directes. Ã€ la diffÃ©rence dâ€™autres modÃ¨les, le mode raisonnement nâ€™a pas Ã©tÃ© surtout entraÃ®nÃ© sur des donnÃ©es mathÃ©matiques, mais adaptÃ© Ã  des cas dâ€™usage rÃ©els.</p>"
      },
      "Command A": {
        "desc": "<p>Grand modÃ¨le, performant pour la programmation, lâ€™utilisation dâ€™outils externes, la â€œgÃ©nÃ©ration augmentÃ©e de rÃ©cupÃ©rationâ€ (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 111 milliards de paramÃ¨tres, ce modÃ¨le fait partie des grands modÃ¨les. Il nÃ©cessite au moins deux cartes graphiques puissantes pour lâ€™hÃ©bergement, ce qui entraÃ®ne un coÃ»t de fonctionnement significatif.</p>\n<p>Sa fenÃªtre de contexte atteint 256 000 jetons, adaptÃ©e Ã  lâ€™analyse de vastes ensembles de documents ou de bases de code.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derriÃ¨re ce modÃ¨le, a Ã©tÃ© fondÃ©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier <a href=\"https://arxiv.org/abs/1706.03762\">Â«â€¯Attention Is All You Needâ€¯Â»</a> paru en 2017 et qui a rÃ©volutionnÃ© l'IA. L'entreprise se dÃ©marque par sa focalisation exclusive sur l'IA gÃ©nÃ©rative pour les entreprises, particuliÃ¨rement les secteurs rÃ©glementÃ©s comme la finance, la santÃ©, la manufacture et l'Ã©nergie, ainsi que le secteur public. L'entreprise est Ã©galement pionniÃ¨re dans les approches multilingues et maintient un laboratoire de recherche Ã  but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modÃ¨le est conÃ§u pour fonctionner dans plus de 23 langues et pour sâ€™intÃ©grer facilement dans les systÃ¨mes dâ€™entreprise.  Il fait partie des rares modÃ¨les distribuÃ©s sous licence <strong>CC-BY-NC 4.0 qui autorise le partage et la modification mais interdit toute utilisation commerciale.</strong> Ce choix de licence reflÃ¨te la volontÃ© de Cohere de contribuer Ã  la recherche et la communautÃ© open source, tout en gardant le contrÃ´le sur les usages commerciaux pour protÃ©ger son modÃ¨le Ã©conomique... Cela exclut par exemple lâ€™intÃ©gration du modÃ¨le dans des produits ou services vendus par une entreprise Ã  des clients mais autorise un usage acadÃ©mique, des tests ou des projets internes, restreints Ã  un cadre non-commercial.</p>"
      },
      "Command R": {
        "desc": "<p>ModÃ¨le de taille moyenne optimisÃ© pour la synthÃ¨se, les questions gÃ©nÃ©rales, lâ€™utilisation dâ€™outils et efficace dans les systÃ¨mes de gÃ©nÃ©ration augmentÃ©e de rÃ©cupÃ©ration (RAG, retrieval augmented generation).</p>",
        "size_desc": "<p>Avec 35 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut Ãªtre hÃ©bergÃ© sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui contribue Ã  limiter les coÃ»ts dâ€™infrastructure.</p>",
        "fyi": "<p>Cohere, l'entreprise canadienne derriÃ¨re ce modÃ¨le, a Ã©tÃ© fondÃ©e en 2019 par d'anciens chercheurs de Google Brain, notamment Aidan Gomez, co-auteur du fameux papier Â«â€¯Attention Is All You Needâ€¯Â» qui a rÃ©volutionnÃ© l'IA. Sa spÃ©cificitÃ© principale rÃ©side dans sa focalisation exclusive sur l'IA gÃ©nÃ©rative pour les entreprises, particuliÃ¨rement les secteurs rÃ©glementÃ©s comme la finance, la santÃ©, la manufacture et l'Ã©nergie, ainsi que le secteur public. L'entreprise est Ã©galement pionniÃ¨re dans les approches multilingues et maintient un laboratoire de recherche Ã  but non lucratif pour soutenir l'innovation ouverte.</p>\n<p>Ce modÃ¨le a Ã©tÃ© Ã©valuÃ© dans plus de 10 langues. Sa fenÃªtre de contexte atteint 128 000 jetons, ce qui facilite lâ€™analyse de documents longs. Cette fenÃªtre a Ã©tÃ© doublÃ©e sur la version suivante du modÃ¨le (Command A).</p>"
      },
      "DeepSeek R1": {
        "desc": "<p>TrÃ¨s grand modÃ¨le trÃ¨s performant sur les tÃ¢ches mathÃ©matiques, scientifiques et de programmation, qui simule une Ã©tape de raisonnement avant de gÃ©nÃ©rer sa rÃ©ponse.</p>",
        "size_desc": "<p>Avec 671 milliards de paramÃ¨tres DeepSeek R1 est un modÃ¨le de trÃ¨s grande taille qui nÃ©cessite plusieurs cartes graphiques puissantes pour fonctionner. Les modÃ¨les de raisonnement de ce type fonctionnent plus longtemps pour produire une rÃ©ponse, ce qui augmente la consommation Ã©nergÃ©tique. Cependant, l'architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. La fenÃªtre de contexte atteint 128 000 jetons, ce qui est adaptÃ© Ã  lâ€™analyse de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le sâ€™appuie sur une architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) avec 61 couches. Il totalise 671 milliards de paramÃ¨tres, dont 37 milliards sont activÃ©s par jeton. L'entraÃ®nement a fait appel Ã  un apprentissage par renforcement Ã  grande Ã©chelle, avec plusieurs Ã©tapes d'ajustement SFT (<em>supervised fine-tuning</em>â€¯: un affinage supervisÃ© oÃ¹ le modÃ¨le apprend Ã  partir d'exemples de rÃ©ponses correctes) et de donnÃ©es de dÃ©marrage.</p>"
      },
      "DeepSeek R1 Llama 70B": {
        "desc": "<p>Grand modÃ¨le basÃ© sur Meta Llama 3.3 70B, rÃ©-entraÃ®nÃ© avec des exemples de raisonnement issus du modÃ¨le DeepSeek R1. Il offre de bonnes capacitÃ©s en mathÃ©matiques et code.</p>",
        "size_desc": "<p>Avec 70 milliards de paramÃ¨tres, ce modÃ¨le est classÃ© parmi les modÃ¨les de grande taille. Il nÃ©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraÃ®ne un coÃ»t de fonctionnement Ã©levÃ©. Les modÃ¨les de raisonnement fonctionnent Ã©galement plus longtemps pour produire une rÃ©ponse, ce qui augmente leur consommation Ã©nergÃ©tique.</p>\n<p>La fenÃªtre de contexte est de 16 000 jetons, ce qui peut Ãªtre limitant pour lâ€™analyse de trÃ¨s grands documents.</p>",
        "fyi": "<p>Le modÃ¨le nâ€™a pas Ã©tÃ© entraÃ®nÃ© depuis zÃ©ro. Il sâ€™appuie sur Llama 3.3 70B, rÃ©-entraÃ®nÃ© en utilisant des rÃ©sultats gÃ©nÃ©rÃ©s par DeepSeek R1. Ce processus a permis de doter Llama 3.3 70B dâ€™une capacitÃ© Ã  simuler le raisonnement, sans possibilitÃ© pour lâ€™utilisateur de choisir dâ€™activer ou non cette fonction.</p>\n<p>ConformÃ©ment aux obligations de la licence Llama 3.3, l'entreprise doit conserver la mention du modÃ¨le source dans le nom du modÃ¨le, soumis au mÃªme rÃ©gime de licence.</p>"
      },
      "DeepSeek V3": {
        "desc": "<p>TrÃ¨s grand modÃ¨le conÃ§u pour des tÃ¢ches complexesâ€¯: gÃ©nÃ©ration de code, utilisation dâ€™outils, analyse de documents longs. Il peut traiter de nombreuses langues, mais il est particuliÃ¨rement adaptÃ© Ã  lâ€™anglais et au chinois.</p>",
        "size_desc": "<p>DeepSeek V3 est un modÃ¨le de trÃ¨s grande taille, nÃ©cessitant plusieurs cartes graphiques pour fonctionner. Lâ€™architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) permet nÃ©anmoins de nâ€™activer quâ€™une partie des paramÃ¨tres, ce qui rÃ©duit lâ€™empreinte par rapport Ã  un modÃ¨le dense de mÃªme taille.</p>\n<p>La fenÃªtre de contexte atteint 163 000 jetons, ce qui est utile pour lâ€™analyse de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le est basÃ© sur une architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts), comptant 671 milliards de paramÃ¨tres mais nâ€™en activant que 37 milliards par jeton gÃ©nÃ©rÃ©. Il est efficace pour les appels dâ€™outils, la gÃ©nÃ©ration de sorties structurÃ©es (JSON) et la gÃ©nÃ©ration de code.</p>"
      },
      "GPT 4.1 Nano": {
        "desc": "<p>Plus petite version allÃ©gÃ©e du modÃ¨le GPT 4.1 , conÃ§ue pour limiter les coÃ»ts tout en restant compÃ©titive sur la plupart des tÃ¢ches. Le modÃ¨le accepte de trÃ¨s longues requÃªtes, ce qui permet de lâ€™utiliser par exemple pour lâ€™analyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de taille moyenne, nÃ©cessitant une carte graphique puissante pour lâ€™exÃ©cution. NÃ©anmoins, l'architecture supposÃ©e de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique.  Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>",
        "fyi": "<p>Il s'agit d'une version distillÃ©e dâ€™un modÃ¨le de plus grande taille, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de lâ€™audio. Sa fenÃªtre de contexte peut atteindre jusquâ€™Ã  1 million de jetons, ce qui le rend particuliÃ¨rement adaptÃ© Ã  lâ€™analyse de corpus de textes ou de dÃ©pÃ´ts de code trÃ¨s longs.</p>"
      },
      "GPT 5": {
        "desc": "<p>Le GPT-5 n'est pas un modÃ¨le unique, mais un systÃ¨me unifiÃ© composÃ© de deux modÃ¨les distinctsâ€¯: un modÃ¨le rapide (<code>gpt-5-main</code>) pour les requÃªtes courantes et un modÃ¨le de raisonnement (<code>gpt-5-thinking</code>) pour les problÃ¨mes complexes. ComparÃ© Ã  ses prÃ©dÃ©cesseurs, OpenAI affirme qu'il est plus utile dans les requÃªtes du monde rÃ©el, avec des amÃ©liorations notables dans les domaines de l'Ã©criture, du codage et de la santÃ©. Il rÃ©duit Ã©galement le phÃ©nomÃ¨ne des hallucinations. GrÃ¢ce Ã  sa fenÃªtre de contexte de 400 000 jetons, il peut accepter de longues requÃªtes, ce rend possible l'analyse de plusieurs documents Ã  la fois.</p>",
        "size_desc": "<p>Le systÃ¨me GPT-5 est composÃ© de modÃ¨les de diffÃ©rentes tailles, mais les tailles exactes ne sont pas connues. Son architecture est conÃ§ue pour inclure plusieurs modÃ¨les, orchestrÃ©s par un systÃ¨me de routage interne, qui sÃ©lectionne le plus petit modÃ¨le adaptÃ© Ã  la tÃ¢che pour optimiser la vitesse et la profondeur du raisonnement. L'architecture est probablement basÃ©e sur un Â« mÃ©lange d'experts Â» (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramÃ¨tres est activÃ©e pour chaque requÃªte. Cela permet une meilleure efficacitÃ© Ã©nergÃ©tique et des performances Ã©levÃ©es. Les estimations disponibles sur la taille des modÃ¨les s'appuient sur des informations publiques et des indices indirects tels que les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>",
        "fyi": "<p>Les dÃ©veloppeurs qui utilisent ce modÃ¨le peuvent configurer un paramÃ¨tre de verbositÃ© pour ajuster la longueur de la phase de raisonnement.</p>\n<p>En matiÃ¨re de sÃ©curitÃ©, le systÃ¨me utilise une nouvelle approche de sÃ©curitÃ© appelÃ©e Â« safe-completions Â» pour prÃ©venir le contenu non autorisÃ© au moment de la rÃ©ponse plutÃ´t quâ€™au moment de la requÃªte. Les crÃ©ateurs du modÃ¨le ont aussi utilisÃ© la phase dâ€™entraÃ®nement au â€œraisonnementâ€ pour le rendre plus â€œrÃ©sistantâ€ aux tentatives de contournement de leurs rÃ¨gles de sÃ©curitÃ© (<em>jailbreaking</em>).</p>"
      },
      "GPT 5 Mini": {
        "desc": "<p>Le GPT-5 Mini est une version allÃ©gÃ©e du modÃ¨le GPT-5 principal. Il est conÃ§u pour Ãªtre utilisÃ© dans des environnements oÃ¹ il est nÃ©cessaire de limiter les coÃ»ts, par exemple Ã  grande Ã©chelle. Son modÃ¨le de raisonnement est presque aussi performant que celui du modÃ¨le principal (<code>gpt-5-thinking</code>) malgrÃ© sa taille plus petite. GrÃ¢ce Ã  sa fenÃªtre de contexte de 400 000 jetons, il peut accepter de longues requÃªtes, ce rend possible l'analyse de plusieurs documents Ã  la fois.</p>",
        "size_desc": "<p>Le modÃ¨le Mini est une dÃ©clinaison plus compacte (taille moyenne supposÃ©e) du systÃ¨me GPT-5. Il est conÃ§u pour fonctionner de maniÃ¨re optimale pour un bon Ã©quilibre entre performance et coÃ»t, grÃ¢ce Ã  un systÃ¨me de routage qui le sÃ©lectionne pour des tÃ¢ches spÃ©cifiques. L'architecture est probablement basÃ©e sur un Â« mÃ©lange d'experts Â» (MoE, Mixture of Experts), ce qui signifie que seule une partie des paramÃ¨tres est activÃ©e pour chaque requÃªte. NÃ©anmoins, les modÃ¨les sont probablement trÃ¨s grands, nÃ©cessitant plusieurs cartes graphiques puissantes pour lâ€™infÃ©rence.</p>",
        "fyi": "<p>Le systÃ¨me utilise une nouvelle approche de sÃ©curitÃ© appelÃ©e Â« safe-completions Â» pour prÃ©venir le contenu non autorisÃ© au moment de la rÃ©ponse plutÃ´t quâ€™au moment de la requÃªte.</p>\n<p>Bien qu'il soit une version plus petite, il se montre trÃ¨s compÃ©titif face au modÃ¨le GPT-5 principal sur de nombreux benchmarks, en particulier dans le domaine mÃ©dical.</p>"
      },
      "GPT 5 Nano": {
        "desc": "<p>Le GPT-5 Nano est la plus petite et la plus rapide version du modÃ¨le de raisonnement GPT-5. Il est conÃ§u pour des contextes oÃ¹ une latence ou un coÃ»t ultra-faible est nÃ©cessaire. GrÃ¢ce Ã  sa fenÃªtre de contexte de 400 000 jetons, il peut accepter de longues requÃªtes, ce rend possible l'analyse de plusieurs documents Ã  la fois.</p>",
        "size_desc": "<p>Le modÃ¨le Nano est le plus compact de la famille GPT-5 (taille petite supposÃ©e). Il est sÃ©lectionnÃ© par le systÃ¨me de routage pour les requÃªtes nÃ©cessitant une latence ultra-faible et des rÃ©ponses instantanÃ©es. Son architecture est probablement basÃ©e sur un Â« mÃ©lange d'experts Â» (MoE, Mixture of Experts), ce qui permet une meilleure efficacitÃ© Ã©nergÃ©tique et des performances Ã©levÃ©es, mÃªme sur des requÃªtes nÃ©cessitant une rÃ©ponse rapide.</p>",
        "fyi": "<p>Le systÃ¨me utilise une nouvelle approche de sÃ©curitÃ© appelÃ©e Â« safe-completions Â» pour prÃ©venir le contenu non autorisÃ© au moment de la rÃ©ponse plutÃ´t quâ€™au moment de la requÃªte.</p>"
      },
      "GPT OSS-120B": {
        "desc": "<p>Le plus grand des deux premiers modÃ¨les semi-ouverts d'OpenAI depuis GPT-2. ConÃ§u en rÃ©ponse Ã  la montÃ©e en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modÃ¨le de raisonnement performant, notamment sur des tÃ¢ches complexes et dans des environnements Â«â€¯agentiquesâ€¯Â».</p>",
        "size_desc": "<p>L'architecture est basÃ©e sur le principe du Â« mÃ©lange d'experts Â» (MoE), ce qui permet une meilleure efficacitÃ© Ã©nergÃ©tique en n'activant qu'une partie des paramÃ¨tres (5,1 milliards par jeton) pour chaque requÃªte. Câ€™est un modÃ¨le de raisonnement, donc sa consommation dâ€™Ã©nergie est plus Ã©levÃ©e car ils gÃ©nÃ¨re une chaÃ®ne de pensÃ©e interne avant de fournir la rÃ©ponse finale. Il dispose d'une fenÃªtre de contexte de 131 000 jetons, ce qui le rend idÃ©al pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce modÃ¨le peut fonctionner sur une seule GPU de 80 Go (comme la NVIDIA H100). Il dispose d'une fenÃªtre de contexte de 131 000 jetons, ce qui le rend idÃ©al pour l'analyse de documents volumineux. </p>\n<p>Dans les configurations du modÃ¨le, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui dÃ©terminent la verbositÃ© du modÃ¨le.</p>"
      },
      "GPT OSS-20B": {
        "desc": "<p>Le plus petit des deux modÃ¨les semi-ouverts d'OpenAI. Il a Ã©tÃ© conÃ§u en rÃ©ponse Ã  la concurrence de l'open source et est destinÃ© aux cas d'utilisation nÃ©cessitant une faible latence ainsi qu'aux dÃ©ploiements locaux ou spÃ©cialisÃ©s.</p>",
        "size_desc": "<p>Avec 20 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. L'architecture est basÃ©e sur le Â« mÃ©lange d'experts Â» (MoE), ce qui permet une meilleure efficacitÃ© Ã©nergÃ©tique en n'activant qu'une partie des paramÃ¨tres (3,6 milliards par jeton) pour chaque requÃªte. Il s'agit d'un modÃ¨le de raisonnement, ce qui se traduit par une consommation d'Ã©nergie plus Ã©levÃ©e car il gÃ©nÃ¨re une chaÃ®ne de pensÃ©e interne avant de fournir la rÃ©ponse finale. Il dispose d'une fenÃªtre de contexte de 131 000 jetons, ce qui le rend idÃ©al pour l'analyse de documents volumineux.</p>",
        "fyi": "<p>Ce modÃ¨le peut Ãªtre exÃ©cutÃ© localement sur un ordinateur portable haut de gamme Ã©quipÃ© de seulement 16 Go de VRAM (ou de RAM systÃ¨me). Cela en fait une option trÃ¨s accessible pour les dÃ©veloppeurs. </p>\n<p>Dans les configurations du modÃ¨le, il est possible de choisir entre trois niveaux de raisonnement (<em>low</em>, <em>medium</em>, et <em>high</em>) qui dÃ©terminent la verbositÃ© du modÃ¨le.</p>"
      },
      "GPT-4.1 Mini": {
        "desc": "<p>Version allÃ©gÃ©e de GPT 4.1 mais qui reste tout de mÃªme de grande taille, conÃ§ue pour limiter les coÃ»ts tout en restant compÃ©titif sur la plupart des tÃ¢ches. Le modÃ¨le accepte de trÃ¨s longues requÃªtes, ce qui permet de lâ€™utiliser par exemple pour lâ€™analyse de corpus de documents.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de grande taille, nÃ©cessitant une carte graphique puissante pour lâ€™exÃ©cution. NÃ©anmoins, l'architecture supposÃ©e de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>",
        "fyi": "<p>Il s'agit d'une version distillÃ©e dâ€™un modÃ¨le plus grand, avec un transfert partiel de ses connaissances. Il peut traiter du texte, des images et de lâ€™audio.  Sa fenÃªtre de contexte peut atteindre jusquâ€™Ã  1 million de jetons, ce qui le rend particuliÃ¨rement adaptÃ© Ã  lâ€™analyse de corpus trÃ¨s longs ou de dÃ©pÃ´ts de code.</p>"
      },
      "Gemini 2.5 Flash": {
        "desc": "<p>Grand modÃ¨le multimodal et multilingue avec deux modalitÃ©s de rÃ©ponses: lâ€™utilisateur peut choisir entre un mode de raisonnement, pour des rÃ©ponses plus approfondies, ou un mode rapide, pour gÃ©nÃ©rer directement Ã  la rÃ©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de grande taille, nÃ©cessitant plusieurs cartes graphiques puissantes pour le fonctionnement. NÃ©anmoins, l'architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse. Sa fenÃªtre de contexte va jusquâ€™Ã  1 millions de jetons, ce qui permet de traiter de trÃ¨s grands corpus documentaires.</p>",
        "fyi": "<p>Ce modÃ¨le repose sur une architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) et a Ã©tÃ© distillÃ© en ne conservant qu'une approximation des prÃ©dictions du modÃ¨le enseignant - Gemini 2.5 Pro. Il a Ã©tÃ© entraÃ®nÃ© sur une architecture TPUv5p intÃ©grant des avancÃ©es comme la possibilitÃ© de poursuivre l'entraÃ®nement automatiquement mÃªme en cas dâ€™erreurs dâ€™entraÃ®nement, de corruption de donnÃ©es ou de problÃ¨mes de mÃ©moire.</p>\n<p>Gemini 2.5 Flash gÃ¨re des contextes allant jusqu'Ã  1 million de jetons, et trois heures de contenu vidÃ©o. L'optimisation du traitement de la vision permet de traiter des vidÃ©os environ trois fois plus longues dans la mÃªme fenÃªtre de contexte: seuls 66 jetons visuels sont nÃ©cessaires pour gÃ©nÃ©rer une image contre 258 auparavant. Ce modÃ¨le permet  Ã©galement la gÃ©nÃ©ration audio native pour les dialogues et la synthÃ¨se vocale.</p>"
      },
      "Gemma 3 12B": {
        "desc": "<p>Petit modÃ¨le multimodal adaptÃ© aux tÃ¢ches courantes comme les questions-rÃ©ponses, les rÃ©sumÃ©s ou lâ€™interprÃ©tation dâ€™images.</p>",
        "size_desc": "<p>Avec 12 milliards de paramÃ¨tres, il fait partie des modÃ¨les de petite taille. Il peut Ãªtre utilisÃ© localement sur un poste pour prÃ©server la confidentialitÃ© des donnÃ©es, ou sur serveur peu coÃ»teux pour limiter les coÃ»ts par rapport Ã  un modÃ¨le plus grand. </p>\n<p>Sa fenÃªtre de contexte va jusquâ€™Ã  128 000 jetons, ce qui permet de traiter de longs documents.</p>",
        "fyi": "<p>Il traite du texte et des images et peut fonctionner en local sur des ordinateurs portables puissants ou des serveurs avec une seule carte graphique. Il a Ã©tÃ© entraÃ®nÃ© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 27B": {
        "desc": "<p>ModÃ¨le de taille moyenne multimodal adaptÃ© aux tÃ¢ches courantes comme les questions-rÃ©ponses, les rÃ©sumÃ©s ou lâ€™interprÃ©tation dâ€™images.</p>",
        "size_desc": "<p>Avec 27 milliards de paramÃ¨tres, il appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut Ãªtre dÃ©ployÃ© sur un serveur avec une seule carte graphique (GPU). </p>\n<p>Il accepte des contextes jusquâ€™Ã  128 000 jetons, ce qui convient pour lâ€™analyse de documents longs.</p>",
        "fyi": "<p>Il peut traiter du texte et des images sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante. Il a Ã©tÃ© entraÃ®nÃ© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3 4B": {
        "desc": "<p>TrÃ¨s petit modÃ¨le multimodal et compact adaptÃ© aux tÃ¢ches courantes comme les questions-rÃ©ponses, les rÃ©sumÃ©s ou lâ€™interprÃ©tation dâ€™images.</p>",
        "size_desc": "<p>Avec 4 milliards de paramÃ¨tres, il fait partie des modÃ¨les de trÃ¨s petite taille. Il peut Ãªtre utilisÃ© localement pour prÃ©server la confidentialitÃ© des donnÃ©es, ou sur serveur pour limiter les coÃ»ts par rapport Ã  un modÃ¨le plus grand. </p>\n<p>Sa fenÃªtre de contexte peut atteindre 128 000 jetons, ce qui permet dâ€™analyser de longs documents.</p>",
        "fyi": "<p>Il peut traiter du texte et des images en fonctionnant sur des appareils peu puissants, y compris smartphones et tablettes. Il a Ã©tÃ© entraÃ®nÃ© pour pouvoir interagir avec des outils externes (recherche internet...) via des appels de fonctions, ce qui le rend utilisable dans des contextes agentiques.</p>"
      },
      "Gemma 3n 4B": {
        "desc": "<p>TrÃ¨s petit modÃ¨le multimodal et compact conÃ§u pour fonctionner localement sur un ordinateur ou un smartphone, sans recours Ã  un serveur - il est capable dâ€™adapter sa puissance selon la capacitÃ© de la capacitÃ© et le besoin.</p>",
        "size_desc": "<p>Avec 4 milliards de paramÃ¨tres, il fait partie des modÃ¨les de trÃ¨s petite taille. Il peut Ãªtre utilisÃ© localement sur un ordinateur ou un smartphone pour prÃ©server la confidentialitÃ© des donnÃ©es, ou sur serveur pour limiter les coÃ»ts par rapport Ã  un modÃ¨le plus grand.</p>\n<p>Sa fenÃªtre de contexte va jusquâ€™Ã  32 000 jetons.</p>",
        "fyi": "<p>Ce modÃ¨le peut traiter du texte, des images et de lâ€™audio. Il repose sur lâ€™architecture MatFormer et un systÃ¨me de cache PLE (per-layer embeddings), qui active uniquement les paramÃ¨tres utiles selon la tÃ¢che, s'adaptant Ã  la capacitÃ© des machines sur lesquelles fonctionne le modÃ¨le.</p>"
      },
      "Grok 3 Mini": {
        "desc": "<p>Version plus lÃ©gÃ¨re du modÃ¨le Grok 3, permettant de rÃ©duire les coÃ»ts tout en conservant de bonnes performances pour de nombreuses tÃ¢ches. Il peut simuler une phase de raisonnement avant de fournir une rÃ©ponse finale.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. MalgrÃ© son nom, Grok 3 Mini est sans doute un trÃ¨s grand modÃ¨le, nÃ©cessitant plusieurs cartes graphiques puissantes pour fonctionner. De plus, il contient une phase de raisonnement facultative qui implique une gÃ©nÃ©ration plus longue et donc une consommation Ã©nergÃ©tique plus Ã©levÃ©e. NÃ©anmoins, l'architecture supposÃ©e de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>",
        "fyi": "<p>Grok 3 Mini est une version distillÃ©e de Grok 3: il sâ€™en approche en termes de capacitÃ©s, tout en Ã©tant plus rapide et moins coÃ»teux.\nLe modÃ¨le propose deux modesâ€¯: un mode rÃ©flexion avec raisonnement Ã©tape par Ã©tape pour les problÃ¨mes complexes, et un mode rapide pour les rÃ©ponses immÃ©diates.\nSa fenÃªtre de contexte atteint 131 000 jetons, ce qui le rend adaptÃ© Ã  lâ€™analyse de longs documents.</p>"
      },
      "Hermes 3 405B": {
        "desc": "<p>TrÃ¨s grand modÃ¨le rÃ©entraÃ®nÃ© Ã  partir du Llama 3.1 405B, ajustÃ© pour mieux rÃ©pondre aux demandes des utilisateurs et faciliter lâ€™utilisation dâ€™outils externes.</p>",
        "size_desc": "<p>Avec 405 milliards de paramÃ¨tres, ce modÃ¨le fait partie des modÃ¨les de trÃ¨s grande taille. Il nÃ©cessite un serveur Ã©quipÃ© de plusieurs cartes graphiques puissantes, ce qui entraÃ®ne un coÃ»t de fonctionnement important.</p>",
        "fyi": "<p>Ce modÃ¨le est le rÃ©sultat dâ€™un rÃ©entraÃ®nement de lâ€™ensemble des paramÃ¨tres de Llama 3.1 405B pour rendre son comportement moins restreint et mieux prendre en compte les nuances du prompt utilisateur et systÃ¨me - lâ€™utilisateur dispose ainsi dâ€™un plus grand contrÃ´le sur la â€œpersonnalitÃ©â€ et comportement du modÃ¨le. Des fonctions de raisonnement spÃ©cifiques telles que <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong>, <strong><code>&lt;THINKING&gt;</code></strong> ont Ã©tÃ© ajoutÃ©es pour simuler un raisonnement sur les tÃ¢ches complexes. L'entraÃ®nement a utilisÃ© un outil appelÃ© AdamW (vitesse d'apprentissage de 3.5Ã—10â»â¶), qui aide le modÃ¨le Ã  apprendre de maniÃ¨re efficace en ajustant progressivement ses paramÃ¨tres. Ensuite, il a Ã©tÃ© affinÃ© avec une mÃ©thode appelÃ©e DPO (direct preference optimisation), qui permet d'amÃ©liorer ses rÃ©ponses en se basant sur des prÃ©fÃ©rences spÃ©cifiques. Pour rendre cet entraÃ®nement plus lÃ©ger et rapide, des adaptateurs LoRA ont Ã©tÃ© utilisÃ©sâ€¯; ce sont des modules plus petits qui modifient seulement une partie du modÃ¨le, ce qui Ã©vite de devoir retravailler tous les paramÃ¨tres en mÃªme temps.</p>"
      },
      "Llama 3.1 405B": {
        "desc": "<p>TrÃ¨s grand modÃ¨le conÃ§u pour des tÃ¢ches complexes ou spÃ©cialisÃ©es. Souvent utilisÃ© en tant que â€œmodÃ¨le professeurâ€ pour lâ€™entraÃ®nement de modÃ¨les plus spÃ©cialisÃ©s.</p>",
        "size_desc": "<p>Avec 405 milliards de paramÃ¨tres, ce modÃ¨le fait partie des modÃ¨les de trÃ¨s grande taille. Il nÃ©cessite un serveur Ã©quipÃ© de plusieurs cartes graphiques puissantes, ce qui entraÃ®ne un coÃ»t de fonctionnement important. Le modÃ¨le est dotÃ© dâ€™une fenÃªtre de contexte jusquâ€™Ã  128 000 jetons, ce qui le rend intÃ©ressant pour des tÃ¢ches dâ€™analyse de longs documents.</p>",
        "fyi": "<p>Le modÃ¨le a Ã©tÃ© entraÃ®nÃ© sur un corpus de 15 billions de jetons avec 16 000 cartes graphiques H100 (une des cartes graphiques les plus puissantes sur le marchÃ© en 2025). L'entraÃ®nement a combinÃ© gÃ©nÃ©ration de donnÃ©es synthÃ©tiques et optimisation par prÃ©fÃ©rences directes (DPO). Ce modÃ¨le est lui-mÃªme souvent utilisÃ© pour gÃ©nÃ©rer des donnÃ©es synthÃ©tiques pour entraÃ®ner de plus petits modÃ¨les. Le modÃ¨le utilise par dÃ©faut une compression 8-bit pour rÃ©duire les besoins en mÃ©moire et permettre l'exÃ©cution sur un seul serveur trÃ¨s puissant.</p>"
      },
      "Llama 3.1 8B": {
        "desc": "<p>Petit modÃ¨le conÃ§u pour un usage local sur ordinateur portable, tout en offrant de bonnes capacitÃ©s pour la synthÃ¨se de texte et les rÃ©ponses simples.</p>",
        "size_desc": "<p>Avec 8 milliards de paramÃ¨tres, ce modÃ¨le fait partie des petits modÃ¨les. Il peut Ãªtre utilisÃ© localement sur un ordinateur puissant, garantissant la confidentialitÃ© des donnÃ©es, ou hÃ©bergÃ© sur un serveur Ã©quipÃ© dâ€™une seule carte graphique, ce qui limite les coÃ»ts dâ€™infrastructure. Sa fenÃªtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le est une version distillÃ©e issue des modÃ¨les Llama 3 de plus grande taillesâ€¯: il a Ã©tÃ© entraÃ®nÃ© grÃ¢ce Ã  un transfert dâ€™une partie des connaissances des plus grands modÃ¨les.</p>"
      },
      "Llama 3.3 70B": {
        "desc": "<p>Grand modÃ¨le destinÃ© Ã  un large Ã©ventail de tÃ¢ches et pouvant rivaliser avec des modÃ¨les plus volumineux.</p>",
        "size_desc": "<p>Avec 70 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des grands modÃ¨les. Il nÃ©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraÃ®ne des coÃ»ts dâ€™exploitation significatifs. Sa fenÃªtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le est une version distillÃ©e issue du modÃ¨le 405B, auquel il doit une partie de ses connaissances transfÃ©rÃ©es. Il a aussi bÃ©nÃ©ficiÃ© de techniques rÃ©centes dâ€™alignement et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modÃ¨le apprenait donc en essayant de rÃ©aliser des tÃ¢ches en ligne de maniÃ¨re autonome.  Son entraÃ®nement sâ€™appuie sur 15 billions de jetons.</p>"
      },
      "Llama 4 Scout": {
        "desc": "<p>Grand modÃ¨le dotÃ© dâ€™une trÃ¨s large fenÃªtre de contexte, adaptÃ© par exemple Ã  la synthÃ¨se d'un ensemble de documents.</p>",
        "size_desc": "<p>Avec 109 milliards de paramÃ¨tres, ce modÃ¨le se place dans la catÃ©gorie des grands modÃ¨les. NÃ©anmoins, grÃ¢ce Ã  une architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts), il peut Ãªtre hÃ©bergÃ© sur un serveur dotÃ© dâ€™une seule carte graphique trÃ¨s puissante. Sa fenÃªtre de contexte va jusquâ€™Ã  10 millions de jetons, ce qui permet de traiter des corpus documentaires extrÃªmement longs.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© codistillÃ© avec Behemoth, ce qui veut dire quâ€™il a appris en mÃªme temps que le modÃ¨le gÃ©ant, et non aprÃ¨s comme dans une distillation classique. Il a Ã©tÃ© entraÃ®nÃ© sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacitÃ©s multimodales natives . Lâ€™architecture repose sur un systÃ¨me de mix dâ€™experts (MoE - Mixture of Experts), avec 17 milliards de paramÃ¨tres actifs, 16 experts et 109 milliards de paramÃ¨tres totaux. Afin d'Ã©quilibrer performances multimodales, raisonnement et qualitÃ© conversationnelle, l'Ã©quipe Meta a dÃ©veloppÃ© une stratÃ©gie de post-entraÃ®nement progressive, combinant filtrage adaptatif des donnÃ©es (pour ne garder que les plus complexes et intÃ©ressantes), fine-tuning ciblÃ© et apprentissage par renforcement avec des environnements en ligne (online reinforcement learning) - le modÃ¨le apprenait donc en essayant de rÃ©aliser des tÃ¢ches en ligne de maniÃ¨re autonome. GrÃ¢ce Ã  lâ€™architecture iRoPE (version optimisÃ©e de lâ€™encodage positionnel), il peut gÃ©rer des fenÃªtres de contexte trÃ¨s longues, jusquâ€™Ã  10 millions de jetons et peut traiter jusquâ€™Ã  8 images simultanÃ©ment. </p>\n<p>Le modÃ¨le a Ã©tÃ© bien reÃ§u Ã  son lancement, notamment pour sa fenÃªtre de contexte impressionnante, une premiÃ¨re dans le domaine, ainsi que pour son rapport qualitÃ©-prix sur des tÃ¢ches comme le rÃ©sumÃ©, lâ€™appel dâ€™outils et la gÃ©nÃ©ration augmentÃ©e (RAG). Cela en fait un choix adaptÃ© pour les pipelines automatisÃ©s.</p>"
      },
      "Llama Maverick": {
        "desc": "<p>TrÃ¨s grand modÃ¨le dotÃ© dâ€™une trÃ¨s large fenÃªtre de contexte, adaptÃ© par exemple au rÃ©sumÃ© de plusieurs documents en mÃªme temps.</p>",
        "size_desc": "<p>Avec 400 milliards de paramÃ¨tres, ce modÃ¨le se place dans la catÃ©gorie des grands modÃ¨les. NÃ©anmoins, grÃ¢ce Ã  une architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts), il nÃ©cessite moins de ressources pour fonctionner que les modÃ¨les â€œdensesâ€ de cette taille. Sa fenÃªtre de contexte va jusquâ€™Ã  1 millions de jetons, ce qui permet de traiter de trÃ¨s grands corpus documentaires.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© codistillÃ© avec Behemoth, ce qui veut dire quâ€™il a appris en mÃªme temps que le modÃ¨le gÃ©ant, et non aprÃ¨s comme dans une distillation classique. Cela permet de transfÃ©rer ses compÃ©tences plus vite et avec moins de calcul.  Il a Ã©tÃ© entraÃ®nÃ© sur 30 billions de jetons, combinant texte en 200 langues et images  pour obtenir des capacitÃ©s multimodales natives - il peut traiter jusquâ€™Ã  8 images simultanÃ©ment. Lâ€™architecture repose sur un systÃ¨me de mix dâ€™experts (MoE - Mixture of Experts), avec 17 milliards de paramÃ¨tres actifs, 16 experts et 109 milliards de paramÃ¨tres totaux. L'Ã©quipe Meta a dÃ©veloppÃ© une stratÃ©gie de post-entraÃ®nement progressive, combinant filtrage adaptatif des donnÃ©es - en gardant seulement les plus complexes et intÃ©ressantes, fine-tuning ciblÃ© et apprentissage par renforcement en ligne, pour Ã©quilibrer performances multimodales, raisonnement et qualitÃ© conversationnelle. GrÃ¢ce Ã  lâ€™architecture iRoPE (version optimisÃ©e de lâ€™encodage positionnel), il peut gÃ©rer des fenÃªtres de contexte trÃ¨s longues, jusquâ€™Ã  10 millions de jetons. </p>\n<p>Le modÃ¨le Llama 4 Maverick a Ã©tÃ© prÃ©sentÃ© comme la rÃ©ponse directe de Meta aux modÃ¨les DeepSeek. Cependant, lors de sa sortie, de nombreux utilisateurs ont estimÃ© quâ€™il ne rÃ©pondait pas aux attentes, en particulier sur les tÃ¢ches de programmation et les travaux crÃ©atifs.</p>"
      },
      "Magistral Medium": {
        "desc": "<p>ModÃ¨le de raisonnement de taille moyenne multimodal et multilingue. AdaptÃ© Ã  des tÃ¢ches de programmation ou autres tÃ¢ches nÃ©cessitant analyse approfondie comprÃ©hension de systÃ¨mes logiques complexes ou planification - par exemple pour des cas dâ€™usages agentiques ou de la rÃ©daction de longs contenus complexes.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de grande taille, nÃ©cessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les modÃ¨les de raisonnement requiÃ¨rent plus de capacitÃ© de calcul pour produire une rÃ©ponse, ce qui augmente leur consommation Ã©nergÃ©tique. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>\n<p>Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  40 000 jetons, utile pour lâ€™analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce modÃ¨le fait partie de la premiÃ¨re gÃ©nÃ©ration des modÃ¨les de raisonnement de Mistral AI (Ã©tÃ© 2025). Contrairement Ã  la plupart des autres modÃ¨les de raisonnement, ce modÃ¨le peut raisonner en plusieurs langues incluant l'anglais, le franÃ§ais, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifiÃ©. Il a Ã©tÃ© entraÃ®nÃ© avec de lâ€™apprentissage par renforcement sur Mistral Medium 3 et n'a pas Ã©tÃ© distillÃ© Ã  partir de modÃ¨les de raisonnement existants. Ce modÃ¨le hÃ©rite des capacitÃ©s multimodales de Mistral Medium 3 mÃªme si l'apprentissage par renforcement n'a Ã©tÃ© rÃ©alisÃ© que sur du texte.</p>"
      },
      "Magistral Small": {
        "desc": "<p>ModÃ¨le de raisonnement de taille moyenne, multimodal et multilingue. AdaptÃ© Ã  des tÃ¢ches nÃ©cessitant une analyse approfondie, comprÃ©hension de systÃ¨mes logiques ou planification - par exemple pour des cas dâ€™usages agentiques ou de la rÃ©daction de longs contenus complexes.</p>",
        "size_desc": "<p>Avec 24 milliards de paramÃ¨tres, ce modÃ¨le est classÃ© parmi les modÃ¨les de taille moyenne. Il nÃ©cessite une seule carte graphiques puissante pour fonctionner. Les modÃ¨les de raisonnement fonctionnent Ã©galement plus longtemps pour produire une rÃ©ponse, ce qui augmente leur consommation Ã©nergÃ©tique.</p>\n<p>Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  40 000 jetons, utile pour lâ€™analyse de documents courts mais insuffisant pour analyser des corpus de documents larges.</p>",
        "fyi": "<p>Ce modÃ¨le fait partie de la premiÃ¨re gÃ©nÃ©ration des modÃ¨les de raisonnement de Mistral AI (Ã©tÃ© 2025). Contrairement Ã  la plupart des autres modÃ¨les de raisonnement, ce modÃ¨le peut raisonner en plusieurs langues incluant l'anglais, le franÃ§ais, l'espagnol, l'allemand, l'italien, l'arabe, le russe et le chinois simplifiÃ©. </p>\n<p>L'entraÃ®nement s'est fait en deux phases. La premiÃ¨re, dite de raisonnement <em>cold-start</em> par distillation (de Mistral Medium 3 et OpenThoughts/OpenR1) permet au modÃ¨le d'acquÃ©rir des capacitÃ©s de base en raisonnement Ã  partir de donnÃ©es d'instruction gÃ©nÃ©rale (10%). La seconde est une phase d'apprentissage par renforcement (RL, <em>renforcement learning</em>) Ã  haute entropie, oÃ¹ le modÃ¨le est encouragÃ© Ã  explorer des solutions diverses et variÃ©es plutÃ´t que de converger vers une seule rÃ©ponse, et Ã  gÃ©nÃ©rer des complÃ©tions longues (jusqu'Ã  32 000 jetons), ce qui permet de dÃ©velopper des capacitÃ©s de raisonnement qui dÃ©passent celles du modÃ¨le enseignant.</p>"
      },
      "Ministral": {
        "desc": "<p>Petit modÃ¨le multilingue conÃ§u pour fonctionner sur un ordinateur portable sans connexion Ã  un serveur, tout en offrant de bonnes capacitÃ©s en synthÃ¨se de texte, rÃ©ponses Ã  des questions simples et utilisation dâ€™outils.</p>",
        "size_desc": "<p>Avec ses 8 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des petits modÃ¨les (entre 7 et 20 milliards de paramÃ¨tres). Il peut Ãªtre dÃ©ployÃ© localement sur un ordinateur assez puissant, garantissant la confidentialitÃ© des donnÃ©es ou hÃ©bergÃ© sur un serveur avec une seule carte graphique pour limiter les coÃ»ts dâ€™infrastructure.</p>",
        "fyi": "<p>Ce modÃ¨le utilise une mÃ©thode d'attention de requÃªte groupÃ©e (GQA, grouped query attention) pour limiter le texte analysÃ© Ã  chaque Ã©tape de gÃ©nÃ©ration et gagner en vitesse et en mÃ©moire: les temps de calculs sont rÃ©duits sans incidence sur la qualitÃ©. Le mÃ©canisme d'attention est amÃ©liorÃ© en appliquant des fenÃªtres de tailles diffÃ©rentes, ce qui permet de gÃ©rer de longs contextes (jusquâ€™Ã  128 000 jetons) tout en restant lÃ©ger. Le tokenizer large (V3-Tekken) compresse mieux les langues et le code, ce qui amÃ©liore ses performances sur des tÃ¢ches multilingues.</p>"
      },
      "Mistral Large 2": {
        "desc": "<p>Grand modÃ¨le prÃ©vu pour traiter des questions et tÃ¢ches complexesâ€¯: par exemple gÃ©nÃ©ration de code, utilisation dâ€™outils, analyse de documents longs ou comprÃ©hension prÃ©cise du langage.</p>",
        "size_desc": "<p>Avec 123 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des grands modÃ¨les. Il nÃ©cessite un serveur Ã©quipÃ© dâ€™au moins une carte graphique puissante, ce qui implique un coÃ»t de fonctionnement important. Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  128 000 jetons, utile pour lâ€™analyse de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© entraÃ®nÃ© avec une forte proportion de donnÃ©es en code (plus de 80 langages de programmation) et de mathÃ©matiques, ce qui amÃ©liore sa capacitÃ© Ã  rÃ©soudre des problÃ¨mes complexes et Ã  utiliser des outils externes.</p>"
      },
      "Mistral Medium 2506": {
        "desc": "<p>ModÃ¨le de taille moyenne multilingue, multimodal et peu couteux par rapport Ã  dâ€™autres modÃ¨les qui offrent des performances similaires. Il est particuliÃ¨rement intÃ©ressant pour des tÃ¢ches de programmation ou des tÃ¢ches de raisonnement, par exemple les mathÃ©matiques.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de grande taille, nÃ©cessitant au moins plusieurs cartes graphiques puissantes pour fonctionner. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>\n<p>Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  128 000 jetons, utile pour lâ€™analyse de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© conÃ§u pour offrir des performances solides Ã  un coÃ»t infÃ©rieur Ã  celui des modÃ¨les propriÃ©taires ou semi-ouverts. Une attention particuliÃ¨re a Ã©tÃ© portÃ©e aux donnÃ©es dâ€™usage professionnel pendant son entraÃ®nement. Il est particuliÃ¨rement bon en comparaison Ã  dâ€™autres modÃ¨les de taille similaire Ã  gÃ©nÃ©rer du code et rÃ©aliser des tÃ¢ches mathÃ©matiques.</p>\n<p>Ce modÃ¨le a servi de base pour entraÃ®ner Magistral Medium - un modÃ¨le de raisonnement.</p>"
      },
      "Mistral Saba": {
        "desc": "<p>ModÃ¨le de taille moyenne conÃ§u pour une comprÃ©hension linguistique et culturelle fine des langues du Moyen-Orient et dâ€™Asie du Sud, notamment lâ€™arabe, le tamoul et le malayalam.</p>",
        "size_desc": "<p>La taille exacte du modÃ¨le nâ€™est pas connue. Des indices laissent penser quâ€™il sâ€™agit dâ€™un modÃ¨le de taille moyenne, nÃ©cessitant au moins une carte graphique puissante pour fonctionner. Les estimations disponibles sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse. </p>\n<p>Le modÃ¨le propose une fenÃªtre de contexte allant jusquâ€™Ã  128 000 jetons, adaptÃ©e Ã  lâ€™analyse de longs documents.</p>",
        "fyi": "<p>Lâ€™entraÃ®nement a portÃ© principalement sur des textes en arabe, tamoul et malayalam. Les corpus rÃ©gionaux ont Ã©tÃ© sÃ©lectionnÃ©s pour reflÃ©ter les usages authentiques, y compris la syntaxe, les registres et les variantes dialectales. Pour la tokenisation (dÃ©coupage du texte en unitÃ©s de base que le modÃ¨le peut traiter), une stratÃ©gie spÃ©cialisÃ©e adaptÃ©e aux langues Ã  morphologie complexe comme l'arabe a Ã©tÃ© employÃ©e. Des optimisations visent Ã  Ã©viter la fragmentation excessive des mots et Ã  maximiser la couverture du vocabulaire.</p>"
      },
      "Mistral Small 3.2": {
        "desc": "<p>MalgrÃ© son nom, câ€™est un modÃ¨le de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se dÃ©marque par un respect prÃ©cis des requÃªtes et sa capacitÃ© Ã  utiliser des outils avancÃ©es.</p>",
        "size_desc": "<p>Avec 32 milliards de paramÃ¨tres, ce modÃ¨le est considÃ©rÃ© comme un modÃ¨le de taille moyenne. Il peut Ãªtre hÃ©bergÃ© sur un serveur disposant dâ€™une seule carte graphique puissante, ce qui limite les coÃ»ts dâ€™infrastructure. Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  128 000 jetons, utile pour lâ€™analyse de longs documents.</p>",
        "fyi": "<p>La version 3.2 de ce modÃ¨le est optimisÃ©e pour gÃ©nÃ©rer des sorties structurÃ©es, notamment en JSON, tout en limitant la rÃ©pÃ©titivitÃ© et les comportements indÃ©sirables lors de longues gÃ©nÃ©rations. Multimodal, il traite Ã  la fois des entrÃ©es textuelles et des images, permettant une analyse conjointe.</p>"
      },
      "Nemotron Llama 3.1 70B": {
        "desc": "<p>Grand modÃ¨le entraÃ®nÃ© Ã  partir de Llama 3.1 70B. Cette version rÃ©entraÃ®nÃ©e (fine-tune) a tendance Ã  dÃ©tailler davantage et fournir des rÃ©ponses plus structurÃ©es.</p>",
        "size_desc": "<p>Avec 70 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des grands modÃ¨les. Il nÃ©cessite plusieurs cartes graphiques puissantes pour fonctionner, ce qui entraÃ®ne des coÃ»ts dâ€™exploitation notables.</p>",
        "fyi": "<p>Ce modÃ¨le est issu dâ€™un rÃ©entraÃ®nement du Llama 3.1 70B, d'oÃ¹ la prÃ©sence de son modÃ¨le-source dans son nomâ€¯! Il introduit des amÃ©liorations grÃ¢ce Ã  lâ€™apprentissage par renforcement avec retour humain (RLHF) et Ã  lâ€™algorithme REINFORCEâ€¯: le modÃ¨le explore diffÃ©rentes rÃ©ponses, reÃ§oit des retours sous forme de rÃ©compenses, puis ajuste ses choix progressivement pour mieux rÃ©pondre aux attentes des utilisateurs. Ce processus d'alignement est souvent utilisÃ© quand on veut que le modÃ¨le sâ€™adapte Ã  des prÃ©fÃ©rences humaines ou quâ€™il optimise ses rÃ©ponses selon des critÃ¨res spÃ©cifiques.</p>"
      },
      "Phi-4": {
        "desc": "<p>Petit modÃ¨le multilingue, capable dâ€™utiliser des outils et performant sur des tÃ¢ches complexes comme la logique, les mathÃ©matiques et le code, tout en restant compact.</p>",
        "size_desc": "<p>Avec 14 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des petits modÃ¨les. Il peut Ãªtre dÃ©ployÃ© localement sur un ordinateur suffisamment puissant, ou hÃ©bergÃ© sur un serveur avec une seule carte graphique, ce qui rÃ©duit les coÃ»ts dâ€™infrastructure. La fenÃªtre de contexte, de 16 000 jetons, peut Ãªtre limitante pour lâ€™analyse de documents trÃ¨s longs.</p>",
        "fyi": "<p>Ce modÃ¨le utilise tiktoken pour la tokenisation, ce qui amÃ©liore ses capacitÃ©s en contexte multilingue. Il a Ã©tÃ© entraÃ®nÃ© sur un total de 9,8 <strong>billions</strong> de jetons, dont 400 milliards proviennent spÃ©cifiquement de donnÃ©es synthÃ©tiques de haute qualitÃ©, le reste Ã©tant constituÃ© de donnÃ©es organiques filtrÃ©es. L'entraÃ®nement s'est dÃ©roulÃ© sur 1 920 cartes graphiques H100 pendant 21 jours. Des techniques innovantes comme l'auto-Ã©valuation â€“ pendant laquelle le modÃ¨le critique et rÃ©Ã©crit ses rÃ©ponses â€“ ainsi que l'inversion des instructions ont Ã©tÃ© utilisÃ©es pour renforcer sa comprÃ©hension des consignes et ses capacitÃ©s de raisonnement.</p>"
      },
      "Qwen 2.5 Coder 32B": {
        "desc": "<p>ModÃ¨le de taille moyenne spÃ©cialisÃ© en programmation et dans lâ€™usage dâ€™outils externes (recherches web, interactions avec des logicielsâ€¦).</p>",
        "size_desc": "<p>Avec 32 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut fonctionner sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui limite les coÃ»ts dâ€™infrastructure.</p>\n<p>Sa fenÃªtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© entraÃ®nÃ© sur 5.5 bilions de jetons et plus de 92 langages de programmation, y compris des langages de code spÃ©cialisÃ©s comme Haskell ou Racket. </p>\n<p>GrÃ¢ce Ã  ses performances en code, il est  capable de bien gÃ©rer les appels Ã  des outils externes, ce qui est utile pour des usages agentiques.</p>"
      },
      "Qwen 2.5 max 0125": {
        "desc": "<p>TrÃ¨s grand modÃ¨le de raisonnement spÃ©cialisÃ© et trÃ¨s performant en mathÃ©matiques, code et rÃ©solution de problÃ¨mes logiques.</p>",
        "size_desc": "<p>Ce modÃ¨le propriÃ©taire basÃ© sur une <strong>architecture MoE Ã  grande Ã©chelle a Ã©tÃ©</strong>entraÃ®nÃ© sur <strong>plus de 20 billions de jetons</strong>. Il est conÃ§u pour des tÃ¢ches nÃ©cessitant plusieurs Ã©tapes de rÃ©flexion. </p>\n<p>La fenÃªtre de contexte va jusquâ€™Ã  32 000 jetons.</p>",
        "fyi": "<p>La taille exacte du modÃ¨le nâ€™est pas connue, mais câ€™est trÃ¨s probablement un trÃ¨s grand modÃ¨le nÃ©cessitant des serveurs Ã©quipÃ©s de plusieurs cartes graphiques. NÃ©anmoins, l'architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. Les estimations de taille sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>"
      },
      "Qwen 3 30B A3B": {
        "desc": "<p>ModÃ¨le de taille moyenne multilingue.</p>",
        "size_desc": "<p>Avec 30 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut fonctionner sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui limite les coÃ»ts dâ€™infrastructure. De plus l'architecture de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres Ã  chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique.</p>",
        "fyi": "<p>Ce modÃ¨le MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activÃ©s par jeton, ce qui permet une infÃ©rence plus rapide et plus efficace. Il utilise un systÃ¨me appelÃ© <em>global-batch</em> pour optimiser la rÃ©partition du travail entre les experts, afin qu'ils soient tous utilisÃ©s de maniÃ¨re Ã©quilibrÃ©e.</p>\n<p>Contrairement Ã  d'autres modÃ¨les comme Qwen 2.5-MoE qui recyclent les mÃªmes experts Ã  travers plusieurs couches du rÃ©seau, Qwen 3 30B A2B attribue des experts uniques Ã  chaque couche. ConcrÃ¨tement, cela signifie que les experts de la premiÃ¨re couche ne sont jamais rÃ©utilisÃ©s dans les couches suivantes - chaque niveau du modÃ¨le dispose de son propre ensemble d'experts spÃ©cialisÃ©s. Cette architecture permet Ã  chaque expert de se concentrer exclusivement sur les tÃ¢ches spÃ©cifiques Ã  sa position dans le rÃ©seau neuronal, rÃ©sultant en une spÃ©cialisation plus fine et des performances optimisÃ©es pour chaque Ã©tape du traitement de l'information.</p>"
      },
      "Qwen 3 32B": {
        "desc": "<p>ModÃ¨le de taille moyenne multilingue avec deux modalitÃ©s de rÃ©ponses: lâ€™utilisateur peut choisir entre un mode de raisonnement, pour des rÃ©ponses plus approfondies, ou un mode rapide, pour gÃ©nÃ©rer directement la rÃ©ponse finale.</p>",
        "size_desc": "<p>Avec 32 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut fonctionner sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui limite les coÃ»ts dâ€™infrastructure.</p>\n<p>Sa fenÃªtre de contexte de 128 000 jetons permet de traiter de longs documents.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© entraÃ®nÃ© sur un trÃ¨s grand volume de donnÃ©esâ€¯: 36 billions de jetons, en 119 langues. L'entraÃ®nement sâ€™est fait en trois Ã©tapes. Le modÃ¨le a d'abord appris Ã  partir de 30 billions de jetons avec un contexte de 4 000 jetons. Ensuite, 5 billions de jetons ont Ã©tÃ© ajoutÃ©s pour renforcer ses connaissances factuelles. Enfin, il a Ã©tÃ© exposÃ© Ã  un corpus spÃ©cifique pour lâ€™aider Ã  mieux gÃ©rer les trÃ¨s longs textes. RÃ©sultatâ€¯: il dispose en fin d'entrainement d'une fenÃªtre de contexte de 128 000 jetons, ce qui est utile pour lire et analyser de longs documents.</p>"
      },
      "o4 mini": {
        "desc": "<p>TrÃ¨s grand modÃ¨le de raisonnement, adaptÃ© pour des tÃ¢ches et questions scientifiques et technologiques complexes.</p>",
        "size_desc": "<p>MalgrÃ© son nom et le fait que la taille exacte nâ€™est pas connue, o4 mini est trÃ¨s probablement un grand modÃ¨le nÃ©cessitant des serveurs Ã©quipÃ©s de plusieurs cartes graphiques. Les modÃ¨les de raisonnement comme o4 mini nÃ©cessitent plus de temps pour rÃ©pondre, car une phase de raisonnement prÃ©cÃ¨de la gÃ©nÃ©ration du rÃ©sultat final, ce qui accroit leur consommation Ã©nergÃ©tique. NÃ©anmoins, l'architecture supposÃ©e de mÃ©lange dâ€™experts (MoE, Mixture of Experts) n'active qu'une partie des paramÃ¨tres pour gÃ©nÃ©rer chaque jeton, limitant ainsi son empreinte Ã©nergÃ©tique. Les estimations de taille sâ€™appuient sur des indices indirects comme les coÃ»ts d'infÃ©rence et la latence de rÃ©ponse.</p>",
        "fyi": "<p>Ce modÃ¨le est trÃ¨s performant pour lâ€™analyse dâ€™images et de graphiques. Il a aussi Ã©tÃ© entraÃ®nÃ© pour interagir avec dâ€™autres systÃ¨mes via des appels de fonctions, ce qui rend possible son utilisation pour des cas dâ€™usage agentiques. En tant que modÃ¨le trÃ¨s puissant de raisonnement, il peut notamment Ãªtre utilisÃ© pour rÃ©partir des tÃ¢ches entre plusieurs modÃ¨les plus petits et/ou plus spÃ©cialisÃ©s.  Il dispose dâ€™une fenÃªtre de contexte allant jusquâ€™Ã  200 000 jetons, ce qui facilite lâ€™analyse de longs documents.</p>"
      },
      "qwq 32B": {
        "desc": "<p>ModÃ¨le de raisonnement de taille moyenne spÃ©cialisÃ© et trÃ¨s performant en mathÃ©matiques, gÃ©nÃ©ration de code, et rÃ©solution de problÃ¨mes logiques.</p>",
        "size_desc": "<p>Avec 32 milliards de paramÃ¨tres, ce modÃ¨le appartient Ã  la catÃ©gorie des modÃ¨les de taille moyenne. Il peut fonctionner sur un serveur Ã©quipÃ© dâ€™une seule carte graphique puissante, ce qui limite les coÃ»ts dâ€™infrastructure. NÃ©anmoins, les modÃ¨les de raisonnement de ce type fonctionnent plus longtemps pour produire une rÃ©ponse car une phase de raisonnement prÃ©cÃ¨de la gÃ©nÃ©ration du rÃ©sultat final, ce qui augmente la consommation Ã©nergÃ©tique.</p>",
        "fyi": "<p>Ce modÃ¨le a Ã©tÃ© entraÃ®nÃ© avec une mÃ©thode dâ€™apprentissage par renforcement (RL) pour optimiser la gestion des problÃ¨mes de mathÃ©matiques et des tÃ¢ches de programmation. Il utilise plusieurs techniques rÃ©centes pour amÃ©liorer la qualitÃ© des rÃ©ponses. Par exemple, la mÃ©thode RoPE (Rotary Position Embedding) lui permet de mieux comprendre lâ€™ordre des mots dans un texte. La fonction d'activation SwiGLU est une maniÃ¨re plus efficace de gÃ©rer les calculs au sein du rÃ©seau de neurones qui aide le modÃ¨le Ã  produire des rÃ©ponses plus fiables. La mÃ©thode d'ajustement QKV (Query Key Value-biais) amÃ©liore la maniÃ¨re dont le modÃ¨le repÃ¨re et sÃ©lectionne les informations importantes. Enfin, grÃ¢ce Ã  la mÃ©thode YaRN (Yet another RoPE extensioN method), il peut traiter de trÃ¨s longs textes allant jusquâ€™Ã  130 000 jetons, ce qui lui permet de travailler sur des documents complexes ou trÃ¨s dÃ©taillÃ©s.</p>"
      }
    }
  }
}
