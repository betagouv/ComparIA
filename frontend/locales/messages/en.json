{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "accessData": "Access data",
        "contact": "Contact us",
        "contactUs": "Contact us",
        "copyLink": {
            "do": "Copy the link",
            "done": "Link copied to clipboard"
        },
        "copyMessage": {
            "do": "Copy the message",
            "done": "Message copied"
        },
        "downloadData": "Download data",
        "home": "Homepage",
        "returnHome": "Return to homepage",
        "scrollLeft": "Scroll left",
        "scrollRight": "Scroll right",
        "searchModel": "Search for a model",
        "seeMore": "See more",
        "selectLanguage": "Select a language",
        "vote": "Give feedback"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 models",
            "help": "If you only choose one, the second will be selected randomly",
            "question": "Which models would you like to compare?"
        },
        "modelSelection": "Model selection",
        "prompt": {
            "label": "Write your first message",
            "placeholder": "Write your first message here"
        },
        "selectModels": {
            "help": "Choose the comparison mode",
            "question": "Which models would you like to compare?"
        },
        "suggestions": {
            "generateAnother": "Generate another prompt",
            "title": "Suggested prompts"
        },
        "title": "How can I help you today?"
    },
    "chatbot": {
        "continuePrompt": "Continue the chat with the AI models",
        "conversation": "Chat",
        "errors": {
            "other": {
                "message": "A temporary error has occurred.",
                "retry": "You can retry to query the models again.",
                "title": "Oops, temporary error",
                "vote": "Or finish the experience by giving your preference on these models."
            },
            "tooLong": {
                "message": "Each model is limited in the size of conversations it can handle.",
                "retry": "You can restart a chat with two new models.",
                "title": "Oops, the conversation is too long for one of the models.",
                "vote": "You can still give your preference on these models or start a conversation with two new ones."
            }
        },
        "loading": "Loading answers",
        "reasoning": {
            "finished": "Reasoning completed",
            "inProgress": "Reasoning in progress…"
        },
        "revealButton": "Reveal the models"
    },
    "closeModal": "Close the popup",
    "components": {
        "pagination": {
            "first": "First page",
            "label": "Pages",
            "last": "Last page",
            "next": "Next page",
            "nth": "Page {count}",
            "previous": "Previous page"
        },
        "table": {
            "linePerPage": "Number of lines per page",
            "pageCount": "{count} lines per page",
            "triage": "Sort"
        },
        "theme": {
            "legend": "Choose a theme to customize the appearance of the site.",
            "options": {
                "dark": "Dark theme",
                "light": "Light theme",
                "system": "System",
                "systemSub": "Use system settings"
            },
            "title": "Display settings"
        }
    },
    "datasets": {
        "access": {
            "catch": "Model publishers, researchers, companies, now it's your turn!",
            "desc": "The platform’s questions and preferences are primarily in French, Danish, Swedish and Lithuanian, capturing organic, real-world usage - not artificial prompts. These datasets are publicly available on <a {linkProps}>data.gouv.fr</a> and Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "All the questions and answers",
                    "title": "/conversations"
                },
                "reactions": {
                    "desc": "All the reactions to messages",
                    "title": "/reactions"
                },
                "votes": {
                    "desc": "All the preferences expressed",
                    "title": "/votes"
                }
            },
            "share": "Show us how you’re using the data",
            "title": "Access compar:IA datasets"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "desc": "Analysis of user conversations with detection of tasks (creation, information search, etc.), topics (arts and culture, education, etc.), complex emotions (curiosity, enthusiasm, etc.), language tones (formal, professional, etc.)",
                    "title": "Access the analysis"
                },
                "conversations": {
                    "desc": "Interactive visualization of conversations where each cluster represents a recurring theme discussed by users (such as education, health, the environment, or even philosophy).",
                    "title": "Explore the data visualization"
                },
                "desc": "The Bunka.ai team conducted a large-scale study of user-AI interactions on the chatbot arena, mapping out dominant themes, key tasks, and the balance between automation vs. human augmentation. Their analysis based on 25,000 real conversations offers rare empirical insight into how people actually use AI.",
                "method": "Learn more about the methodology"
            },
            "desc": "Examples of compar:IA dataset reuses",
            "title": "How is this data used?"
        }
    },
    "errors": {
        "404": {
            "desc": "If you typed the URL into your browser, check if it's correct. The page may no longer be available. <br />You can continue by visiting our homepage. <br /> If you struggle to find a page you are looking for, contact us so we can redirect you to the correct URL.",
            "error": "Error 404",
            "sorry": "The page you are looking for cannot be found. We apologize for the inconvenience.",
            "title": "Page not found"
        },
        "unexpected": {
            "desc": "Please try refreshing the page or try again later.",
            "error": "Error {code}",
            "sorry": "Our apologies, there is an issue with the service, we are working to resolve it as quickly as possible.",
            "title": "Unexpected error"
        },
        "unknown": "An error has occurred"
    },
    "faq": {
        "datasets": {
            "questions": {
                "1": {
                    "desc": "<p> Preference data is used to improve models during future training. </p> <p> By blindly comparing the responses of two models, compar:IA users express their preferences, indicating which responses are most relevant. This preference data can be used to improve model alignment—that is, to train them to generate responses that are more in line with user expectations and preferences. </p> <p> This is an iterative process, where the model gradually learns to generate better responses based on feedback from humans on the quality of the responses. By being exposed to preference data, models adjust their response style. </p>",
                    "title": "Does preference data have an immediate effect on model performance?"
                },
                "2": {
                    "desc": "<p> The uniqueness of the data collected on the compar:IA platform is that it is in lower ressourced languages and corresponds to real-life user tasks. This data reflects human preferences in specific linguistic and cultural contexts. This data allows us to adjust the models to make them more relevant, accurate, and adapted to user needs, while attempting to address any biases or gaps. </p>",
                    "title": "Why are the votes on compar:IA useful?"
                },
                "3": {
                    "desc": "<p> compar:IA positions itself as a low-ressourced language assessment and alignment tool for language models, focused on response quality and preference data collection, thus distinguishing itself from the global ranking approach of <a href='https://lmarena.ai/' target='_blank'> chatbot arena </a> developed by <a href='http://lmsys.org' target='_blank'> lmsys.org </a> and the ethical alignment of AI models of <a href='https://hannahkirk.github.io/prism-alignment/' target='_blank'> Prism Alignment Project </a> . </p>",
                    "title": "What makes compar:IA different from other similar initiatives?"
                }
            },
            "title": "Dataset"
        },
        "ecology": {
            "questions": {
                "1": {
                    "desc": "<p> compar:AI uses the methodology developed by <a target='_blank' href='https://ecologits.ai/latest/'> <strong> Ecologits </strong> (GenAI Impact) </a> to provide an estimation for energy consumed that allows users to compare the environmental impact of different AI models for the same query. This transparency is essential to encourage the development and adoption of more eco-responsible AI models. </p> <p> Ecologits applies the principles of Life Cycle Assessment (LCA) in accordance with ISO 14044, focusing for the moment on the impact of <strong> inference </strong> (i.e., the use of models to answer queries) and the <strong> manufacturing of graphics cards </strong> (resource extraction, manufacturing, and transportation). </p> <p> The model's power consumption is estimated by taking into account various parameters such as the size of the AI model used, the location of the servers where the models are deployed, and the number of output tokens. The calculation of the global warming potential indicator expressed in CO2 equivalent is derived from the model's power consumption measurement. </p> <p> It is important to note that methodologies for assessing the environmental impact of AI are still under development. </p>",
                    "title": "How are environmental metrics calculated?"
                },
                "2": {
                    "desc": "<p> Data center locations play a very significant role in AI's carbon footprint. If a model is trained or used in a country heavily dependent on fossil fuels, its environmental impact will be greater than if it is hosted in a country that uses mostly renewable energy. </p> <p> The AI environmental impact analysis method developed by <a target='_blank' href='https://ecologits.ai/latest/'> Ecologits (from GenAI Impact) </a> incorporates data on the energy mix of the different countries where the servers are located. This allows for a more accurate and nuanced estimate of the actual carbon footprint of inference on different generative AI models. </p>",
                    "title": "Do environmental metrics take into account the energy mix of different countries?"
                },
                "3": {
                    "desc": "<p> Many environmental impact metrics focus primarily on the impact of <strong> inference </strong> , that is, the use of AI models to answer queries. This approach may give the illusion that inference is less energy-intensive than model training. However, <strong> the reality is more complex. </strong> Consider the car analogy: </p> <ul> <li> Building a car (training) is a one-time, resource-intensive process. </li> <li> Each car journey (inference) uses less energy, but these journeys are repeated daily, and their number is potentially immense. </li> </ul> <p> Similarly <strong> the cumulative impact of inference, across millions of users querying daily, is often signficantly greater than the impact of initial training <strong> This is why it is crucial that AI carbon footprint assessment tools consider the entire lifecycle </strong> models, from training to inference </strong>  </p>",
                    "title": "Do environmental impact estimations take into account the resources used to train the models?"
                }
            },
            "title": "Environmental data"
        },
        "i18n": {
            "questions": {
                "1": {
                    "desc": "<p> Yes, the internationalization of compar:AI is underway. We are starting with an expansion to three pilot countries: Lithuania, Sweden, and Denmark. This first phase will allow us to test the approach and adapt the interface to different European linguistic and cultural contexts. Eventually, the circle may expand to more European languages based on feedback from these pilot countries. The objective is to gradually build a true European digital common for human evaluation of conversational AI, with collaborative governance that remains to be defined between the different participating countries. </p>",
                    "title": "compar:IA initially focused on French: are there plans for other European languages?"
                },
                "2": {
                    "desc": "<p> The development of a European platform for comparing conversational AI models offers several concrete advantages. It allows for the collection of preference data reflecting the real needs of European users, thus improving the relevance of model answers for this userbase. The publication of the datasets guarantees better representation of European languages and cultures, often underrepresented in global datasets and evaluations dominated by English. It also ensures compliance with European regulations (GDPR, AI Act) and integrates evaluation criteria aligned with European priorities such as environmental impact reduction and algorithmic transparency. Finally, it fosters the emergence of a competitive and autonomous European AI ecosystem. </p>",
                    "title": "What are the advantages of a specifically European preference collection platform?"
                }
            },
            "title": "Internationalization"
        },
        "models": {
            "questions": {
                "1": {
                    "desc": "<p> We choose models based on their popularity, diversity, and relevance to users. We pay particular attention to making <em> open weights </em> and different size models available. </p>",
                    "title": "How do you choose the models present in the arena ?"
                },
                "2": {
                    "desc": "<p> Inference, meaning the ability to query the models, is payed for by the project. For most models, we use the Open Router and Hugging Face Inference Providers on a per-token usage basis. </p>",
                    "title": "How do you manage to keep this service free?"
                },
                "3": {
                    "desc": "<p> Quantized models are optimized to consume fewer resources by simplifying certain calculations while aiming for the best response quality. </p> <p> Quantization is an optimization technique that involves reducing the precision of the numbers used to represent the parameters of an AI model. This allows <strong> to reduce the size of the model </strong> and <strong> to speed up calculations </strong> , which is particularly advantageous for inference on resource-constrained machines. </p>",
                    "title": "What are “quantized models” ?"
                },
                "4": {
                    "desc": "<p> <strong> A model's ability to speak multiple languages is related to the linguistic diversity of its training data, not to the country it was developed in </strong> . <strong> LLMs use huge corpora of data with many languages </strong> , but the distribution of languages in the training data is not uniform. An overrepresentation of English can lead to limitations in other languages. These limitations are visible, for example, in <strong> anglicisms or the inability to generate content in certain languages classified as \"endangered\" by UNESCO </strong> . </p> <p> <strong> The accuracy and richness of a model's vocabulary depend on the data used for its training </strong> . </p>",
                    "title": "Is there a link between the nationality of the company or lab that created the model and its ability to speak several languages?"
                },
                "5": {
                    "desc": "<p> Few stakeholders are “transparent” about the data sources used in training corpora. This information is often confidential for legal and commercial reasons. </p>",
                    "title": "Can we see the training data of the models?"
                }
            },
            "title": "Models"
        },
        "title": "Frequently Asked Questions",
        "usage": {
            "questions": {
                "1": {
                    "desc": "<p> Current conversational language models are <strong> unable to cite their sources </strong> they used to generate an answer. They work by predicting the most likely next token based on the statistical distribution of the training data. While they can synthesize information from various sources, they do not keep track of where that information comes from. </p> <p> However, there are techniques like <strong> Retrieval Augmented Generation (RAG) </strong> that aim to overcome this limitation. RAG allows models to access external knowledge bases and <strong> provide contextualized information by citing the sources </strong> . This approach is useful for improving the transparency and reliability of model answers. </p>",
                    "title": "Can models cite their sources?"
                },
                "2": {
                    "desc": "<p> Have you asked the question: “Explain to me the latest trendy cheesecake recipe and cite your sources” and been disappointed with the answers? That's normal... </p> <p> <strong> “Raw” conversational AI models cannot answer questions about the most recent news. </strong> They are trained on static datasets and cannot interact with the web or open links. They do not have the ability to update themselves in real time with events unfolding in the world. The information the model has access to is limited to the date of its last training. </p> <p> Therefore, if you ask a question about a recent news event, the model will rely on outdated information, risking generating inaccurate answers. </p> <p> In the case of Perplexity, Copilot, or ChatGPT, the so-called “raw” conversational AI models are combined with other system blocks that allow them to connect to the internet to access information in real time. These are called “conversational agents.” </p>",
                    "title": "If I ask a question about the latest news, can the model answer?"
                },
                "3": {
                    "desc": "<p> If you include a URL in a query, the conversational model will not be able to access it directly. Language models process the query text but do not have the ability to interact with the web or open links. They are trained on a fixed text dataset, and their answers are based on this training data. When a question is asked, the models use this training to generate an answer but cannot access new information online if they are not enabled to do so by a surrounding system. </p> <p> As an analogy, imagine a student taking an exam without internet access. They can use their acquired knowledge to answer questions, but cannot visit websites for additional information. </p>",
                    "title": "If I include an URL in a query, can the model access it?"
                },
                "4": {
                    "desc": "<p> Models sometimes lose track of a conversation due to their <strong> limited context window and the \"needle in the haystack\" problem. </strong> This \"window\" represents the amount of previous information the model can retain, acting like a short-term memory. The smaller the window, the more likely the model is to forget key parts of the conversation, leading to inconsistent responses. Long or complex conversations can quickly saturate the context window, increasing the risk of inconsistency. </p> <p> As an analogy, imagine a person who only remembers the last five sentences of a conversation. If the conversation is short, the person can keep up. But if the conversation becomes long, the person will forget crucial information, making their responses inconsistent. Similarly, an AI model with a small context window can \"lose track\" of a conversation when too much information is exchanged, missing key elements and producing responses that no longer make sense. </p>",
                    "title": "Why do some models quickly lose the thread of the conversation?"
                },
                "5": {
                    "desc": "<p> The wording of \"prompts,\" influences the answers significantly. To get the best results from a language model, it is essential to master the art of <em> prompting </em> , that is, the wording of requests or instructions. <strong> Clarity is key </strong> : </p> <ul> <li> Use simple and direct language, avoiding questions that are too long or complex. Break requests down into several simpler questions for more precise answers. </li> <li> <strong> Specify specific format constraints if necessary </strong> : If you need a response in a certain format (list, table, summary, etc.), specify it in the prompt. You can also specify the steps to follow and the desired quality criteria. </li> <li> <strong> Specify the role of the model </strong> For example, start with “Act like an expert in…” or “Imagine you are a teacher…” to guide the tone and perspective of the answer. </li> <li> <strong> Contextualize your questions </strong> If necessary, provide relevant examples to guide the model. </li> <li> <strong> Encourage reasoning </strong> Use chain-of-thought prompting ( <em> Chain-of-Thought Prompting </em> ) to ask the model to explain its reasoning, which makes the answers more robust. </li> </ul> <p> Conversational models are sensitive to variations in wording: simple language, short questions, and rephrasing when necessary can help guide the model toward relevant answers. Test and refine your prompts to find the most effective wording! </p>",
                    "title": "What are the best practices for prompting?"
                },
                "6": {
                    "desc": "<p> Raw conversational AI models answer directly by predicting tokens (words) based on their probability, while a search engine offers links and resources for the user to explore on their own. </p>",
                    "title": "What's the difference between asking a question to a conversation AI model and searching on Google?"
                }
            },
            "title": "Use"
        }
    },
    "footer": {
        "backHome": "Back to home - compar:IA",
        "dpg": "The service is recognized as a digital public good by the Digital Public Goods Alliance",
        "helpUs": "Help us improve the product!",
        "license": {
            "linkTitle": "Etalab license - new window",
            "mention": "Unless otherwise explicitly stated as third-party intellectual property, the contents of this site are offered under the <a {linkProps}>Etalab 2.0 license</a>"
        },
        "links": {
            "accessibility": "Accessibility: non-compliant",
            "legal": "Legal notice",
            "privacy": "Privacy policy",
            "rgesn": "Eco-design",
            "sources": "Source code",
            "tos": "Terms of use"
        },
        "writeUs": "If you encounter a problem or have feedback on the chatbot arena, feel free to write to us <a {formLinkProps}>using this form</a> - we read every message.<br />Thank you!"
    },
    "general": {
        "a11y": {
            "desc": "This accessibility statement applies to the website <strong> comparia.beta.gouv.fr </strong> .",
            "disclaimer": "<strong> compar:IA </strong> is committed to making its digital services accessible, in accordance with Article 47 of Law No. 2005-102 of February 11, 2005.",
            "improveAdress": "Address: DINUM, 20 avenue de Ségur 75007 Paris",
            "improveDelay": "We try to respond within 2 business days.",
            "improveDesc": "If you are unable to access any content or service, you can contact the manager of beta.gouv.fr to be directed to an accessible alternative or obtain the content in another format.",
            "improveMail": "E-mail: <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Improvement and contact",
            "remedyAdvocate": "Write a message to the <a {linkProps}>Defender of Rights</a>",
            "remedyAdvocateAdress": "Send a letter by post (free, do not put a stamp): Defender of Rights - Free response 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Contact the <a {linkProps}>Defender of Rights representative in your region</a>",
            "remedyDesc": "This procedure is to be used in the following case: you have reported to the website manager an accessibility defect which prevents you from accessing content or one of the portal's services and you have not received a satisfactory response.",
            "remedyList": "You can :",
            "remedyTitle": "Appeal",
            "stateDesc": "The comparia.beta.gouv.fr website is non-compliant with RGAA 4.1. The site has not yet been audited <strong>. However, it has been designed to be accessible to as many people as possible </strong> . You should therefore be able to:",
            "stateNavigate": "navigate all pages of the site using a keyboard",
            "statePrefs": "adapt the site to your preferences (font size, screen zoom, change of typography, etc.) without loss of content",
            "stateScreenReader": "view the website with a screen reader.",
            "stateTitle": "Compliance Status",
            "title": "Accessibility statement"
        },
        "legal": {
            "a11yDesc": "Compliance with digital accessibility standards is a future goal, but we strive to make this site accessible to everyone.",
            "a11yTitle": "Accessibility",
            "directorDesc": "Mr. Romain Delassus, Head of the Digital Department at the Ministry of Culture",
            "directorTitle": "Director of the publication",
            "editorDesc": "This site is published by the French Ministry of Culture, 182 Rue Saint-Honoré, 75001 Paris",
            "editorTitle": "Published",
            "hostingDesc": "This site is hosted by OVH SAS (<a {linkProps}>https://www.ovh.com</a>), whose registered office is located at 2 Rue Kellermann, 59100 Roubaix, France.",
            "hostingTitle": "Hosting of the site",
            "reportA11y": "If you encounter an accessibility issue preventing you from accessing any content or functionality on the site, please let us know.",
            "reportA11yDesc": "To learn more about the State’s digital accessibility policy: <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "reportDesc": "If you do not receive a prompt response from us, you have the right to submit your complaint or a request for referral to the Defender of Rights.",
            "reportTitle": "Report a problem",
            "securityCertif": "The site is protected by an electronic certificate, represented in most browsers by a padlock. This protection helps ensure the confidentiality of exchanges.",
            "securityNoMail": "Under no circumstances will the services associated with the platform be the source of emails asking for the input of personal information.",
            "securityTitle": "Security",
            "sources": "Unless otherwise stated, all texts on this site are under the <a {etalabLinkProps}>Etalab Open 2.0 license</a>. The source code of this application is freely reusable and accessible on <a {githubLinkProps}>GitHub</a>.",
            "title": "Legal notice"
        },
        "privacy": {
            "cookiesBannerDesc": "It's true, you didn't have to click on a block covering half of the page to say that you agree to the use of cookies -even if you don't know what that means!",
            "cookiesBannerNoNeed": "Nothing exceptional, no special treatment related to a .gouv.fr domain. We simply respect the law, which states that certain audience tracking tools, properly configured to respect privacy, are exempt from prior authorization.",
            "cookiesBannerTitle": "Why doesn't this site display a cookie consent banner?",
            "cookiesBannerTools": "We use <a {matomoLinkProps}>Matomo</a>, a <a {libreLinkProps}>free</a> tool, configured to comply with the CNIL's \"Cookies\" <a {cnilLinkProps}>recommendation</a>. This means that your IP address, for example, is anonymized before being recorded. It is therefore impossible to associate your visits to this site with your person.",
            "cookiesDesc": "This website places a small text file (a \"cookie\") on your computer when you visit it. This allows us to measure the number of visits and understand which pages are the most viewed.",
            "cookiesDescMore": "You can opt out of tracking your browsing on this website. This will protect your privacy, but it will also prevent the owner from learning from your actions and creating a better experience for you and other users.",
            "cookiesTitle": "Cookies and Consent",
            "dataAccessDatasets": "User dialogue and preference data are distributed under Etalab's Open License 2.0 on the Hugging Face platform as well as on Data.gouv.fr through the Ministry of Culture's account (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "dataAccessDesc": "Of course! The site's usage statistics are freely accessible at <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessTitle": "I contribute to enriching your data, can I access it?",
            "dataExtraCountry": "Destination country: France",
            "dataExtraHost": "Subcontractor: OVH",
            "dataExtraTitle": "Who helps us process the data?",
            "dataExtraWarranty": "Guarantees: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>",
            "dataExtraWhat": "Processing carried out: Accommodation",
            "dataRespDesc": "The French Ministry of Culture's digital department is responsible for processing your personal data.",
            "dataRespTitle": "Who is responsible for data processing?",
            "dataTimeDesc": "Data relating to users and their conversations with models are retained from the time the preference vote is recorded.",
            "dataTimeTitle": "How long do we keep this data?",
            "dataUseDesc": "In any case, the publisher commits to implementing means to ensure the anonymization of dialogue data before making it publicly available.",
            "dataUseTitle": "What processing is done on the conversation data?",
            "desc": "The service is published by the Digital Department of the French Ministry of Culture.",
            "privacyData": "The data collected on the site are as follows:",
            "privacyDataArena": "Data related to user conversations with the models: questions asked by users, model responses, and user preferences expressed between the two models",
            "privacyDataForm": "Data related to the questionnaire \"Help us improve compar:IA\".",
            "privacyDesc": "The service does not process personal data as defined by the CNIL, meaning any information relating to an identifiable natural person, directly or indirectly.",
            "privacyResp": "The user is responsible for the data or content they enter in the prompt provided by the platform. By accepting the <a {linkProps}>terms of use</a>, the user agrees not to transmit any information that could identify themselves or a third party.",
            "privacyTitle": "Do we process personal data?",
            "title": "Privacy policy"
        },
        "rgesn": {
            "1": {
                "1": "The compar:IA website provides easy and free access to a wide variety of conversational AI models, raising public awareness of the various challenges of generative AI: diversity, bias, and environmental impact. Furthermore, the datasets of questions and preferences compiled are shared as open source under the MIT/Etalab 2.0 license.",
                "2": "The target users of the Compar:IA service were identified by conducting user interviews and a preliminary phase of needs investigation.",
                "3": "Thus, all citizens are concerned as potential users, but particularly teachers, pupils, students, workshop facilitators, AI trainers, as well as researchers (computer science, AI, humanities and social sciences) and companies interested in analyzing or using open-source shared datasets. Accordingly, the digital service meets their needs through the following features: testing and comparing models for free, and using open-source shared data.",
                "4": "The compar:IA service is part of an eco-design approach aimed at reducing environmental impacts. To this end, this declaration was drafted within the framework of the implementation of the General Eco-design Framework for Digital Services (RGESN, version 2024). The RGESN, produced by Arcep and Arcom, in collaboration with ADEME, DINUM, CNIL, and Inria, is available on the <a {linkProps}>Arcep website</a>.",
                "title": "Purpose of this service"
            },
            "2": {
                "1": "Eco-design of digital services is an approach aimed at reducing and limiting the environmental impacts of a service from its design phase and throughout its life cycle.",
                "2": {
                    "1": "Designing more sustainable digital services to extend the lifespan of devices;",
                    "2": "Promote an approach based on environmental sobriety in response to user attention capture strategies, in line with international environmental objectives;",
                    "3": "Reduce the IT resources used, optimize data traffic and the use of digital infrastructure;",
                    "4": "Increase the level of transparency on the environmental footprint of the digital service.",
                    "title": "Its implementation pursues four main objectives:"
                },
                "3": {
                    "1": "Limit the size of a page",
                    "2": "Limit the complexity of a page",
                    "3": "Limit the number of server requests",
                    "4": "Limit the energy consumption required to host the service",
                    "title": "In practical terms, eco-design translates into a set of best practices to be followed before, during, and after the creation or improvement of a service, enabling, in particular:"
                },
                "4": "This is a process of continuous and collective improvement.",
                "title": "What is eco-design?"
            },
            "3": {
                "1": {
                    "1": "Simplifying the user experience: navigation paths are optimized and centered around the essential features of the Compar:IA service (arena of models to test, datasets, ranking) with regular user testing.",
                    "2": "Raising awareness among users about the ecological footprint of the service: highlighting energy impact indicators on questions asked to AI; promoting \"frugal\" uses, with a mode dedicated to querying \"small\" conversational AI models whose environmental impact is smaller.",
                    "3": "Use of State Design System (DSFR) interface components: they are accessible and their footprint is minimal.",
                    "4": "The most intuitive navigation possible to reduce the time spent finding information.",
                    "5": "Mobile-first design: all content is viewable on mobile devices. The site adapts to all screen sizes.",
                    "6": "Exclusion of video integration on the site.",
                    "title": "The main challenge is to ensure that the compar:IA service works on the oldest possible user devices, under varying connectivity conditions. Several measures were implemented during the design phase, including:"
                },
                "title": "Strategy implemented and objectives regarding the reduction or limitation of environmental impacts"
            },
            "4": {
                "1": "The self-assessment, conducted between May and October 2025 by the product team (two people were responsible for reviewing and validating all criteria), covered all pages of the site. The tools used to evaluate certain technical indicators were the browser's developer tools' \"network\" inspector, the <a {greenitLinkProps}>Green IT Analysis</a> extension, and the <a {ecoindexLinkProps}>EcoIndex</a> tool.",
                "title": "RGESN Diagnosis"
            },
            "5": {
                "1": {
                    "1": "Progress score in the implementation of the RGESN, as of 10/12/2025: 89%",
                    "title": "The self-diagnosis gives a compliance rate with the <a {linkProps}>RGESN</a> (general ecodesign reference) of 89%."
                },
                "2": "The digital service compar:IA aims to improve this progress score to reach 95% by 2027.",
                "3": {
                    "1": "Strategy: 100%",
                    "2": "Specifications: 78%",
                    "3": "Architecture: 100%",
                    "4": "User Experience and Interface (UX/UI): 100%",
                    "5": "Contents: 67%",
                    "6": "Frontend: 82%",
                    "7": "Backend: 64%",
                    "8": "Hosting: 90%",
                    "9": "Algorithms: 100%",
                    "title": "Score by theme:"
                },
                "4": {
                    "date": "December 10, 2025",
                    "title": "RGESN Audit Table"
                },
                "title": "Score in the implementation of the framework"
            },
            "6": {
                "1": "The site is designed to be accessible for any mobile device dating from 2017 or later.",
                "2": "Minimum connection required for comfortable access and use of the service: 3G on mobile and 512 Kbps on fixed connection.",
                "3": "The site is designed for different screen sizes (minimum 320 pixels wide).",
                "4": "The site is not accessible via the Internet Explorer browser. ",
                "title": "Minimum requirements to access the site"
            },
            "7": {
                "1": "Design the service with a design review and a code review, one of the objectives of which is to reduce the environmental impacts of each feature (criterion 2.1)",
                "2": "Implement data retention periods for the purpose of deletion or archiving: regular deletion of logs in the \"S3\" file server and the database (Postgres) in particular.",
                "3": {
                    "1": "Maximum number of server requests per screen: 40",
                    "2": "Maximum resource size per screen: 600 KB",
                    "title": "To adhere to a maximum weight and a limit on the number of requests per screen (criterion 6.1):"
                },
                "4": "Convert the illustrations to WebP, AVIF, JPEG XL or a more efficient image format for raster images (criterion 5.1).",
                "5": "Allow the possibility to interrupt inference (loading of responses) during model comparison.",
                "6": "EcoIndex analysis scores demonstrate a significant discrepancy in impact measurements, with some pages receiving scores of up to E or F (relatively large page weight, complexity, and number of requests); the challenge now is to understand these results more precisely on a case-by-case basis, despite the good practices implemented, in order to identify further improvements.",
                "desc": "Several improvements have been or are still being identified, including:",
                "title": "Improvements identified"
            },
            "desc": "published on December 11, 2025",
            "title": "Ecodesign statement"
        },
        "tos": {
            "contactDesc": "For any questions about the service, you can write to <a {linkProps}>{contactLink}</a>.",
            "contactTitle": "9. Contact",
            "defsEditor": "“Publisher” refers to the Digital Department of the Ministry of Culture.",
            "defsModels": "\"Models\" refers to the large language models (LLMs) reused under their usage license by the platform to fulfill its purposes.",
            "defsPlatform": "“Platform” refers to the website that makes the services accessible.",
            "defsServices": "\"Services\" refers to the features offered by the platform to fulfill its purposes.",
            "defsTitle": "2. Definitions",
            "defsUser": "“User” refers to any natural person consulting the platform and benefiting from its services.",
            "descDatasets": "These datasets will be made accessible under an open license, particularly to encourage research uses.",
            "descEditor": "Published by the Digital Department of the French Ministry of Culture, the arena is a platform for comparing conversational models aimed at the general public with the goal of (1) raising citizens' awareness of large language models (LLMs), and (2) collecting user preferences to create alignment datasets.",
            "descTitle": "3. Platform description",
            "descUse": "The user asks a question in a given language and receives answers from two anonymous large language models (LLMs). They vote for the model that provides their preferred response and are then shown the identities of the models. This participatory production system, inspired by the \"<a {linkProps}>chatbot arena</a>\" platform (LMarena), allows for the creation of datasets of human preferences for real-world tasks in French, which can be used for model alignment.",
            "dispoDesc": "The platform is accessible, except in cases of force majeure or events beyond the control of its publisher.",
            "dispoResp": "In this regard, the publisher cannot be held responsible for any losses or damages of any kind that may result from a malfunction or unavailability of the service. Such situations will not entitle any financial compensation.",
            "dispoRight": "The publisher reserves the right to suspend, interrupt, or limit, without prior notice, access to all or part of the services, particularly for maintenance and update operations necessary for the proper functioning of the service and related equipment, or for any other reason, including technical reasons.",
            "dispoTitle": "7. Service availability",
            "dispoWarranty": "It is not guaranteed that the service will be free of anomalies or errors. Therefore, the service is provided without any warranty regarding its availability and performance.",
            "evoDesc": "The terms of use may be modified or supplemented at any time without prior notice, depending on changes made to the services, changes in legislation, or for any other reason deemed necessary.",
            "evoDescMore": "These modifications and updates are binding on the user, who should therefore regularly refer to this section to check the current general terms.",
            "evoTitle": "8. Changes to the Terms of Use",
            "featuresDatasets": "The service collects conversation data (questions, responses from both models) and user preferences (votes, associated metadata). This data is used both to create public datasets and to build a ranking of AI models displayed on the platform, based on the votes cast by the users.",
            "featuresDatasetsMore": "The publisher reserves the right to distribute the user's dialogue and preference data under an etalab 2.0 license. The dataset is disseminated on Data.gouv and the Hugging Face platform through the French Ministry of Culture's account (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "featuresDesc": "To meet the dual objective of raising citizen awareness about large language models and collecting user preferences, the platform provides the following services without access restrictions:",
            "featuresDescMore": "A human-machine interface that allows users to dialogue simultaneously with two conversational models and vote for the preferred response.",
            "featuresModels": "The models integrated into the platform are deployed on the inference servers of the various hosting providers and model publishers. Requests are handled primarily by the Open Router service. Inference conditions are defined on the platform to ensure transparent model usage.",
            "featuresModelsMore": "The model comparison interface.",
            "featuresTitle": "4. Features",
            "featuresVote": "The user can view the list of models on the arena and access more information on these models.",
            "featuresVoteMore": "Sharing, providing access and reuse of datasets generated by user preferences.",
            "licenceCode": "The platform's source code is open and available here: <a {linkProps}>https://github.com/betagouv/ComparIA</a>",
            "licenceLLM": "The LLMs used to power the services are governed by the following licenses:",
            "licenceLLMEvolution": "The list of language models integrated into the platform is subject to change over time and is updated with each modification.",
            "licenceLLMLicence": "License",
            "licenceLLMModel": "Conversational AI model",
            "licenceLLMNoticeLink": "Link to the model licenses",
            "licenceLLMUnavailable": "Not available",
            "licenceTitle": "6. Code and licenses",
            "respEditor": "In general, the publisher disclaims any liability in the event of non-compliance with the terms of use.",
            "respLegal": "The platform is not intended to be used for generating illegal content or content that is contrary to public order, and more generally, any content generation that violates the current legal framework.",
            "respLegalMore": "In this regard, the user does not enter content or information in the prompt that is contrary to current legal and regulatory provisions.",
            "respPrivacy": "Since the data entered by the user on the platform is intended to be made available publicly, the user is responsible not to input any information that could identify them or a third party.",
            "respPrivacyMore": "In any case, the publisher has to implement measures to ensure as much as possible the anonymization of dialogue data before making it available. If, despite the publisher's efforts, sensitive data were to be published in the datasets, you can report it via this form: [https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ](https://adtk8x51mbw.eu.typeform.com/to/B49aloXZ).",
            "respTitle": "5. Responsabilities",
            "respUser": "The user is responsible for the data or content they enter in the prompt provided by the platform.",
            "scopeDesc": "Access to the platform is free, does not require registration, and entails the application of specific conditions, listed in these terms of use.",
            "scopeTitle": "1. Scope of application",
            "title": "Terms of use"
        }
    },
    "generated": {
        "archs": {
            "dense": {
                "desc": "Dense architecture refers to a type of neural network in which each neuron in one layer is connected to all neurons in the next layer. This allows all parameters in the layer to contribute to the output calculation.",
                "name": "Dense",
                "title": "Dense Architecture"
            },
            "matformer": {
                "desc": "Imagine **Russian dolls** (matryoshkas → matryoshka transformer → Matformer): each block contains several **nested sub-models** of increasing size, sharing the same parameters. This allows, for each request, to select a model of adapted capacity, according to the available memory or latency, without needing to re-train different models.",
                "name": "Matformer",
                "title": "Matformer Architecture"
            },
            "moe": {
                "desc": "The Mixture of Experts (MoE) architecture uses a routing mechanism to activate, depending on the input, only certain specialized subsets (“experts”) of the neural network. This allows for the construction of very large models while keeping computational costs low, because only a part of the network is used at each step.",
                "name": "MoE",
                "title": "MoE Architecture"
            },
            "na": {
                "desc": "The publisher has not made information about the model's architecture public.",
                "name": "Proprietary",
                "title": "Architecture N/A"
            }
        },
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "license_desc": "<p> This license allows you to freely use, modify, and distribute the model, including for commercial purposes. In addition to freedom of use, it guarantees legal protection by including a patent grant clause that acts as insurance: if you use this model, the contributors agree not to sue you for violating their patents related to the project. This mutual protection avoids legal conflicts between users and developers. When distributing modified versions, significant changes must be indicated with appropriate notices, ensuring transparency for the user. </p>"
                },
                "CC-BY-NC-4.0": {
                    "license_desc": "<p> This license allows you to freely share and adapt the content as long as you credit the author, but prohibits any commercial use. It provides flexibility for non-commercial uses while protecting the author's rights. </p>",
                    "reuse_specificities": "but only for non-commercial uses"
                },
                "Gemma": {
                    "license_desc": "<p> This license is designed to encourage the use, modification, and redistribution of the software, but includes a clause stating that all modified or improved versions must be shared with the community under the same source license, thus promoting collaboration and transparency in software development. </p>"
                },
                "Jamba Open Model": {
                    "commercial_use_specificities": "below $50 million in annual revenue",
                    "license_desc": "<p> This license allows free use, reproduction, modification, and distribution of the code with attribution, but imposes restrictions for organizations exceeding $50 million in annual revenue. </p>"
                },
                "LFM 1.0": {
                    "license_desc": "<p> This worldwide, free, and non-exclusive license allows the use, modification, and redistribution of the model and its derivatives, including the reuse of outputs to train other models. It remains permissive but limits commercial use to organizations with annual revenues below $10 million; beyond this threshold, usage is no longer covered. </p>"
                },
                "Llama 3 Community": {
                    "commercial_use_specificities": "under 700 million users",
                    "license_desc": "<p> This license allows free use, modification, and distribution of the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users and prohibits reuse of the code or generated content for training or improving competing models, thus protecting Meta's technology investments and brand. </p>"
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "below 700 million users",
                    "license_desc": "<p> This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users. Reuse of the code or generated content for training or improving derivative models is permitted provided that you display “built with llama” and include “Llama” in their name for any distribution. </p>"
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "below 700 million users",
                    "license_desc": "<p> This <strong> non-exclusive, worldwide, royalty-free </strong> license allows you to freely use, reproduce, modify, and distribute the Llama 3.3 code and Materials with attribution. It notably permits reuse for improving derivative models, but imposes restrictions on very large-scale commercial operations. </p>"
                },
                "Llama 4": {
                    "commercial_use_specificities": "below 700 million users\n",
                    "license_desc": "<p> This non-exclusive, worldwide, royalty-free license allows you to use, reproduce, modify, and distribute the Llama 4 Materials (models and documentation) with attribution. However, it imposes two major restrictions: (1) companies exceeding 700 million monthly active users must obtain a special license from Meta, and (2) <strong> total exclusion </strong> of EU residents and companies headquartered in the EU from directly using the multimodal models, due to regulatory uncertainties related to the European AI Act. European end users may nevertheless access services integrating Llama 4, provided they are provided from outside the EU. </p>"
                },
                "MIT": {
                    "license_desc": "<p> The MIT License is a permissive free software license: it allows anyone to reuse, modify, and distribute the model, even for commercial purposes, provided they include the original license and copyright notices. </p>"
                },
                "Mistral AI Research License": {
                    "license_desc": "<p> This non-exclusive, royalty-free license authorizes the use, copying, modification, and distribution of Mistral models and their derivatives (including modified or refined versions). However, it is strictly limited to research purposes. </p>",
                    "reuse_specificities": "but only for non-commercial uses"
                },
                "Modified MIT": {
                    "license_desc": "<p>A variant of the MIT license. The MIT license is a permissive free software license: it allows anyone to reuse, modify, and distribute the model, even for commercial purposes, provided they include the original license and copyright notices.</p>"
                }
            },
            "proprio": {
                "Alibaba": {
                    "license_desc": "The model is available under paid license and accessible via API on Alibaba company platforms, requiring pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "Amazon": {
                    "license_desc": "The model is available under paid licensing and accessed through Amazon Bedrock, requiring to pay-as-you-go based on the number of tokens processed or infrastructure reserved.",
                    "reuse_specificities": "except to distill or train other models on Amazon's platforms."
                },
                "Anthropic": {
                    "license_desc": "The model is available under paid license and accessible via API on the Anthropic platform or partner platforms, requiring to pay-per-use based on the number of tokens processed or based on the infrastructure reserved to host the model."
                },
                "Google": {
                    "license_desc": "The model is available under paid license and accessible via API on Google platforms, requiring to pay-per-use based on the number of tokens processed or on infrastructure reserved.",
                    "reuse_specificities": "except for training other models on Vertex AI"
                },
                "Liquid": {
                    "license_desc": "This model is available under paid license and accessible via API from platforms by Liquid AI and partners, requiring to pay by token or infrastructure reserved."
                },
                "Mistral AI": {
                    "license_desc": "The model is available under a paid license and accessible via the Mistral API, Amazon Sagemaker, and several other infrastructure providers, requiring to pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "OpenAI": {
                    "license_desc": "The model is available under a paid license and accessible via API on OpenAI's platforms or through Microsoft Azure services, requiring to pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "xAI": {
                    "license_desc": "The model is available under a paid license and accessible via X and xAI, requiring to pay-per-use based on the number of tokens processed or infrastructure reserved."
                }
            }
        },
        "models": {
            "Apertus 70B Instruct": {
                "desc": "<p> Open source and reproducible medium-sized model developed by a consortium of Swiss institutions. Its weights and training code are published under a permissive license. It was trained on over 1,800 languages on over 15 trillion tokens. Training took place on the CSCS Alps supercomputer in Lugano, powered by carbon-neutral hydroelectric power. </p>",
                "fyi": "<p> The model was trained on the Alps supercomputer in Lugano, using over 10 million GPU hours powered by carbon-neutral hydroelectric power. The model was pre-trained on 15 trillion tokens covering over 1,800 languages, including a significant proportion of underrepresented languages. </p>\n <p> Apertus is based on a 131,000-entry byte-level BPE tokenizer, derived from Mistral AI's \"tekken\" tokenizer, optimized for multilingualism, code, and mathematical expressions. The architecture combines several innovations: Rotary Positional Embeddings (RoPE) with an extended base and NTK-aware tuning for long contexts, Grouped Query Attention (GQA) for better memory efficiency, QK-Norm normalization to stabilize training, and an xIELU (extended Integrated ELU) activation function improving MLP performance. </p>\n <p> The final refinement of the model relies on an alignment algorithm called QRPO (Quantile Reward-Preferring Optimization), an alternative to classic RLHF, which uses absolute reward signals for more stable learning that is better aligned with human preferences. While it doesn't directly compete with the most advanced proprietary models, Apertus stands out for its level of transparency. </p>",
                "size_desc": "<p> With 70 billion parameters, this model is in the large model category. It requires at least two powerful GPUs for hosting, which leads to significant operating costs. Its 65,536-token context window allows it to process fairly long documents.</p>"
            },
            "Apertus 8B Instruct": {
                "desc": "<p> Small, open-source, and reproducible model developed by a consortium of Swiss institutions. Its weights and training code are published under a permissive license. It was trained on over 1,800 languages on over 15 trillion tokens. Training took place on the CSCS Alps supercomputer in Lugano, powered by carbon-neutral hydroelectric power. </p>",
                "fyi": "<p> The model was trained on the Alps supercomputer in Lugano, using over 10 million GPU hours powered by carbon-neutral hydroelectric power. The model was pre-trained on 15 trillion tokens covering over 1,800 languages, including a significant proportion of underrepresented languages. </p>\n <p> Apertus is based on a 131,000-entry byte-level BPE tokenizer, derived from Mistral AI's \"tekken\" tokenizer, optimized for multilingualism, code, and mathematical expressions. The architecture combines several innovations: Rotary Positional Embeddings (RoPE) with an extended base and NTK-aware tuning for long contexts, Grouped Query Attention (GQA) for better memory efficiency, QK-Norm normalization to stabilize training, and an xIELU (extended Integrated ELU) activation function improving MLP performance. </p>\n <p> The final refinement of the model relies on an alignment algorithm called QRPO (Quantile Reward-Preferring Optimization), an alternative to classic RLHF, which uses absolute reward signals for more stable learning that is better aligned with human preferences. While it doesn't directly compete with the most advanced proprietary models, Apertus stands out for its level of transparency. </p>",
                "size_desc": "<p> With 8 billion parameters, this model is one of the smallest models. It can be used locally on a powerful computer, ensuring data confidentiality, or hosted on a server equipped with a single graphics card, which limits infrastructure costs. Its context window of 65,536 tokens allows it to process fairly long documents. </p>"
            },
            "Aya 23 8B": {
                "desc": "<p>Small multilingual language model trained on a large portion of generally under-represented languages.</p>",
                "fyi": "<p>Aya 23 8B by Cohere is a small language model from the Command R family, specifically trained on a multilingual corpus.</p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Aya Expanse 32B": {
                "desc": "<p> Mid-sized multilingual model, trained to generate text in 23 languages. </p>",
                "fyi": "<p> Cohere, the Canadian company that trained this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous “Attention Is All You Need” paper that revolutionized modern AI. Its main uniqueness lies in its exclusive focus on generative AI for businesses, particularly regulated sectors such as finance, healthcare, manufacturing, and energy, as well as the public sector. The company is also a pioneer in multilingual approaches and maintains a non-profit research lab to support open innovation. </p>\n <p> This model was designed to provide good capabilities in each of the 23 languages in its training corpus. </p>",
                "size_desc": "<p> With 32 billion parameters, this model falls is considered mid-sized. It can be hosted on a server with a single powerful graphics card, which helps keep infrastructure costs low. </p>\n <p> It has a context window of up to 130,000 tokens, useful for analyzing long documents. </p>"
            },
            "Aya Expanse 8B": {
                "desc": "<p>Small multilingual model, second iteration of the Aya series. It was trained on a large portion of under-represented languages.</p>",
                "fyi": "<p> Aya Expanse 8B from Canadian company Cohere is a small model of the Command R family that has been specially trained on a multilingual corpus. </p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Aya23-35B": {
                "desc": "<p> Medium-sized multilingual model, specifically trained in large proportion on generally underrepresented languages. </p>",
                "fyi": "<p> Cohere's Aya 23 35B is a medium-sized model from the Command R family that has been specially trained on a multilingual corpus. </p>",
                "size_desc": "<p>Medium sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Chocolatine 14B": {
                "desc": "<p> Based on Microsoft's Phi-3 Medium model, this model has been specialized for the French language. </p>",
                "fyi": "<p> Based on Microsoft's Phi-3 Medium model, this model has been specialized for the French language. The model's name 'Chocolatine' is a nod to the CroissantLLM project, which was one of the first initiatives to create a small, open-source model optimized for the French language. </p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Chocolatine 2 14B": {
                "desc": "<p> Based on Alibaba's Qwen2.5 model, this model has been specialized in the French language. </p>",
                "fyi": "<p> Based on Alibaba's Qwen2.5 model, this model is specialized for the French language. The name of the model 'Chocolatine' is a nod to the CroissantLLM project, which was one of the first initiatives to create a small, open-source model optimized for French. </p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Claude 3.5 Sonnet v2": {
                "desc": "<p> Very efficient model in code, made after a post-training improvement compared to Claude 3 </p>",
                "fyi": "<p> The best model in the Claude 3.5 family, this model specializes in generating literary texts and a more natural tone. Version v2 was released in October 2024. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p> Very large, multimodal and multilingual model, efficient for code generation, with two response modes: the user can choose between a reasoning mode, for more in-depth answers, or a fast mode, to directly generate the final answer. </p>",
                "fyi": "<p> Claude 4 Opus is the most advanced model in the Claude 4 family. It is optimized for raw power and complex tasks requiring sustained reasoning over long periods of time: for example, it can work on long-term tasks (Anthropic claims it can work independently for up to seven hours). On the other hand, Opus is more expensive to use, slower to respond, and requires more resources to run. </p>\n <p> The model offers two modes of use: a reasoning mode with step-by-step reasoning for complex problems, and a quick mode for direct answers. Unlike other models, the reasoning mode was not primarily trained on mathematical data, but adapted to real-life use cases. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a very large model, requiring servers with multiple powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. It has a context window of up to 200,000 tokens, suitable for analyzing long documents or code repositories. </p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p> Very large multimodal and multilingual model, very powerful in code. The user or developer using this model can choose between several levels of reasoning. </p>",
                "fyi": "<p> Claude 4 Sonnet is a more compact version of Claude 4 Opus optimized for speed, efficiency, and accessibility. It is slightly less effective at tasks requiring complex, multi-step reasoning. However, it is significantly less expensive, faster, and more energy-efficient than Opus 4. </p>\n <p> The model offers the possibility to choose the intensity of \"reasoning\". Unlike other models, the reasoning mode was not primarily trained on mathematical data, but mainly on real-life use cases. </p>",
                "size_desc": "<p> The exact size is unknown. Evidence suggests that this is a very large model, requiring servers with multiple powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. The model has a context window of up to 1,000,000 tokens, suitable for analyzing very long documents or code repositories. </p>"
            },
            "Claude 4.5 Sonnet": {
                "desc": "<p> Very large multimodal and multilingual model, extremely powerful in coding, reasoning, and mathematics. The user or developer using this model can choose between multiple levels of reasoning. </p>",
                "fyi": "<p> Claude Sonnet 4.5 is a direct evolution of Sonnet 4. The \".5\" refers to the major changes introduced during post-training, which result in significant gains in reasoning, mathematics, and especially in the practical use of computers. At the time of its release, it was considered the best model in the world for coding and excelled at solving long and complex multi-step tasks. Its performance on benchmarks such as SWE-bench Verified and OSWorld marked a clear improvement over previous versions, with the ability to maintain \"focus\" for more than thirty hours on the same problem. </p>",
                "size_desc": "<p> The exact size is unknown. All of the available information sources suggest that this is a very large model, requiring servers with multiple powerful graphics cards to run it. Size and power consumption estimates are based on indirect clues such as inference costs and observed latency. Claude Sonnet 4.5 has a context window of up to 1,000,000 tokens, suitable for analyzing entire code repositories or very large documents. </p>"
            },
            "Claude 4.6 Sonnet": {
                "desc": "<p> A very large, multimodal, and multilingual model, highly efficient in coding, standalone computer operation, and reasoning. The user or developer can choose between several levels of reasoning. </p>",
                "fyi": "<p>Claude Sonnet 4.6 succeeds Sonnet 4.5 and represents a major upgrade to Anthropic's mid-range lineup. The model achieves 79.6% on SWE-bench Verified, a benchmark score for code generation, and 72.5% on OSWorld for computer usage, approaching the performance of Opus 4.6 at a cost five times lower. Math performance (89%) is also significantly improved compared to its predecessor.</p>\n<p>The model supports adaptive reasoning, allowing the intensity of its reasoning to be modulated according to the complexity of the task. Its context window reaches 1,000,000 tokens in beta, a first for a model in the Sonnet family, enabling the processing of entire codebases or large document sets in a single query. In internal comparative tests, users preferred Sonnet 4.6 to Sonnet 4.5 in approximately 70% of cases.</p>",
                "size_desc": "<p>The exact size is unknown. All indications are that it is a very large model, requiring servers equipped with multiple powerful graphics cards to run it. Size and power consumption estimates are based on indirect indicators such as inference costs and observed latency. Claude Sonnet 4.6 has a context window of up to 1,000,000 tokens, suitable for analyzing entire code repositories or very large documents.</p>"
            },
            "Command A": {
                "desc": "<p> Large model, efficient for programming, use of tools, and “retrieval augmented generation” (RAG). </p>",
                "fyi": "<p> Cohere, the Canadian company behind this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous paper <a href=\"https://arxiv.org/abs/1706.03762\"> \"Attention Is All You Need\" </a> published in 2017 and which revolutionized AI. The company stands out for its exclusive focus on generative AI for businesses, particularly regulated sectors such as finance, healthcare, manufacturing and energy, as well as the public sector. The company is also a pioneer in multilingual approaches and maintains a non-profit research lab to support open innovation. </p>\n <p> This model is designed to work in more than 23 languages and to integrate easily into enterprise systems. It is one of the few models distributed under <strong> CC-BY-NC 4.0 license, which allows sharing and modification but prohibits any commercial use. </strong> This choice of license reflects Cohere's desire to contribute to research and the open source community, while maintaining control over commercial uses to protect its business model... This excludes, for example, the integration of the model into products or services sold by a company to customers but allows academic use, testing, or internal projects, restricted to a non-commercial framework. </p>",
                "size_desc": "<p> With 111 billion parameters, this model is considered large. It requires at least two powerful graphics cards to host, which results in significant running costs. </p>\n <p> Its context window reaches 256,000 tokens, suitable for analyzing large sets of documents or code bases. </p>"
            },
            "Command R": {
                "desc": "<p> Medium-sized model optimized for summarization, general questions, tool usage, and efficient in retrieval augmented generation (RAG) systems. </p>",
                "fyi": "<p> Cohere, the Canadian company that trained this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous “Attention Is All You Need” paper that revolutionized AI. Cohere's main uniqueness lies in its exclusive focus on generative AI for enterprise, particularly regulated sectors such as finance, healthcare, manufacturing, and energy, as well as the public sector. The company is also a pioneer in multilinguality and has a non-profit research lab focus on open innovation. </p>\n <p> This model has been evaluated in over 10 languages. Its context window reaches 128,000 tokens, which helps it the analyse long documents. This window is doubled in the next version of the model (Command A). </p>",
                "size_desc": "<p> With 35 billion parameters, this model is mid-sized. It can be hosted on a server with a single powerful graphics card, which helps keep infrastructure costs down. </p>"
            },
            "Command R+": {
                "desc": "<p> Multilingual model specifically trained in 10 languages, specialized for business use cases. </p>",
                "fyi": "<p> The big brother of Cohere's Command R family, this language model is geared towards professional use and designed specifically for information search and retrieval tasks. </p>",
                "size_desc": "<p>Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.</p>"
            },
            "DeepSeek R1": {
                "desc": "<p> Very large model with high performance on mathematical, scientific and programming tasks, which simulates a reasoning step before generating its answer. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture with 61 layers. It has a total of 671 billion parameters, 37 billion of which are token-activated. Training used large-scale reinforcement learning, with multiple SFT (supervised fine-tuning) steps where the model learns from examples of correct answers. </p>",
                "size_desc": "<p> With 671 billion parameters, DeepSeek R1 is a very large model that requires multiple powerful graphics cards to run. Reasoning models of this type take longer to produce an answer, which increases energy consumption. However, the Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token, thus limiting its energy footprint. The context window reaches 163,840 tokens, which is suitable for analyzing long documents. </p>"
            },
            "DeepSeek R1 0528": {
                "desc": "<p> A very large model, specialized in mathematical, scientific, and programming tasks. It simulates a reasoning step before generating its answer and, with the May 2025 update, has gained in analysis depth and accuracy thanks to post-training optimization. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture with 61 layers. It has a total of 671 billion parameters, 37 billion of which are activated per token. Training used large-scale reinforcement learning, with multiple SFT ( <em> supervised fine-tuning </em> - where the model learns from examples of correct answers) adjustments. Its latest version (DeepSeek-R1-0528) significantly improves its reasoning capabilities, reducing the hallucination rate and increasing efficiency in programming, logic, and function calling. On the AIME 2025 test, its score increased from 70% to 87.5%, bringing it closer to models like o3 and Gemini 2.5 Pro. </p>",
                "size_desc": "<p> With 671 billion parameters, DeepSeek R1 is a very large model that requires multiple powerful graphics cards to run. Reasoning models of this type take longer to produce an answer, which increases energy consumption. However, the Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token, thus limiting its energy footprint. The context window reaches 163,840 tokens, which is suitable for analyzing long documents. </p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p> Large model based on Meta Llama 3.3 70B, retrained with reasoning examples from the DeepSeek R1 model. It offers good math and coding capabilities. </p>",
                "fyi": "<p> The model was not trained from scratch. It relies on Llama 3.3 70B, retrained using results generated by DeepSeek R1. This process gave Llama 3.3 70B the ability to simulate reasoning, without the user being able to choose whether or not to enable this feature. </p>\n <p> In accordance with the obligations of the Llama 3.3 license, the company must retain the mention of the source model in the name of the model, subject to the same licensing regime. </p>",
                "size_desc": "<p> With 70 billion parameters, this model is considered large. It requires multiple powerful graphics cards to run, resulting in high inference costs. Reasoning models also take longer to produce an answer, increasing their energy consumption. </p>\n <p> The context window is 16,000 tokens, which can be limiting for analyzing large documents. </p>"
            },
            "DeepSeek V3": {
                "desc": "<p> Very large model designed for complex tasks: code generation, tool usage, long document analysis. It can handle many languages, but is particularly well-suited to English and Chinese. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture, with 671 billion parameters but only activating 37 billion per generated token. It is well suited for tool calling, generating structured output (JSON), and code generation. </p>",
                "size_desc": "<p> DeepSeek V3 is a very large model, requiring multiple graphics cards to run. The Mixture of Experts (MoE) architecture, however, allows only a portion of the parameters to be used for generating the next token, reducing the footprint compared to a dense model of the same size. </p>\n <p> The context window reaches 163,000 tokens, which is useful for analyzing long documents. </p>"
            },
            "DeepSeek V3.2": {
                "desc": "<p> A very large model designed for complex tasks: agentic orchestration, code generation, and analysis of long documents. This version is particularly strong in tool usage and can simulate a reasoning phase before providing the final answer. </p>",
                "fyi": "<p> For this version, the API cost and computational requirements are reduced by approximately 50% or more for long contexts. This improvement is based on \"DeepSeek Sparse Attention (DSA),\" a fine-grained sparse attention mechanism that selectively calculates attention to decrease complexity over long sequences while preserving the essential context. </p>\n <p> The model was trained prioritizing reasoning abilities and agentic uses. </p>",
                "size_desc": "<p> DeepSeek V3.2 is a very large model, requiring multiple graphics cards to run. However, the Mixture of Experts (MoE) architecture allows only a portion of the parameters to be activated, reducing the footprint compared to a dense model of the same size. The context window reaches 163,000 tokens, which is useful for analyzing very long documents or codebases. </p>"
            },
            "DeepSeek v3": {
                "desc": "<p> Released in December 2024, the DeepSeek V3 model features a Mixture-of-Experts architecture that allows it to be very large while reducing inference costs. </p>",
                "fyi": "<p> Released in December 2024, this flagship model from the Chinese company DeepSeek features a Mixture-of-Experts architecture that allows it to be very large while reducing inference costs. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "DeepSeek v3.1": {
                "desc": "<p> Very large model designed for complex tasks: code generation, long document analysis. This version is particularly strong in tool use and can simulate a reasoning phase before providing the final answer. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture, with 671 billion parameters but only activating 37 billion per generated token. It is efficient for tool calls, generating structured output (JSON), and code generation. Training uses FP8 microscaling, which reduces computational and memory costs while maintaining accuracy. The model was trained in two phases: first on sequences of 32,000 tokens, then extended to 163,000 tokens, allowing for better stability and increased performance on very long contexts. </p>",
                "size_desc": "<p> DeepSeek V3.1 is a very large model, requiring multiple graphics cards to run. The Mixture of Experts (MoE) architecture, however, allows only a portion of the parameters to be enabled, reducing the footprint compared to a dense model of the same size. </p>\n <p> The context window now reaches 163,000 tokens, up from 128,000 in the previous version, which improves the parsing of very long documents. </p>"
            },
            "EuroLLM 22B Instruct": {
                "desc": "<p> A multilingual model designed specifically for European linguistic diversity, with strong multilingual translation and comprehension capabilities. </p>",
                "fyi": "<p> EuroLLM-22B is a model created by a consortium comprising Sorbonne University, Paris-Saclay University, Artefact Research Center, Instituto Superior Técnico – University of Lisbon, Instituto de Telecomunicações, University of Edinburgh, Aveni, Unbabel, University of Amsterdam, and Naver Labs. It was trained with the primary objective of balanced coverage of European languages. It covers the 24 official languages of the European Union as well as 11 additional languages. Training was performed on approximately 4 trillion tokens, using 400 Nvidia H100 GPUs on the MareNostrum 5 supercomputer, operated by the Barcelona Supercomputing Center. The project received European institutional support through Horizon Europe and EuroHPC, within the framework of an \"extreme-scale\" computing allocation. </p>\n <p> The training data combines web data, multilingual parallel data (en–xx and xx–en), and carefully selected high-quality datasets, with a strong emphasis on balance between European languages. The model performs particularly well for multilingual translation tasks. </p>",
                "size_desc": "<p> With 22 billion parameters, this model falls into the category of medium-sized models. Its use requires a very powerful personal computer or, more generally, a server with at least one high-performance graphics card. The context window contains 32,000 tokens. </p>"
            },
            "GLM 4.5": {
                "desc": "<p> A large model specialised in code created by Zhipu AI, a Chinese AI model maker founded in 2019 by professors at Tsinghua University and backed by major players like Alibaba and Tencent. The model has two response modes: a reasoning mode, for more in-depth answers, or a quick mode, to generate the final answer right-away. </p>",
                "fyi": "<p> This model has good agentic capabilities, allowing it to make function calls with reliably. It offers solid coding performance, thus allowing the model to create complete web applications and generate artefacts, which are single-file programs that can be used within the interfaces of conversational agents. For training, a specific reinforcement learning infrastructure, named Slime, was designed to optimize performance on complex and agentic tasks by efficiently managing long workflows - thus the model is able to long-running tasks - making the best use of its tools and remaining consistent from start to finish. </p>",
                "size_desc": "<p> With 355 billion parameters, this model is considered very large. Thanks to a Mixture of Experts (MoE) architecture, it is more efficient than dense models of similar size, but it still requires a server with several very powerful graphics cards to host. Its context window goes up to 128,000 tokens, which allows it to process fairly long documents. </p>"
            },
            "GLM 4.6": {
                "desc": "<p> A large model specialized in code created by Zhipu AI, a Chinese AI model developer founded in 2019 by professors from Tsinghua University and backed by major players like Alibaba and Tencent. This update increases the context window size, improves its code performance, aligns more closely with human preferences, and is more capable in agent-based/tool-based use cases. </p>",
                "fyi": "<p> This model has good agentic capabilities, allowing it to make function calls with high reliability. Its coding performance is high, and the model has a good ability to create complete web applications and generate artifacts, which are single-file programs that can be used within the interfaces of conversational agents. For training, a specific reinforcement learning framework, called slime, was designed to optimize performance on complex and agentic tasks by efficiently handling long workflows - the model is able to handle complex and long-running tasks, such as creating an application from scratch, making the best use of its tools and remaining consistent from start to finish. </p>",
                "size_desc": "<p> With 357 billion parameters, this model falls into the very large model category. Thanks to a Mixture of Experts (MoE) architecture, it is more efficient than some other models of similar size, but it still requires a server with several very powerful graphics cards to host it. Its context window goes up to 200,000 tokens, which allows it to process very long documents. </p>"
            },
            "GLM 4.7": {
                "desc": "<p> A large, specialized code model created by Zhipu AI, a Chinese AI model developer founded in 2019 by professors from Tsinghua University and backed by major players like Alibaba and Tencent. This update improves its code performance (especially for web interfaces), interacts better with AI-assisted coding environments, and generally performs better in agentic contexts. </p>",
                "fyi": "<p> This model has good agentic capabilities, allowing it to make function calls with high reliability. Its coding performance is high, and the model has a good ability to create applications and interfaces. For training, a specific reinforcement learning infrastructure, named Slime, was designed to optimize performance on complex and agentic tasks by efficiently managing long workflows—the model is capable of handling complex and lengthy tasks, such as creating an application from start to finish, making the best use of its tools and remaining consistent from beginning to end. </p>",
                "size_desc": "<p> With 357 billion parameters, this model falls into the very large model category. Thanks to a Mixture of Experts (MoE) architecture, it is more efficient than some other models of similar size, but it still requires a server with several very powerful graphics cards to host it. Its context window goes up to 200,000 tokens, which allows it to process very long documents. </p>"
            },
            "GLM 5": {
                "desc": "<p>A very large model designed for complex tasks: software engineering, agentic orchestration, and advanced reasoning. It simulates a reasoning step before generating its answer.</p>",
                "fyi": "<p>GLM 5 is the flagship model of Zhipu AI, a Chinese software company founded in 2019 by professors from Tsinghua University. It is based on a mixture of experts (MoE) architecture with a total of 744 billion parameters, but only activating 40 billion per token generated, thanks to 256 expert modules, 8 of which are selected at each stage. Pre-training covered 28.5 trillion tokens, up from 23 trillion for the previous generation (GLM 4.5). The model incorporates the DeepSeek Sparse Attention (DSA) mechanism, which reduces deployment costs while preserving the capacity to process long contexts.</p>\n<p>For post-training, Zhipu developed an asynchronous reinforcement learning infrastructure called slime, which decouples data generation from policy updates, achieving throughput up to three times higher than traditional synchronous methods. This framework is particularly well-suited for long-duration agentic tasks. GLM 5 excels in code evaluations (SWE-bench Verified: 77.8%) and web browsing (BrowseComp: 75.9%). The model was trained on Huawei Ascend chips using the MindSpore framework.</p>",
                "size_desc": "<p>With 744 billion parameters, GLM 5 is among the very large models. The Mixture of Experts (MoE) architecture—256 experts, 8 of which are token-activated—allows it to use only 40 billion parameters per inference, reducing its footprint compared to a dense model of the same size, while still requiring a server with multiple powerful graphics cards. The context window reaches approximately 200,000 tokens, which is useful for analyzing very long documents or codebases. Reasoning models of this type run longer to produce a response, which increases energy consumption.</p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p> Smaller, lightweight version of the GPT 4.1 model, designed to keep costs down while remaining competitive on most tasks. The model supports very long queries, making it suitable for use in long document corpus analysis. </p>",
                "fyi": "<p> This is a distilled version of a larger model, with partial knowledge transfer. It can process text, images, and audio. Its context window can reach up to 1 million tokens, making it particularly suitable for analyzing text corpora or very long code repositories. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a medium-sized model, requiring a powerful graphics card to run. However, the supposed Mixture of Experts (MoE) architecture activates only a subset of parameters at each token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "GPT 5": {
                "desc": "<p> GPT-5 is not a single model, but a unified system composed of two separate models: a fast model ( <code> gpt-5-main </code> ) for common queries and a reasoning model ( <code> gpt-5-thinking </code> ) for complex problems. Compared to its predecessors, OpenAI claims it is more useful in real-world queries, with notable improvements in the areas of writing, coding, and health. Its creators also claim it produces less hallucinations. Thanks to its context window of 400,000 tokens, it can accept long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> Developers using this model can configure a verbosity parameter to adjust the length of the reasoning phase. </p>\n <p> In terms of security, the system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than at request time. The model's creators also used the \"reasoning\" training phase to make it more \"resistant\" to attempts to circumvent their security rules ( <em> jailbreaking </em> ). </p>",
                "size_desc": "<p> The GPT-5 system is composed of models of various sizes, but the exact sizes are unknown. Its architecture is designed to include multiple models, orchestrated by an internal routing system, which selects the smallest model suited to the task to optimize the speed and depth of reasoning. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a portion of the parameters are activated for each query. This allows for greater energy efficiency and high performance. Available estimates of model sizes are based on public information and indirect indices such as inference costs and response latency. </p>"
            },
            "GPT 5 Mini": {
                "desc": "<p> GPT-5 Mini is a lightweight version of the main GPT-5 model. It is designed for use in environments where cost constraints are needed, such as large scale. Its reasoning model performs almost as well as the main model ( <code> gpt-5-thinking </code> ) despite its smaller size. Thanks to its 400,000 token context window, it can handle long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> The system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than request time. </p>\n <p> Although it is a smaller version, it is very competitive against the leading GPT-5 model on many benchmarks, especially in the medical field. </p>",
                "size_desc": "<p> The Mini model is a more compact (medium-sized according to estimations) version of the GPT-5 system. It is designed to perform optimally for a good balance between performance and cost, thanks to a routing system that selects it for specific tasks. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a portion of the parameters are activated for each query. However, the models are likely very large, requiring multiple powerful graphics cards for inference. </p>"
            },
            "GPT 5 Nano": {
                "desc": "<p> GPT-5 Nano is the smallest and fastest version of the GPT-5 reasoning model. It is designed for contexts where ultra-low latency or cost is required. Thanks to its 400,000 token context window, it can accept long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> The system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than request time. </p>",
                "size_desc": "<p> The Nano model is the most compact of the GPT-5 family (estimated to be small). It is selected by the routing system for queries requiring ultra-low latency and instant responses. Its architecture is likely based on a \"Mixture of Experts\" (MoE), which allows for better energy efficiency and high performance, even on queries requiring a fast response. </p>"
            },
            "GPT 5.1": {
                "desc": "<p> Second iteration of the GPT 5 family, with a style deemed (by the publisher) more natural, with better results in code and agent tasks. The model has the unique feature of adjusting its reasoning time according to the difficulty. </p>",
                "fyi": "<p> The arrival of this new iteration brings automatic routing between two modes. For simple requests, the model chooses a quick response. For more complex tasks, it switches to a mode that requires reasoning before responding. This logic avoids wasting time on easy requests while maintaining true depth when needed. </p>\n <p> The first release of the GPT 5 family provoked a lot of negative reactions regarding its writing style. Many users found the model less warm than GPT 4o. This iteration aims to return to a warmer style. </p>\n <p> This new version also marks progress in coding and mathematics, which is confirmed by benchmarks such as SWE Bench for coding and AIME for mathematical skills. </p>",
                "size_desc": "<p> The model sizes behind the GPT-5.1 system are not known. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a subset of parameters are activated for each query. This allows for greater energy efficiency and high performance. Available estimates of model sizes rely on publicly available information and indirect indicators such as inference costs and response latency. </p>"
            },
            "GPT 5.2": {
                "desc": "<p> Third iteration of GPT 5, with special attention paid to its usefulness in professional tasks. </p>",
                "fyi": "<p> The model has been presented as reaching, or even exceeding, the performance level of human experts on the GDPval benchmark, which evaluates well-defined digital professional tasks. It offers different levels of reasoning effort configurable by the user or developer, allowing for trade-offs between cost, latency, and quality of responses depending on the complexity of the task. GPT 5.2 also stands out for its ability to retrieve precise information within very large contexts, including when such information is scarce and scattered across long corpora (needle in the haystack). </p>",
                "size_desc": "<p> The model sizes behind the GPT-5.2 system are not known. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a subset of parameters are enabled for each query. This allows for greater energy efficiency and high performance. Available estimates of model sizes rely on publicly available information and indirect indicators such as inference costs and response latency. </p>"
            },
            "GPT OSS-120B": {
                "desc": "<p> The larger of OpenAI's first two open-weight models since GPT-2. Designed in response to the rise of open source players like Meta (LLaMA) and Mistral, it is a powerful reasoning model, particularly on complex tasks and in \"agentic\" environments. </p>",
                "fyi": "<p> This model can run on a single 80GB GPU (like the NVIDIA H100). It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>\n <p> In the model configurations, it is possible to choose between three levels of reasoning ( <em> low </em> , <em> medium </em> , and <em> high </em> ) which determine the verbosity of the model. </p>",
                "size_desc": "<p> The architecture is a \"mixture of experts\" (MoE), which allows for greater energy efficiency by activating only a portion of the parameters (5.1 billion per token) for each token prediction. It is a reasoning model, thus its energy consumption is higher because it generates an internal chain of thought before providing the final answer. It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>"
            },
            "GPT OSS-20B": {
                "desc": "<p> The smaller of OpenAI's two open-weight models. It was designed in response to open source competition and is intended for use cases requiring low latency as well as local or specialized deployments. </p>",
                "fyi": "<p> This model can be run locally on a high-end laptop with only 16GB of VRAM (or system RAM). This makes it a very accessible option for developers. </p>\n <p> In the model configurations, it is possible to choose between three levels of reasoning ( <em> low </em> , <em> medium </em> , and <em> high </em> ) which determine the verbosity of the model at the reasoning step. </p>",
                "size_desc": "<p> With 20 billion parameters, this model belongs to the medium-sized model category. The architecture is based on a \"mixture of experts\" (MoE), which allows for greater energy efficiency by activating only a portion of the parameters (3.6 billion per token) for each token generation. It is a reasoning model, which results in higher energy consumption because it generates an internal chain of thought before providing the final answer. It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>"
            },
            "GPT-3.5": {
                "desc": "<p> Launched in March 2023, GPT-3.5 is a smaller OpenAI model sufficient for various natural language processing tasks. </p>",
                "fyi": "<p> Launched in March 2023, GPT-3.5 is a smaller OpenAI model sufficient for various natural language processing tasks. </p>",
                "size_desc": "<p>Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.</p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p> A lightweight version of GPT 4.1, but still large in size, designed to keep costs down while remaining competitive on most tasks. The model supports very long queries, making it suitable for use in document corpus analysis. </p>",
                "fyi": "<p> This is a distilled version of a larger model, with partial knowledge transfer. It can process text, images, and audio. Its context window can reach up to 1 million tokens, making it particularly suitable for analyzing very long corpora or code repositories. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Indicators suggest that it is a large model, requiring a powerful graphics card to run. However, the supposed Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token generation, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "GPT-4o": {
                "desc": "<p> The larger of the two models that underpined OpenAI's ChatGPT, launched in August 2024. </p>",
                "fyi": "<p> Model launched in August 2024 and successor to GPT-4, GPT-4o is an improved version of GPT-4, designed for various natural language processing tasks via, for example, the ChatGPT application from the American company OpenIA. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "GPT-4o mini": {
                "desc": "<p> The smaller of the two models that underpined OpenAI's ChatGPT, launching in July 2024. </p>",
                "fyi": "<p> Launched in July 2024 and replacing GPT-3.5, GPT-4o mini is a smaller version of GPT-4, designed for various natural language processing tasks, such as the ChatGPT application from the American company OpenIA. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Gemini 1.5 Pro": {
                "desc": "<p> Released in September 2024 (version 002), this multimodal model can be used for text generation but also interpret images, videos, and audio. </p>",
                "fyi": "<p> Announced in February  2024 and released in May of the year, this multilingual and multimodal model is capable of processing a very large volume of input data, whether text, image, sound (up to 11 hours of audio) or video (up to one hour). It was the LLM model that powered Google's Gemini chatbot. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "Gemini 2.0 Flash": {
                "desc": "<p> Released in December 2024, this smaller multilingual and multimodal model is from Gemini's Flash family, enabling very fast response for tasks that require less advanced reasoning. </p>",
                "fyi": "<p> Released in December 2024, this smaller multilingual and multimodal model is from Gemini's Flash family, enabling very fast response for tasks that require less advanced reasoning than the Gemini Pro models. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p> Large multimodal and multilingual model with two response modalities: the user can choose between a reasoning mode, for more in-depth answers, or a fast mode, to generate the final answer directly. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture and has been distilled by keeping only an approximation of the predictions of the teacher model - Gemini 2.5 Pro. It has been trained on a TPUv5p architecture incorporating advances such as the ability to continue training automatically even in the event of training errors, data corruption, or memory issues. </p>\n <p> Gemini 2.5 Flash supports contexts of up to 1 million tokens and three hours of video content. Optimized vision processing allows for approximately three times longer video to be processed in the same context window: only 66 visual tokens are needed to generate a frame, compared to 258 previously. This model also enables native audio generation for dialogue and speech synthesis. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring multiple powerful graphics cards to run. However, the Mixture of Experts (MoE) architecture only activates a portion of the parameters to predict each new token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. Its context window extends up to 1 million tokens, which allows it to process very large document corpora. </p>"
            },
            "Gemini 3 Flash": {
                "desc": "<p> Large, native, multimodal, and multilingual model, distilled from Gemini 3 Pro. It incorporates an advanced reasoning capability (\"Deep Think\") that can be activated on demand, with a reasoning level configurable by the user or developer. The model natively supports text, code, audio, images, video, and PDFs. </p>",
                "fyi": "<p> The model was trained on an infrastructure composed solely of TPUs (Tensor Processing Units). Gemini 3 Flash can process large volumes of multimodal content per prompt: up to 900 images, 900 PDFs of 900 pages each, videos from 45 minutes to 1 hour, and up to 8.4 hours of audio. At the time of its release, Gemini 3 Flash proved to be very powerful while being approximately nine times less expensive than Gemini 3 Pro. </p>",
                "size_desc": "<p> The exact size of the model is not disclosed, but it is based on a mixture of experts (MoE) architecture. This architecture activates only a subset of parameters for each token, thus reducing the required computing power. Its context window extends to 1 million tokens. The model is approximately 9 times less expensive than Gemini 3 Pro. The environmental impact varies depending on the level of reasoning chosen: deeper reasoning generates more tokens and therefore consumes more resources. </p>"
            },
            "Gemini 3 Pro": {
                "desc": "<p> Large, natively multimodal, and multilingual model. It integrates an advanced reasoning capability (\"Deep Think\") that can be activated on demand for complex tasks (mathematics, logic, coding), separate from its faster, standard generation capability. The model natively supports text, code, audio, images, video, and 3D. </p>",
                "fyi": "<p> The model was trained on an infrastructure composed solely of Tensor Processing Units (TPUs). At the time of its release, the model represents a significant leap forward in assessments—for example, on Humanity's Last Exam, ARC-AGI-2, and MathArena Apex. The model is optimized for agentic use: it has been trained to simulate planning, tool usage, code execution, and self-correction within coding environments. Its multimodal understanding has been refined to significantly reduce the number of tokens required for video and audio encoding. </p>",
                "size_desc": "<p> The exact size of the model is not disclosed, but it is based on a mixture of experts (MoE) architecture. This architecture activates only a subset of parameters for each input token, requiring less computing power to generate. Its context window extends to 1 million tokens, suitable for analyzing large document sets, including entire code repositories and video files. </p>"
            },
            "Gemma 2 27B": {
                "desc": "<p> A high-performance model with a decent size, allowing it to tackle specific use cases requiring great precision. </p>",
                "fyi": "<p> With three times more parameters than its little brother in the Gemma 2 family, this model is more accurate in responding to instructions. The model used here is the quantized version (q8). </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Gemma 2 2B": {
                "desc": "<p> Very small model that offered very competitive performance for its size and most tasks. </p>",
                "fyi": "<p> The little brother of the Gemma 2 family, this very small model released in July 2024 manages to compete with much larger models. </p>",
                "size_desc": "<p>Very small models, with fewer than 7 billion parameters, are the least complex and most resource-efficient, providing sufficient performance for simple tasks such as text classification.</p>"
            },
            "Gemma 2 9B": {
                "desc": "<p> The younger brother of the Gemma 2 family, this model, released in June 2024, is trained to respond to specific instructions, handle complex requests, and offer creative solutions. </p>",
                "fyi": "<p> The little brother of the Gemma 2 family, this model, released in June 2024, is trained to respond to specific instructions, handle complex requests, and offer creative solutions. </p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Gemma 3 12B": {
                "desc": "<p> Small multimodal model suitable for common tasks such as question-and-answer, summarization, or image interpretation. </p>",
                "fyi": "<p> It processes text and images and can run locally on powerful laptops or servers with a single graphics card. It has been trained to be able to interact with external tools (web search, etc.) via function calls, which makes it useful in agentic use cases. </p>",
                "size_desc": "<p> With 12 billion parameters, it is one of the smallest models. It can be used locally on a personal computer to preserve data confidentiality, or on an inexpensive server to limit costs compared to a larger model. </p>\n <p> Its context window can hold up to 128,000 tokens, making it easy to process long documents. </p>"
            },
            "Gemma 3 27B": {
                "desc": "<p> Medium-sized, multimodal model suitable for common tasks such as question-and-answering, summarization, or image interpretation. </p>",
                "fyi": "<p> It can process text and images on a server equipped with a single powerful graphics card. It has been trained to be able to interact with external tools (internet search, etc.) via function calls, which makes it useful in agentic use cases. </p>",
                "size_desc": "<p> With 27 billion parameters, it is a medium-sized model. It can be deployed on a server with a single graphics card (GPU). </p>\n <p> It accepts contexts of up to 128,000 tokens, making it suitable for parsing long documents. </p>"
            },
            "Gemma 3 4B": {
                "desc": "<p> Very small, compact, multimodal model suitable for common tasks such as question-and-answer, summarization, or image interpretation. </p>",
                "fyi": "<p> It can process text and images while running on less powerful computers, including smartphones and tablets. It has been trained to interact with external tools (web search, etc.) via function calls, making it usable in agentic use cases. </p>",
                "size_desc": "<p> With 4 billion parameters, it is considered a very small model. It can be used locally to preserve data confidentiality, or on a server to limit costs compared to a larger model. </p>\n <p> Its context window can reach 128,000 tokens, allowing it to analyze long documents. </p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p> Very small, compact, multimodal model designed to run locally on a computer or smartphone, without the need for a server - it is capable of adapting its ressource consumption according to the capacity of the device it is running on. </p>",
                "fyi": "<p> This model can process text, images, and audio. It is based on the MatFormer architecture and a PLE (per-layer embeddings) cache system, which activates only the useful parameters depending on the task, adapting to the capacity of the machines on which the model runs. </p>",
                "size_desc": "<p> With 4 billion parameters, it is one of the smallest models available. It can be used locally on a computer or smartphone to maintain data confidentiality, or on a server to limit costs compared to a larger model. </p>\n <p> Its context window goes up to 32,000 tokens. </p>"
            },
            "Grok 3 Mini": {
                "desc": "<p> A lighter version of the Grok 3 model, reducing costs while maintaining good performance for many tasks. It can go through a reasoning phase before providing a final answer. </p>",
                "fyi": "<p> Grok 3 Mini is a distilled version of Grok 3: it is close to it in terms of capabilities, while being faster and less expensive.\n The model offers two modes: a thinking mode with step-by-step reasoning for complex problems, and a quick mode for immediate answers.\n Its context window reaches 131,000 tokens, making it suitable for analyzing long documents. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Despite its name, Grok 3 Mini is likely a very large model, requiring multiple powerful graphics cards to run. Additionally, it contains an optional reasoning phase that involves longer generation times and therefore higher power consumption. However, the supposed Mixture of Experts (MoE) architecture only activates a portion of the parameters at each token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "Grok 4 Fast": {
                "desc": "<p> Grok 4 Fast is a model focused on balancing performance, speed, and cost, particularly for information retrieval tasks and other \"agentic\" actions. </p>",
                "fyi": "<p> The exact size of the model is unknown. Despite its name, Grok 4 Fast is likely a very large model, requiring multiple powerful graphics cards to run. Additionally, it contains an optional reasoning phase that involves longer generation times and thus higher energy consumption. However, the supposed Mixture of Experts (MoE) architecture only activates a portion of the parameters at each token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> Trained by reinforcement learning, Grok 4 Fast achieves scores close to the very large model - Grok 4 while remaining more efficient. It has been trained to perform well on web browsing and specifically on the X platform, as well as on tool calling capabilities and code execution. </p>",
                "size_desc": "<p> Featuring a 2 million token context window, Grok 4 Fast combines reasoning and direct response modes in a single model. It uses approximately 40% fewer reasoning tokens than Grok 4, resulting in a significant reduction in execution cost and latency. </p>"
            },
            "Grok 4.1 Fast": {
                "desc": "<p> Grok 4.1 Fast is a new iteration of the fourth generation of Grok. Based on the same core model, according to the developer it benefits from an improved post-training process that gives it better style, superior agentic behavior, and greater consistency in responses, especially during extended discussions. </p>",
                "fyi": "<p>Available evaluations tend to indicate a lower rate of hallucinations for this version of the model . The Grok 4.1 training reportedly incorporated expanded reward modeling, including not only human evaluations but also feedback from models acting as AI evaluators.</p>",
                "size_desc": "<p> Grok 4 Fast integrates a reasoning mode and a direct response mode. The user or developer can control whether or not to enable reasoning. The model has a context window of 2 million tokens, allowing it to analyze very large document corpora or massive codebases. </p>"
            },
            "Hermes 3 405B": {
                "desc": "<p> Very large model retrained from the Llama 3.1 405B, adjusted to better meet user prompts and improve it's capacity to use tools. </p>",
                "fyi": "<p> This model is the result of retraining the Llama 3.1 405B parameter set to make its behavior less restricted and better account for the nuances of user and system prompts - thus giving the user greater control over the model's “personality” and behavior. Specific reasoning functions such as <strong> <code> &lt; SCRATCHPAD &gt; </code> </strong> , <strong> <code> &lt; REASONING &gt; </code> </strong> , <strong> <code> &lt; THINKING &gt; </code> </strong> have been added to simulate reasoning about complex tasks. The training used a tool called AdamW (learning rate of 3.5×10⁻⁶), which helps the model learn efficiently by gradually adjusting its parameters. Then, it was fine-tuned with a method called DPO (direct preference optimization), which improves its responses based on specific preferences. To make this training lighter and faster, LoRA adapters were used; these are smaller modules that modify only a part of the model, avoiding the need to rework all the parameters at once. </p>",
                "size_desc": "<p> With 405 billion parameters, this model is considered to be very large. It requires a server equipped with several powerful graphics cards, which results in significant operating costs. </p>"
            },
            "Hermes 4 70B": {
                "desc": "<p> Large model retrained from the Llama 3.1 70B, adjusted to better meet users' stylistic requests and instructions. </p>",
                "fyi": "<p> Hermes 4-70B was trained on 56 billion tokens using a combination of Fully Sharded Data Parallel (FSDP) and Tensor Parallelism to manage its size. The model is based on the Llama 3.1 70B framework, adapted with TorchTitan and enriched with approximately 19 billion synthetic reasoning-focused tokens. Its training follows a multi-phase approach, with supervised fine-tuning on reasoning chains that can exceed 30,000 tokens. It also leverages the Atropos environment, used to generate and verify complex trajectories (code, JSON, agentic tasks), thanks to massive rejection sampling that guarantees data quality. </p>",
                "size_desc": "<p> Hermes 4-70B is a very large model, requiring at least a powerful graphics card to run. </p>\n <p> The context window reaches 40,960 tokens in reasoning and 32,768 tokens for other tasks, with fine-tuning mechanisms used to teach it to “close” the thinking sequence to around 30k tokens. </p>"
            },
            "Jamba 1.5 Large": {
                "desc": "<p> Released in August 2024, this model from the AI21 company is a special hybrid type of model called 'SSM' and Mixture of Experts, whose architecture seeks to make the most of the number of parameters. </p>",
                "fyi": "<p> Released in August 2024, this model from the AI21 company is a special hybrid type of model called 'SSM' (State Space Models) and Mixture of Experts, whose architecture seeks to make the most of the number of parameters. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "Kimi K2": {
                "desc": "<p> Developed by Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), a Beijing-based company, Kimi K2 is a very large code-oriented and agentic model. It is praised for its performance in programming in agentic contexts (e.g., in Cursor or Windsurf), particularly for its capacity to act as the orchestrator. It does not expose an explicit “reasoning mode,” but for large tasks it subdivides its response into steps and alternates between actions (tool calls) and text writing. </p>",
                "fyi": "<p> To stabilize training at very large scales, Moonshot AI introduced MuonClip, a training “speed limiter” that allows training a model of this size and on a corpus of 15.5 trillion tokens without major derailing. </p>\n <p> On the data side, K2 has extensively used “simulations” with real tools (browser, terminal, code executors, APIs, etc.). Like a pilot on a simulator, he learns to plan, try, fail, then try again, and to chain several actions together to achieve a goal. As a result, the model is particularly good at orchestrating tools and successfully completing multi-step tasks. </p>",
                "size_desc": "<p> With 1 trillion parameters, this model is one of the largest that exists. Thanks to a Mixture of Experts (MoE) architecture, it is more efficient than some other models, but it still requires a server with several very powerful graphics cards to host it. Its context window goes up to 128 000 tokens, which allows it to process fairly long documents. </p>"
            },
            "Kimi K2 Thinking": {
                "desc": "<p> This version of Kimi K2 incorporates a more advanced reasoning phase, improving its performance compared to the original iteration. It was developed by Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), a company based in Beijing. </p>",
                "fyi": "<p> The model appears to demonstrate enhanced performance on long reasoning tasks and tasks requiring tool use. These characteristics are assumed to make the model more autonomous for longer task execution. </p>",
                "size_desc": "<p> With 1 trillion parameters (32 billion of which are active per token), this model is among the very large models. Its Mixture of Experts (MoE) architecture nevertheless allows for greater efficiency compared to some models of comparable size. Furthermore, it has been natively quantized at the INT4 level. This quantization, which consists of \"rounding\" the model weights to fewer digits to simplify calculations, is presented by Moonshot AI as reducing inference time by a factor of 2. The context window reaches 256,000 tokens, offering the capacity to process very long documents or codebases. </p>"
            },
            "Kimi K2.5": {
                "desc": "<p>Built from Kimi K2, the editor continued pre-training on approximately 15 trillion additional tokens, blending text and visual data. The editor highlights its native multimodality as well as its agentic capabilities, particularly suited to tasks such as software development.</p>",
                "fyi": "The model was trained using a specific mode called \"Agent Swarm.\" In this orchestration mode, the problem is not solved in a strictly sequential manner, but rather broken down into several subtasks executed in parallel by sub-agents. During its training <p>the model learned to act like a master agent: it analyzes the demand, decides how many sub-agents to launch, assigns them objectives, and then aggregates their results. This capability was achieved through multi-agent reinforcement learning methods, where the orchestrator was rewarded when it chose a good decomposition of the problem and effective coordination between agents, and penalized when it adopted overly linear or</p> reasoning.",
                "size_desc": "<p>With 1 trillion parameters in total, of which approximately 32 billion are activated for each generated token, this model is among the very large models. Its mixture of experts (MoE) architecture, however, allows for more efficient resource utilization than dense models of comparable size. Furthermore, the model is natively quantized using INT4, a technique that reduces the precision of the weights to simplify calculations and limit computation time for inference. Its context window can accommodate up to 256,000 tokens, enabling the processing of very large documents or extensive codebases.</p>"
            },
            "LFM 2 8B A1B": {
                "desc": "<p> A model specifically designed for efficient inference on local devices (on-device deployment). Its architecture aims to offer output quality competitive with that of larger, dense models, while minimizing latency and computational resource requirements. </p>",
                "fyi": "<p> LFM2-8B-A1B is based on a hybrid architecture integrating convolutional blocks and Grouped Query Attention (GQA) mechanisms. The mixture of experts (MoE) implementation consists of 32 experts per transformation block. The router selects and activates the top 4 experts (top-4 gating) for processing each token. This design aims to establish a benchmark for expert mixture models optimized for local inference. </p>",
                "size_desc": "<p> The model has a total of 8.3 billion parameters. Thanks to a Mixture of Experts (MoE) architecture, activation is limited to approximately 1.5 billion parameters per token during inference. This configuration allows it to achieve performance typically associated with dense models in the 3-4 billion parameter class, with execution speed comparable to that of 1.5 billion parameter models. The model is offered in quantized variants optimized for local environments (such as a smartphone). The context window is limited to 32,000 tokens. </p>"
            },
            "LFM 40B": {
                "desc": "<p> Released in September 2024, this model from the American company Liquid is a Mixture of Experts type model, whose architecture seeks to make the most of the number of parameters. </p>",
                "fyi": "<p> Released in September 2024, this model from the American company Liquid is a Mixture of Experts type model, whose architecture seeks to make the most of the number of parameters. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Llama 3 70B": {
                "desc": "<p> Launched in April 2024, this model has been trained on over 15 trillion tokens but supports a relatively small context of 8,000 tokens. </p>",
                "fyi": "<p> Launched in April 2024, this model was trained on over 15 trillion tokens, then specialized for dialogue using instruction data and human-made annotations. It supports a context of 8,000 tokens. </p>",
                "size_desc": "<p>Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.</p>"
            },
            "Llama 3 8B": {
                "desc": "<p> The little brother of the Llama 3 family, this model is optimized for dialogue, with particular attention paid to efficiency and safety. </p>",
                "fyi": "<p> The little brother of the Llama 3 family, this model is optimized for dialogue, with particular attention paid to efficiency and safety. </p>",
                "size_desc": "<p>A small model is less complex and costly to run compared to larger models, while also offering sufficient performance for many tasks such as summarization, translation, etc.</p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p> Very large model designed for complex or specialized tasks. Often used as a “teacher model” for training more specialized models. </p>",
                "fyi": "<p> The model was trained on a corpus of 15 trillion tokens with 16,000 H100 graphics cards (one of the most powerful graphics cards on the market in 2025). Training combined synthetic data generation and direct preference optimization (DPO). This model is itself often used to generate synthetic data to train smaller models. The model uses 8-bit compression by default to reduce memory requirements and allow execution on a single, high-powered server. </p>",
                "size_desc": "<p> With 405 billion parameters, this model is one of the very large models. It requires a server equipped with several powerful graphics cards, which results in significant running costs. The model has a context window of up to 128,000 tokens, making it suitable for long document analysis tasks. </p>"
            },
            "Llama 3.1 70B": {
                "desc": "<p> With 70 billion parameters and released in April 2024, this model is effective at generating and understanding complex texts in various languages. </p>",
                "fyi": "<p> Like the other models in the Llama 3.1 family, this model, released in April 2024, was trained on data dating back to December 2023. No need to ask it about the highlights of the Paris 2024 Olympic Games! With 70 billion parameters, this model is efficient at generating and understanding complex texts in various languages. </p>",
                "size_desc": "<p>Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.</p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p> Small model designed for local use on a laptop, while offering good capabilities for text synthesis and straight-forward questions and answers. </p>",
                "fyi": "<p> This model is a distilled version of the larger Llama 3 models: it was trained by transferring some of the knowledge from the larger models. </p>",
                "size_desc": "<p> With 8 billion parameters, this model is small. It can be run locally on a powerful computer, ensuring data confidentiality, or hosted on a server equipped with a single graphics card, which limits infrastructure costs. Its context window of 128,000 tokens allows it to process long documents. </p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p> Large model designed for a wide range of tasks and able to compete with larger models. </p>",
                "fyi": "<p> This model is a distilled version of the 405B model, to which it owes some of its transferred knowledge. It also benefited from recent alignment and reinforcement learning techniques with online environments (online reinforcement learning) - the model learned by trying to perform online tasks autonomously. Its training was done on 15 billion tokens of data. </p>",
                "size_desc": "<p> With 70 billion parameters, this model is large. It requires multiple powerful graphics cards to run, which results in significant operating costs. Its context window of 128,000 tokens allows it to process long documents. </p>"
            },
            "Llama 4 Maverick": {
                "desc": "<p> Very large model with a very large context window. Useful, for example, for summarizing several documents at once. </p>",
                "fyi": "<p> This model was co-distilled with Behemoth, meaning it learned alongside the giant model, rather than afterward, as in traditional distillation. This allows for faster and less computational transfer of skills. It was trained on 30 trillion tokens of data, combining text in 200 languages and images to achieve native multimodal capabilities - it can process up to 8 images simultaneously. The architecture is based on a Mixture of Experts (MoE) system, with 17 billion active parameters, 16 experts, and 109 billion total parameters. The Meta team developed a progressive post-training strategy, combining adaptive data filtering - keeping only the most complex and interesting data-targeted fine-tuning, and online reinforcement learning to balance multimodal performance, reasoning, and conversational quality. Thanks to the iRoPE architecture (optimized version of positional encoding), it can handle very long context windows, up to 10 million tokens. </p>\n <p> The Llama 4 Maverick model was presented as Meta's direct response to the DeepSeek models. However, upon its release, many users felt that it did not live up to expectations, especially for programming tasks and creative work. </p>",
                "size_desc": "<p> With 400 billion parameters, this model is considered to be very large. However, thanks to a Mixture of Experts (MoE) architecture, it requires fewer resources to run than “dense” models of this size. Its context window extends up to 1 million tokens, which allows it to process very large document corpora. </p>"
            },
            "Llama 4 Scout": {
                "desc": "<p> Large model with a very large context window, useful, for example, for summarizing a set of documents. </p>",
                "fyi": "<p> This model was co-distilled with Behemoth, meaning it learned alongside the giant model, not afterward as in a traditional distillation. It was trained on 30 trillion tokens of data, combining text in 200 languages and images to achieve native multimodal capabilities. The architecture is based on a Mixture of Experts (MoE), with 17 billion active parameters, 16 experts, and 109 billion total parameters. To balance multimodal performance, reasoning, and conversational quality, the Meta team developed a progressive post-training strategy, combining adaptive data filtering (to keep only the most complex and interesting data), targeted fine-tuning, and online reinforcement learning - the model learned by trying to perform online tasks autonomously. Thanks to the iRoPE architecture (optimized version of positional encoding), it can handle very long context windows, up to 10 million tokens and can process up to 8 images simultaneously. </p>\n <p> The model was well received upon launch, notably for its impressive context window, a first in the field, as well as its cost-effectiveness on tasks such as summarization, tool invocation, and augmented generation (RAG). This makes it a suitable choice for automated pipelines. </p>",
                "size_desc": "<p> With 109 billion parameters, this model falls into the large model category. However, thanks to a Mixture of Experts (MoE) architecture, it can be hosted on a server with a single, high-performance graphics card. Its context window can hold up to 10 million tokens, making it useful for processing extremely long document corpora. </p>"
            },
            "Magistral Medium": {
                "desc": "<p> Medium-sized, multimodal, and multilingual reasoning model. Suitable for programming tasks or other tasks requiring in-depth analysis, understanding of complex logical systems, or planning - for example, for agentic use cases or writing long, complex content. </p>",
                "fyi": "<p> This model is part of the first generation of Mistral AI reasoning models (Summer 2025). Unlike most other reasoning models, this model can reason in multiple languages, including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese. It was trained with reinforcement learning on Mistral Medium 3 and was not distilled from existing reasoning models. This model inherits the multimodal capabilities of Mistral Medium 3, even though reinforcement learning was only performed on text. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring at least several powerful graphics cards to run. Reasoning models require more computing power to produce a response, which increases their power consumption. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> It has a context window of up to 40,000 tokens, useful for analyzing short documents but insufficient for analyzing large document corpora. </p>"
            },
            "Magistral Small": {
                "desc": "<p> Medium-sized, multimodal, and multilingual reasoning model. Suitable for tasks requiring in-depth analysis, understanding of logical systems, or planning- for example, for agentic use cases or writing long, complex content. </p>",
                "fyi": "<p> This model is part of Mistral AI's first generation of reasoning models (Summer 2025). Unlike most other reasoning models, this model can reason in multiple languages, including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese. </p>\n <p> Training was done in two phases. The first, called <em> cold-start </em> by distillation (from Mistral Medium 3 and <em> /OpenR1), allows the model to acquire basic reasoning capabilities from general instructional data (10%). The second is a high-entropy reinforcement learning (RL) phase, where the model is encouraged to explore diverse and varied solutions rather than converging on a single answer, and to generate long completions (up to 32,000 tokens), which allows the development of reasoning capabilities that exceed those of the teaching model. </em> </p>",
                "size_desc": "<p> With 24 billion parameters, this model is considered medium-sized. It requires a single powerful graphics card to run. Reasoning models also take longer to produce an answer, which increases their power consumption. </p>\n <p> It has a context window of up to 40,000 tokens, useful for analyzing short documents but insufficient for analyzing large document corpora. </p>"
            },
            "MiniMax M2": {
                "desc": "<p> A model specializing in coding with a very competitive quality/speed/price ratio. It was trained by MiniMax, a company based in Shanghai, China. </p>",
                "fyi": "<p> A model specifically designed for agentic tasks (like coding), with training to adhere to strict agent control protocols (planning, tool calls, verification). </p>",
                "size_desc": "<p> A mixture of experts (MoE) model with 230 billion parameters (10 billion of which are active per token generation). The context window supports 200,000 tokens, enabling the processing of codebases and long documents. </p>"
            },
            "MiniMax M2.5": {
                "desc": "<p>A very large, semi-open model specializing in coding and agentic tasks, with an integrated reasoning mode. It is designed by MiniMax, a company based in Shanghai, China.</p>",
                "fyi": "<p>This model uses a MoE architecture with 256 experts, 8 of which are token-activated. Its training is based on Forge, an agentic reinforcement learning framework developed by MiniMax, which decouples training from inference and allows the model to be optimized on over 200,000 real-world environments (code repositories, web browsers, desktop applications). The CISPO (Clipped Importance Sampling Policy Optimization) algorithm ensures the stability of the MoE training at scale. A tree-like sample merging mechanism accelerates training by approximately 40 times.</p>\n <p>The model scores 80.2% on SWE-Bench Verified (automatic software troubleshooting) and 51.3% on Multi-SWE-Bench. It also integrates office skills (Word, PowerPoint, Excel). Its unique feature is that it writes specifications before starting to code, breaking down complex tasks into sub-steps.</p>",
                "size_desc": "<p>With 229 billion parameters, MiniMax M2.5 is a very large model. The Mixture of Experts (MoE) architecture allows only 10 billion parameters to be activated per generated token, which significantly reduces the footprint compared to a dense model of the same size. The context window reaches approximately 200,000 tokens. Reasoning models of this type run longer to produce a response, which increases energy consumption.</p>"
            },
            "Ministral": {
                "desc": "<p> Small multilingual model designed to run on a laptop without a server connection, while still offering good capabilities in text summarization, simple question answering, and basic tool use. </p>",
                "fyi": "<p> This model uses a grouped query attention (GQA) method to limit the analyzed text at each generation step and gain speed and memory: computation times are reduced without impacting quality. The attention mechanism is improved by applying windows of different sizes, which allows handling large contexts (up to 128,000 tokens) while remaining lightweight. The large tokenizer (V3-Tekken) better compresses languages and code, which improves its performance on multilingual tasks. </p>",
                "size_desc": "<p> With its 8 billion parameters, this model is considered small. It can be deployed locally on a fairly powerful computer, ensuring data confidentiality, or hosted on a server with a single graphics card to limit infrastructure costs. </p>"
            },
            "Mistral 3 Large": {
                "desc": "<p> A very large, semi-open, multimodal model that performs well in multilingual code and contexts. </p>",
                "fyi": "<p> This model was trained on 3,000 Nvidia H200 GPUs. It directly competes with semi-open Chinese models. At launch, it is competitive with DeepSeek V3.1, Kimi K2, GLM 4.6, and other benchmarks in terms of code and general use cases. </p>",
                "size_desc": "<p> With 675 billion parameters, this model falls into the category of very large models. Thanks to a Mixture of Experts (MoE) architecture, it is more efficient than dense models of similar size, but it still requires a server with several very powerful graphics cards to host. This model activates 41 billion parameters per token generation. Its context window can hold up to 256,000 tokens, allowing it to process very long documents. </p>"
            },
            "Mistral Large 2": {
                "desc": "<p> Large model designed to handle complex questions and tasks: for example, code generation, tool use, long document analysis, or specific-domain language understanding. </p>",
                "fyi": "<p> This model was trained with a high proportion of code data (over 80 programming languages) and mathematics, which improves its ability to solve complex problems and use tools. </p>",
                "size_desc": "<p> With 123 billion parameters, this model is considered large. It requires a server equipped with at least one powerful graphics card, which implies a significant operating cost. It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Mistral Medium 3.1": {
                "desc": "<p> A medium-sized, multilingual, multimodal, and inexpensive model compared to other models offering similar performance. It became particularly attractive after an update in August 2025 with significant improvements to overall performance, an \"enhanced\" tone, and better internet search capabilities. </p>",
                "fyi": "<p> This model was designed to deliver solid performance at a lower cost than other similar models. Special attention was paid to professional usage data during its training. It performs particularly well compared to other models of similar size at generating code and performing mathematical tasks. </p>\n <p> This model was used as the basis for training Magistral Medium - a reasoning model. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring at least several powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Mistral Nemo": {
                "desc": "<p> Optimized for fast response time, this model is ideal for applications requiring immediate responses and can support 128k context tokens in over 100 languages. Released July 2024. </p>",
                "fyi": "<p> Released in July 2024, this small model is trained for reasoning, general knowledge, and programming tasks. It uses the Tekken tokenizer, which is effective at compressing texts of up to 128,000 tokens in over 100 languages. </p>",
                "size_desc": "<p> A small-sized model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.) </p>"
            },
            "Mistral Saba": {
                "desc": "<p> Medium-sized model designed for a detailed linguistic and cultural understanding of Middle Eastern and South Asian languages, including Arabic, Tamil, and Malayalam. </p>",
                "fyi": "<p> Training focused primarily on texts in Arabic, Tamil, and Malayalam. Regional corpora were selected to reflect authentic usage, including syntax, registers, and dialectal variants. For tokenization (splitting the text into basic units that the model can process), a specialized strategy adapted to languages with complex morphology such as Arabic was used. Optimizations aimed to avoid excessive word fragmentation and maximize vocabulary coverage. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a medium-sized model, requiring at least a powerful graphics card to run. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> The model offers a context window of up to 128,000 tokens, suitable for analyzing long documents. </p>"
            },
            "Mistral Small 3": {
                "desc": "<p> Released in January 2025, this model specializes in multilingualism and has advanced reasoning capabilities. </p>",
                "fyi": "<p> Released in January 2025, this model is specialized in multilingualism, has a function call mode and a context of 32,000 tokens. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Mistral Small 3.1 24B": {
                "desc": "<p> Mistral Small 3.1 24B Instruct is an improved variant of the Mistral Small 3 (January 2025), featuring 24 billion parameters and advanced multimodal capabilities. </p>",
                "fyi": "<p> Mistral Small 3.1 24B Instruct is a multimodal model that delivers state-of-the-art performance in text- and vision-based reasoning tasks, including image analysis, programming, mathematical reasoning, and multilingual support for dozens of languages. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p> Despite its name, this is a medium-sized model. It is multimodal (capable of processing text and images) and stands out for its precise query processing and its ability to use tools. </p>",
                "fyi": "<p> Version 3.2 of this model is optimized to generate structured output, particularly in JSON, while limiting repetitiveness and undesirable behavior during long generation chains. Multimodal, it processes both text and image inputs, allowing for joint analysis. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered medium-sized. It can be hosted on a server with a single powerful graphics card, limiting infrastructure costs. It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Mistral Small 3.2 24B": {
                "desc": "<p> Mistral Small 3.2 24B Instruct is an improved variant of Mistral Small 3.1 (March 2025), featuring 24 billion parameters and advanced multimodal capabilities. </p>",
                "fyi": "<p> Mistral Small 3.2 24B Instruct is a multimodal model that delivers state-of-the-art performance in text- and vision-based reasoning tasks, including image analysis, programming, mathematical reasoning, and multilingual support for dozens of languages. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Mixtral 8x22B": {
                "desc": "<p> This multilingual model, released in April 2024, has been specifically trained in English, French, German, Italian, and Spanish, as well as on math, programming, and reasoning tasks. </p>",
                "fyi": "<p> This model's SMoE (sparse mixture of experts) architecture makes it faster and optimizes its size-to-cost ratio. Only 39 billion parameters are active out of 141 billion. The 64,000-token pop-up window allows for precise information recall from large documents. Released in April 2024. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "Mixtral-8x7B": {
                "desc": "<p> This model trained on a multilingual corpus is effective for varied and low-complexity tasks. </p>",
                "fyi": "<p> The younger brother of the Mixtral family, this model is capable of processing contexts of 32,000 tokens and supports English, French, Italian, German, and Spanish. Thanks to the SMoE (sparse mixture of experts) architecture, only a fraction of the parameters are activated for each inference, thus reducing costs and latency. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p> Large model trained from Llama 3.1 70B. This retrained (fine-tune) version tends to detail more and provide more structured responses. </p>",
                "fyi": "<p> This model is based on retraining the Llama 3.1 70B, hence the presence of its source model in its name. It introduces improvements achieved thanks to reinforcement learning with human feedback (RLHF) and the REINFORCE algorithm: the model explores different responses, receives feedback in the form of rewards, then gradually adjusts its choices to better meet user expectations. This alignment process is often used when the model is intended to adapt to human preferences or to optimize its responses according to specific criteria. </p>",
                "size_desc": "<p> With 70 billion parameters, this model belongs to the large model category. It requires several powerful graphics cards to run, which results in significant operating costs. </p>"
            },
            "OLMo-2 32B": {
                "desc": "<p> OLMo 2 32B is a fully open source model (corpus and training code included) created by the Allen AI Institute (Ai2), released in March 2025. </p>",
                "fyi": "<p> OLMo 2 32B is a fully open source model: both the corpus and the training code are fully accessible. This OLMo model family was designed by the Allen Institute for AI (Ai2). </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Olmo 3 32B Think": {
                "desc": "<p> A reasoning model whose code and data are completely open. It was trained by AI2, a non-profit research institute. </p>",
                "fyi": "<p> This model is a rare example of total transparency. The code, the data (6 trillion tokens), the training checkpoints, and the evaluation recipes are all public. While fully reproducing the training is infrastructure-intensive and likely rare, this availability of checkpoints and the base models allows developers to fine-tune the model for specific tasks, notably by blending specific data with the existing dataset, thus ensuring more stable training. </p>",
                "size_desc": "<p> With 32 billion parameters, it is considered to be a medium-sized model. It can be deployed on a server equipped with a single graphics card (GPU). Its context window reaches 65,000 tokens, making it suitable for analyzing fairly long documents. </p>"
            },
            "Phi-3-Mini": {
                "desc": "<p> Powerful for code generation and summarization tasks, this compact model supports a restricted context of 4000 tokens. </p>",
                "fyi": "<p> Little brother of the Phi3 family, this model supports a context of 4000 tokens and was trained on synthetic and filtered web datasets. </p>",
                "size_desc": "<p>Very small models, with fewer than 7 billion parameters, are the least complex and most resource-efficient, providing sufficient performance for simple tasks such as text classification.</p>"
            },
            "Phi-3-small-8k-Instruct": {
                "desc": "<p> Optimized for logical reasoning, this small model supports a context of 8000 tokens, suitable for code generation and complex tasks. </p>",
                "fyi": "<p> Big brother of the Phi3 family, this model supports a context of 8000 tokens and was trained on synthetic and filtered web datasets. </p>",
                "size_desc": "<p> A small-sized model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.) </p>"
            },
            "Phi-3.5-mini": {
                "desc": "<p> Powerful for code generation and summarization tasks, this model handles a large context of 128k tokens. </p>",
                "fyi": "<p> A small model of the Phi family, replacing Phi-3-mini, this model supports a large context of 128,000 tokens and was trained on synthetic and filtered web datasets. </p>",
                "size_desc": "<p> Very small models, with fewer than 7 billion parameters, are the least complex and most resource-efficient, providing sufficient performance for simple tasks such as text classification. </p>"
            },
            "Phi-4": {
                "desc": "<p> Small, multilingual model that can use tools and performs well on complex tasks like logic, math, and code, while remaining compact. </p>",
                "fyi": "<p> This model uses TikTok for tokenization, which improves its capabilities in a multilingual context. It was trained on a total of 9.8 <strong> trillion </strong> tokens, 400 billion of which were specifically sourced from high-quality synthetic data, and the rest from filtered organic data. Training took place on 1,920 H100 graphics cards for 21 days. Techniques such as self-assessment - during which the model critiques and rewrites its responses - and instruction reversal were used to strengthen its understanding of instructions and reasoning abilities. </p>",
                "size_desc": "<p> With 14 billion parameters, this model is considered small. It can be deployed locally on a sufficiently powerful computer, or hosted on a server with a single graphics card, which reduces infrastructure costs. The context window, of 16,000 tokens, can be limiting for analyzing very long documents. </p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p> Medium-sized model specializing in programming and the use of tools (web searches, interactions with APIs, etc.). </p>",
                "fyi": "<p> This model has been trained on 5.5 trillion tokens and over 92 programming languages, including specialized coding languages like Haskell and Racket. </p>\n <p> Thanks to its code performance, it is able to handle calls to tools well, which is useful for agentic uses. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered mid-sized. It can run on a server equipped with a single powerful graphics card, which limits infrastructure costs. </p>\n <p> Its 128,000-token context window allows it to process long documents. </p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p> Multilingual medium-sized model. </p>",
                "fyi": "<p> This MoE (Mixture of Experts) model features a configuration of 128 experts in total, with only 8 experts activated per token generated, allowing for faster and more efficient inference. It uses a system called <em> global-batch </em> to optimize the distribution of work among the experts, so that they are all used in a balanced manner. </p>\n <p> Unlike other models like Qwen 2.5-MoE that recycle the same experts across multiple layers of the network, Qwen 3 30B A2B assigns unique experts to each layer. In practical terms, this means that experts from the first layer are never reused in subsequent layers—each level of the model has its own set of specialized experts. This architecture allows each expert to focus exclusively on tasks specific to their position in the neural network, resulting in finer-grained specialization and optimized performance for each stage of information processing. </p>",
                "size_desc": "<p> With 30 billion parameters, this model falls into the mid-size category. It can run on a server with a single powerful graphics card, limiting infrastructure costs. In addition, the Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token generation, thus limiting its energy footprint. </p>"
            },
            "Qwen 3 32B": {
                "desc": "<p> Medium-sized multilingual model with two response methods: the user can choose between a reasoning mode, for more in-depth answers, or a quick mode, to directly generate the final answer. </p>",
                "fyi": "<p> This model was trained on a very large data set: 36 billion tokens in 119 languages. Training was done in three stages. The model first learned from 30 billion tokens with a context of 4,000 tokens. Then, 5 billion tokens were added to strengthen its factual knowledge. Finally, it was exposed to a specific corpus to help it better handle very long texts. As a result, it has a context window of 128,000 tokens at the end of training, which is useful for reading and analyzing long documents. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered mid-sized. It can run on a server equipped with a single powerful graphics card, which limits infrastructure costs. </p>\n <p> Its 128,000-token context window allows it to process long documents. </p>"
            },
            "Qwen 3 8B": {
                "desc": "<p> Small, dense multilingual model of the Qwen 3 family, offering a “reasoning” mode for complex tasks (mathematics, code) and a “direct response” mode for faster responses. </p>",
                "fyi": "<p> Qwen 3 8B was trained on the same corpus as the larger models in the Qwen family: 36 billion tokens covering 119 languages. Its training follows three stages: pre-training on 30 billion tokens with a window of 4,000, factual enrichment with 5 billion tokens, and then a specialized phase for long contexts. </p>",
                "size_desc": "<p> With 8 billion parameters, it is one of the smallest models. It can be used locally on a workstation to preserve data confidentiality, or on an inexpensive server to limit costs compared to a larger model. </p>\n <p> Its context window can hold up to 128,000 tokens, making it useful to process long documents. </p>"
            },
            "Qwen 3 Max": {
                "desc": "<p> Among Qwen's few proprietary models, this is the largest and most powerful of the third generation. It has been trained with particular attention to enterprise usage and agentic use cases. </p>",
                "fyi": "<p> This model was trained on 36 trillion tokens, almost double that of Qwen 2.5, and is officially capable of responding in 100 languages. </p>",
                "size_desc": "<p> The exact size is unknown. Evidence suggests that this is a very large model, requiring servers with multiple powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. The model has a context window of up to 256,000 tokens, suitable for analyzing long documents or code repositories. </p>"
            },
            "Qwen 3.5 397B": {
                "desc": "<p>A very large, multimodal, and multilingual model with a reasoning mode. Thanks to a hybrid architecture combining linear attention (Gated DeltaNet) and expert mixing, it activates only 17 billion parameters out of a total of 397 billion.</p>",
                "fyi": "<p>This model is based on an innovative hybrid architecture combining Gated DeltaNet (linear attention) and classical attention with a mix of ultra-sparse experts: 512 experts in total, of which only 10 routed and 1 shared are activated per token. The network has 60 layers organized into blocks of 4: 3 Gated DeltaNet layers followed by 1 classical attention layer, each associated with a MoE module. This design significantly reduces the inference cost while preserving the model's ability to handle complex reasoning tasks.</p>\n <p>Qwen 3.5 is natively multimodal, trained using early fusion on trillions of multimodal tokens (text and images). It supports 201 languages and dialects. Post-training employs large-scale reinforcement learning, deployed across massively parallel environments (million-agent environments), with task distributions of increasing complexity. The training infrastructure uses an FP8 pipeline and an asynchronous reinforcement learning framework to optimize efficiency.</p>",
                "size_desc": "<p>With 397 billion parameters, Qwen 3.5 is a very large model, requiring several powerful graphics cards to run. However, the Mixture of Experts (MoE) architecture allows only 17 billion parameters to be activated per generated token, reducing the footprint compared to a dense model of the same size. The native context window reaches 262,144 tokens and can be extended to approximately 1 million using extrapolation techniques (YaRN). Reasoning models of this type run longer to produce an answer, which increases energy consumption.</p>"
            },
            "Qwen1.5-32B": {
                "desc": "<p> Medium-sized model with a training process highly focused on aligning with user preferences. </p>",
                "fyi": "<p> The model went through a phase of alignment with user preferences through techniques such as DPO (Direct Preference Optimization) and PPO (Proximal Policy Optimization). In addition to these techniques, which were highly innovative at the time of the model's design, Alibaba's teams also optimized the training data to make it highly multilingual, particularly for European, East Asian, and Southeast Asian languages. </p>",
                "size_desc": "<p>Medium-sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.</p>"
            },
            "Qwen2-57B-A14B-Instruct": {
                "desc": "<p> Mid-sized model with expert mix architecture, performing well in coding, math, and multilingual tasks. </p>",
                "fyi": "<p> This iteration of the Qwen models has longer context windows. </p>",
                "size_desc": "<p> Medium models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning. </p>"
            },
            "Qwen2-72b-instruct": {
                "desc": "<p> Large model performing well in coding, mathematics and multilingual tasks. </p>",
                "fyi": "<p> This iteration of the Qwen models has longer context windows. </p>",
                "size_desc": "<p>Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.</p>"
            },
            "Qwen2-7B": {
                "desc": "<p> Supporting 130k context tokens, this small, versatile, multilingual model performs well on translation, summarization, analysis, and reasoning tasks. </p>",
                "fyi": "<p> The little brother of the Qwen2 family and produced by the Chinese company Alibaba, this model can support up to 130,000 tokens to process long texts. </p>",
                "size_desc": "<p> A small-sized model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.) </p>"
            },
            "Qwen2.5-32B": {
                "desc": "<p> Supporting 130k context tokens, this multilingual and versatile model performs well on translation, summarization, analysis, and reasoning tasks. </p>",
                "fyi": "<p> A mid-range model in the Qwen2.5 family, produced by the Chinese company Alibaba, this model can support up to 130,000 tokens to process long texts. </p>",
                "size_desc": "<p> Medium models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning. </p>"
            },
            "Qwen2.5-7B": {
                "desc": "<p> Supporting 130k context tokens, this multilingual and versatile model performs well on translation, summarization, analysis, and reasoning tasks. </p>",
                "fyi": "<p> A small model in the Qwen2.5 family and produced by the Chinese company Alibaba, this model can support up to 130,000 tokens to process long texts. </p>",
                "size_desc": "<p> A small-sized model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.) </p>"
            },
            "Qwen3 Coder 480B A35B": {
                "desc": "<p> Very large model specialized in code generation, analysis of entire repositories and solving multi-step problems. This version is particularly strong in tool use and can simulate a reasoning phase before providing the final answer. </p>",
                "fyi": "<p> This model was pre-trained on 7.5 trillion tokens (70% of which are code), and uses an advanced post-training process - Code RL (Hard to Solve, Easy to Verify) to reinforce correct code execution and Agent RL (long-horizon reinforcement learning) to optimize the solving of multi-round software tasks, with a massively parallel environment (20,000 parallel simulations on Alibaba Cloud). </p>",
                "size_desc": "<p> Qwen3-Coder-480B-A35B-Instruct is a very large model, requiring multiple graphics cards to run. The Mixture of Experts (MoE) architecture, however, allows only a fraction of the parameters (35B out of 480B) to be enabled, which significantly reduces the environmental impact and cost compared to an equivalent dense model.\n The context window natively reaches 256,000 tokens and can be extended up to 1 million using extrapolation techniques (YaRN), making it ideal for analyzing large code bases. </p>"
            },
            "Qwen3 Coder Next": {
                "desc": "<p>A model designed specifically for code (especially for local development). Thanks to a hybrid architecture combining Gated DeltaNet, Gated Attention, and MoE, it achieves performance comparable to much larger models while activating only 3 billion parameters out of a total of 80 billion.</p>",
                "fyi": "<p>This model has been trained on over 800,000 verifiable code tasks via a reinforcement learning process. It excels at long-horizon reasoning, using complex tools, and recovering from execution errors.</p>",
                "size_desc": "<p>Qwen3-Coder-Next uses an ultra-sparse Mix of Experts (MoE) architecture with 512 experts, of which only 10 are activated per token. This design allows the model to run on high-end but mainstream hardware (MacBook 64GB, RTX 5090 or AMD Radeon 7900 XTX) while maintaining a native context window of 256,000 tokens.</p>"
            },
            "Trinity Large": {
                "desc": "<p>A semi-open American model created by Arcee AI, a 30-person startup.</p>",
                "fyi": "<p>Trinity Large was designed and trained entirely in the United States by Arcee AI, a 30-person startup. It was trained on 17 trillion tokens.</p>\n<p>The complete training was run over 33 days on 2,048 Nvidia B300 GPUs, at a </p> cost of approximately $20 million. The architecture incorporates advancements such as gated attention and Muon.</p>",
                "size_desc": "<p>With 398 billion parameters, Trinity Large is a very large model. Nevertheless, thanks to its highly optimized Mixture of Experts (MoE) architecture, it activates only 13 billion parameters per generated token, significantly reducing its energy footprint and inference costs compared to an equivalent dense model. Its context window reaches 512,000 tokens, making it particularly well-suited for analyzing very long documents or massive codebases.</p>"
            },
            "Yi-1.5 9B": {
                "desc": "<p> Yi 1.5 is a model from the Chinese company 01-ai, specializing in coding, math, reasoning, and instruction following, with a strong understanding of language. </p>",
                "fyi": "<p> Yi 1.5 is a model from the Chinese company 01-ai, specializing in coding, math, reasoning, and instruction following, with a strong understanding of language. </p>",
                "size_desc": "<p> A small-sized model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.) </p>"
            },
            "o3-mini": {
                "desc": "<p> o3-mini is made for reasoning and coding. It offers a good balance between performance, cost, and latency, while being smaller than other OpenAI models. </p>",
                "fyi": "<p> Model optimized for STEM (science, technology, engineering, mathematics) reasoning tasks and coding. It excels in science, mathematics, and programming. </p>",
                "size_desc": "<p>These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments.</p>"
            },
            "o4 mini": {
                "desc": "<p> Very large reasoning model, suitable for complex scientific and technological tasks and questions. </p>",
                "fyi": "<p> This model is very powerful for analyzing images and graphs. It has also been trained to interact with other tools via function calls, making it suitable for agentic use cases. As a very powerful reasoning model, it can be used to distribute tasks among several smaller and/or more specialized models. It has a context window of up to 200,000 tokens, making it easy to analyze long documents. </p>",
                "size_desc": "<p> Despite its name and the fact that the exact size is unknown, o4 mini is most likely a large model requiring servers equipped with multiple graphics cards. Reasoning models like o4 mini require more time to respond because a reasoning phase precedes the generation of the final result, which increases their energy consumption. However, the assumed Mixture of Experts (MoE) architecture only activates a subset of the parameters to generate each token, thus limiting its energy footprint. Size estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "qwq 32B": {
                "desc": "<p> Medium-sized reasoning model specialized and highly efficient in mathematics, code generation, and logical problem solving. </p>",
                "fyi": "<p> This model was trained with a reinforcement learning (RL) method to optimize its handling of math problems and programming tasks. It uses several recent techniques to improve the quality of answers. For example, the RoPE (Rotary Position Embedding) method allows it to better understand the word order in a text. The SwiGLU activation function is a more efficient way of handling computations within the neural network, which helps the model produce more reliable answers. The QKV (Query Key Value-bias) adjustment method improves how the model identifies and selects important information. Finally, thanks to the YaRN (Yet another RoPE extensioN method), it can process very long texts of up to 130,000 tokens, allowing it to work on complex or very detailed documents. </p>",
                "size_desc": "<p> With 32 billion parameters, this model falls into the mid-size category. It can run on a server with a single powerful graphics card, which limits infrastructure costs. However, reasoning models of this type take longer to produce an answer because a reasoning phase precedes the generation of the final result, which increases energy consumption. </p>"
            }
        }
    },
    "header": {
        "banner": "New models just joined the arena!",
        "chatbot": {
            "newDiscussion": "New chat",
            "step": "Step",
            "stepOne": {
                "description": "Pay attention to both content and form, then evaluate each response.",
                "title": "What do you think of the answers?"
            },
            "stepTwo": {
                "description": "Discover the environmental impact of your conversations with each model",
                "title": "Models are revealed!"
            }
        },
        "help": {
            "link": {
                "content": "Help us improve compar:IA",
                "title": "Give feedback on the arena – opens a new window"
            }
        },
        "homeTitle": "Home - compar:IA",
        "logoAlt": "French Republic",
        "menu": "Menu",
        "startDiscussion": "New chat",
        "subtitle": "The chatbot arena",
        "title": "compar:AI",
        "votes": {
            "count": "{count} votes",
            "legend": "Legend",
            "objective": "Goal: {count}",
            "tooltip": "Discuss, vote, and help us reach this goal!<br /><strong>Your votes matter</strong>: they feed the compar:IA dataset, which is freely available to help refine future models in less-resourced languages.<br />This digital commons contributes to better <strong>respect for linguistic and cultural diversity in future language models.</strong>"
        }
    },
    "home": {
        "europe": {
            "desc": "Lithuania, Sweden, and Denmark are joining France in adopting the comparator to refine future AI models in their national languages.",
            "languages": {
                "da": "in Danish",
                "fr": "in French",
                "lt": "in Lithuanian",
                "sv": "in Swedish"
            },
            "question": "Would you like to have the chatbot arena in your language?",
            "title": "The comparator <span {props}>becomes European!</span>"
        },
        "faq": {
            "discover": "See other questions",
            "title": "Your frequently asked questions"
        },
        "intro": {
            "desc": "Have a blind discussion with two AIs and evaluate their answers",
            "steps": {
                "a11yDesc": "1. I chat with two hidden AIs: Chat for as long as you like. 2. I give my preference: By doing so, you'll help improve the AI models. 3. The model identities are revealed: Learn more about them and their characteristics.",
                "one": {
                    "desc": "Chat as long as you like",
                    "title": "I chat with two hidden AIs"
                },
                "three": {
                    "desc": "Learn more about AI models and their characteristics",
                    "title": "The model identities are revealed!"
                },
                "title": "How it works",
                "two": {
                    "desc": "By doing so, you'll help improve the AI models",
                    "title": "I give my preference"
                }
            },
            "title": "Don't trust the answers <span {props}>of a single AI</span>",
            "tos": {
                "accept": "I accept the <a {linkProps}>terms of use</a>",
                "error": "You must accept the terms of use to continue",
                "help": "Data is shared for research purposes"
            }
        },
        "origin": {
            "project": {
                "desc": "The chatbot arena was designed and developed as part of a government startup led by the French Ministry of Culture, integrated into the <a {linkProps}>Beta.gouv.fr</a> program by the Interministerial Digital Directorate (DINUM). This initiative supports French public administrations in building useful, simple, and user-friendly digital services.",
                "title": "Who initiated the project?"
            },
            "team": {
                "desc": "The chatbot arena is led within the French Ministry of Culture by a multidisciplinary team - AI experts, developers, deployment specialists, and designers - with a mission to make conversational AI more transparent and accessible to everyone.",
                "title": "Who are we?"
            }
        },
        "usage": {
            "desc": "The tool is also aimed at AI experts and educators for more specific use cases",
            "educate": {
                "desc": "Use the chatbot arena as an educational tool to discuss AI with your audience",
                "title": "Train and raise awareness"
            },
            "explore": {
                "desc": "Find all model specifications and terms of use in one place",
                "title": "Explore the models"
            },
            "title": "Specific use cases of compar:IA",
            "use": {
                "desc": "Developers, researchers, model publishers - access compar:IA’s datasets to enhance models for low-resource languages",
                "title": "Reuse the data"
            }
        },
        "use": {
            "compare": {
                "alt": "Compare",
                "desc": "Discuss and develop your critical thinking by giving your preference",
                "title": "Compare the responses of different AI models"
            },
            "desc": "compar:IA is a free tool that helps raise awareness among citizens about generative AI and its challenges.",
            "measure": {
                "alt": "Measure",
                "desc": "Discover the environmental impact of your conversations with each model",
                "title": "Measure the environmental footprint of questions asked to AI"
            },
            "test": {
                "alt": "Test",
                "desc": "Test different models: open, proprietary, small, large...",
                "title": "Test the latest AI in the ecosystem in one place"
            },
            "title": "What is compar:AI for?"
        },
        "vote": {
            "datasetAccess": "Access the datasets",
            "desc": "The tool is also useful to AI experts, developers and for educational purposes.",
            "steps": {
                "datasets": {
                    "desc": "All questions and votes are compiled into datasets and published openly after anonymization.",
                    "title": "Datasets by language"
                },
                "finetune": {
                    "desc": "Companies and academia can use the datasets to train new models that are more respectful of linguistic and cultural diversity.",
                    "title": "Models fine-tuned for specific languages"
                },
                "prefs": {
                    "desc": "After discussing with the AIs, you are invited to indicate your preference for a model on given criteria, such as the relevance or usefulness of the answers.",
                    "title": "Your preferences"
                }
            },
            "title": "Why is your vote important?"
        }
    },
    "models": {
        "arch": {
            "title": "Did you know?"
        },
        "conditions": {
            "commercialUse": {
                "question": "Is commercial use of the model allowed?",
                "title": "Commercial use"
            },
            "reuse": {
                "question": "Can I use the model outputs to train new models?",
                "subTitle": "You cannot reuse outputs to train other models",
                "title": "Reuse of generated outputs"
            },
            "title": "Terms of Use",
            "types": {
                "allowed": "Allowed",
                "conditions": "Under conditions",
                "forbidden": "Forbidden"
            }
        },
        "conso": {
            "count": {
                "L": "> 100 Wh",
                "M": "from 10 to 100 Wh",
                "S": "< 10 Wh"
            },
            "filterLegend": "Filter by average energy consumption per 1000 tokens"
        },
        "extra": {
            "experts": {
                "api-only": "To dive deeper, check out the <a {linkProps}>official model website</a>",
                "open-weights": "To dive deeper, check out the <a {linkProps}>model page on Hugging Face</a>"
            },
            "impacts": "Environmental impact calculations are based on the <a {linkProps1}>EcoLogits</a> and <a {linkProps2}>Impact CO<sub>2</sub></a> projects.",
            "title": "To learn more"
        },
        "licenses": {
            "type": {
                "openSource": "Open source",
                "proprietary": "Proprietary",
                "semiOpen": "Open-weight"
            }
        },
        "list": {
            "filters": {
                "archived": {
                    "checkedLabel": "Visible",
                    "help": "By default, only active models in the arena are visible.",
                    "label": "Archived models",
                    "uncheckedLabel": "Not visible"
                },
                "display": "Show filters",
                "editor": {
                    "legend": "Publisher"
                },
                "license": {
                    "legend": "License"
                },
                "reset": "Clear all filters",
                "size": {
                    "labels": {
                        "L": "100 to 400 billion",
                        "M": "60 to 100 billion",
                        "S": "15 to 60 billion",
                        "XL": "> 400 billion",
                        "XS": "< 15 billion"
                    },
                    "legend": "Size (parameters)"
                }
            },
            "intro": "Explore the different conversational AI models available, their specifications, and licenses.",
            "model": "model",
            "models": "models",
            "noresults": "No models match your search criteria.",
            "title": "Discover the models",
            "triage": {
                "label": "Sort by",
                "options": {
                    "date-desc": "Release date (newest to oldest)",
                    "name-asc": "Model name (A to Z)",
                    "org-asc": "Publisher (A to Z)",
                    "params-asc": "Size (smallest to largest)"
                }
            }
        },
        "names": {
            "a": "Model A",
            "b": "Model B"
        },
        "openWeight": {
            "tooltips": {
                "copyleft": "Once modified, the model must be redistributed under the same license as the source model.",
                "free": "Once modified, the model may be redistributed under a different license than the source model.",
                "openSource": "The training data, code, and weights of this model (i.e., the parameters learned during its training) are fully downloadable and modifiable by the public, allowing them to run and modify the model on their own hardware. Whether a model is \"open source\" is more restrictive than \"open weights,\" in particular because of the need for transparency of the training corpus, and few models are considered \"open source.\"",
                "openWeight": "A so-called \"open weights\" model whose weights, i.e. the parameters learned during training, are downloadable by the public, allowing them to run the model on their own hardware. Whether a model is \"open source\" is more restrictive (mainly in relation to the transparency of the training corpus), and few models are considered \"open source\".",
                "params": "Parameters or weights, counted in billions, are the variables learned by a model during training that determine its responses. The greater the number of parameters, the more learning capacity they have.",
                "ram": "RAM (random access memory) stores data processed by an LLM in real time. The larger the model, the more RAM it needs to run."
            }
        },
        "parameters": "{number} parameters",
        "ram": "{min} to {max} GB",
        "release": "Released on {date}",
        "size": {
            "count": {
                "L": "100 to 400 billion",
                "M": "60 to 100 billion",
                "S": "15 to 60 billion",
                "XL": ">400 billion",
                "XS": "<15 billion"
            },
            "estimated": "Estimated size ({size})",
            "title": "Size"
        }
    },
    "modes": {
        "big-vs-small": {
            "altLabel": "David vs Goliath model selection",
            "description": "One small model against one big model, both chosen randomly",
            "label": "David vs Goliath",
            "title": "David vs Goliath mode"
        },
        "custom": {
            "altLabel": "Manual model selection",
            "description": "Will you recognize the two models you chose?",
            "label": "Manual selection",
            "title": "Manual selection mode"
        },
        "random": {
            "altLabel": "Random model selection",
            "description": "Two models chosen randomly from the full list",
            "label": "Random",
            "title": "Random mode"
        },
        "reasoning": {
            "altLabel": "Reasoning model selection",
            "description": "Two reasoning models chosen randomly",
            "label": "Reasoning",
            "title": "Reasoning mode"
        },
        "small-models": {
            "altLabel": "Frugal model selection",
            "description": "Two small models chosen randomly",
            "label": "Frugal",
            "title": "Frugal mode"
        }
    },
    "product": {
        "community": {
            "countries": {
                "da": "Denmark",
                "fr": "France"
            },
            "tabLabel": "Community",
            "teams": {
                "fr": {
                    "people": {
                        "aurelien": {
                            "date": "Since june 2024",
                            "job": "UX/UI product designer"
                        },
                        "elie": {
                            "date": "Since november 2025",
                            "job": "Full-stack developer"
                        },
                        "hadrien": {
                            "date": "June 2024 - November 2025",
                            "job": "Full-stack developer"
                        },
                        "lucie": {
                            "date": "January 2024 - December 2025",
                            "job": "Founder - Product Lead"
                        },
                        "mathilde": {
                            "date": "Since September 2024",
                            "job": "Head of \"Atelier numérique\" / Product Ops"
                        },
                        "nicolas": {
                            "date": "Since June 2025",
                            "job": "Full-stack developer"
                        },
                        "simonas": {
                            "date": "Since December 2024",
                            "job": "Product Manager - ex-deployment manager"
                        }
                    },
                    "title": "The compar:IA France team"
                }
            },
            "title": "Partners"
        },
        "comparator": {
            "challenges": {
                "bias": {
                    "desc": "Highlight AI biases stemming from the underrepresentation of non-English data in models and raise awareness of their real-world impact.",
                    "title": "Cultural and linguistic bias"
                },
                "impacts": {
                    "desc": "Show the environmental impact of generative AI, still widely unknown to the general public.",
                    "title": "Environmental impact"
                },
                "pluralism": {
                    "desc": "Ensure citizens have access to a diverse range of AI models, empowering them to make informed choices and cultivate a critical understanding of these technologies.",
                    "title": "Model diversity"
                },
                "thinking": {
                    "desc": "Encourage critical thinking on the role of generative AI in personal and professional practices.",
                    "title": "Critical thinking and societal questions"
                },
                "title": "The platform addresses multiple challenges"
            },
            "cta": "Access the arena",
            "europe": {
                "adventure": "As of summer 2025, Lithuania, Sweden, and Denmark are joining the initiative!",
                "catch": "Would you like to have the chatbot arena in your language?",
                "desc": "The arena is now available to their citizens in national languages, with a core mission: build preference datasets to improve future AI model performance in low-resource languages.",
                "title": "The arena <span {props}>goes European</span>!"
            },
            "screenshotAlt": "Screenshot of the arena, with the initial question, the two model responses, and the voting buttons.",
            "tabLabel": "The arena",
            "title": "The arena enables the creation of <span {props}>preference datasets</span> focused on <span {props}>real-world usage</span> in <span {props}>European languages</span>."
        },
        "faq": {
            "tabLabel": "FAQ"
        },
        "history": {
            "steps": {
                "acceleration": {
                    "items": {
                        "1": {
                            "date": "March 2025",
                            "desc": "Creation and publishing of the first compar:IA dataset in French including the questions and preferences of the platform's users.",
                            "title": "50,000 votes on the arena!"
                        },
                        "2": {
                            "date": "May 2025",
                            "desc": "And first reuse of the dataset with Bunka.ai which conducted an in-depth study on the interactions between users of the platform.",
                            "title": "Goal of 100,000 votes reached!"
                        },
                        "3": {
                            "date": "June 2025",
                            "desc": "Three datasets, conversations, reactions, votes, are available on <a {hgLinkProps}>HuggingFace</a> and <a {dataLinkProps}>Data.gouv.fr</a>.",
                            "title": "Publication of the three datasets"
                        }
                    },
                    "tag": "Acceleration"
                },
                "construction": {
                    "items": {
                        "1": {
                            "date": "June-September 2024",
                            "desc": "Development of the first MVP of the arena and integration of feedback from beta testers.",
                            "title": "Minimum viable product design"
                        },
                        "2": {
                            "date": "October 2024",
                            "desc": "Official presentation and first deployment of the tool.",
                            "title": "Official launch during the Francophonie Summit in Villers Cotterêts!"
                        },
                        "3": {
                            "date": "January 2025",
                            "desc": "Launch of the feature allowing to choose models manually.",
                            "title": "v2 of the arena"
                        },
                        "4": {
                            "date": "February 2025",
                            "desc": "More than 300 people in conferences and workshops on ethical, cultural and environmental challenges of conversational AI systems.",
                            "title": "<a {linkProps}>ComparIA Day at the BnF</a> during the AI Action Summit"
                        }
                    },
                    "tag": "Building"
                },
                "i18n": {
                    "items": {
                        "1": {
                            "date": "Summer 2025",
                            "desc": "Partnership with Denmark, Sweden and Lithuania to open the platform in their language and region.",
                            "title": "Scaling at the European level"
                        },
                        "2": {
                            "date": "September 2025",
                            "desc": "Creation of a new workshop format open to all audiences, to discover the inner workings of generative AI and reflect on its environmental impact, and publication of the facilitation kit associated with the \"AI Duels\" extension.",
                            "title": "Launch of <a {linkProps}>“AI Duels”</a>"
                        },
                        "3": {
                            "desc": "Built in partnership with the Digital Regulation Expertise Center (PEReN), the arena ranking is based on all votes and reactions collected since the service was opened to the public in October 2024.",
                            "title": "Publication of the <a {linkProps}>French ranking</a>"
                        },
                        "4": {
                            "desc": "Denmark joins the adventure by offering the arena in its national language with a dedicated domain name.",
                            "title": "Opening the arena <a {linkProps}>in Danish</a>"
                        },
                        "5": {
                            "desc": "The service is recognized as a digital public good by the Digital Public Goods Alliance",
                            "title": "compar:AI recognized as a <a {linkProps}>Digital Public Good</a>"
                        },
                        "6": {
                            "date": "November 2025",
                            "desc": "With over 500,000 unique conversations since the launch of compar:IA.",
                            "title": "Goal of 200,000 votes reached!"
                        }
                    },
                    "tag": "Internationalization"
                },
                "investigation": {
                    "items": {
                        "1": {
                            "date": "January-March 2024",
                            "desc": "Interviews with stakeholders in the ecosystem and initial hypotheses for a solution to the problem: \"How to facilitate access to data in French for training language models?\".",
                            "title": "Investigation phase"
                        }
                    },
                    "tag": "Investigation"
                }
            },
            "tabLabel": "Key Dates",
            "title": "The arena's milestones"
        },
        "partners": {
            "academy": {
                "catch": "Working on a research project? Have suggestions or need clarification on our methodology or datasets?",
                "desc": "We’re committed to ensuring the datasets we generate fuel multidisciplinary research, bridging humanities, social sciences, and data science.",
                "title": "Academic partners"
            },
            "diffusion": {
                "catch": "Would you like to use the chatbot arena in a professional context?",
                "cta": "Let us know",
                "desc": "We’re building a network of partners who integrate the chatbot arena into their services and training offerings.",
                "title": "Communication partners"
            },
            "institution": {
                "title": "Institutional partners"
            },
            "services": {
                "desc": "Environmental impact calculations are based on the tools above.",
                "title": "Services used"
            },
            "tabLabel": "Partners"
        },
        "problem": {
            "alignment": {
                "alignment": {
                    "a": "Alignment comes after a language model’s pre-training phase, acting as a final \"refinement\" or \"polishing\" step. During pre-training, the model learns to predict the next word, gaining the ability to generate coherent text - but alignment is what tailors it to human preferences.",
                    "b": "The alignment phase trains the model to better meet human needs by making it <strong>more relevant</strong> (answering questions more accurately), <strong>more honest</strong> (admitting when it lacks sufficient data), and <strong>safer</strong> (avoiding harmful or inappropriate content).",
                    "c": "<strong>Without alignment, an LLM might be technically capable yet impractical to use - failing to grasp what users truly expect in a conversation.</strong>",
                    "title": "Alignment: a critical post-training phase"
                },
                "datasets": {
                    "a": "Alignment relies on highly specialized datasets, meticulously designed to teach the model \"proper\" behavior.",
                    "b": "<strong>Preference data</strong> is a critical component of alignment, working alongside <strong>demonstration data</strong> (expert-crafted human-AI dialogues with precise tone/style guidelines), <strong>safety data</strong> (curated examples teaching models to reject harmful requests), and <strong>domain-specific datasets</strong> (tailored for fields like medicine, law, or education).",
                    "c": "Preference data presents multiple potential answers to the same question, ranked by human evaluators based on criteria like relevance, usefulness, or harm potential. Users indicate which response performs best, and these curated datasets are then used to fine-tune models - aligning them with expressed human preferences.",
                    "title": "Specific datasets"
                },
                "desc": "Alignment: A bias-mitigation technique based on crowdsourcing user preferences to refine model behavior",
                "diversity": {
                    "a": "To reflect the diversity of cultures and languages in model outputs, <strong>alignment datasets must incorporate a broad range of languages</strong>, contexts, and real-world user tasks. Diversifying alignment data ultimately improves a model’s performance in two key ways:",
                    "b": "First, it <strong>reduces cultural bias</strong> by preventing a single - often Anglophone -perspective from dominating the AI’s responses. The model learns that valid answers vary by cultural context, recognizing multiple legitimate ways to address the same question.",
                    "c": "Second, exposure to linguistic and cultural diversity enables context-aware responses: a French user gets advice tailored to France’s systems, while a Danish user receives information aligned with their national context.",
                    "d": "The result? A more inclusive conversational AI - one that acknowledges and adapts to diverse cultural perspectives.",
                    "title": "Diversify data sources to reduce bias"
                },
                "english": {
                    "a": "Preference data is expensive to produce because <strong>each example requires skilled human evaluation</strong>. Platforms like chat.lmsys.org help crowdsource these datasets—but few users contribute in their native language, leaving low-resource languages underrepresented.",
                    "b": "Preference datasets for European languages are scarce - or nonexistent. In LMSYS’s dataset, for instance, French queries represent less than 1% of the total.",
                    "c": "compar:IA is a chatbot arena designed to gather multilingual conversations - capturing region-specific cultural references like daily tasks, local culinary traditions, education systems, or historical and literary touchstones.",
                    "title": "European languages suffer from a shortage of preference data"
                },
                "title": "How can we reduce cultural and linguistic biases in these models?"
            },
            "diversity": {
                "diversity": {
                    "desc": "These biases can lead to incomplete or outright incorrect responses, sidelining the diversity of European languages and cultures.",
                    "title": "Overlooked cultural and linguistic diversity"
                },
                "english": {
                    "desc": "Conversational AI relies on large language models (LLMs) trained primarily on English data, creating linguistic and cultural biases in their outputs.",
                    "title": "Training data overwhelmingly in English"
                },
                "stereotypes": {
                    "desc": "Conversational AI systems seem fluent in every language - but their outputs can still be stereotypical or discriminatory.",
                    "title": "Bias-reinforcing answers"
                }
            },
            "tabLabel": "The initial problem",
            "title": "Do conversational AI models respect the <span {props}>diversity</span> of European languages?"
        },
        "title": "Everything you need to know about the chatbot arena"
    },
    "ranking": {
        "energy": {
            "desc": "This graph represents the satisfaction score (Bradley Terry score) for each model as a function of the estimated average energy consumption per 1000 tokens. Energy consumption is estimated using the Ecologits methodology and is based on two parameters: the size of the models (number of parameters) and their architecture. For proprietary models, this information is either not provided or only partially available. Therefore, they are excluded from the graph below.",
            "tabLabel": "Energy focus",
            "title": "Are the most popular models energy efficient?",
            "views": {
                "graph": {
                    "desc": "Select a model to find out its Bradley-Terry (BT) score and estimated energy consumption",
                    "legends": {
                        "arch": "Model architecture",
                        "size": "Filter by size",
                        "sizeSub": "(billions of parameters)"
                    },
                    "tabLabel": "Graph view",
                    "title": "Bradley-Terry (BT) Satisfaction Score VS Average Consumption per 1000 Tokens",
                    "tooltip": {
                        "active_params": "Active parameters (billions)",
                        "arch": "Architecture",
                        "consumption_wh": "Average consumption (Wh)",
                        "elo": "BT score",
                        "params": "Parameters (billions)"
                    },
                    "xLabel": "Average consumption per 1000 tokens (Wh)",
                    "yLabel": "Bradley-Terry Score (BT)"
                },
                "methodo": {
                    "1": {
                        "list": {
                            "1": "<strong> Higher a model is located on the graph </strong> the higher its Bradley-Terry satisfaction score. <strong> further to the left a model is located on the graph </strong> the less energy it consumes compared to other models.",
                            "2": "<strong> At the top left </strong> are the models that are popular and consume less energy compared to other models.",
                            "3": "Beyond size, architecture has an impact on the average energy consumption of models: for example, with a similar size, the Llama 3 405B model (dense architecture, 405 billion parameters) consumes 10 times more energy on average than the GLM 4.5 model (MOE architecture, 355 billion parameters and 32 billion active parameters)."
                        },
                        "subTitle": "Examples of how to read the graph",
                        "title": "How to find the right balance between perceived performance and energy efficiency?"
                    },
                    "2": {
                        "descs": {
                            "1": "The estimation of energy consumption for model inference relies on the Ecologits methodology, which takes into account the size and architecture of the models. However, this information is not made public by model developers for proprietary models.",
                            "2": "We have therefore decided not to integrate proprietary models into the graph until the information contributing used for the calculation of energy consumption is transparent."
                        },
                        "title": "Why are the proprietary models not displayed on the graph?"
                    },
                    "3": {
                        "descs": {
                            "1": "The arena uses the methodology developed by Ecologits (GenAI Impact) to provide an estimate of the energy footprint associated with inferring conversational generative AI models. This estimate allows users to compare the environmental impact of different AI models for the same query. This transparency is essential to encourage the development and adoption of more eco-friendly AI models.",
                            "2": "Ecologits applies the principles of life cycle assessment (LCA) in accordance with ISO 14044, focusing for the moment on the impact of inference (i.e., the use of models to answer queries) and the <strong> manufacturing of graphics cards </strong> (resource extraction, manufacturing and transport).",
                            "3": "The model's power consumption is estimated by taking into account various parameters such as the size and architecture of the AI model used, the location of the servers where the models are deployed, and the number of output tokens. The calculation of the global warming potential indicator, expressed in CO2 equivalent, is derived from the measurement of the model's power consumption.",
                            "4": "It is important to note that methodologies for assessing the environmental impact of AI are still under development."
                        },
                        "title": "How is the energy impact of the models calculated?"
                    }
                },
                "table": {
                    "title": "Chart data in table form"
                }
            }
        },
        "methodo": {
            "desc": {
                "1": "Since 2024, thousands of users have used the arena to compare the responses of different models, generating hundreds of thousands of votes. Simply counting the number of wins is not enough to establish a ranking. A fair system must be statistically robust, adjust after each matchup, and truly reflect the value of the performances achieved.",
                "2": "It is with this in mind that a <strong>ranking based on the Bradley-Terry model</strong> was established, developed in collaboration with the <strong>French Center of expertise for digital platform regulation (<a {perenLinkProps}>PEReN</a>) teams</strong>, based on all the votes and reactions collected on the platform. To learn more, see our <a {lnotebookLinkProps}>methodological notebook</a>."
            },
            "impacts": {
                "elo": {
                    "desc": {
                        "1": "The <strong> Bradley-Terry </strong> model transforms a set of local and potentially incomplete comparisons into a consistent and statistically robust global ranking system, where the empirical win rate remains limited to direct observations."
                    },
                    "title": "Top 10 models in the ranking based on estimated win rate with the Bradley-Terry model"
                },
                "title": "Impact of methodological choice on model ranking",
                "winrate": {
                    "desc": {
                        "1": "Based solely on the <strong> average win rate </strong> , an overall ranking can be obtained, but this calculation assumes that each model has played against all others.",
                        "2": "This method is not ideal because it requires data from all combinations of models and as soon as the number of models increases, it quickly becomes expensive and cumbersome to maintain."
                    },
                    "title": "Top 10 models in the ranking based on \"empirical\" win rates"
                }
            },
            "methods": {
                "cons": "Main problems",
                "elo": {
                    "def": "<strong> Definition </strong>: Ranking system where the gain or loss of points depends on the result (victory/defeat/draw <strong> and </strong> the estimated level of the opponent: if a weaker model beats a stronger model, its progression in the ranking is greater.",
                    "list": {
                        "1": "<strong> Probabilistic model </strong>: we can estimate the probable outcome of any matchup, even between models that have never been directly competed.",
                        "2": "<strong> Taking match difficulty into account </strong> :The scores estimated from the Bradley Terry model take into account the level of the opponents encountered, allowing for a fair comparison between models.",
                        "3": "<strong> Better Uncertainty Management </strong>:The confidence interval integrates the entire network of comparisons. This allows for a more accurate estimation of uncertainty, especially for models with few direct confrontations but many common opponents."
                    },
                    "title": "Bradley-Terry (BT) leaderboard"
                },
                "pros": "Benefits",
                "title": "Two ways to classify models",
                "winrate": {
                    "def": "<strong> Definition </strong> An empirical ranking system for models based on the percentage of duels won by a model against all other models.",
                    "list": {
                        "1": "<strong> Game Count Bias </strong> A model that has won three out of three duels has a 100% win rate, but this score is not very meaningful because it is based on very little data.",
                        "2": "<strong> No consideration of duel difficulty: </strong> beating a “beginner” or an “expert” model counts the same. Win rates are unfair since they do not take match difficulty into account.",
                        "3": "<strong> Stagnation: </strong> In the long run, many good models end up around 50% win rate because they are facing models of their own skill level, which makes the rankings less discriminating."
                    },
                    "title": "Ranking by win rate"
                }
            },
            "tabLabel": "Methodology",
            "title": "How to choose the model classification method?"
        },
        "preferences": {
            "desc": "When you vote, you can classify your preference according to different positive and negative labels. Compare their distribution across models.",
            "modal": {
                "cta": "What is a preference?",
                "title": "Positive and negative preferences"
            },
            "tabLabel": "Preference focused view",
            "table": {
                "cols": {
                    "clear_formatting": "Clear formatting<br>",
                    "complete": "Complete",
                    "creative": "Creative",
                    "incorrect": "Incorrect",
                    "instructions_not_followed": "Instructions<br> not followed",
                    "n_match": "Total matches",
                    "name": "Model",
                    "positive_prefs_ratio": "Distribution of preferences",
                    "superficial": "Superficial",
                    "total_negative_prefs": "Total preferences<br> Negative",
                    "total_positive_prefs": "Total preferences<br> Positive",
                    "useful": "Useful"
                },
                "percentLabel": "Preferences in percentage",
                "tooltips": {
                    "positive_prefs_ratio": "When voting, labels are used to indicate the reasons for your preference. This column shows the percentage distribution of these labels (positive or negative) for all votes."
                }
            },
            "title": "How are user preferences distributed?"
        },
        "ranking": {
            "desc": "Thank you for your contributions!<br> The <strong>arena ranking</strong> is based on all votes and reactions from the <strong>blind comparison</strong> of the models, collected since the service opened to the public in October 2024.<br>Developed in partnership with the Digital Regulation Expertise Center (<a {linkProps}>PEReN</a>), the model ranking is based on the <strong>satisfaction score</strong> calculated using the <strong>Bradley Terry</strong> statistical model, a widely used method for converting binary votes into a probabilistic ranking.<br> The compar:IA ranking is not intended to be an official recommendation or to evaluate the technical performance of the models. It reflects the <strong>subjective preferences</strong> of the platform's users and not the factual accuracy or veracity of the responses.",
            "tabLabel": "Leaderboard"
        },
        "table": {
            "data": {
                "billions": "{count} billion",
                "cols": {
                    "arch": "Architecture",
                    "consumption_wh": "Average energy<br>(per 1000 tokens)",
                    "elo": "BT score <br> of satisfaction",
                    "license": "Licence",
                    "n_match": "Total votes",
                    "name": "Model",
                    "organisation": "Organization",
                    "rank": "Rank",
                    "release": "Release date",
                    "size": "Size<br>(parameters)",
                    "trust_range": "Confidence (±)"
                },
                "estimation": "(estimate)",
                "tooltips": {
                    "arch": "The architecture of an LLM model refers to the design principles that define how the components of a neural network are arranged and interact to transform input data into predictive outputs, including the activation mode of parameters (dense vs. sparse), the specialization of components, and the information processing mechanisms (transformers, convolutional networks, hybrid architectures).",
                    "consumption_wh": "Measured in watt-hours, energy consumption represents the electricity used by the model to process a request and generate the corresponding response. Model energy consumption depends on the model's size and architecture. We have chosen to display proprietary models for which we do not have transparent information about their size and architecture in gray as \"unanalyzed\" (N/A).",
                    "elo": "Estimated statistical score based on the Bradley-Terry model, reflecting the probability that one model is preferred over another. This score is calculated from all user votes and reactions. For more information, visit the methodology tab.",
                    "rank": "Rank assigned based on Bradley-Terry satisfaction score",
                    "size": "Model size in billions of parameters, categorized into five classes. For proprietary models, this size is not reported.",
                    "trust_range": "Interval indicating the reliability of the rank: the narrower the interval, the more reliable the rank estimate. There is a 95% chance that the model's true rank is within this range."
                }
            },
            "lastUpdate": "Updated on {date}",
            "totalModels": "Total models:",
            "totalVotes": "Total votes:"
        },
        "title": "From votes to a leaderboard"
    },
    "reveal": {
        "equivalent": {
            "co2": {
                "label": "CO <sub> 2 </sub> emitted",
                "tooltip": "The CO <sub> 2 </sub> emitted is equivalent to the carbon dioxide emissions produced by the energy used to run the model. It reflects the environmental impact linked to energy consumption. The Watt-hour/CO <sub> 2 </sub> equivalence calculation differs depending on the energy mix of each country. However, the servers used for model inference are not all located in Europe. Thus, the equivalence calculation is based on the global average CO <sub> 2 </sub> emission rate per energy consumed."
            },
            "lightbulb": {
                "label": "LED bulb",
                "tooltip": "Data calculated based on the consumption of a standard 5W LED bulb (E14)"
            },
            "streaming": {
                "label": "online videos",
                "tooltip": "Data calculated based on the carbon impact of one hour of streamed video in high definition, on a television, with a Wi-Fi connection (source <a {linkProps}>ADEME</a>)"
            },
            "title": "Which corresponds to:"
        },
        "feedback": {
            "description": "Share compar:AI with others by sharing the AI models you've interacted with! Only the names and energy impact of the discussion will be visible via this link, with no access to the messages in the conversation.",
            "example": "Example of shared results",
            "moreOnVotes": "Learn more about votes",
            "shareResult": "Share your result"
        },
        "impacts": {
            "energy": {
                "label": "energy consumed",
                "tooltip": "Measured in watt-hours, energy consumption represents the electricity used by the model to process a query and generate the corresponding response. Generally, the larger a model (in billions of parameters), the more energy is required to produce a token."
            },
            "size": {
                "count": "billion param.",
                "estimated": "(est.)",
                "label": "model size",
                "quantized": "(quantized)"
            },
            "title": "Estimated energy consumption of the chat",
            "tokens": {
                "label": "text size",
                "tokens": "tokens",
                "tooltip": "AI analyzes and generates sentences from words or parts of words of approximately four letters; this unit of text is called a token. The longer a text, the greater the number of tokens."
            },
            "tooltip": "Estimate based on model size, architecture, and generated response length."
        },
        "thanks": {
            "cta": "Access the rankings",
            "desc": "Discover how your preference compares to the overall ranking of models in the arena.",
            "graphAlt": "Overview of the graph representing the score and electricity consumption of the models",
            "rankingAlt": "Overview of the ranking in table format",
            "title": "Thank you for your contribution!"
        }
    },
    "seo": {
        "desc": "compar:IA is a tool that enables the blind comparison of different conversational AI models, raising awareness of the issues surrounding generative AI (plurality, bias, environmental impact) and helping to build language preference datasets for less-resourced languages.",
        "title": "compar:IA, the AI chatbot arena",
        "titles": {
            "accessibilite": "Accessibility statement",
            "arene": "Chat",
            "community": "Community",
            "comparator": "The arena",
            "datasets": "Datasets",
            "donnees-personnelles": "Privacy policy",
            "duel": "AI Duel Workshop",
            "faq": "FAQ",
            "history": "Key Dates",
            "home": "Home",
            "mentions-legales": "Legal notice",
            "modalites": "Terms of use",
            "modeles": "Model list",
            "news": "News and resources",
            "partners": "Partners",
            "problem": "The initial challenge",
            "product": "About us",
            "ranking": "Leaderboard",
            "rgesn": "Ecodesign statement",
            "share": "My results"
        }
    },
    "vote": {
        "bothEqual": "Both are equally good",
        "choices": {
            "altText": "{choice} for model {model}",
            "negative": {
                "incorrect": "Incorrect",
                "instructions_not_followed": "Instructions not followed",
                "question": "Why did you not like the answer",
                "superficial": "Superficial"
            },
            "other": "Other…",
            "positive": {
                "clear_formatting": "Clear formatting",
                "complete": "Complete",
                "creative": "Creative",
                "question": "What did you like about the answer?",
                "useful": "Useful"
            }
        },
        "comment": {
            "add": "Add comments",
            "placeholder": "You can add details about model {model}'s response"
        },
        "dislike": {
            "label": "I dislike",
            "selectedLabel": "I dislike (selected)"
        },
        "introA": "Before finding out the identity of the models, we need your vote.",
        "introB": "It allows us to improve the compar:IA datasets, the objective of which is to refine future AI models on less-resourced languages",
        "like": {
            "label": "I like",
            "selectedLabel": "I like (selected)"
        },
        "qualify": {
            "addDetails": "Add details",
            "placeholder": "The responses from the {model} model are...",
            "question": "How would you describe its answers?"
        },
        "title": "Which AI model do you prefer",
        "yours": "Your vote"
    },
    "welcome": {
        "errors": "AI can make mistakes: we encourage you to check the information provided",
        "go": "Here we go",
        "privacy": "Do not share personal information such as your name, surname or address.",
        "title": "Welcome to compar:IA!",
        "tos": {
            "desc": "The conversations and preferences you express on compar:IA are used anonymously to create datasets representative of European languages and usage, in order to reduce cultural bias and propose more inclusive future AI models.",
            "moreInfos": "Learn more about the project."
        },
        "use": "Do not use the comparator for illegal or harmful purposes"
    },
    "words": {
        "NA": "N / A",
        "activated": "Activated",
        "archived": "Archived",
        "back": "Back",
        "close": "Close",
        "deactivated": "Disabled",
        "loading": "Loading",
        "new": "New",
        "random": "Random",
        "regenerate": "Regenerate",
        "reset": "Reset",
        "restart": "Start again",
        "retry": "Start again",
        "search": "Search",
        "send": "Send",
        "tooltip": "Tooltip",
        "validate": "Validate"
    }
}
