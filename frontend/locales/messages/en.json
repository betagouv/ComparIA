{
    "arenaHome": {
        "title": "How can I help you today?",
        "modelSelection": "Model selection",
        "prompt": {
            "label": "Write your first message",
            "placeholder": "Write your first message here"
        },
        "selectModels": {
            "question": "Which models would you like to compare?",
            "help": "Choose the comparison mode"
        },
        "compareModels": {
            "question": "Which models would you like to compare?",
            "count": "{count}/2 models",
            "help": "If you only choose one, the second will be selected randomly"
        },
        "suggestions": {
            "title": "Suggested prompts",
            "generateAnother": "Generate another prompt",
            "choices": {
                "iasummit": {
                    "iconAlt": "AI Action Summit",
                    "title": "Prompts from a citizen consultation on AI",
                    "tooltip": "These questions are the outcomes of a citizen consultation on AI held from 09/16/2024 to 11/08/2024. It aimed to broadly involve citizens and civil society in the AI Action Summit, gathering their ideas on how to make AI an opportunity for all, while limiting misuse or abuse."
                },
                "ideas": {
                    "iconAlt": "Ideas",
                    "title": "Generate new ideas"
                },
                "explanations": {
                    "iconAlt": "Explanation",
                    "title": "Explain a concept"
                },
                "languages": {
                    "iconAlt": "Translation",
                    "title": "Write in another language"
                },
                "administrative": {
                    "iconAlt": "Administrative",
                    "title": "Write an administrative document"
                },
                "recipes": {
                    "iconAlt": "Cooking",
                    "title": "Show me new recipes"
                },
                "coach": {
                    "iconAlt": "Advice",
                    "title": "Give me advice on health and fitness"
                },
                "stories": {
                    "iconAlt": "Stories",
                    "title": "Tell me a story"
                },
                "recommendations": {
                    "iconAlt": "Recommendations",
                    "title": "Suggest films, books, music"
                }
            }
        }
    },
    "closeModal": "Close the popup",
    "models": {
        "licenses": {
            "type": {
                "proprietary": "Proprietary",
                "openSource": "Open source",
                "semiOpen": "Semi-open"
            },
            "name": "License {licence}",
            "commercial": "Commercial license",
            "noDesc": "Licensing information has not been filled for this model.",
            "descriptions": {
                "MIT": "The MIT License is a permissive free software license: it allows anyone to reuse, modify, and distribute the model, even for commercial purposes, provided they include the original license and copyright notices.",
                "Apache 2.0": "This license allows for free use, modification, and distribution, even for commercial purposes. In addition to freedom of use, it guarantees legal protection by including a non-infringement clause and transparency: all modifications must be documented and are therefore traceable.",
                "Gemma": "This license is designed to encourage the use, modification, and redistribution of the software but includes a clause stating that all modified or improved versions must be shared with the community under the same license, thus promoting collaboration and transparency in software development.",
                "Llama 3 Community": "This license allows the free use, modification, and distribution of the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users and prohibits the reuse of the code or generated content for training or improving competing models, thus protecting Meta's technology investments and brand.",
                "Llama 3.1": "This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users. Reuse of the code or generated content for training or improving derivative models is permitted provided that you display “built with llama” and include “Llama” in their name for any distribution.",
                "Llama 3.3": "This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users. Reuse of the code or generated content for training or improving derivative models is permitted provided that you display “built with llama” and include “Llama” in their name for any distribution.",
                "Llama 4": "This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users. Reuse of the code or generated content for training or improving derivative models is permitted provided that you display “built with llama” and include “Llama” in their name for any distribution.",
                "Jamba Open Model": "This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for organizations with over $50 million in annual revenue.",
                "CC-BY-NC-4.0": "This license allows you to share and adapt the content as long as you credit the author, but prohibits any commercial use. It provides flexibility for non-commercial uses while protecting the author's rights.",
                "propriétaire Gemini": "The model is available under a paid license and accessible via the Gemini API available on the Google AI Studio and Vertex AI platforms, requiring a pay-per-use fee based on the number of tokens processed or according to the company's terms.",
                "propriétaire Mistral": "The model is available under a paid license and accessible via the Mistral and other partner APIs, requiring a pay-per-use fee based on the number of tokens processed.",
                "propriétaire xAI": "The model is accessible via the xAI API, requiring pay-per-use based on the number of tokens processed or according to the company's terms.",
                "propriétaire Liquid": "The model is available under a paid license and accessible via API on Liquid AI's platforms, requiring a pay-per-use fee based on the number of tokens processed.",
                "propriétaire OpenAI": "The model is available under a paid license and accessible via API on OpenAI's platforms, requiring a pay-per-use fee based on the number of tokens processed or according to the company's terms.",
                "propriétaire Anthropic": "The model is available under a paid license and accessible via API on Anthropic's platforms, requiring a pay-per-use fee based on the number of tokens processed or according to the company's terms.",
                "Mistral AI Non-Production": "This license allows you to share and adapt the content as long as you credit the author, but prohibits any commercial use. It provides flexibility for non-commercial uses while protecting the author's rights."
            }
        },
        "release": "Released on {date}",
        "size": {
            "estimated": "Estimated size ({size})",
            "title": "Size",
            "descriptions": {
                "XS": "Very small models, with fewer than 7 billion parameters, are the least complex and most resource-efficient, providing sufficient performance for simple tasks such as text classification.",
                "S": "A small model is less complex and resource-intensive compared to larger models, while still providing sufficient performance for various tasks (summarization, translation, text classification, etc.)",
                "M": "Medium sized models offer a good balance between complexity, cost, and performance: they are much less resource-intensive than large models while still being able to handle complex tasks such as sentiment analysis or reasoning.",
                "L": "Large models require significant resources, but offer the best performance for advanced tasks like creative writing, dialogue modeling, and applications requiring a fine-grained understanding of context.",
                "XL": "These models, with hundreds of billions of parameters, are the most complex and advanced in terms of performance and accuracy. The computing and memory resources required to deploy these models are such that they are intended for the most advanced applications and highly specialized environments."
            }
        },
        "parameters": "{number} parameters",
        "names": {
            "a": "Model A",
            "b": "Model B"
        },
        "conditions": "Terms of Use",
        "openWeight": {
            "conditions": {
                "free": "Permissive",
                "copyleft": "Copyleft",
                "restricted": "Conditional"
            },
            "tooltips": {
                "openSource": "The training data, code, and weights of this model (i.e., the parameters learned during its training) are fully downloadable and modifiable by the public, allowing them to run and modify the model on their own hardware. Whether a model is \"open source\" is more restrictive than \"open weights,\" in particular because of the need for transparency of the training corpus, and few models are considered \"open source.\"",
                "openWeight": "A so-called \"open weights\" model whose weights, i.e. the parameters learned during training, are downloadable by the public, allowing them to run the model on their own hardware. Whether a model is \"open source\" is more restrictive (mainly in relation to the transparency of the training corpus), and few models are considered \"open source\".",
                "params": "Parameters or weights, counted in billions, are the variables learned by a model during training that determine its responses. The greater the number of parameters, the more learning capacity they have.",
                "free": "Once modified, the model may be redistributed under a different license than the source model.",
                "copyleft": "Once modified, the model must be redistributed under the same license as the source model.",
                "ram": "RAM (random access memory) stores data processed by an LLM in real time. The larger the model, the more RAM it needs to run."
            },
            "descriptions": {
                "XS": "With {paramsCount} billion parameters, this model is part of the very small model category (less than 7 billion parameters).",
                "S": "With {paramsCount} billion parameters, this model is part of the small model category (between 7 and 20 billion parameters).",
                "M": "With {paramsCount} billion parameters, this model is part of the medium-sized model category (between 20 and 70 billion parameters).",
                "L": "With {paramsCount} billion parameters, this model is part of the large model category (between 70 and 100 billion parameters).",
                "XL": "With {paramsCount} billion parameters, this model is part of the very large model category."
            },
            "use": {
                "commercial": "Commercial Use",
                "modification": "Modification authorized",
                "attribution": "Attribution required",
                "licenseType": "License type",
                "requiredRam": "RAM required"
            }
        },
        "ram": "{min} to {max} GB",
        "extra": {
            "title": "To learn more",
            "experts": {
                "open-weights": "To dive deeper, check out the <a {linkProps}>model page on Hugging Face</a>",
                "api-only": "To dive deeper, check out the <a {linkProps}>official model website</a>"
            },
            "impacts": "Environmental impact calculations are based on the <a {linkProps1}>EcoLogits</a> and <a {linkProps2}>Impact CO<sub>2</sub></a> projects."
        },
        "list": {
            "title": "Discover the models",
            "intro": "Explore the different conversational AI models available, their specifications, and licenses.",
            "filters": {
                "editor": {
                    "legend": "Publisher"
                },
                "size": {
                    "legend": "Size (in billions of parameters)",
                    "labels": {
                        "XS": "< 7 billion",
                        "S": "7 to 20 billion",
                        "M": "20 to 70 billion",
                        "L": "70 to 150 billion",
                        "XL": "> 150 billion"
                    }
                },
                "license": {
                    "legend": "License"
                },
                "display": "Show filters"
            },
            "triage": {
                "label": "Sort by",
                "options": {
                    "name-asc": "Model name (A to Z)",
                    "date-desc": "Release date (newest to oldest)",
                    "params-asc": "Size (smallest to largest)",
                    "org-asc": "Publisher (A to Z)"
                }
            },
            "model": "model",
            "models": "models",
            "noresults": "No models match your search criteria."
        }
    },
    "modes": {
        "random": {
            "title": "Random mode",
            "label": "Random",
            "altLabel": "Random model selection",
            "description": "Two models chosen randomly from the full list"
        },
        "custom": {
            "title": "Manual selection mode",
            "label": "Manual selection",
            "altLabel": "Manual model selection",
            "description": "Will you recognize the two models you chose?"
        },
        "small-models": {
            "title": "Frugal mode",
            "label": "Frugal",
            "altLabel": "Frugal model selection",
            "description": "Two small models chosen randomly"
        },
        "big-vs-small": {
            "title": "David vs Goliath mode",
            "label": "David vs Goliath",
            "altLabel": "David vs Goliath model selection",
            "description": "One small model against one big model, both chosen randomly"
        },
        "reasoning": {
            "title": "Reasoning mode",
            "label": "Reasoning",
            "altLabel": "Reasoning model selection",
            "description": "Two reasoning models chosen randomly"
        }
    },
    "vote": {
        "title": "Which AI model do you prefer",
        "bothEqual": "Both are equally good",
        "comment": {
            "add": "Add comments",
            "placeholder": "You can add details about model {model}'s response"
        },
        "choices": {
            "positive": {
                "question": "What did you like about the answer?",
                "useful": "Useful",
                "complete": "Complete",
                "creative": "Creative",
                "clear-formatting": "Clear formatting"
            },
            "negative": {
                "question": "Why did you not like the answer",
                "incorrect": "Incorrect",
                "superficial": "Superficial",
                "instructions-not-followed": "Instructions not followed"
            },
            "altText": "{choice} for model {model}"
        },
        "introA": "Before finding out the identity of the models, we need your vote.",
        "introB": "It allows us to improve the compar:IA datasets, the objective of which is to refine future AI models on less-resourced languages",
        "qualify": {
            "question": "How would you describe its answers?",
            "placeholder": "The responses from the {model} model are...",
            "addDetails": "Add details"
        },
        "like": {
            "label": "I like",
            "selectedLabel": "I like (selected)"
        },
        "dislike": {
            "label": "I dislike",
            "selectedLabel": "I dislike (selected)"
        },
        "yours": "Your vote"
    },
    "words": {
        "back": "Back",
        "close": "Close",
        "random": "Random",
        "regenerate": "Regenerate",
        "send": "Send",
        "validate": "Validate",
        "reset": "Reset",
        "restart": "Start again",
        "retry": "Start again",
        "tooltip": "Tooltip",
        "loading": "Loading",
        "NA": "N / A",
        "search": "Search"
    },
    "a11y": {
        "externalLink": "{text}"
    },
    "seo": {
        "title": "compar:IA, the AI chatbot arena",
        "desc": "compar:IA is a tool that enables the blind comparison of different conversational AI models, raising awareness of the issues surrounding generative AI (plurality, bias, environmental impact) and helping to build language preference datasets for less-resourced languages.",
        "titles": {
            "home": "Home",
            "product": "Product and partners",
            "modeles": "Model list",
            "datasets": "Datasets",
            "comparator": "The arena",
            "problem": "The initial challenge",
            "history": "Project history",
            "faq": "FAQ",
            "partners": "Partners",
            "news": "News",
            "mentions-legales": "Legal notice",
            "modalites": "Terms of use",
            "donnees-personnelles": "Privacy policy",
            "accessibilite": "Accessibility statement",
            "arene": "Chat",
            "share": "My results",
            "ranking": "Leaderboard"
        }
    },
    "header": {
        "subtitle": "The chatbot arena",
        "homeTitle": "Home - compar:IA",
        "logoAlt": "French Republic",
        "startDiscussion": "Start the discussion",
        "help": {
            "link": {
                "title": "Give feedback on the arena – opens a new window",
                "content": "Help us improve compar:IA"
            }
        },
        "banner": "The chatbot arena is now available in Lithuanian 🇱🇹, Swedish 🇸🇪, and Danish 🇩🇰!",
        "votes": {
            "count": "{count} votes",
            "objective": "Goal: {count}",
            "legend": "Legend",
            "tooltip": "Discuss, vote, and help us reach this goal!<br /><strong>Your votes matter</strong>: they feed the compar:IA dataset, which is freely available to help refine future models in less-resourced languages.<br />This digital commons contributes to better <strong>respect for linguistic and cultural diversity in future language models.</strong>"
        },
        "chatbot": {
            "step": "Step",
            "stepOne": {
                "title": "What do you think of the answers?",
                "description": "Pay attention to both content and form, then evaluate each response."
            },
            "stepTwo": {
                "title": "Models are revealed!",
                "description": "Discover the environmental impact of your conversations with each model"
            },
            "newDiscussion": "New chat"
        },
        "menu": "Menu",
        "title": {
            "compar": "compar",
            "ia": "AI"
        }
    },
    "footer": {
        "backHome": "Back to home - compar:IA",
        "helpUs": "Help us improve the product!",
        "writeUs": "If you encounter a problem or have feedback on the chatbot arena, feel free to write to us <a {linkProps}>using this form</a> - we read every message.<br />Thank you!",
        "links": {
            "legal": "Legal notice",
            "tos": "Terms of use",
            "privacy": "Privacy policy",
            "accessibility": "Accessibility: non-compliant",
            "sources": "Source code"
        },
        "license": {
            "mention": "Unless otherwise explicitly stated as third-party intellectual property, the contents of this site are offered under the <a {linkProps}>Etalab 2.0 license</a>",
            "linkTitle": "Etalab license - new window"
        }
    },
    "general": {
        "legal": {
            "title": "Legal notice",
            "editorTitle": "Published",
            "editorDesc": "This site is published by the French Ministry of Culture, 182 Rue Saint-Honoré, 75001 Paris",
            "directorTitle": "Director of the publication",
            "directorDesc": "Mr. Romain Delassus, Head of the Digital Department at the Ministry of Culture",
            "hostingTitle": "Hosting of the site",
            "hostingDesc": "This site is hosted by OVH SAS (<a {linkProps}>https://www.ovh.com</a>), whose registered office is located at 2 Rue Kellermann, 59100 Roubaix, France.",
            "a11yTitle": "Accessibility",
            "a11yDesc": "Compliance with digital accessibility standards is a future goal, but we strive to make this site accessible to everyone.",
            "reportTitle": "Report a problem",
            "reportA11y": "If you encounter an accessibility issue preventing you from accessing any content or functionality on the site, please let us know.",
            "reportDesc": "If you do not receive a prompt response from us, you have the right to submit your complaint or a request for referral to the Defender of Rights.",
            "reportA11yDesc": "To learn more about the State’s digital accessibility policy: <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "securityTitle": "Security",
            "securityCertif": "The site is protected by an electronic certificate, represented in most browsers by a padlock. This protection helps ensure the confidentiality of exchanges.",
            "securityNoMail": "Under no circumstances will the services associated with the platform be the source of emails asking for the input of personal information.",
            "sources": "Unless otherwise stated, all texts on this site are under the <a {etalabLinkProps}>Etalab Open 2.0 license</a>. The source code of this application is freely reusable and accessible on <a {githubLinkProps}>GitHub</a>."
        },
        "tos": {
            "title": "Terms of use",
            "scopeTitle": "1. Scope of application",
            "scopeDesc": "Access to the platform is free, does not require registration, and entails the application of specific conditions, listed in these terms of use.",
            "defsTitle": "2. Definitions",
            "defsUser": "“User” refers to any natural person consulting the platform and benefiting from its services.",
            "defsEditor": "“Publisher” refers to the Digital Department of the Ministry of Culture.",
            "defsPlatform": "“Platform” refers to the website that makes the services accessible.",
            "defsModels": "\"Models\" refers to the large language models (LLMs) reused under their usage license by the platform to fulfill its purposes.",
            "defsServices": "\"Services\" refers to the features offered by the platform to fulfill its purposes.",
            "descTitle": "3. Platform description",
            "descEditor": "Published by the Digital Department of the French Ministry of Culture, the arena is a platform for comparing conversational models aimed at the general public with the goal of (1) raising citizens' awareness of large language models (LLMs), and (2) collecting user preferences to create alignment datasets.",
            "descUse": "The user asks a question in a given language and receives answers from two anonymous large language models (LLMs). They vote for the model that provides their preferred response and are then shown the identities of the models. This participatory production system, inspired by the \"<a {linkProps}>chatbot arena</a>\" platform (LMarena), allows for the creation of datasets of human preferences for real-world tasks in French, which can be used for model alignment.",
            "descDatasets": "These datasets will be made accessible under an open license, particularly to encourage research uses.",
            "featuresTitle": "4. Features",
            "featuresDesc": "To meet the dual objective of raising citizen awareness about large language models and collecting user preferences, the platform provides the following services without access restrictions:",
            "featuresDescMore": "A human-machine interface that allows users to dialogue simultaneously with two conversational models and vote for the preferred response.",
            "featuresModels": "The models integrated into the platform are deployed on the inference servers of various partners (Scaleway, OVH, Hugging Face, Google Cloud, Mistral AI). The conditions for standardized inference are specified on the platform to ensure transparency in the use of the models.",
            "featuresModelsMore": "The model comparison interface.",
            "featuresVote": "At the end of the voting process, the user can view the list of models integrated into the chatbot arena and access a list of information about these models. The information documenting the models is sourced.",
            "featuresVoteMore": "Sharing and providing access to datasets resulting from the collection of user preferences.",
            "featuresDatasets": "The service collects user dialogue and preference data. The shared datasets will include the user's questions, responses from the two models, the vote, and the user's preferences.",
            "featuresDatasetsMore": "The publisher reserves the right to distribute the user's dialogue and preference data under an etalab 2.0 license. The dataset is disseminated on Data.gouv and the Hugging Face platform through the French Ministry of Culture's account (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "respTitle": "5. Responsabilities",
            "respUser": "The user is responsible for the data or content they enter in the prompt provided by the platform.",
            "respLegal": "The platform is not intended to be used for generating illegal content or content that is contrary to public order, and more generally, any generation that violates the current legal framework.",
            "respLegalMore": "In this regard, the user does not enter content or information in the prompt that is contrary to current legal and regulatory provisions.",
            "respPrivacy": "Since the data entered by the user on the platform is intended to be made available, they undertake not to transmit any information that could identify them or a third party.",
            "respPrivacyMore": "In any case, the publisher undertakes to implement means to ensure the anonymization of dialogue data before making it available.",
            "respEditor": "In general, the publisher disclaims any liability in the event of non-compliance with the terms of use.",
            "licenceTitle": "6. Code and licenses",
            "licenceCode": "The platform's source code is open and available here: <a {linkProps}>https://github.com/betagouv/ComparIA</a>",
            "licenceLLM": "The LLMs used to power the services are governed by the following licenses:",
            "licenceLLMModel": "Conversational AI model",
            "licenceLLMNoticeLink": "Link to the model licenses",
            "licenceLLMLicence": "License",
            "licenceLLMUnavailable": "Not available",
            "licenceLLMEvolution": "The list of language models integrated into the platform is subject to change over time and is updated with each modification.",
            "dispoTitle": "7. Service availability",
            "dispoDesc": "The platform is accessible, except in cases of force majeure or events beyond the control of its publisher.",
            "dispoRight": "The publisher reserves the right to suspend, interrupt, or limit, without prior notice, access to all or part of the services, particularly for maintenance and update operations necessary for the proper functioning of the service and related equipment, or for any other reason, including technical reasons.",
            "dispoWarranty": "It is not guaranteed that the service will be free of anomalies or errors. Therefore, the service is provided without any warranty regarding its availability and performance.",
            "dispoResp": "In this regard, the publisher cannot be held responsible for any losses or damages of any kind that may result from a malfunction or unavailability of the service. Such situations will not entitle any financial compensation.",
            "evoTitle": "8. Changes to the Terms of Use",
            "evoDesc": "The terms of use may be modified or supplemented at any time without prior notice, depending on changes made to the services, changes in legislation, or for any other reason deemed necessary.",
            "evoDescMore": "These modifications and updates are binding on the user, who should therefore regularly refer to this section to check the current general terms.",
            "contactTitle": "9. Contact",
            "contactDesc": "For any questions about the service, you can write to <a {linkProps}>contact@comparia.beta.gouv.fr</a>."
        },
        "privacy": {
            "title": "Privacy policy",
            "desc": "The service is published by the Digital Department of the French Ministry of Culture.",
            "cookiesTitle": "Cookies and Consent",
            "cookiesDesc": "This website places a small text file (a \"cookie\") on your computer when you visit it. This allows us to measure the number of visits and understand which pages are the most viewed.",
            "cookiesDescMore": "You can opt out of tracking your browsing on this website. This will protect your privacy, but it will also prevent the owner from learning from your actions and creating a better experience for you and other users.",
            "cookiesBannerTitle": "Why doesn't this site display a cookie consent banner?",
            "cookiesBannerDesc": "It's true, you didn't have to click on a block covering half of the page to say that you agree to the use of cookies -even if you don't know what that means!",
            "cookiesBannerNoNeed": "Nothing exceptional, no special treatment related to a .gouv.fr domain. We simply respect the law, which states that certain audience tracking tools, properly configured to respect privacy, are exempt from prior authorization.",
            "cookiesBannerTools": "We use <a {matomoLinkProps}>Matomo</a>, a <a {libreLinkProps}>free</a> tool, configured to comply with the CNIL's \"Cookies\" <a {cnilLinkProps}>recommendation</a>. This means that your IP address, for example, is anonymized before being recorded. It is therefore impossible to associate your visits to this site with your person.",
            "dataAccessTitle": "I contribute to enriching your data, can I access it?",
            "dataAccessDesc": "Of course! The site's usage statistics are freely accessible at <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessDatasets": "User dialogue and preference data are distributed under Etalab's Open License 2.0 on the Hugging Face platform as well as on Data.gouv.fr through the Ministry of Culture's account (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "privacyTitle": "Do we process personal data?",
            "privacyDesc": "The service does not process personal data as defined by the CNIL, meaning any information relating to an identifiable natural person, directly or indirectly.",
            "privacyData": "The data collected on the site are as follows:",
            "privacyDataArena": "Data related to user conversations with the models: questions asked by users, model responses, and user preferences expressed between the two models",
            "privacyDataForm": "Data related to the questionnaire \"Help us improve compar:IA\".",
            "privacyResp": "The user is responsible for the data or content they enter in the prompt provided by the platform. By accepting the <a {linkProps}>terms of use</a>, the user agrees not to transmit any information that could identify themselves or a third party.",
            "dataUseTitle": "What processing is done on the conversation data?",
            "dataUseDesc": "In any case, the publisher commits to implementing means to ensure the anonymization of dialogue data before making it publicly available.",
            "dataTimeTitle": "How long do we keep this data?",
            "dataTimeDesc": "Data relating to users and their conversations with models are retained from the time the preference vote is recorded.",
            "dataRespTitle": "Who is responsible for data processing?",
            "dataRespDesc": "The French Ministry of Culture's digital department is responsible for processing your personal data.",
            "dataExtraTitle": "Who helps us process the data?",
            "dataExtraHost": "Subcontractor: OVH",
            "dataExtraCountry": "Destination country: France",
            "dataExtraWhat": "Processing carried out: Accommodation",
            "dataExtraWarranty": "Guarantees: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>"
        },
        "a11y": {
            "disclaimer": "<strong> compar:IA </strong> is committed to making its digital services accessible, in accordance with Article 47 of Law No. 2005-102 of February 11, 2005.",
            "title": "Accessibility statement",
            "desc": "This accessibility statement applies to the website <strong> comparia.beta.gouv.fr </strong> .",
            "stateTitle": "Compliance Status",
            "stateDesc": "The comparia.beta.gouv.fr website is non-compliant with RGAA 4.1. The site has not yet been audited <strong>. However, it has been designed to be accessible to as many people as possible </strong> . You should therefore be able to:",
            "stateNavigate": "navigate all pages of the site using a keyboard",
            "stateScreenReader": "view the website with a screen reader.",
            "statePrefs": "adapt the site to your preferences (font size, screen zoom, change of typography, etc.) without loss of content",
            "improveTitle": "Improvement and contact",
            "improveDesc": "If you are unable to access any content or service, you can contact the manager of beta.gouv.fr to be directed to an accessible alternative or obtain the content in another format.",
            "improveMail": "E-mail: <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveAdress": "Address: DINUM, 20 avenue de Ségur 75007 Paris",
            "improveDelay": "We try to respond within 2 business days.",
            "remedyTitle": "Appeal",
            "remedyDesc": "This procedure is to be used in the following case: you have reported to the website manager an accessibility defect which prevents you from accessing content or one of the portal's services and you have not received a satisfactory response.",
            "remedyList": "You can :",
            "remedyAdvocate": "Write a message to the <a {linkProps}>Defender of Rights</a>",
            "remedyDelegateAdvocate": "Contact the <a {linkProps}>Defender of Rights representative in your region</a>",
            "remedyAdvocateAdress": "Send a letter by post (free, do not put a stamp): Defender of Rights - Free response 71120 75342 Paris CEDEX 07"
        }
    },
    "welcome": {
        "title": "Welcome to compar:IA!",
        "goodPractices": "Here are some best practices:",
        "errors": "AI can make mistakes: we encourage you to check the information provided",
        "privacy": "Do not share personal information such as your name, surname or address",
        "use": "Do not use the comparator for illegal or harmful purposes",
        "go": "Here we go"
    },
    "home": {
        "intro": {
            "title": "Don't trust the answers <span {props}>of a single AI</span>",
            "desc": "Have a blind discussion with two AIs and evaluate their answers",
            "tos": {
                "accept": "I accept the <a {linkProps}>terms of use</a>",
                "help": "Data is shared for research purposes",
                "error": "You must accept the terms of use to continue"
            },
            "steps": {
                "title": "How it works",
                "a11yDesc": "1. I chat with two hidden AIs: Chat for as long as you like. 2. I give my preference: By doing so, you'll help improve the AI models. 3. The model identities are revealed: Learn more about them and their characteristics.",
                "one": {
                    "title": "I chat with two hidden AIs",
                    "a": "Chat as long as",
                    "b": "as you like"
                },
                "two": {
                    "title": "I give my preference",
                    "a": "By doing so,",
                    "b": "you'll help improve the AI models"
                },
                "three": {
                    "title": "The model identities are revealed!",
                    "a": "Learn more about them and their characteristics",
                    "b": "of AI and their characteristics"
                }
            }
        },
        "use": {
            "title": "What is compar:AI for?",
            "desc": "compar:IA is a free tool that helps raise awareness among citizens about generative AI and its challenges.",
            "compare": {
                "title": "Compare the responses of different AI models",
                "desc": "Discuss and develop your critical thinking by giving your preference",
                "alt": "Compare"
            },
            "test": {
                "title": "Test the latest AI in the ecosystem in one place",
                "alt": "Test",
                "desc": "Test different models: open, proprietary, small, large..."
            },
            "measure": {
                "desc": "Discover the environmental impact of your conversations with each model",
                "title": "Measure the environmental footprint of questions asked to AI",
                "alt": "Measure"
            }
        },
        "europe": {
            "title": "The comparator <span {props}>becomes European!</span>",
            "desc": "Lithuania, Sweden, and Denmark are joining France in adopting the comparator to refine future AI models in their national languages.",
            "question": "Would you like to have the chatbot arena in your language?",
            "languages": {
                "da": "in Danish",
                "fr": "in French",
                "lt": "in Lithuanian",
                "sv": "in Swedish"
            }
        },
        "vote": {
            "title": "Why is your vote important?",
            "desc": "Your preferences enrich the compar:IA datasets, which aim to refine future AI models on French, Swedish, Lithuanian and Danish",
            "steps": {
                "prefs": {
                    "title": "Your preferences",
                    "desc": "After discussing with the AIs, you are invited to indicate your preference for a model on given criteria, such as the relevance or usefulness of the answers."
                },
                "datasets": {
                    "title": "Datasets by language",
                    "desc": "All questions and votes are compiled into datasets and published openly after anonymization."
                },
                "finetune": {
                    "title": "Models fine-tuned for specific languages",
                    "desc": "Companies and academia can use the datasets to train new models that are more respectful of linguistic and cultural diversity."
                }
            },
            "datasetAccess": "Access the datasets"
        },
        "usage": {
            "title": "Specifc use cases of compar:IA",
            "desc": "The tool is also aimed at AI experts and educators for more specific use cases",
            "use": {
                "title": "Reuse the data",
                "desc": "Developers, researchers, model publishers - access compar:IA’s datasets to enhance models for low-resource languages"
            },
            "explore": {
                "title": "Explore the models",
                "desc": "Find all model specifications and terms of use in one place"
            },
            "educate": {
                "title": "Train and raise awareness",
                "desc": "Use the chatbot arena as an educational tool to discuss AI with your audience"
            }
        },
        "origin": {
            "team": {
                "title": "Who are we?",
                "desc": "The chatbot arena is led within the French Ministry of Culture by a multidisciplinary team - AI experts, developers, deployment specialists, and designers - with a mission to make conversational AI more transparent and accessible to everyone."
            },
            "project": {
                "title": "Who initiated the project?",
                "desc": "The chatbot arena was designed and developed as part of a government startup led by the French Ministry of Culture, integrated into the <a {linkProps}>Beta.gouv.fr</a> program by the Interministerial Digital Directorate (DINUM). This initiative supports French public administrations in building useful, simple, and user-friendly digital services."
            }
        },
        "faq": {
            "title": "Your frequently asked questions",
            "discover": "See other questions"
        }
    },
    "product": {
        "title": "Everything you need to know about the chatbot arena",
        "comparator": {
            "title": "The arena enables the creation of <span {props}>preference datasets</span> focused on <span {props}>real-world usage</span> in <span {props}>European languages</span>.",
            "cta": "Access the arena",
            "challenges": {
                "title": "The platform addresses multiple challenges",
                "bias": {
                    "title": "Cultural and linguistic bias",
                    "desc": "Highlight AI biases stemming from the underrepresentation of non-English data in models and raise awareness of their real-world impact."
                },
                "impacts": {
                    "title": "Environmental impact",
                    "desc": "Show the environmental impact of generative AI, still widely unknown to the general public."
                },
                "pluralism": {
                    "title": "Model diversity",
                    "desc": "Ensure citizens have access to a diverse range of AI models, empowering them to make informed choices and cultivate a critical understanding of these technologies."
                },
                "thinking": {
                    "title": "Critical thinking and societal questions",
                    "desc": "Encourage critical thinking on the role of generative AI in personal and professional practices."
                }
            },
            "europe": {
                "title": "The arena <span {props}>goes European</span>!",
                "adventure": "As of summer 2025, Lithuania, Sweden, and Denmark are joining the initiative!",
                "desc": "The arena is now available to their citizens in national languages, with a core mission: build preference datasets to improve future AI model performance in low-resource languages.",
                "catch": "Would you like to have the chatbot arena in your language?"
            },
            "tabLabel": "The arena"
        },
        "problem": {
            "title": "Do conversational AI models respect the <span {props}>diversity</span> of European languages?",
            "diversity": {
                "stereotypes": {
                    "title": "Bias-reinforcing answers",
                    "desc": "Conversational AI systems seem fluent in every language - but their outputs can still be stereotypical or discriminatory."
                },
                "english": {
                    "title": "Training data overwhelmingly in English",
                    "desc": "Conversational AI relies on large language models (LLMs) trained primarily on English data, creating linguistic and cultural biases in their outputs."
                },
                "diversity": {
                    "title": "Overlooked cultural and linguistic diversity",
                    "desc": "These biases can lead to incomplete or outright incorrect responses, sidelining the diversity of European languages and cultures."
                }
            },
            "alignment": {
                "title": "How can we reduce cultural and linguistic biases in these models?",
                "desc": "Alignment: A bias-mitigation technique based on crowdsourcing user preferences to refine model behavior",
                "alignment": {
                    "title": "Alignment: a critical post-training phase",
                    "a": "Alignment comes after a language model’s pre-training phase, acting as a final \"refinement\" or \"polishing\" step. During pre-training, the model learns to predict the next word, gaining the ability to generate coherent text - but alignment is what tailors it to human preferences.",
                    "b": "The alignment phase trains the model to better meet human needs by making it <strong>more relevant</strong> (answering questions more accurately), <strong>more honest</strong> (admitting when it lacks sufficient data), and <strong>safer</strong> (avoiding harmful or inappropriate content).",
                    "c": "<strong>Without alignment, an LLM might be technically capable yet impractical to use - failing to grasp what users truly expect in a conversation.</strong>"
                },
                "datasets": {
                    "title": "Specific datasets",
                    "a": "Alignment relies on highly specialized datasets, meticulously designed to teach the model \"proper\" behavior.",
                    "b": "<strong>Preference data</strong> is a critical component of alignment, working alongside <strong>demonstration data</strong> (expert-crafted human-AI dialogues with precise tone/style guidelines), <strong>safety data</strong> (curated examples teaching models to reject harmful requests), and <strong>domain-specific datasets</strong> (tailored for fields like medicine, law, or education).",
                    "c": "Preference data presents multiple potential answers to the same question, ranked by human evaluators based on criteria like relevance, usefulness, or harm potential. Users indicate which response performs best, and these curated datasets are then used to fine-tune models - aligning them with expressed human preferences."
                },
                "english": {
                    "title": "European languages suffer from a shortage of preference data",
                    "a": "Preference data is expensive to produce because <strong>each example requires skilled human evaluation</strong>. Platforms like chat.lmsys.org help crowdsource these datasets—but few users contribute in their native language, leaving low-resource languages underrepresented.",
                    "b": "Preference datasets for European languages are scarce - or nonexistent. In LMSYS’s dataset, for instance, French queries represent less than 1% of the total.",
                    "c": "compar:IA is a chatbot arena designed to gather multilingual conversations - capturing region-specific cultural references like daily tasks, local culinary traditions, education systems, or historical and literary touchstones."
                },
                "diversity": {
                    "title": "Diversify data sources to reduce bias",
                    "a": "To reflect the diversity of cultures and languages in model outputs, <strong>alignment datasets must incorporate a broad range of languages</strong>, contexts, and real-world user tasks. Diversifying alignment data ultimately improves a model’s performance in two key ways:",
                    "b": "First, it <strong>reduces cultural bias</strong> by preventing a single - often Anglophone -perspective from dominating the AI’s responses. The model learns that valid answers vary by cultural context, recognizing multiple legitimate ways to address the same question.",
                    "c": "Second, exposure to linguistic and cultural diversity enables context-aware responses: a French user gets advice tailored to France’s systems, while a Danish user receives information aligned with their national context.",
                    "d": "The result? A more inclusive conversational AI - one that acknowledges and adapts to diverse cultural perspectives."
                }
            },
            "tabLabel": "The initial problem"
        },
        "partners": {
            "institution": {
                "title": "Institutional partners"
            },
            "diffusion": {
                "title": "Communication partners",
                "desc": "We’re building a network of partners who integrate the chatbot arena into their services and training offerings.",
                "catch": "Would you like to use the chatbot arena in a professional context?",
                "cta": "Let us know"
            },
            "academy": {
                "title": "Academic partners",
                "desc": "We’re committed to ensuring the datasets we generate fuel multidisciplinary research, bridging humanities, social sciences, and data science.",
                "catch": "Working on a research project? Have suggestions or need clarification on our methodology or datasets?"
            },
            "services": {
                "title": "Services used",
                "desc": "Environmental impact calculations are based on the tools above."
            },
            "tabLabel": "Partners"
        },
        "faq": {
            "tabLabel": "FAQ"
        },
        "history": {
            "tabLabel": "Project history"
        }
    },
    "datasets": {
        "access": {
            "title": "Access compar:IA datasets",
            "desc": "The platform’s questions and preferences are primarily in French, Danish, Swedish and Lithuanian, capturing organic, real-world usage - not artificial prompts. These datasets are publicly available on <a {linkProps}>data.gouv.fr</a> and Hugging Face.",
            "catch": "Model publishers, researchers, companies, now it's your turn!",
            "share": "Show us how you’re using the data",
            "repos": {
                "conversations": {
                    "title": "/conversations",
                    "desc": "All the questions and answers"
                },
                "reactions": {
                    "title": "/reactions",
                    "desc": "All the reactions to messages"
                },
                "votes": {
                    "title": "/votes",
                    "desc": "All the preferences expressed"
                }
            }
        },
        "reuse": {
            "title": "How is this data used?",
            "desc": "Examples of compar:IA dataset reuses",
            "bunka": {
                "desc": "The Bunka.ai team conducted a large-scale study of user-AI interactions on the chatbot arena, mapping out dominant themes, key tasks, and the balance between automation vs. human augmentation. Their analysis based on 25,000 real conversations offers rare empirical insight into how people actually use AI.",
                "conversations": {
                    "title": "Explore the data visualization",
                    "desc": "Interactive visualization of conversations where each cluster represents a recurring theme discussed by users (such as education, health, the environment, or even philosophy)."
                },
                "analyze": {
                    "title": "Access the analysis",
                    "desc": "Analysis of user conversations with detection of tasks (creation, information search, etc.), topics (arts and culture, education, etc.), complex emotions (curiosity, enthusiasm, etc.), language tones (formal, professional, etc.)"
                },
                "method": "Learn more about the methodology"
            }
        }
    },
    "chatbot": {
        "continuePrompt": "Continue the chat with the AI models",
        "revealButton": "Reveal the models",
        "conversation": "Chat",
        "errors": {
            "tooLong": {
                "title": "Oops, the conversation is too long for one of the models.",
                "message": "Each model is limited in the size of conversations it can handle.",
                "vote": "You can still give your preference on these models or start a conversation with two new ones.",
                "retry": "You can restart a chat with two new models."
            },
            "other": {
                "title": "Oops, temporary error",
                "message": "A temporary error has occurred.",
                "vote": "Or finish the experience by giving your preference on these models.",
                "retry": "You can retry to query the models again."
            }
        },
        "loading": "Loading answers"
    },
    "reveal": {
        "impacts": {
            "title": "Energy consumption of the chat",
            "size": {
                "label": "model size",
                "count": "billion param.",
                "estimated": "(est.)",
                "quantized": "(quantized)"
            },
            "tokens": {
                "label": "text size",
                "tooltip": "AI analyzes and generates sentences from words or parts of words of approximately four letters; this unit of text is called a token. The longer a text, the greater the number of tokens.",
                "tokens": "tokens"
            },
            "energy": {
                "label": "energy consumed",
                "tooltip": "Measured in watt-hours, energy consumption represents the electricity used by the model to process a query and generate the corresponding response. Generally, the larger a model (in billions of parameters), the more energy is required to produce a token."
            }
        },
        "equivalent": {
            "title": "Which corresponds to:",
            "co2": {
                "label": "CO <sub> 2 </sub> emitted",
                "tooltip": "The CO <sub> 2 </sub> emitted is equivalent to the carbon dioxide emissions produced by the energy used to run the model. It reflects the environmental impact linked to energy consumption. The Watt-hour/CO <sub> 2 </sub> equivalence calculation differs depending on the energy mix of each country. However, the servers used for model inference are not all located in Europe. Thus, the equivalence calculation is based on the global average CO <sub> 2 </sub> emission rate per energy consumed."
            },
            "lightbulb": {
                "label": "LED bulb",
                "tooltip": "Data calculated based on the consumption of a standard 5W LED bulb (E14)"
            },
            "streaming": {
                "label": "online videos",
                "tooltip": "Data calculated based on the carbon impact of one hour of streamed video in high definition, on a television, with a Wi-Fi connection (source <a {linkProps}>ADEME</a>)"
            }
        },
        "feedback": {
            "shareResult": "Share your result",
            "moreOnVotes": "Learn more about votes",
            "description": "Share compar:AI with others by sharing the AI models you've interacted with! Only the names and energy impact of the discussion will be visible via this link, with no access to the messages in the conversation.",
            "example": "Example of shared results"
        }
    },
    "errors": {
        "unknown": "An error has occurred",
        "404": {
            "title": "Page not found",
            "error": "Error 404",
            "sorry": "The page you are looking for cannot be found. We apologize for the inconvenience.",
            "desc": "If you typed the URL into your browser, check if it's correct. The page may no longer be available. <br />You can continue by visiting our homepage. <br /> If you struggle to find a page you are looking for, contact us so we can redirect you to the correct URL."
        },
        "unexpected": {
            "title": "Unexpected error",
            "error": "Error {code}",
            "sorry": "Our apologies, there is an issue with the service, we are working to resolve it as quickly as possible.",
            "desc": "Please try refreshing the page or try again later."
        }
    },
    "actions": {
        "copyMessage": {
            "do": "Copy the message",
            "done": "Message copied"
        },
        "copyLink": {
            "do": "Copy the link",
            "done": "Link copied to clipboard"
        },
        "contact": "Contact us",
        "contactUs": "Contact us",
        "home": "Homepage",
        "returnHome": "Return to homepage",
        "seeMore": "See more",
        "selectLanguage": "Select a language"
    },
    "generated": {
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "license_desc": "<p> This license allows you to freely use, modify, and distribute the model, including for commercial purposes. In addition to freedom of use, it guarantees legal protection by including a patent grant clause that acts as insurance: if you use this model, the contributors agree not to sue you for violating their patents related to the project. This mutual protection avoids legal conflicts between users and developers. When distributing modified versions, significant changes must be indicated with appropriate notices, ensuring transparency for the user. </p>"
                },
                "CC-BY-NC-4.0": {
                    "license_desc": "<p> This license allows you to freely share and adapt the content as long as you credit the author, but prohibits any commercial use. It provides flexibility for non-commercial uses while protecting the author's rights. </p>",
                    "reuse_specificities": "but only for non-commercial uses"
                },
                "Gemma": {
                    "license_desc": "<p> This license is designed to encourage the use, modification, and redistribution of the software, but includes a clause stating that all modified or improved versions must be shared with the community under the same source license, thus promoting collaboration and transparency in software development. </p>"
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "below 700 million users",
                    "license_desc": "<p> This license allows you to freely use, reproduce, modify, and distribute the code with attribution, but imposes restrictions for operations exceeding 700 million monthly users. Reuse of the code or generated content for training or improving derivative models is permitted provided that you display “built with llama” and include “Llama” in their name for any distribution. </p>"
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "below 700 million users",
                    "license_desc": "<p> This <strong> non-exclusive, worldwide, royalty-free </strong> license allows you to freely use, reproduce, modify, and distribute the Llama 3.3 code and Materials with attribution. It notably permits reuse for improving derivative models, but imposes restrictions on very large-scale commercial operations. </p>"
                },
                "Llama 4": {
                    "commercial_use_specificities": "below 700 million users\n",
                    "license_desc": "<p> This non-exclusive, worldwide, royalty-free license allows you to use, reproduce, modify, and distribute the Llama 4 Materials (models and documentation) with attribution. However, it imposes two major restrictions: (1) companies exceeding 700 million monthly active users must obtain a special license from Meta, and (2) <strong> total exclusion </strong> of EU residents and companies headquartered in the EU from directly using the multimodal models, due to regulatory uncertainties related to the European AI Act. European end users may nevertheless access services integrating Llama 4, provided they are provided from outside the EU. </p>"
                },
                "Mistral AI Research License": {
                    "license_desc": "<p> This non-exclusive, royalty-free license authorizes the use, copying, modification, and distribution of Mistral models and their derivatives (including modified or refined versions). However, it is strictly limited to research purposes. </p>",
                    "reuse_specificities": "but only for non-commercial uses"
                },
                "MIT": {
                    "license_desc": "<p> The MIT License is a permissive free software license: it allows anyone to reuse, modify, and distribute the model, even for commercial purposes, provided they include the original license and copyright notices. </p>"
                }
            },
            "proprio": {
                "Alibaba": {
                    "license_desc": "The model is available under paid license and accessible via API on Alibaba company platforms, requiring pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "Amazon": {
                    "license_desc": "The model is available under paid licensing and accessed through Amazon Bedrock, requiring to pay-as-you-go based on the number of tokens processed or infrastructure reserved.",
                    "reuse_specificities": "except to distill or train other models on Amazon's platforms."
                },
                "Anthropic": {
                    "license_desc": "The model is available under paid license and accessible via API on the Anthropic platform or partner platforms, requiring to pay-per-use based on the number of tokens processed or based on the infrastructure reserved to host the model."
                },
                "Google": {
                    "license_desc": "The model is available under paid license and accessible via API on Google platforms, requiring to pay-per-use based on the number of tokens processed or on infrastructure reserved.",
                    "reuse_specificities": "except for training other models on Vertex AI"
                },
                "Mistral AI": {
                    "license_desc": "The model is available under a paid license and accessible via the Mistral API, Amazon Sagemaker, and several other infrastructure providers, requiring to pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "OpenAI": {
                    "license_desc": "The model is available under a paid license and accessible via API on OpenAI's platforms or through Microsoft Azure services, requiring to pay-per-use based on the number of tokens processed or the infrastructure reserved."
                },
                "xAI": {
                    "license_desc": "The model is available under a paid license and accessible via X and xAI, requiring to pay-per-use based on the number of tokens processed or infrastructure reserved."
                }
            }
        },
        "models": {
            "DeepSeek R1": {
                "desc": "<p> Very large model with high performance on mathematical, scientific and programming tasks, which simulates a reasoning step before generating its answer. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture with 61 layers. It has a total of 671 billion parameters, 37 billion of which are token-activated. Training used large-scale reinforcement learning, with multiple SFT (supervised fine-tuning) steps where the model learns from examples of correct answers. </p>",
                "size_desc": "<p> With 671 billion parameters, DeepSeek R1 is a very large model that requires multiple powerful graphics cards to run. Reasoning models of this type take longer to produce an answer, which increases energy consumption. However, the Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token, thus limiting its energy footprint. The context window reaches 128,000 tokens, which is suitable for analyzing long documents. </p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p> Large model based on Meta Llama 3.3 70B, retrained with reasoning examples from the DeepSeek R1 model. It offers good math and coding capabilities. </p>",
                "fyi": "<p> The model was not trained from scratch. It relies on Llama 3.3 70B, retrained using results generated by DeepSeek R1. This process gave Llama 3.3 70B the ability to simulate reasoning, without the user being able to choose whether or not to enable this feature. </p>\n <p> In accordance with the obligations of the Llama 3.3 license, the company must retain the mention of the source model in the name of the model, subject to the same licensing regime. </p>",
                "size_desc": "<p> With 70 billion parameters, this model is considered large. It requires multiple powerful graphics cards to run, resulting in high inference costs. Reasoning models also take longer to produce an answer, increasing their energy consumption. </p>\n <p> The context window is 16,000 tokens, which can be limiting for analyzing large documents. </p>"
            },
            "DeepSeek V3": {
                "desc": "<p> Very large model designed for complex tasks: code generation, tool usage, long document analysis. It can handle many languages, but is particularly well-suited to English and Chinese. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture, with 671 billion parameters but only activating 37 billion per generated token. It is well suited for tool calling, generating structured output (JSON), and code generation. </p>",
                "size_desc": "<p> DeepSeek V3 is a very large model, requiring multiple graphics cards to run. The Mixture of Experts (MoE) architecture, however, allows only a portion of the parameters to be used for generating the next token, reducing the footprint compared to a dense model of the same size. </p>\n <p> The context window reaches 163,000 tokens, which is useful for analyzing long documents. </p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p> Large multimodal and multilingual model with two response modalities: the user can choose between a reasoning mode, for more in-depth answers, or a fast mode, to generate the final answer directly. </p>",
                "fyi": "<p> This model is based on a Mixture of Experts (MoE) architecture and has been distilled by keeping only an approximation of the predictions of the teacher model - Gemini 2.5 Pro. It has been trained on a TPUv5p architecture incorporating advances such as the ability to continue training automatically even in the event of training errors, data corruption, or memory issues. </p>\n <p> Gemini 2.5 Flash supports contexts of up to 1 million tokens and three hours of video content. Optimized vision processing allows for approximately three times longer video to be processed in the same context window: only 66 visual tokens are needed to generate a frame, compared to 258 previously. This model also enables native audio generation for dialogue and speech synthesis. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring multiple powerful graphics cards to run. However, the Mixture of Experts (MoE) architecture only activates a portion of the parameters to predict each new token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. Its context window extends up to 1 million tokens, which allows it to process very large document corpora. </p>"
            },
            "Gemma 3 12B": {
                "desc": "<p> Small multimodal model suitable for common tasks such as question-and-answer, summarization, or image interpretation. </p>",
                "fyi": "<p> It processes text and images and can run locally on powerful laptops or servers with a single graphics card. It has been trained to be able to interact with external tools (web search, etc.) via function calls, which makes it useful in agentic use cases. </p>",
                "size_desc": "<p> With 12 billion parameters, it is one of the smallest models. It can be used locally on a personal computer to preserve data confidentiality, or on an inexpensive server to limit costs compared to a larger model. </p>\n <p> Its context window can hold up to 128,000 tokens, making it easy to process long documents. </p>"
            },
            "Gemma 3 27B": {
                "desc": "<p> Medium-sized, multimodal model suitable for common tasks such as question-and-answering, summarization, or image interpretation. </p>",
                "fyi": "<p> It can process text and images on a server equipped with a single powerful graphics card. It has been trained to be able to interact with external tools (internet search, etc.) via function calls, which makes it useful in agentic use cases. </p>",
                "size_desc": "<p> With 27 billion parameters, it is a medium-sized model. It can be deployed on a server with a single graphics card (GPU). </p>\n <p> It accepts contexts of up to 128,000 tokens, making it suitable for parsing long documents. </p>"
            },
            "Gemma 3 4B": {
                "desc": "<p> Very small, compact, multimodal model suitable for common tasks such as question-and-answer, summarization, or image interpretation. </p>",
                "fyi": "<p> It can process text and images while running on less powerful computers, including smartphones and tablets. It has been trained to interact with external tools (web search, etc.) via function calls, making it usable in agentic use cases. </p>",
                "size_desc": "<p> With 4 billion parameters, it is considered a very small model. It can be used locally to preserve data confidentiality, or on a server to limit costs compared to a larger model. </p>\n <p> Its context window can reach 128,000 tokens, allowing it to analyze long documents. </p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p> Very small, compact, multimodal model designed to run locally on a computer or smartphone, without the need for a server - it is capable of adapting its ressource consumption according to the capacity of the device it is running on. </p>",
                "fyi": "<p> This model can process text, images, and audio. It is based on the MatFormer architecture and a PLE (per-layer embeddings) cache system, which activates only the useful parameters depending on the task, adapting to the capacity of the machines on which the model runs. </p>",
                "size_desc": "<p> With 4 billion parameters, it is one of the smallest models available. It can be used locally on a computer or smartphone to maintain data confidentiality, or on a server to limit costs compared to a larger model. </p>\n <p> Its context window goes up to 32,000 tokens. </p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p> Smaller, lightweight version of the GPT 4.1 model, designed to keep costs down while remaining competitive on most tasks. The model supports very long queries, making it suitable for use in long document corpus analysis. </p>",
                "fyi": "<p> This is a distilled version of a larger model, with partial knowledge transfer. It can process text, images, and audio. Its context window can reach up to 1 million tokens, making it particularly suitable for analyzing text corpora or very long code repositories. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a medium-sized model, requiring a powerful graphics card to run. However, the supposed Mixture of Experts (MoE) architecture activates only a subset of parameters at each token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "GPT 5": {
                "desc": "<p> GPT-5 is not a single model, but a unified system composed of two separate models: a fast model ( <code> gpt-5-main </code> ) for common queries and a reasoning model ( <code> gpt-5-thinking </code> ) for complex problems. Compared to its predecessors, OpenAI claims it is more useful in real-world queries, with notable improvements in the areas of writing, coding, and health. Its creators also claim it produces less hallucinations. Thanks to its context window of 400,000 tokens, it can accept long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> Developers using this model can configure a verbosity parameter to adjust the length of the reasoning phase. </p>\n <p> In terms of security, the system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than at request time. The model's creators also used the \"reasoning\" training phase to make it more \"resistant\" to attempts to circumvent their security rules ( <em> jailbreaking </em> ). </p>",
                "size_desc": "<p> The GPT-5 system is composed of models of various sizes, but the exact sizes are unknown. Its architecture is designed to include multiple models, orchestrated by an internal routing system, which selects the smallest model suited to the task to optimize the speed and depth of reasoning. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a portion of the parameters are activated for each query. This allows for greater energy efficiency and high performance. Available estimates of model sizes are based on public information and indirect indices such as inference costs and response latency. </p>"
            },
            "GPT 5 Mini": {
                "desc": "<p> GPT-5 Mini is a lightweight version of the main GPT-5 model. It is designed for use in environments where cost constraints are needed, such as large scale. Its reasoning model performs almost as well as the main model ( <code> gpt-5-thinking </code> ) despite its smaller size. Thanks to its 400,000 token context window, it can handle long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> The system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than request time. </p>\n <p> Although it is a smaller version, it is very competitive against the leading GPT-5 model on many benchmarks, especially in the medical field. </p>",
                "size_desc": "<p> The Mini model is a more compact (medium-sized according to estimations) version of the GPT-5 system. It is designed to perform optimally for a good balance between performance and cost, thanks to a routing system that selects it for specific tasks. The architecture is likely based on a \"mixture of experts\" (MoE), meaning that only a portion of the parameters are activated for each query. However, the models are likely very large, requiring multiple powerful graphics cards for inference. </p>"
            },
            "GPT 5 Nano": {
                "desc": "<p> GPT-5 Nano is the smallest and fastest version of the GPT-5 reasoning model. It is designed for contexts where ultra-low latency or cost is required. Thanks to its 400,000 token context window, it can accept long queries, making it possible to analyze multiple documents at once. </p>",
                "fyi": "<p> The system uses a new security approach called \"safe-completions\" to prevent unauthorized content at response time rather than request time. </p>",
                "size_desc": "<p> The Nano model is the most compact of the GPT-5 family (estimated to be small). It is selected by the routing system for queries requiring ultra-low latency and instant responses. Its architecture is likely based on a \"Mixture of Experts\" (MoE), which allows for better energy efficiency and high performance, even on queries requiring a fast response. </p>"
            },
            "GPT OSS-120B": {
                "desc": "<p> The larger of OpenAI's first two semi-open models since GPT-2. Designed in response to the rise of open source players like Meta (LLaMA) and Mistral, it is a powerful reasoning model, particularly on complex tasks and in \"agentic\" environments. </p>",
                "fyi": "<p> This model can run on a single 80GB GPU (like the NVIDIA H100). It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>\n <p> In the model configurations, it is possible to choose between three levels of reasoning ( <em> low </em> , <em> medium </em> , and <em> high </em> ) which determine the verbosity of the model. </p>",
                "size_desc": "<p> The architecture is a \"mixture of experts\" (MoE), which allows for greater energy efficiency by activating only a portion of the parameters (5.1 billion per token) for each token prediction. It is a reasoning model, thus its energy consumption is higher because it generates an internal chain of thought before providing the final answer. It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>"
            },
            "GPT OSS-20B": {
                "desc": "<p> The smaller of OpenAI's two semi-open models. It was designed in response to open source competition and is intended for use cases requiring low latency as well as local or specialized deployments. </p>",
                "fyi": "<p> This model can be run locally on a high-end laptop with only 16GB of VRAM (or system RAM). This makes it a very accessible option for developers. </p>\n <p> In the model configurations, it is possible to choose between three levels of reasoning ( <em> low </em> , <em> medium </em> , and <em> high </em> ) which determine the verbosity of the model at the reasoning step. </p>",
                "size_desc": "<p> With 20 billion parameters, this model belongs to the medium-sized model category. The architecture is based on a \"mixture of experts\" (MoE), which allows for greater energy efficiency by activating only a portion of the parameters (3.6 billion per token) for each token generation. It is a reasoning model, which results in higher energy consumption because it generates an internal chain of thought before providing the final answer. It has a context window of 131,000 tokens, making it ideal for analyzing large documents. </p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p> A lightweight version of GPT 4.1, but still large in size, designed to keep costs down while remaining competitive on most tasks. The model supports very long queries, making it suitable for use in document corpus analysis. </p>",
                "fyi": "<p> This is a distilled version of a larger model, with partial knowledge transfer. It can process text, images, and audio. Its context window can reach up to 1 million tokens, making it particularly suitable for analyzing very long corpora or code repositories. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Indicators suggest that it is a large model, requiring a powerful graphics card to run. However, the supposed Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token generation, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "Grok 3 Mini": {
                "desc": "<p> A lighter version of the Grok 3 model, reducing costs while maintaining good performance for many tasks. It can go through a reasoning phase before providing a final answer. </p>",
                "fyi": "<p> Grok 3 Mini is a distilled version of Grok 3: it is close to it in terms of capabilities, while being faster and less expensive.\n The model offers two modes: a thinking mode with step-by-step reasoning for complex problems, and a quick mode for immediate answers.\n Its context window reaches 131,000 tokens, making it suitable for analyzing long documents. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Despite its name, Grok 3 Mini is likely a very large model, requiring multiple powerful graphics cards to run. Additionally, it contains an optional reasoning phase that involves longer generation times and therefore higher power consumption. However, the supposed Mixture of Experts (MoE) architecture only activates a portion of the parameters at each token, thus limiting its energy footprint. Available estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "Hermes 3 405B": {
                "desc": "<p> Very large model retrained from the Llama 3.1 405B, adjusted to better meet user prompts and improve it's capacity to use tools. </p>",
                "fyi": "<p> This model is the result of retraining the Llama 3.1 405B parameter set to make its behavior less restricted and better account for the nuances of user and system prompts - thus giving the user greater control over the model's “personality” and behavior. Specific reasoning functions such as <strong> <code> &lt; SCRATCHPAD &gt; </code> </strong> , <strong> <code> &lt; REASONING &gt; </code> </strong> , <strong> <code> &lt; THINKING &gt; </code> </strong> have been added to simulate reasoning about complex tasks. The training used a tool called AdamW (learning rate of 3.5×10⁻⁶), which helps the model learn efficiently by gradually adjusting its parameters. Then, it was fine-tuned with a method called DPO (direct preference optimization), which improves its responses based on specific preferences. To make this training lighter and faster, LoRA adapters were used; these are smaller modules that modify only a part of the model, avoiding the need to rework all the parameters at once. </p>",
                "size_desc": "<p> With 405 billion parameters, this model is considered to be very large. It requires a server equipped with several powerful graphics cards, which results in significant operating costs. </p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p> Very large model designed for complex or specialized tasks. Often used as a “teacher model” for training more specialized models. </p>",
                "fyi": "<p> The model was trained on a corpus of 15 trillion tokens with 16,000 H100 graphics cards (one of the most powerful graphics cards on the market in 2025). Training combined synthetic data generation and direct preference optimization (DPO). This model is itself often used to generate synthetic data to train smaller models. The model uses 8-bit compression by default to reduce memory requirements and allow execution on a single, high-powered server. </p>",
                "size_desc": "<p> With 405 billion parameters, this model is one of the very large models. It requires a server equipped with several powerful graphics cards, which results in significant running costs. The model has a context window of up to 128,000 tokens, making it suitable for long document analysis tasks. </p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p> Small model designed for local use on a laptop, while offering good capabilities for text synthesis and straight-forward questions and answers. </p>",
                "fyi": "<p> This model is a distilled version of the larger Llama 3 models: it was trained by transferring some of the knowledge from the larger models. </p>",
                "size_desc": "<p> With 8 billion parameters, this model is small. It can be run locally on a powerful computer, ensuring data confidentiality, or hosted on a server equipped with a single graphics card, which limits infrastructure costs. Its context window of 128,000 tokens allows it to process long documents. </p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p> Large model designed for a wide range of tasks and able to compete with larger models. </p>",
                "fyi": "<p> This model is a distilled version of the 405B model, to which it owes some of its transferred knowledge. It also benefited from recent alignment and reinforcement learning techniques with online environments (online reinforcement learning) - the model learned by trying to perform online tasks autonomously. Its training was done on 15 billion tokens of data. </p>",
                "size_desc": "<p> With 70 billion parameters, this model is large. It requires multiple powerful graphics cards to run, which results in significant operating costs. Its context window of 128,000 tokens allows it to process long documents. </p>"
            },
            "Llama 4 Scout": {
                "desc": "<p> Large model with a very large context window, useful, for example, for summarizing a set of documents. </p>",
                "fyi": "<p> This model was co-distilled with Behemoth, meaning it learned alongside the giant model, not afterward as in a traditional distillation. It was trained on 30 trillion tokens of data, combining text in 200 languages and images to achieve native multimodal capabilities. The architecture is based on a Mixture of Experts (MoE), with 17 billion active parameters, 16 experts, and 109 billion total parameters. To balance multimodal performance, reasoning, and conversational quality, the Meta team developed a progressive post-training strategy, combining adaptive data filtering (to keep only the most complex and interesting data), targeted fine-tuning, and online reinforcement learning - the model learned by trying to perform online tasks autonomously. Thanks to the iRoPE architecture (optimized version of positional encoding), it can handle very long context windows, up to 10 million tokens and can process up to 8 images simultaneously. </p>\n <p> The model was well received upon launch, notably for its impressive context window, a first in the field, as well as its cost-effectiveness on tasks such as summarization, tool invocation, and augmented generation (RAG). This makes it a suitable choice for automated pipelines. </p>",
                "size_desc": "<p> With 109 billion parameters, this model falls into the large model category. However, thanks to a Mixture of Experts (MoE) architecture, it can be hosted on a server with a single, high-performance graphics card. Its context window can hold up to 10 million tokens, making it useful for processing extremely long document corpora. </p>"
            },
            "Llama Maverick": {
                "desc": "<p> Very large model with a very large context window. Useful, for example, for summarizing several documents at once. </p>",
                "fyi": "<p> This model was co-distilled with Behemoth, meaning it learned alongside the giant model, rather than afterward, as in traditional distillation. This allows for faster and less computational transfer of skills. It was trained on 30 trillion tokens of data, combining text in 200 languages and images to achieve native multimodal capabilities - it can process up to 8 images simultaneously. The architecture is based on a Mixture of Experts (MoE) system, with 17 billion active parameters, 16 experts, and 109 billion total parameters. The Meta team developed a progressive post-training strategy, combining adaptive data filtering - keeping only the most complex and interesting data-targeted fine-tuning, and online reinforcement learning to balance multimodal performance, reasoning, and conversational quality. Thanks to the iRoPE architecture (optimized version of positional encoding), it can handle very long context windows, up to 10 million tokens. </p>\n <p> The Llama 4 Maverick model was presented as Meta's direct response to the DeepSeek models. However, upon its release, many users felt that it did not live up to expectations, especially for programming tasks and creative work. </p>",
                "size_desc": "<p> With 400 billion parameters, this model is considered to be very large. However, thanks to a Mixture of Experts (MoE) architecture, it requires fewer resources to run than “dense” models of this size. Its context window extends up to 1 million tokens, which allows it to process very large document corpora. </p>"
            },
            "Magistral Medium": {
                "desc": "<p> Medium-sized, multimodal, and multilingual reasoning model. Suitable for programming tasks or other tasks requiring in-depth analysis, understanding of complex logical systems, or planning - for example, for agentic use cases or writing long, complex content. </p>",
                "fyi": "<p> This model is part of the first generation of Mistral AI reasoning models (Summer 2025). Unlike most other reasoning models, this model can reason in multiple languages, including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese. It was trained with reinforcement learning on Mistral Medium 3 and was not distilled from existing reasoning models. This model inherits the multimodal capabilities of Mistral Medium 3, even though reinforcement learning was only performed on text. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring at least several powerful graphics cards to run. Reasoning models require more computing power to produce a response, which increases their power consumption. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> It has a context window of up to 40,000 tokens, useful for analyzing short documents but insufficient for analyzing large document corpora. </p>"
            },
            "Magistral Small": {
                "desc": "<p> Medium-sized, multimodal, and multilingual reasoning model. Suitable for tasks requiring in-depth analysis, understanding of logical systems, or planning- for example, for agentic use cases or writing long, complex content. </p>",
                "fyi": "<p> This model is part of Mistral AI's first generation of reasoning models (Summer 2025). Unlike most other reasoning models, this model can reason in multiple languages, including English, French, Spanish, German, Italian, Arabic, Russian, and Simplified Chinese. </p>\n <p> Training was done in two phases. The first, called <em> cold-start </em> by distillation (from Mistral Medium 3 and <em> /OpenR1), allows the model to acquire basic reasoning capabilities from general instructional data (10%). The second is a high-entropy reinforcement learning (RL) phase, where the model is encouraged to explore diverse and varied solutions rather than converging on a single answer, and to generate long completions (up to 32,000 tokens), which allows the development of reasoning capabilities that exceed those of the teaching model. </em> </p>",
                "size_desc": "<p> With 24 billion parameters, this model is considered medium-sized. It requires a single powerful graphics card to run. Reasoning models also take longer to produce an answer, which increases their power consumption. </p>\n <p> It has a context window of up to 40,000 tokens, useful for analyzing short documents but insufficient for analyzing large document corpora. </p>"
            },
            "Ministral": {
                "desc": "<p> Small multilingual model designed to run on a laptop without a server connection, while still offering good capabilities in text summarization, simple question answering, and basic tool use. </p>",
                "fyi": "<p> This model uses a grouped query attention (GQA) method to limit the analyzed text at each generation step and gain speed and memory: computation times are reduced without impacting quality. The attention mechanism is improved by applying windows of different sizes, which allows handling large contexts (up to 128,000 tokens) while remaining lightweight. The large tokenizer (V3-Tekken) better compresses languages and code, which improves its performance on multilingual tasks. </p>",
                "size_desc": "<p> With its 8 billion parameters, this model is considered small. It can be deployed locally on a fairly powerful computer, ensuring data confidentiality, or hosted on a server with a single graphics card to limit infrastructure costs. </p>"
            },
            "Mistral Large 2": {
                "desc": "<p> Large model designed to handle complex questions and tasks: for example, code generation, tool use, long document analysis, or specific-domain language understanding. </p>",
                "fyi": "<p> This model was trained with a high proportion of code data (over 80 programming languages) and mathematics, which improves its ability to solve complex problems and use tools. </p>",
                "size_desc": "<p> With 123 billion parameters, this model is considered large. It requires a server equipped with at least one powerful graphics card, which implies a significant operating cost. It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Mistral Medium 2506": {
                "desc": "<p> A medium-sized, multilingual, multimodal model that is inexpensive compared to other models that offer similar performance. It is particularly interesting for programming tasks or reasoning tasks, such as mathematics. </p>",
                "fyi": "<p> This model was designed to deliver solid performance at a lower cost than other similar models. Special attention was paid to professional usage data during its training. It performs particularly well compared to other models of similar size at generating code and performing mathematical tasks. </p>\n <p> This model was used as the basis for training Magistral Medium - a reasoning model. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a large model, requiring at least several powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Mistral Saba": {
                "desc": "<p> Medium-sized model designed for a detailed linguistic and cultural understanding of Middle Eastern and South Asian languages, including Arabic, Tamil, and Malayalam. </p>",
                "fyi": "<p> Training focused primarily on texts in Arabic, Tamil, and Malayalam. Regional corpora were selected to reflect authentic usage, including syntax, registers, and dialectal variants. For tokenization (splitting the text into basic units that the model can process), a specialized strategy adapted to languages with complex morphology such as Arabic was used. Optimizations aimed to avoid excessive word fragmentation and maximize vocabulary coverage. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a medium-sized model, requiring at least a powerful graphics card to run. Available estimates rely on indirect indices such as inference costs and response latency. </p>\n <p> The model offers a context window of up to 128,000 tokens, suitable for analyzing long documents. </p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p> Despite its name, this is a medium-sized model. It is multimodal (capable of processing text and images) and stands out for its precise query processing and its ability to use tools. </p>",
                "fyi": "<p> Version 3.2 of this model is optimized to generate structured output, particularly in JSON, while limiting repetitiveness and undesirable behavior during long generation chains. Multimodal, it processes both text and image inputs, allowing for joint analysis. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered medium-sized. It can be hosted on a server with a single powerful graphics card, limiting infrastructure costs. It has a context window of up to 128,000 tokens, useful for analyzing long documents. </p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p> Large model trained from Llama 3.1 70B. This retrained (fine-tune) version tends to detail more and provide more structured responses. </p>",
                "fyi": "<p> This model is based on retraining the Llama 3.1 70B, hence the presence of its source model in its name. It introduces improvements achieved thanks to reinforcement learning with human feedback (RLHF) and the REINFORCE algorithm: the model explores different responses, receives feedback in the form of rewards, then gradually adjusts its choices to better meet user expectations. This alignment process is often used when the model is intended to adapt to human preferences or to optimize its responses according to specific criteria. </p>",
                "size_desc": "<p> With 70 billion parameters, this model belongs to the large model category. It requires several powerful graphics cards to run, which results in significant operating costs. </p>"
            },
            "o4 mini": {
                "desc": "<p> Very large reasoning model, suitable for complex scientific and technological tasks and questions. </p>",
                "fyi": "<p> This model is very powerful for analyzing images and graphs. It has also been trained to interact with other tools via function calls, making it suitable for agentic use cases. As a very powerful reasoning model, it can be used to distribute tasks among several smaller and/or more specialized models. It has a context window of up to 200,000 tokens, making it easy to analyze long documents. </p>",
                "size_desc": "<p> Despite its name and the fact that the exact size is unknown, o4 mini is most likely a large model requiring servers equipped with multiple graphics cards. Reasoning models like o4 mini require more time to respond because a reasoning phase precedes the generation of the final result, which increases their energy consumption. However, the assumed Mixture of Experts (MoE) architecture only activates a subset of the parameters to generate each token, thus limiting its energy footprint. Size estimates rely on indirect indices such as inference costs and response latency. </p>"
            },
            "Phi-4": {
                "desc": "<p> Small, multilingual model that can use tools and performs well on complex tasks like logic, math, and code, while remaining compact. </p>",
                "fyi": "<p> This model uses TikTok for tokenization, which improves its capabilities in a multilingual context. It was trained on a total of 9.8 <strong> trillion </strong> tokens, 400 billion of which were specifically sourced from high-quality synthetic data, and the rest from filtered organic data. Training took place on 1,920 H100 graphics cards for 21 days. Techniques such as self-assessment - during which the model critiques and rewrites its responses - and instruction reversal were used to strengthen its understanding of instructions and reasoning abilities. </p>",
                "size_desc": "<p> With 14 billion parameters, this model is considered small. It can be deployed locally on a sufficiently powerful computer, or hosted on a server with a single graphics card, which reduces infrastructure costs. The context window, of 16,000 tokens, can be limiting for analyzing very long documents. </p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p> Medium-sized model specializing in programming and the use of tools (web searches, interactions with APIs, etc.). </p>",
                "fyi": "<p> This model has been trained on 5.5 trillion tokens and over 92 programming languages, including specialized coding languages like Haskell and Racket. </p>\n <p> Thanks to its code performance, it is able to handle calls to tools well, which is useful for agentic uses. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered mid-sized. It can run on a server equipped with a single powerful graphics card, which limits infrastructure costs. </p>\n <p> Its 128,000-token context window allows it to process long documents. </p>"
            },
            "Qwen 2.5 max 0125": {
                "desc": "<p> Very large, specialized reasoning model with high performance in mathematics, coding, and logical problem solving. </p>",
                "fyi": "<p> The exact size of the model is unknown, but it is most likely a very large model requiring servers equipped with multiple graphics cards. However, the Mixture of Experts (MoE) architecture only activates a subset of parameters at each token generation, thus limiting its energy footprint. Size estimates rely on indirect indices such as inference costs and response latency. </p>",
                "size_desc": "<p> This proprietary model based on a <strong> large-scale MoE architecture has been </strong> trained on <strong> more than 20 trillion tokens </strong> . It is designed for tasks requiring multiple thinking stages. </p>\n <p> The context window goes up to 32,000 tokens. </p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p> Multilingual medium-sized model. </p>",
                "fyi": "<p> This MoE (Mixture of Experts) model features a configuration of 128 experts in total, with only 8 experts activated per token generated, allowing for faster and more efficient inference. It uses a system called <em> global-batch </em> to optimize the distribution of work among the experts, so that they are all used in a balanced manner. </p>\n <p> Unlike other models like Qwen 2.5-MoE that recycle the same experts across multiple layers of the network, Qwen 3 30B A2B assigns unique experts to each layer. In practical terms, this means that experts from the first layer are never reused in subsequent layers—each level of the model has its own set of specialized experts. This architecture allows each expert to focus exclusively on tasks specific to their position in the neural network, resulting in finer-grained specialization and optimized performance for each stage of information processing. </p>",
                "size_desc": "<p> With 30 billion parameters, this model falls into the mid-size category. It can run on a server with a single powerful graphics card, limiting infrastructure costs. In addition, the Mixture of Experts (MoE) architecture activates only a portion of the parameters at each token generation, thus limiting its energy footprint. </p>"
            },
            "Qwen 3 32B": {
                "desc": "<p> Medium-sized multilingual model with two response methods: the user can choose between a reasoning mode, for more in-depth answers, or a quick mode, to directly generate the final answer. </p>",
                "fyi": "<p> This model was trained on a very large data set: 36 billion tokens in 119 languages. Training was done in three stages. The model first learned from 30 billion tokens with a context of 4,000 tokens. Then, 5 billion tokens were added to strengthen its factual knowledge. Finally, it was exposed to a specific corpus to help it better handle very long texts. As a result, it has a context window of 128,000 tokens at the end of training, which is useful for reading and analyzing long documents. </p>",
                "size_desc": "<p> With 32 billion parameters, this model is considered mid-sized. It can run on a server equipped with a single powerful graphics card, which limits infrastructure costs. </p>\n <p> Its 128,000-token context window allows it to process long documents. </p>"
            },
            "qwq 32B": {
                "desc": "<p> Medium-sized reasoning model specialized and highly efficient in mathematics, code generation, and logical problem solving. </p>",
                "fyi": "<p> This model was trained with a reinforcement learning (RL) method to optimize its handling of math problems and programming tasks. It uses several recent techniques to improve the quality of answers. For example, the RoPE (Rotary Position Embedding) method allows it to better understand the word order in a text. The SwiGLU activation function is a more efficient way of handling computations within the neural network, which helps the model produce more reliable answers. The QKV (Query Key Value-bias) adjustment method improves how the model identifies and selects important information. Finally, thanks to the YaRN (Yet another RoPE extensioN method), it can process very long texts of up to 130,000 tokens, allowing it to work on complex or very detailed documents. </p>",
                "size_desc": "<p> With 32 billion parameters, this model falls into the mid-size category. It can run on a server with a single powerful graphics card, which limits infrastructure costs. However, reasoning models of this type take longer to produce an answer because a reasoning phase precedes the generation of the final result, which increases energy consumption. </p>"
            },
            "Aya Expanse 32B": {
                "fyi": "<p> Cohere, the Canadian company that trained this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous “Attention Is All You Need” paper that revolutionized modern AI. Its main uniqueness lies in its exclusive focus on generative AI for businesses, particularly regulated sectors such as finance, healthcare, manufacturing, and energy, as well as the public sector. The company is also a pioneer in multilingual approaches and maintains a non-profit research lab to support open innovation. </p>\n <p> This model was designed to provide good capabilities in each of the 23 languages in its training corpus. </p>",
                "size_desc": "<p> With 32 billion parameters, this model falls is considered mid-sized. It can be hosted on a server with a single powerful graphics card, which helps keep infrastructure costs low. </p>\n <p> It has a context window of up to 130,000 tokens, useful for analyzing long documents. </p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p> Very large, multimodal and multilingual model, efficient for code generation, with two response modes: the user can choose between a reasoning mode, for more in-depth answers, or a fast mode, to directly generate the final answer. </p>",
                "fyi": "<p> Claude 4 Opus is the most advanced model in the Claude 4 family. It is optimized for raw power and complex tasks requiring sustained reasoning over long periods of time: for example, it can work on long-term tasks (Anthropic claims it can work independently for up to seven hours). On the other hand, Opus is more expensive to use, slower to respond, and requires more resources to run. </p>\n <p> The model offers two modes of use: a reasoning mode with step-by-step reasoning for complex problems, and a quick mode for direct answers. Unlike other models, the reasoning mode was not primarily trained on mathematical data, but adapted to real-life use cases. </p>",
                "size_desc": "<p> The exact size of the model is unknown. Evidence suggests that it is a very large model, requiring servers with multiple powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. It has a context window of up to 200,000 tokens, suitable for analyzing long documents or code repositories. </p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p> Very large multimodal and multilingual model, very powerful in code, with two response modes: the user can choose between a reasoning mode, for more in-depth answers, or a fast mode, to directly generate the final answer. </p>",
                "fyi": "<p> Claude 4 Sonnet is a more compact version of Claude 4 Opus optimized for speed, efficiency, and accessibility. It is slightly less adept at tasks requiring complex, multi-step reasoning. On the other hand, it is significantly less expensive, faster, can generate longer texts, and consumes less power than Opus. </p>\n <p> The model offers two modes of use: a reasoning mode with step-by-step reasoning for complex problems, and a rapid mode for direct answers. Unlike other models, the reasoning mode was not primarily trained on mathematical data, but adapted to real-life use cases. </p>",
                "size_desc": "<p> The exact size is unknown. Evidence suggests that this is a very large model, requiring servers with multiple powerful graphics cards to run. Available estimates rely on indirect indices such as inference costs and response latency. The model has a context window of up to 200,000 tokens, suitable for analyzing long documents or code repositories. </p>"
            },
            "Command A": {
                "desc": "<p> Large model, efficient for programming, use of tools, and “retrieval augmented generation” (RAG). </p>",
                "fyi": "<p> Cohere, the Canadian company behind this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous paper <a href=\"https://arxiv.org/abs/1706.03762\"> \"Attention Is All You Need\" </a> published in 2017 and which revolutionized AI. The company stands out for its exclusive focus on generative AI for businesses, particularly regulated sectors such as finance, healthcare, manufacturing and energy, as well as the public sector. The company is also a pioneer in multilingual approaches and maintains a non-profit research lab to support open innovation. </p>\n <p> This model is designed to work in more than 23 languages and to integrate easily into enterprise systems. It is one of the few models distributed under <strong> CC-BY-NC 4.0 license, which allows sharing and modification but prohibits any commercial use. </strong> This choice of license reflects Cohere's desire to contribute to research and the open source community, while maintaining control over commercial uses to protect its business model... This excludes, for example, the integration of the model into products or services sold by a company to customers but allows academic use, testing, or internal projects, restricted to a non-commercial framework. </p>",
                "size_desc": "<p> With 111 billion parameters, this model is considered large. It requires at least two powerful graphics cards to host, which results in significant running costs. </p>\n <p> Its context window reaches 256,000 tokens, suitable for analyzing large sets of documents or code bases. </p>"
            },
            "Command R": {
                "desc": "<p> Medium-sized model optimized for summarization, general questions, tool usage, and efficient in retrieval augmented generation (RAG) systems. </p>",
                "fyi": "<p> Cohere, the Canadian company behind this model, was founded in 2019 by former Google Brain researchers, including Aidan Gomez, co-author of the famous “Attention Is All You Need” paper that revolutionized AI. Its main uniqueness lies in its exclusive focus on generative AI for businesses, particularly regulated sectors such as finance, healthcare, manufacturing, and energy, as well as the public sector. The company is also a pioneer in multilingual approaches and maintains a non-profit research lab to support open innovation. </p>\n <p> This model has been evaluated in over 10 languages. Its context window reaches 128,000 tokens, which facilitates the analysis of long documents. This window was doubled in the next version of the model (Command A). </p>",
                "size_desc": "<p> With 35 billion parameters, this model is mid-sized. It can be hosted on a server with a single powerful graphics card, which helps keep infrastructure costs down. </p>"
            }
        }
    },
    "ranking": {
        "desc": "Find out how the best AI models stack up based on their preference scores, based on votes from the compar:AI community. To learn more, see our ranking methodology.",
        "graphs": {
            "title": "Graphs"
        },
        "table": {
            "data": {
                "cols": {
                    "consumption_wh": "Energy<br>(1000 tokens)",
                    "elo": "Preference score",
                    "license": "Licence",
                    "name": "Model",
                    "organisation": "Organization",
                    "rank": "Rank",
                    "release": "Release date",
                    "size": "Size<br>(active parameters)",
                    "total_votes": "Total votes",
                    "trust_range": "Confidence (±)"
                }
            },
            "lastUpdate": "Updated on {date}",
            "search": "Search for a model",
            "totalModels": "Total models:",
            "totalVotes": "Total votes:"
        },
        "title": "Model leaderboard"
    }
}
