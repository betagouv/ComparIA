{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "contact": "Kontakta oss",
        "contactUs": "Kontakta oss",
        "copyLink": {
            "do": "Kopiera länk",
            "done": "Länk kopierad"
        },
        "copyMessage": {
            "do": "Kopiera meddelande",
            "done": "Meddelandet har kopierats"
        },
        "home": "Startsida",
        "returnHome": "Tillbaka till startsida",
        "seeMore": "Visa mer",
        "selectLanguage": "Välj språk",
        "vote": "Skicka åsikt"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 modeller",
            "help": "Om bara en är vald, kommer den andra att väljas slumpmässigt",
            "question": "Vilka modeller vill du jämföra?"
        },
        "modelSelection": "Val av modeller",
        "prompt": {
            "label": "Skriv ditt första meddelande",
            "placeholder": "Skriv ditt första meddelande här"
        },
        "selectModels": {
            "help": "Välj jämförelseläge",
            "question": "Vilka modeller vill du jämföra?"
        },
        "suggestions": {
            "choices": {
                "administrative": {
                    "iconAlt": "Administrativt",
                    "title": "Skriv ett administrativt dokument"
                },
                "coach": {
                    "iconAlt": "Tips",
                    "title": "Få råd om kost och träning"
                },
                "explanations": {
                    "iconAlt": "Förklaringar",
                    "title": "Förklara ett koncept enkelt"
                },
                "iasummit": {
                    "iconAlt": "AI Action Summit",
                    "title": "Prompter from medborgarkonsultation om AI ",
                    "tooltip": "Dessa frågor är resultatet av en medborgarkonsultation om IA som hölls 16/9 till 8/11 2024. Syftet var att låta medborgare och samhälle delta i AI Action Summit, och samla deras idéer om hur AI kan skapa möjligheter för alla, medan missbruk minimeras."
                },
                "ideas": {
                    "iconAlt": "Idéer",
                    "title": "Generera nya idéer"
                },
                "languages": {
                    "iconAlt": "Översättning",
                    "title": "Skriv på annat språk"
                },
                "recipes": {
                    "iconAlt": "Recept",
                    "title": "Upptäck ett nytt recept"
                },
                "recommendations": {
                    "iconAlt": "Rekommendationer",
                    "title": "Föreslå filmer, böcker och musik"
                },
                "stories": {
                    "iconAlt": "Berättelser",
                    "title": "Berätta en historia"
                }
            },
            "generateAnother": "Skapa ett nytt meddelande",
            "title": "Förslag på prompter"
        },
        "title": "Hur kan jag hjälpa dig idag?"
    },
    "chatbot": {
        "continuePrompt": "Fortsätt chatta med AI-modellerna",
        "conversation": "Konversation",
        "errors": {
            "other": {
                "message": "Ett tillfälligt fel har uppstått.",
                "retry": "Du kan försöka begära modellerna igen.",
                "title": "Ett tillfälligt fel har uppstått",
                "vote": "Eller avsluta din upplevelse genom att ge din åsikt om modellerna."
            },
            "tooLong": {
                "message": "Varje modell har en gräns för hur stora samtal den kan hantera.",
                "retry": "Du kan återuppta en konversation med två nya modeller.",
                "title": "Konversationen är för lång för en av modellerna.",
                "vote": "Du kan fortfarande ge din åsikt om dessa modeller eller starta en konversation med två nya."
            }
        },
        "loading": "Läser in svar",
        "reasoning": {
            "finished": "Resonemang avslutat",
            "inProgress": "Resonemang pågår…"
        },
        "revealButton": "Visa vilka modeller som användes"
    },
    "closeModal": "Stäng modalfönstret",
    "components": {
        "pagination": {
            "first": "Första sidan",
            "label": "Sidnummer",
            "last": "Sista sidan",
            "next": "Nästa sida",
            "nth": "Sida {count}",
            "previous": "Föregående sida"
        },
        "table": {
            "linePerPage": "Antal rader per sida",
            "pageCount": "{count} rader per sida",
            "triage": "Sortera"
        },
        "theme": {
            "legend": "Välj ett tema för att anpassa webbplatsens utseende.",
            "options": {
                "dark": "Mörkt tema",
                "light": "Ljust tema",
                "system": "System",
                "systemSub": "Använd systeminställningar"
            },
            "title": "Visningsinställningar"
        }
    },
    "datasets": {
        "access": {
            "catch": "Modellskapare, forskare, företag, det är er tur!",
            "desc": "Frågorna som ställs på plattformen är mestadels på franska och återspeglar verklig användning utan restriktioner. Dessa datamängder finns tillgängliga på <a {linkProps}>data.gouv</a> och Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "Frågor och svar",
                    "title": "/konversationer"
                },
                "reactions": {
                    "desc": "Alla reaktioner",
                    "title": "/reaktioner"
                },
                "votes": {
                    "desc": "Alla uttryckta preferenser",
                    "title": "/röster"
                }
            },
            "share": "Visa hur du använder datan",
            "title": "Åtkomst till datauppsättningar från compar:IA"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "title": "Visa analys för indikator"
                },
                "conversations": {
                    "desc": "Interaktiv visualisering av samtal där varje punkt representerar ett kluster av diskussioner som nämns av användare (som tex. utbildning, hälsa, miljö eller filosofi).",
                    "title": "Utforska visualisering av data"
                },
                "desc": "Bunka.ai-teamet genomförde en djupgående studie av interaktioner mellan användare av Compar:AI-plattformen och AI-modeller, där de undersökte vilka ämnen man föredrog, vilka frågor som ställdes, och ifall modellerna främst användes som automatiseringsverktyg eller för att assistera användaren. Denna analys baseras på ett stort urval av 25 000 konversationer.",
                "method": "Läs mer om metoderna"
            },
            "title": "Hur används dessa uppgifter?"
        }
    },
    "errors": {
        "404": {
            "desc": "Om du skrev in webbadressen i din webbläsare, kontrollera att den är korrekt. Sidan kanske inte längre är tillgänglig. <br /> I så fall kan du gå tillbaka till startsidan. <br />Kontakta oss så hjälper vi dig hitta rätt information.",
            "error": "Fel 404",
            "sorry": "Sidan kunde inte visas.",
            "title": "Sidan hittades inte"
        },
        "unexpected": {
            "desc": "Prova att ladda om sidan, eller försök igen senare.",
            "error": "Felkod {code}",
            "sorry": "Ett problem har uppstått med tjänsten. Vi arbetar på att lösa det så snabbt som möjligt.",
            "title": "Oväntat fel"
        },
        "unknown": "Ett fel har uppstått"
    },
    "faq": {
        "datasets": {
            "questions": {
                "2": {
                    "desc": "<p>Det unika med data som samlas in på compar:IA-plattformen är att de använder språk med mindre resurser, och motsvarar verkliga användaruppgifter. Denna data återspeglar mänskliga preferenser i ett specifikt språkligt och kulturellt sammanhang. Den gör det möjligt att justera modellerna för att göra dem mer relevanta, korrekta och anpassade till användarnas behov, samtidigt som vi åtgärdar eventuell bias eller luckor i nuvarande modeller.</p>"
                },
                "3": {
                    "title": "Vad skiljer compar:IA från andra liknande initiativ?"
                }
            },
            "title": "Datamängd"
        },
        "ecology": {
            "questions": {
                "1": {
                    "title": "Hur beräknas miljömässiga indikatorer?"
                },
                "2": {
                    "title": "Tar miljöindikatorer hänsyn till olika typer av energikällor i olika länder?"
                },
                "3": {
                    "title": "Tar indikatorer för miljöpåverkan hänsyn till de resurser som används för att träna modellerna?"
                }
            },
            "title": "Miljömässiga indikatorer"
        },
        "i18n": {
            "questions": {
                "1": {
                    "desc": "<p> Ja, internationaliseringen av compar:AI är igång. Vi börjar med en expansion till tre pilotländer: Litauen, Sverige och Danmark. Denna första fas kommer att göra det möjligt för oss att testa tillvägagångssättet och anpassa gränssnittet till olika europeiska språkliga och kulturella sammanhang. Så småningom kan vi expandera till fler europeiska språk baserat på feedback från dessa pilotländer. Målet är att gradvis bygga en gemensam europeisk digital plattform för mänsklig utvärdering av konversationsbaserad AI, med samarbetsinriktad styrning som återstår att definiera mellan de olika deltagande länderna. </p>",
                    "title": "compar:IA fokuserade från början på franska – finns det planer för andra europeiska språk?"
                }
            },
            "title": "Internationalisering"
        },
        "models": {
            "questions": {
                "1": {
                    "desc": "<p>Vi väljer modeller baserat på deras popularitet, mångfald och relevans för användarna. Vi lägger särskild vikt vid att ta med modeller i olika storlekar, och modeller med så kallade <em>open weights</em>.</p>",
                    "title": "Hur väljs modellerna som finns tillgängliga i jämförelseverktyget?"
                },
                "2": {
                    "desc": "<p>Funktionen att fråga modellerna möjliggörs tack vare donationer från molnleverantörerna som stöder projektet: Google Cloud Platform, Hugging Face, Microsoft Azure, OVH, Scaleway.</p>",
                    "title": "Hur finansieras den här tjänsten?"
                },
                "3": {
                    "desc": "<p>Kvantiserade modeller är optimerade för att förbruka färre resurser genom att förenkla vissa beräkningar. Tekniken innebär att man minskar precisionen hos parametrarna i en AI-modell. På det sättet kan man göra <strong> modellen mindre </strong>och<strong> beräkningarna snabbare</strong>, vilket är särskilt användbart för maskiner med begränsade resurser.</p>",
                    "title": "Vad är en \"kvantiserad modell\"?"
                },
                "4": {
                    "desc": "<p> <strong>Vilka språk en modell kan hantera väl beror på språken som används i dess träningsdata.</strong> <strong> En LLM använder enorma korpusar på många språk</strong> men språkfördelningen i träningsdatan skiljer mellan modeller. En överrepresentation av engelska kan leda till begrränsningar på andra språk. Dessa begränsningar återspeglas till exempel i <strong> anglicismer eller en oförmåga att generera innehåll på vissa språk som klassificeras som \"hotade\" av UNESCO.</strong></p><p><strong>En modells noggrannhet och ordförråd beror på de data som använts för träning</strong>.</p>",
                    "title": "Finns det ett samband mellan nationaliteten hos skaparen av modellen och dess förmåga att tala flera språk?"
                },
                "5": {
                    "desc": "<p>Bara ett fåtal modeller är \"transparenta\" i den bemärkelsen. I de flesta fall är träningsdatan inte tillgänglig för allmänheten, av juridiska eller kommersiella skäl.</p>",
                    "title": "Går det att få tag på modellernas träningsdata?"
                }
            },
            "title": "Modeller"
        },
        "title": "Vanliga frågor",
        "usage": {
            "questions": {
                "1": {
                    "desc": "<p> Nuvarande konversationsmodeller kan <strong>inte citera källorna</strong> de använde för att generera ett svar. De fungerar genom att förutsäga det mest sannolika nästa ordet baserat på den statistiska fördelningen av träningsdata. Informationen är en kombination av många olika källor, och metoden kan inte hålla reda på vilka källor som har påverkat ett visst svar.</p><p>Det finns dock tekniker som <strong>Retrieval Augmented Generation (RAG)</strong> som syftar till att övervinna denna begränsning. Med RAG kan modeller få tillgång till externa kunskapsbaser och <strong>tillhandahålla kontextualiserad information genom att citera källorna</strong> Denna metod är avgörande för att förbättra transparensen och tillförlitligheten hos modellgenererade svar.</p>",
                    "title": "Kan modellerna ange specifika källor för ett svar?"
                },
                "2": {
                    "desc": "<p><strong> Nej, \"råa\" konversationsbaserade AI-modeller kan inte svara på frågor om aktuella händelser.</strong>De är tränade på statiska datamängder och kan inte interagera med webben eller öppna länkar. De har inte möjlighet att uppdatera sig själva med händelser i världen. Informationen som modellen har tillgång till är begränsad till datumet för dess senaste träning.</p><p>Om du ställer en fråga om en nyhetshändelse kommer modellen därför att förlita sig på potentiellt föråldrad information och riskera att generera felaktiga svar.</p><p>När det gäller Perplexity, Copilot och ChatGPT kombineras de \"råa\" konversationsmodellerna med andra tekniska byggstenar som gör att de kan ansluta till internet för att få tillgång till information i realtid. Dessa kallas \"konversationsagenter\".</p>",
                    "title": "Om jag ställer en fråga om aktuella händelser, kan modellen svara?"
                },
                "3": {
                    "desc": "<p>Språkmodeller bearbetar frågetexten men har inte möjlighet att interagera med webben eller öppna länkar. De tränas på en fast textdatauppsättning, och deras svar baseras på denna träningsdata. När en fråga ställs använder modellerna denna träning för att generera ett svar men kan inte komma åt ny information online.</p><p>Som jämförelse, föreställ dig en student som gör ett prov utan internetåtkomst. De kan använda sina förvärvade kunskaper för att svara på frågor, men kan inte besöka webbplatser för ytterligare information.</p>",
                    "title": "Om min fråga innehåller en länk, kan modellen komma åt den?"
                },
                "4": {
                    "desc": "<p>Varje modell har ett begränsat <strong>kontextfönster</strong>, som representerar hur mycket tidigare information modellen kan komma ihåg. I långa och komplexa konversationer kan kontextfönstret snabbt ta slut, så att modellen glömmer viktiga delar av konversationen, vilket leder till inkonsekventa svar. Ju mindre fönstret är, desto snabbare uppstår problemet.</p>",
                    "title": "Varför tappar vissa modeller snabbt tråden i samtalet?"
                },
                "5": {
                    "desc": "<p>För att få bästa resultat från en språkmodell är det viktigt att behärska konsten att skriva bra <em>prompter</em>, det vill säga förfrågningar eller instruktioner. <strong>Tydlighet är nyckeln</strong>:</p><ul><li>Använd enkelt och direkt språk och undvik frågor som är för långa eller komplexa. Dela upp förfrågningar i flera enklare frågor för mer exakta svar.</li><li><strong>Ange specifika formatbegränsningar om det behövs</strong>: Om du behöver ett svar i ett visst format (lista, tabell, sammanfattning etc.), ange det i prompten. Du kan också ange vilka steg som ska följas och andra kriterier.</li><li><strong>Specificera modellens roll</strong> Börja till exempel med \"Agera som en expert på...\" eller \"Tänk dig att du är lärare...\" för att bestämma tonen och perspektivet i svaret.</li><li><strong>Kontextualisera dina frågor</strong>. Ge om nödvändigt relevanta exempel för att vägleda modellen.</li><li><strong>Uppmuntra resonemang</strong>. Använd en tankekedja (<em>Chain-of-Thought Prompting</em>) för att be modellen förklara sitt resonemang, vilket gör svaren mer robusta.</li></ul><p>Konversationsmodeller är känsliga för variationer i formuleringar – enkelt språk, korta frågor och omformuleringar vid behov kan hjälpa till att vägleda modellen mot relevanta svar. Testa och förfina dina frågor för att hitta den mest effektiva formuleringen!</p>",
                    "title": "Hur skriver man en effektiv prompt (fråga/uppmaning) till modellen?"
                },
                "6": {
                    "desc": "<p>Konversations-AI svarar direkt genom att formulera meningar från en stor datamängd som modellen har tränats på, medan en sökmotor erbjuder länkar och resurser som användaren kan utforska på egen hand.</p>",
                    "title": "Vad är skillnaden mellan att ställa en fråga till en konversationsbaserad AI-modell och att söka på Google?"
                }
            },
            "title": "Användning"
        }
    },
    "footer": {
        "backHome": "Tillbaka till startsidan",
        "helpUs": "Hjälp oss att förbättra den här tjänsten!",
        "license": {
            "linkTitle": "Etalab-licens – nytt fönster",
            "mention": "Innehållet på denna webbplats är tillgängligt under <a {linkProps}>etalab-2.0-licensen</a>, förutom där rättigheter som innehas av tredje part uttryckligen anges"
        },
        "links": {
            "accessibility": "Brister i tillgänglighet",
            "legal": "Rättsliga meddelanden",
            "privacy": "Integritetspolicy",
            "sources": "Källkod",
            "tos": "Användarvillkor"
        },
        "writeUs": "Om du stöter på problem eller har kommentarer om jämförelseverktyget är du välkommen att skriva till oss med hjälp av <a {linkProps}>det här formuläret</a>. Vi läser alla meddelanden.<br />Tack!"
    },
    "general": {
        "a11y": {
            "desc": "Denna tillgänglighetspolicy gäller webbplatsen <strong>comparia.beta.gouv.fr</strong>.",
            "disclaimer": "<strong> compar:IA </strong> åtar sig att göra sina digitala tjänster tillgängliga, i enlighet med artikel 47 i lag nr 2005-102 av den 11 februari 2005.",
            "improveAdress": "Adress: DINUM, 20 avenue de Ségur, 75007 Paris",
            "improveDelay": "Vi försöker svara inom 2 arbetsdagar.",
            "improveDesc": "Om du har problem att komma åt innehåll eller tjänst kan du kontakta administratören för beta.gouv.fr, för att bli hänvisad till ett tillgängligt alternativ eller få innehållet i ett annat format.",
            "improveMail": "E-post: <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Feedback och kontakt",
            "remedyAdvocate": "Skriv ett meddelande till <a {linkProps}>Frankrikes ombudsman för mänskliga rättigheter (le Défenseur des Droits)</a>",
            "remedyAdvocateAdress": "Skicka ett brev med posten (gratis, inget frimärke behövs): Défenseur des droits - Libre réponse 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Kontakta <a {linkProps}>Frankrikes ombudsman för mänskliga rättigheter (le Défenseur des Droits)</a>",
            "remedyDesc": "Denna procedur ska användas i följande fall: Du har rapporterat ett tillgänglighetsfel till webbplatsansvarig som hindrar dig från att komma åt innehåll eller en av portalens tjänster och du inte har fått ett tillfredsställande svar.",
            "remedyList": "Du kan:",
            "remedyTitle": "Överklaga",
            "stateDesc": "Webbplatsen comparia.beta.gouv.fr uppfyller inte RGAA 4.1. Webbplatsen har ännu inte granskats. <strong>Den har dock utformats för att vara tillgänglig för så många som möjligt</strong>. Du bör därför kunna:",
            "stateNavigate": "navigera på alla sidor på webbplatsen med hjälp av ett tangentbord",
            "statePrefs": "anpassa webbplatsen efter dina preferenser (teckenstorlek, skärmzoom, ändring av typsnitt etc.) utan att innehåll förloras",
            "stateScreenReader": "visa webbplatsen med en skärmläsare.",
            "stateTitle": "Efterlevnad",
            "title": "Tillgänglighetspolicy"
        },
        "legal": {
            "a11yDesc": "Efterlevnad av digitala tillgänglighetsstandarder är ett framtida mål, men vi arbetar för att göra webbplatsen tillgänglig för alla.",
            "a11yTitle": "Tillgänglighet",
            "directorDesc": "Romain Delassus, chef för kulturministeriets digitala avdelning",
            "directorTitle": "Publikationsansvarig",
            "editorDesc": "Denna webbplats publiceras av franska kulturministeriet. Postadress: Ministère de la culture, 182 Rue Saint-Honoré, 75001 Paris",
            "editorTitle": "Redaktör",
            "hostingDesc": "Denna webbplats drivs av OVH SAS (<a {linkProps}>https://www.ovh.com</a>) vars huvudkontor ligger på 2 rue Kellermann, 59100 Roubaix, Frankrike.",
            "hostingTitle": "Servertjänst för sidan",
            "reportA11y": "Om du träffar på ett tillgänglighetsproblem som hindrar dig från att komma åt innehåll eller funktioner på webbplatsen, meddela oss gärna.",
            "reportA11yDesc": "För att läsa mer om Frankrikes policy för digital tillgänglighet: <a {linkProps}>references.modernisation.gouv.fr/accessibilite-numerique</a>",
            "reportDesc": "Om du inte får ett snabbt svar från oss har du rätt att skicka ditt klagomål eller en begäran om hänvisning till Frankrikes ombudsman för mänskliga rättigheter (le Défenseur des Droits).",
            "reportTitle": "Rapportera ett problem",
            "securityCertif": "Webbplatsen är skyddad av ett elektroniskt certifikat, som hjälper till att säkerställa konfidentialiteten för dina uppgifter. I de flesta webbläsare visas detta med en symbol i form av ett hänglås.",
            "securityNoMail": "De tjänster som är kopplade till plattformen kommer inte att användas som källa för att skicka e-postmeddelanden med begäran om att ange personlig information.",
            "securityTitle": "Säkerhet",
            "sources": "Om inget annat anges är all text på denna webbplats licensierad under <a {etalabLinkProps}>Etalab Open 2.0-licensen</a>. Källkoden för denna applikation är fritt återanvändbar och tillgänglig på <a {githubLinkProps}>GitHub</a>.",
            "title": "Rättsliga meddelanden"
        },
        "privacy": {
            "cookiesBannerDesc": "Just det, du behövde inte klicka på ett block som täcker halva sidan för att säga att du godkänner cookies!",
            "cookiesBannerNoNeed": "Inget speciellt, ingen specialbehandling i samband med .gouv.fr. Vi respekterar helt enkelt lagen, som anger att vissa verktyg för publikspårning, korrekt konfigurerade för att respektera integritet, är undantagna från förhandsgodkännande.",
            "cookiesBannerTitle": "Den här webbplatsen visar ingen banner för samtycke till cookies, varför?",
            "cookiesBannerTools": "För detta använder vi <a {matomoLinkProps}>Matomo</a>, ett <a {libreLinkProps}>gratis</a> verktyg, konfigurerat för att följa CNIL:s <a {cnilLinkProps}>rekommendation om cookies</a>. Det innebär att din IP-adress, till exempel, anonymiseras innan den registreras. Det är därför omöjligt att koppla dina besök på denna webbplats till dig personligen.",
            "cookiesDesc": "Den här webbplatsen placerar en liten textfil (en \"cookie\") på din dator när du besöker den. Detta gör att vi kan mäta antalet besök och se vilka sidor som besöks mest.",
            "cookiesDescMore": "Du kan välja bort att din aktivitet på den här webbplatsen spåras. Detta skyddar din integritet, men hindrar också sidans ägare från att sammanställa statistik som kan användas för att skapa en bättre upplevelse för dig och andra användare.",
            "cookiesTitle": "Cookies och samtycke",
            "dataAccessDatasets": "Data för användardialog och preferenser distribueras under Etalabs Open License 2.0 på Hugging Face-plattformen via franska kulturministeriets konto (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "dataAccessDesc": "Självklart! Statistik över webbplatsens användning finns fritt tillgänglig på <a {linkProps}>stats.beta.gouv.fr</a>.",
            "dataAccessTitle": "Jag bidrar till att berika era data, kan jag få tillgång till dem?",
            "dataExtraCountry": "Destinationsland: Frankrike",
            "dataExtraHost": "Underleverantör: OVH",
            "dataExtraTitle": "Vem hjälper oss att behandla uppgifterna?",
            "dataExtraWarranty": "Garantier: <a {linkProps}>https://storage.gra.cloud.ovh.net/v1/AUTH_325716a587c64897acbef9a4a4726e38/contracts/9e74492-OVH_Data_Protection_Agreement-FR-6.0.pdf</a>",
            "dataExtraWhat": "Utförd process: Ackommodering",
            "dataRespDesc": "Franska kulturdepartementets digitala avdelning ansvarar för behandlingen av dina personuppgifter.",
            "dataRespTitle": "Vem är ansvarig för databehandlingen?",
            "dataTimeDesc": "Data som rör användare och deras samtal med modeller sparas när du anger vilken modell du föredrar.",
            "dataTimeTitle": "Hur länge sparas dessa uppgifter?",
            "dataUseDesc": "Utgivaren åtar sig att vidta åtgärder för att säkerställa anonymisering av dialogdata innan de görs tillgängliga för allmänheten.",
            "dataUseTitle": "Vilken behandling utförs på konversationsdata?",
            "desc": "Tjänsten publiceras av franska kulturministeriets digitala avdelning.",
            "privacyData": "Uppgifterna som samlas in på webbplatsen är följande:",
            "privacyDataArena": "Data relaterade till användarkonversationer med modeller: frågor ställda av användare, svar från modeller, och vilken av de två modellerna användaren föredrar",
            "privacyDataForm": "Data relaterade till frågeformuläret ”Hjälp oss att förbättra compar:IA”.",
            "privacyDesc": "Tjänsten behandlar inte personuppgifter enligt CNIL:s definition, nämligen information som rör en fysisk person som kan identifieras, direkt eller indirekt.",
            "privacyResp": "Användaren ansvarar för de uppgifter eller det innehåll som de anger i de frågor som erbjuds av plattformen. Genom att acceptera <a {linkProps}>användarvillkoren</a> samtycker användaren till att inte överföra någon information som kan identifiera dem eller en tredje part.",
            "privacyTitle": "Behandlar vi personuppgifter?",
            "title": "Integritetspolicy"
        },
        "tos": {
            "contactDesc": "Om du har frågor om tjänsten kan du skriva till <a {linkProps}>contact@comparia.beta.gouv.fr</a>.",
            "contactTitle": "9. Kontakt",
            "defsEditor": "”Utgivare” avser franska kulturministeriets digitala tjänst.",
            "defsModels": "”Modeller” avser de stora språkmodeller (LLM) som används av plattformen inom ramen för sina licenser.",
            "defsPlatform": "”Plattform” avser webbplatsen som gör tjänsterna tillgängliga.",
            "defsServices": "”Tjänster” avser de funktioner som plattformen erbjuder för att uppfylla sina syften.",
            "defsTitle": "2. Definitioner",
            "defsUser": "”Användare” avser varje fysisk person som använder plattformen och drar nytta av dess tjänster.",
            "descDatasets": "Dessa datamängder kommer att göras tillgängliga under en öppen licens, särskilt för att främja forskningsanvändning.",
            "descEditor": "Tjänsten, som publiceras av kulturministeriets digitala avdelning, är en plattform för att jämföra konversationsmodeller riktade till allmänheten i syfte att (1) öka medborgarnas medvetenhet om stora språkmodeller (LLM), (2) samla in användarpreferenser för att skapa datamängder för jämförelse.",
            "descTitle": "3. Beskrivning av plattformen",
            "descUse": "Användaren ställer en fråga på ett givet språk och får svar från två anonyma stora språkmodeller (LLM). Denne röstar på den modell som de bedömer ger det bästa svaret, och får sedan reda på vilka modellerna var. Detta crowdsourcing-system, inspirerat av <a {linkProps}>\"chatbot arena\" (LMSYS)</a>-plattformen, möjliggör skapandet av datamängder av mänskliga preferenser för verkliga uppgifter, vilka kan användas för att förbättra modellerna.",
            "dispoDesc": "Plattformen är tillgänglighetsanpassad, förutom i fall av force majeure eller händelser utanför utgivarens kontroll.",
            "dispoResp": "Utgivaren kan därför inte hållas ansvarig för förluster eller skador, av något slag, som kan uppstå till följd av fel eller bristande tillgänglighet i tjänsten. Sådana situationer ger inte rätt till någon ekonomisk ersättning.",
            "dispoRight": "Utgivaren förbehåller sig rätten att utan förvarning stänga av, avbryta eller begränsa åtkomsten till hela eller delar av tjänsterna, särskilt för underhållsåtgärder och uppdateringar som är nödvändiga för att tjänsten och relaterat material ska fungera korrekt, eller av andra skäl, särskilt tekniska.",
            "dispoTitle": "7. Tjänsternas tillgänglighet",
            "dispoWarranty": "Tjänsten garanteras inte vara fri från avvikelser eller fel. Tjänsten tillhandahålls därför utan någon garanti för dess tillgänglighet och prestanda.",
            "evoDesc": "Användarvillkoren kan ändras eller kompletteras när som helst, utan förvarning, beroende på ändringar i tjänsterna, lagändringar eller av annan anledning som bedöms nödvändig.",
            "evoDescMore": "Dessa ändringar och uppdateringar är bindande för användaren som därför regelbundet bör läsa detta avsnitt för att kontrollera villkoren.",
            "evoTitle": "8. Ändringar av användarvillkoren",
            "featuresDatasets": "Tjänsten samlar in data om användardialog och preferenser. Delade datamängder kommer att inkludera användarfrågor, svar från båda modellerna, röster och användarpreferenser.",
            "featuresDatasetsMore": "Utgivaren förbehåller sig rätten att distribuera användardialog- och preferensdata under en öppen licens 2.0. Datamängden distribueras på Hugging Face-plattformen via franska kulturministeriets konto (<a {linkProps}>https://huggingface.co/ministere-culture</a>).",
            "featuresDesc": "För att uppnå det tvåfaldiga målet att öka medborgarnas medvetenhet om stora språkmodeller och samla in användarpreferenser, tillhandahålls följande tjänster av plattformen utan åtkomstbegränsningar:",
            "featuresDescMore": "Ett människa-maskin-gränssnitt som möjliggör samtidig dialog med två konversationsmodeller och val av det föredragna svaret.",
            "featuresModels": "Modellerna som är integrerade i plattformen distribueras på inferensservrar hos de olika partnerna (Scaleway, OVH, Hugging Face, Google Cloud, Mistral AI). Standardiseringsvillkoren för inferens tillhandahålls på plattformen för att garantera transparens i användningen av modellerna.",
            "featuresModelsMore": "Ett gränssnitt för modelljämförelse.",
            "featuresTitle": "4. Funktioner",
            "featuresVote": "Efter valprocessen kan användaren se listan över modeller som ingår i jämförelseverktyget och få tillgång till en lista med information om dessa modeller. Informationen som dokumenterar modellerna är hämtad från källor.",
            "featuresVoteMore": "Dela och tillgängliggöra datamängder som är resultatet av insamling av användarpreferenser.",
            "licenceCode": "Plattformens källkod är öppen och fritt tillgänglig här: <a {linkProps}>https://github.com/betagouv/languia</a>",
            "licenceLLM": "De språkmodeller som används för att driva tjänsterna regleras av följande licenser:",
            "licenceLLMEvolution": "Listan över språkmodeller som ingår i plattformen kan förändras över tid och uppdateras med varje modifiering.",
            "licenceLLMLicence": "Licens",
            "licenceLLMModel": "Konversationsbaserad AI-modell",
            "licenceLLMNoticeLink": "Länk till modellernas licenser",
            "licenceLLMUnavailable": "Inte tillgänglig",
            "licenceTitle": "6. Kod och licenser",
            "respEditor": "Generellt sett frånsäger sig utgivaren allt ansvar vid användning som inte överensstämmer med användarvillkoren.",
            "respLegal": "Plattformen är inte avsedd att användas för att generera olagligt innehåll eller innehåll som strider mot allmän ordning, eller mer generellt, för någon generering som strider mot gällande rättsliga ramar.",
            "respLegalMore": "I detta avseende anger användaren inte något innehåll eller information i prompten som strider mot gällande lagar och förordningar.",
            "respPrivacy": "Eftersom de uppgifter som användaren anger på plattformen är avsedda att göras tillgängliga, förbinder sig denne att inte överföra information som kan identifiera denne eller en tredje part.",
            "respPrivacyMore": "Under alla omständigheter åtar sig utgivaren att vidta åtgärder för att säkerställa anonymisering av dialogdata innan de görs tillgängliga.",
            "respTitle": "5. Ansvar",
            "respUser": "Användaren ansvarar för de uppgifter och det innehåll som denne anger i den prompt som plattformen erbjuder.",
            "scopeDesc": "Åtkomst till plattformen är gratis, utan registrering, och medför tillämpning av specifika förbehåll, som anges i dessa användarvillkor.",
            "scopeTitle": "1. Tillämpningsområde",
            "title": "Användarvillkor"
        }
    },
    "generated": {
        "licenses": {
            "os": {
                "Apache 2.0": {
                    "license_desc": "<p> Denna licens ger dig rätt att fritt använda, modifiera och distribuera modellen, inklusive för kommersiella ändamål. Förutom användningsfrihet garanterar den rättsligt skydd genom att inkludera en patentklausul som fungerar som en försäkring: om du använder modellen samtycker bidragsgivarna till att inte stämma dig för brott mot deras patent relaterade till projektet. Detta ömsesidiga skydd undviker juridiska konflikter mellan användare och utvecklare. Vid distribution av modifierade versioner måste betydande ändringar meddelas, vilket säkerställer transparens för användaren. </p>"
                },
                "CC-BY-NC-4.0": {
                    "license_desc": "<p> Denna licens tillåter dig att fritt dela och anpassa innehållet så länge du anger upphovsmannen, men förbjuder all kommersiell användning. Den ger flexibilitet för icke-kommersiell användning samtidigt som skaparens rättigheter skyddas. </p>",
                    "reuse_specificities": "men endast för icke-kommersiellt bruk"
                },
                "Gemma": {
                    "license_desc": "<p>Denna licens är skapad för att uppmuntra att mjukvaran används, ändras och sprids, men innehåller en klausul som säger att alla ändrade versioner måste delas under samma licens, för att främja samarbete och transparens inom mjukvaruutveckling.</p>"
                },
                "Llama 3.1": {
                    "commercial_use_specificities": "under 700 miljoner användare",
                    "license_desc": "<p>Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.</p>"
                },
                "Llama 3.3": {
                    "commercial_use_specificities": "under 700 miljoner användare",
                    "license_desc": "Denna <p><strong>icke-exklusiva, globala, royaltyfria</strong> licens tillåter dig att fritt använda, reproducera, modifiera och distribuera Llama 3.3 med tillhörande kod och material, så länge upphovsmannen nämns. Den tillåter specifikt återanvändning för att förbättra derivatmodeller, men sätter begränsningar för mycket storskaliga kommersiella verksamheter.</p>"
                },
                "Llama 4": {
                    "commercial_use_specificities": "under 700 miljoner användare\n",
                    "license_desc": "<p>Denna icke-exklusiva, globala, royaltyfria licens tillåter dig att använda, reproducera, modifiera och distribuera Llama 4-materialet (modeller och dokumentation) förutsatt att upphovsmannen nämns. Den sätter dock två huvudsakliga begränsningar: (1) företag med över 700 miljoner månatliga aktiva användare måste erhålla en särskild licens från Meta, och (2) <strong>totalt uteslutande</strong> av EU-invånare och företag med huvudkontor i EU från att direkt använda de multimodala modellerna, på grund av regulatoriska osäkerheter relaterade till den europeiska AI-lagen. Europeiska slutanvändare kan ändå få tillgång till tjänster som integrerar Llama 4, förutsatt att de tillhandahålls utanför EU.</p>"
                },
                "MIT": {
                    "license_desc": "<p>MIT-licensen är en licens för fri programvara. Den tillåter att vem som helst återanvänder, ändrar eller sprider en modell, även för kommersiella ändamål, förutsatt att den ursprungliga licensen och upphovsrättstexten skickas med.</p>"
                },
                "Mistral AI Research License": {
                    "license_desc": "<p>Denna icke-exklusiva, royaltyfria licens tillåter användning, kopiering, modifiering och distribution av Mistral-modeller och deras derivat (inklusive modifierade versioner). Den är dock strikt begränsad till forskningsändamål.</p>",
                    "reuse_specificities": "men endast för icke-kommersiellt bruk"
                }
            },
            "proprio": {
                "Alibaba": {
                    "license_desc": "Modellen är tillgänglig under betald licens och åtkomlig via API på Alibabas företagsplattformar, vilket kräver betalning per användning baserat på antalet bearbetade tokens eller den reserverade infrastrukturen."
                },
                "Amazon": {
                    "license_desc": "Modellen är tillgänglig under betald licens och åtkomlig via Amazon Bedrock, vilket kräver betalning per användning baserat på antalet bearbetade tokens eller reserverad infrastruktur.",
                    "reuse_specificities": "förutom för att destillera eller träna andra modeller på Amazons plattformar."
                },
                "Anthropic": {
                    "license_desc": "Modellen är tillgänglig under betald licens och åtkomlig via API på Anthropic-företagets eller partnerföretagens plattformar, vilket kräver en betalning per användning baserat på antalet bearbetade tokens eller den reserverade infrastrukturen."
                },
                "Google": {
                    "license_desc": "Modellen är tillgänglig under betald licens och åtkomlig via API på Googles företagsplattformar, vilket kräver betalning per användning baserat på antalet bearbetade tokens eller reserverad infrastruktur.",
                    "reuse_specificities": "förutom för att träna andra modeller på Vertex AI"
                },
                "Mistral AI": {
                    "license_desc": "Modellen är tillgänglig under en betald licens och åtkomlig via Mistral API, Amazon Sagemaker, och flera andra värdar, vilket kräver betalning per användning baserat på antalet bearbetade tokens eller den reserverade infrastrukturen."
                },
                "OpenAI": {
                    "license_desc": "Modellen är tillgänglig under en betald licens och åtkomlig via API på OpenAI:s plattformar eller via Microsoft Azure-tjänster, vilket kräver en avgift per användning baserad på antalet bearbetade tokens eller den reserverade infrastrukturen."
                },
                "xAI": {
                    "license_desc": "Modellen är tillgänglig under en betald licens och åtkomlig via X och xAI, vilket kräver betalning per användning baserat på antalet bearbetade tokens eller reserverad infrastruktur."
                }
            }
        },
        "models": {
            "Aya Expanse 32B": {
                "desc": "<p>Mellanstor flerspråkig modell, som klarar 23 språk.</p>",
                "fyi": "<p>Cohere, det kanadensiska företaget bakom denna modell, grundades av tidigare Google Brain-forskare, inklusive Aidan Gomez, medförfattare till den berömda artikeln \"Attention Is All You Need\" som revolutionerade AI. Dess främsta särprägel ligger i dess exklusiva fokus på generativ AI för företag, särskilt reglerade sektorer som finans, sjukvård, tillverkning och energi, samt den offentliga sektorn. Företaget är också en pionjär inom flerspråkiga metoder och har ett ideellt forskningslabb för att stödja öppen innovation.\n</p><p>Denna modell utformades för att ge goda funktioner i vart och ett av de 23 språken i dess utbildningskorpus.</p>",
                "size_desc": "<p>Med 32 miljarder parametrar faller den här modellen i medelstor kategori. Den kan hanteras på en server med ett enda kraftfullt grafikkort, vilket hjälper till att hålla nere infrastrukturkostnaderna.</p>\n<p>Den har ett kontextfönster på upp till 130 000 tokens, vilket är användbart för att analysera långa dokument.</p>"
            },
            "Claude 3.7 Sonnet": {
                "desc": "<p>Mycket stor, multimodal och flerspråkig modell, effektiv för kodgenerering, med två svarslägen: Användaren kan välja mellan ett resonemangsläge, för mer djupgående svar, eller ett snabbläge, för att direkt generera det slutliga svaret.</p>",
                "fyi": "<p>Claude 4 Opus är den mest avancerade versionen av Claude 4-familjen. Den är optimerad för rå kraft och komplexa uppgifter som kräver långvarigt resonemang. Den kan bland annat arbeta med långsiktiga uppgifter (Anthropic hävdar att den kan arbeta självständigt i upp till sju timmar). Å andra sidan är Opus dyrare att använda, svarar långsammare och kräver mer resurser för att köras. </p>\n<p> Modellen har två användningslägen: ett reflektionsläge med stegvis resonemang för komplexa problem, och ett snabbläge för direkta svar. Till skillnad från andra modeller tränades inte resonemanget primärt på matematiska data, utan anpassades till verkliga användningsfall. </p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns bevis som tyder på att det är en mycket stor modell som kräver servrar med flera kraftfulla grafikkort för att köras. Tillgängliga uppskattningar bygger på indirekta index som inferenskostnader och svarslatens. Den har ett kontextfönster på upp till 200 000 tokens, vilket är lämpligt för att analysera långa dokument eller koddatabaser.</p>"
            },
            "Claude 4 Sonnet": {
                "desc": "<p>Mycket stor multimodal och flerspråkig modell, med två svarsmetoder: användaren kan välja mellan ett resonemangsläge, för mer djupgående svar, eller ett snabbt läge, för att direkt generera det slutliga svaret.</p>",
                "fyi": "<p>Claude 4 Sonnet är en mer kompakt version av Claude 4 Opus, optimerad för hastighet, effektivitet och tillgänglighet. Den är något mindre effektiv för uppgifter som kräver komplext resonemang i flera steg. Å andra sidan är den betydligt billigare, snabbare, kan generera längre texter och förbrukar mindre ström än Opus.</p>\n<p>Modellen erbjuder två användningslägen: ett reflektionsläge med stegvis resonemang för komplexa problem, och ett snabbläge för direkta svar. Till skillnad från andra modeller tränades inte resonemanget primärt på matematiska data, utan anpassades till verkliga användningsfall.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns bevis som tyder på att det är en mycket stor modell som kräver servrar med flera kraftfulla grafikkort för att köras. Tillgängliga uppskattningar bygger på indirekta index som inferenskostnader och svarslatens. Den har ett kontextfönster på upp till 200 000 tokens, vilket är lämpligt för att analysera långa dokument eller koddatabaser.</p>"
            },
            "Command A": {
                "desc": "<p>Stor modell, effektiv för programmering, användning av externa verktyg, och \"retrieval augmented generation\" (RAG).</p>",
                "fyi": "<p>Cohere, det kanadensiska företaget bakom denna modell, grundades 2019 av tidigare Google Brain-forskare, inklusive Aidan Gomez, medförfattare till den berömda artikeln <a href=\"https://arxiv.org/abs/1706.03762\">\"Attention Is All You Need\"</a> som publicerades 2017 och som revolutionerade AI. Företaget utmärker sig för sitt exklusiva fokus på generativ AI för företag, särskilt reglerade sektorer som finans, sjukvård, tillverkningsindustri och energi, samt den offentliga sektorn. Företaget är också en pionjär inom flerspråkiga metoder och har ett ideellt forskningslabb för stödja öppen innovation.</p>\n<p>Denna modell är utformad för att fungera på fler än 23 språk och för att enkelt integreras i företagssystem. Det är en av få modeller som distribueras under <strong>CC-BY-NC 4.0-licensen, som tillåter delning och modifiering men förbjuder all kommersiell användning.</strong> Detta licensval återspeglar Coheres önskan att bidra till forskning och öppen källkod, samtidigt som man bibehåller kontrollen över kommersiell användning för att skydda sin affärsmodell. Detta utesluter till exempel integrationen av modellen i produkter eller tjänster som säljs av ett företag till kunder, men tillåter akademisk användning, testning eller interna projekt, begränsade till ett icke-kommersiellt ramverk.</p>",
                "size_desc": "<p>Med 111 miljarder parametrar är den här modellen att betrakta som stor. Den kräver minst två kraftfulla grafikkort, vilket resulterar i betydande driftskostnader.</p>\n<p>Dess kontextfönster når 256 000 tokens, lämpligt för analysera stora mängder dokument eller kodbaser.</p>"
            },
            "Command R": {
                "desc": "<p>Medelstor modell optimerad för syntes, allmänna frågor och verktygsanvändning, och effektiv i system för retrieval augmented generation (RAG).</p>",
                "fyi": "<p>Cohere, det kanadensiska företaget bakom denna modell, grundades 2019 av tidigare Google Brain-forskare, inklusive Aidan Gomez, medförfattare till den berömda artikeln \"Attention Is All You Need\" som revolutionerade AI. Det som gör det unikt är dess exklusiva fokus på generativ AI för företag, särskilt reglerade sektorer som finans, sjukvård, tillverkningsindustri och energi, samt den offentliga sektorn. Företaget är också en pionjär inom flerspråkiga metoder och har ett ideellt forskningslabb för att stödja öppen innovation.</p>\n<p>Denna modell har utvärderats på över 10 språk. Dess kontextfönster når 128 000 tokens, vilket underlättar analysen av långa dokument. Detta fönster fördubblades i nästa version av modellen (Command A).</p>",
                "size_desc": "<p>Med 35 miljarder parametrar faller den här modellen i kategorin medelstora modeller. Den kan hanteras på en server med ett enda kraftfullt grafikkort, vilket hjälper till att hålla nere infrastrukturkostnaderna.</p>"
            },
            "DeepSeek R1": {
                "desc": "<p>Mycket stor modell med hög prestanda för matematiska, vetenskapliga och programmeringsrelaterade uppgifter, som simulerar ett resonemangssteg innan svaret genereras.</p>",
                "fyi": "<p>Denna modell är baserad på en Mixture of Experts (MoE)-arkitektur med 61 lager. Den har totalt 671 miljarder parametrar, varav 37 miljarder är token-aktiverade. Träningen använde storskalig förstärkningsinlärning, med flera SFT-steg (<em>supervised fine-tuning</em>, där modellen lär sig av exempel på korrekta svar) och bootstrap-data.</p>",
                "size_desc": "<p>Med 671 miljarder parametrar är DeepSeek R1 en mycket stor modell som kräver flera kraftfulla grafikkort för att köras. Resonemangsmodeller av den här typen tar längre tid att producera ett svar, vilket ökar energiförbrukningen. Arkitekturen Mixture of Experts aktiverar dock bara en del av parametrarna vid varje token, vilket begränsar dess energiförbrukning. Kontextfönstret når 128 000 tokens, vilket är lämpligt för att analysera långa dokument.</p>"
            },
            "DeepSeek R1 Llama 70B": {
                "desc": "<p>Stor modell baserad på Meta Llama 3.3 70B, omtränad med resonemangsexempel från DeepSeek R1-modellen. Den erbjuder bra matematik- och kodningsfunktioner.</p>",
                "fyi": "<p>Modellen tränades inte från grunden. Den använder Llama 3.3 70B, omtränad med hjälp av resultat genererade av DeepSeek R1. Denna process ger Llama 3.3 70B möjligheten att simulera resonemang, utan att användaren kan välja om den här funktionen skulle aktiveras eller inte.</p>\n<p>I enlighet med Llama 3.3-licensen måste företaget behålla omnämnandet av källmodellen i modellens namn. Medellen faller under samma licensieringsregler.</p>",
                "size_desc": "<p>Med 70 miljarder parametrar klassificeras denna modell som en stor modell. Den kräver flera kraftfulla grafikkort för att köras, vilket resulterar i höga driftskostnader. Resonemangsmodeller tar också längre tid att producera ett svar, vilket ökar deras energiförbrukning.</p>\n<p>Kontextfönstret är 16 000 tokens, vilket kan vara begränsande för att analysera mycket stora dokument.</p>"
            },
            "DeepSeek V3": {
                "desc": "<p>Mycket stor modell utformad för komplexa uppgifter: kodgenerering, verktygsanvändning, och analys av långa dokument. Den kan hantera många språk, men är särskilt väl lämpad för engelska och kinesiska.</p>",
                "fyi": "<p>Denna modell är baserad på en Mixture of Experts (MoE)-arkitektur med 671 miljarder parametrar, men aktiverar endast 37 miljarder per genererad token. Den är effektiv för verktygsanrop, generering av strukturerad utdata (JSON) och kodgenerering.</p>",
                "size_desc": "<p>DeepSeek V3 är en mycket stor modell som kräver flera grafikkort för att köras. Arkitekturen Mixture of Experts (MoE) tillåter dock bara att en del av parametrarna aktiveras, vilket minskar minneskraven jämfört med en tät modell av samma storlek.</p>\n<p> Kontextfönstret når 163 000 tokens, vilket är användbart för att analysera långa dokument.</p>"
            },
            "GLM 4.5": {
                "desc": "<p>En mycket stor modell skapad av Zhipu AI, en kinesisk AI-modelltillverkare som grundades 2019 av professorer vid universitetet i Tsinghua och stöds av stora aktörer som Alibaba och Tencent. Modellen har två svarslägen: Användaren kan välja mellan ett resonemangsläge för mer djupgående svar, eller ett snabbläge för att direkt generera det slutliga svaret.</p>",
                "fyi": "<p>Denna modell har goda agentfunktioner, vilket gör att den kan göra funktionsanrop med hög tillförlitlighet. Dess kodningsprestanda är hög, och modellen har en god förmåga att skapa kompletta webbapplikationer och generera artefakter, vilket är program med en enda fil som kan användas i gränssnitten för konversationsagenter. För träning utformades en specifik infrastruktur för förstärkningsinlärning, kallad slime, för att optimera prestandan för komplexa uppgifter och agentuppgifter genom att effektivt hantera långa arbetsflöden – modellen kan hantera komplexa och långvariga uppgifter, som att skapa en applikation från grunden, utnyttja sina verktyg på bästa sätt och förbli konsekvent från början till slut.</p>",
                "size_desc": "<p>Med 355 miljarder parametrar faller den här modellen inom kategorin mycket stora modeller. Tack vare en Mixture of Experts (MoE)-arkitektur är den effektivare än vissa andra modeller av liknande storlek, men den kräver fortfarande en server med flera mycket kraftfulla grafikkort. Dess kontextfönster går upp till 128 000 tokens, vilket gör att den kan bearbeta ganska långa dokument.</p>"
            },
            "GPT 4.1 Nano": {
                "desc": "<p>Mindre, lättare version av GPT 4.1-modellen, utformad för att hålla nere kostnaderna samtidigt som den förblir konkurrenskraftig på de flesta uppgifter. Modellen stöder mycket långa frågor, vilket gör den lämplig för användning vid exempelvis korpusanalys med långa dokument.</p>",
                "fyi": "<p>Detta är en destillerad version av en större modell, med viss kunskapsöverföring. Den kan bearbeta text, bilder och ljud. Dess kontextfönster kan nå upp till 1 miljon tokens, vilket gör den särskilt lämplig för att analysera textkorpusar eller mycket långa koddatabaser.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns bevis som tyder på att det är en medelstor modell som kräver ett kraftfullt grafikkort för att fungera. Den förmodade arkitekturen Mixture of Experts (MoE) aktiverar dock endast en delmängd av parametrar vid varje token, vilket begränsar dess energiavtryck. Tillgängliga uppskattningar förlitar sig på indirekta index som inferenskostnader och svarslatens.</p>"
            },
            "GPT 5": {
                "desc": "<p>GPT-5 är inte en enda modell, utan ett enhetligt system som består av två separata modeller: en snabb modell (<code>gpt-5-main</code>) för vanliga frågor och en resonemangsmodell (<code>gpt-5-thinking</code>) för komplexa problem. Jämfört med dess föregångare hävdar OpenAI att den är mer användbar i verkliga frågor, med märkbara förbättringar inom områdena skrivande, kodning och hälsa. Den minskar också mängden hallucinationer. Tack vare sitt kontextfönster på 400 000 tokens kan den acceptera långa frågor, vilket gör det möjligt att analysera flera dokument samtidigt.</p>",
                "fyi": "<p>Utvecklare som använder den här modellen kan konfigurera en utförlighetsparameter för att justera längden på resonemangsfasen. </p>\n<p> När det gäller säkerhet använder systemet en ny metod som kallas \"safe-completions\" för att förhindra obehörigt innehåll i svarsfasen snarare än när begäran görs. Modellens skapare använde också en \"resonemangsträningsfas\" för att göra den mer motståndskraftig mot försök att kringgå deras säkerhetsregler (<em>jailbreaking</em>).</p>",
                "size_desc": "<p>GPT-5-systemet består av modeller i olika storlekar, men de exakta storlekarna är okända. Dess arkitektur är utformad för att inkludera flera modeller, sammankopplade av ett internt routingsystem, som väljer den minsta modellen som passar uppgiften för att optimera resonemangets hastighet och djup. Arkitekturen är sannolikt baserad på en \"mix of experts\" (MoE), vilket innebär att endast en del av parametrarna aktiveras för varje fråga. Detta möjliggör större energieffektivitet och hög prestanda. Tillgängliga uppskattningar av modellstorlekar baseras på offentlig information och indirekta index såsom inferenskostnader och svarslatens.</p>"
            },
            "GPT 5 Mini": {
                "desc": "<p>GPT-5 Mini är en lättviktsversion av huvudmodellen för GPT-5. Den är utformad för användning i miljöer där kostnadsbegränsningar krävs, till exempel i stor skala. Dess resonemangsmodell presterar nästan lika bra som huvudmodellen (<code>gpt-5-thinking</code>) trots sin mindre storlek. Tack vare sitt kontextfönster med 400 000 tokens kan den hantera långa frågor, vilket gör det möjligt att analysera flera dokument samtidigt.</p>",
                "fyi": "<p>Systemet använder en ny säkerhetsmetod som kallas \"safe-completions\" för att förhindra obehörigt innehåll i svarsfasen snarare än när förfrågan görs.</p>\n<p>Även om det är en mindre version är den mycket konkurrenskraftig mot den ledande GPT-5-modellen på många riktmärken, särskilt inom det medicinska området.</p>",
                "size_desc": "<p>Mini-modellen är en mer kompakt (medelstor, enligt uppskattningar) version av GPT-5-systemet. Den är utformad för en bra balans mellan prestanda och kostnad, tack vare ett routingsystem som väljer metod för specifika uppgifter. Arkitekturen är sannolikt baserad på en \"mix of experts\" (MoE), vilket innebär att endast en del av parametrarna aktiveras för varje fråga. Modellerna är dock sannolikt mycket stora och kräver flera kraftfulla grafikkort för inferens.</p>"
            },
            "GPT 5 Nano": {
                "desc": "<p>GPT-5 Nano är den minsta och snabbaste versionen av GPT-5s resonemangsmodell. Den är utformad för sammanhang där mycket låg latens eller kostnad krävs. Tack vare sitt kontextfönster på 400 000 tokens kan den acceptera långa frågor, vilket gör det möjligt att analysera flera dokument samtidigt.</p>",
                "fyi": "<p>Systemet använder en ny säkerhetsmetod som kallas \"safe-completions\" för att förhindra obehörigt innehåll i svarsfasen snarare än när förfrågan görs.</p>",
                "size_desc": "<p>Nano-modellen är den mest kompakta i GPT-5-familjen (den hamnar en ligt uppskattningar i kategorin små modeller). Den väljs av routingsystemet för frågor som kräver mycket låg latens och omedelbara svar. Dess arkitektur är sannolikt baserad på en \"Mixture of Experts\" (MoE), vilket möjliggör bättre energieffektivitet och prestanda, även på frågor som kräver snabba svar.</p>"
            },
            "GPT OSS-120B": {
                "desc": "<p>Den större av OpenAI:s två första halvöppna modeller sedan GPT-2. Den skapades som svar på det växande antalet aktörer som använder öppen källkod, däribland Meta (LLaMA) och Mistral, och är en kraftfull resonemangsmodell, särskilt för komplexa uppgifter och i \"agentiska\" miljöer. </p>",
                "fyi": "<p>Den här modellen kan köras på en enda 80 GB GPU (som NVIDIA H100). Den har ett kontextfönster på 131 000 tokens, vilket gör den lämplig för att analysera stora dokument.</p>\n<p> I modellkonfigurationerna är det möjligt att välja mellan tre resonemangsnivåer (<em>låg</em>, <em>medel</em> och <em>hög</em>) vilket avgör modellens utförlighet.</p>",
                "size_desc": "<p>Arkitekturen är baserad på principen \"mixture of experts\" (MoE), vilket möjliggör större energieffektivitet genom att endast aktivera en del av parametrarna (5,1 miljarder per token) för varje fråga. Det är en resonemangsmodell, så dess energiförbrukning är högre eftersom den genererar en intern tankekedja innan den ger det slutgiltiga svaret. Den har ett kontextfönster på 131 000 tokens, vilket gör den lämplig för att analysera stora dokument.</p>"
            },
            "GPT OSS-20B": {
                "desc": "<p>Den mindre av OpenAI:s två halvöppna modeller. Den designades som svar på konkurrens från öppen källkod och är avsedd för användningsfall som kräver låg latens samt lokala eller specialiserade fall.</p>",
                "fyi": "<p>Den här modellen kan köras lokalt på en avancerad bärbar dator med endast 16 GB VRAM (eller system-RAM). Detta gör den till ett mycket tillgängligt alternativ för utvecklare.</p>\n<p> I modellkonfigurationen är det möjligt att välja mellan tre resonemangsnivåer (<em>låg</em>, <em>medel</em> och <em>hög</em>) vilket avgör modellens utförlighet.</p>",
                "size_desc": "<p>Med 20 miljarder parametrar tillhör denna modell kategorin medelstora modeller. Arkitekturen är baserad på \"mixture of experts\" (MoE), vilket möjliggör större energieffektivitet genom att endast aktivera en del av parametrarna (3,6 miljarder per token) för varje fråga. Det är en resonemangsmodell, vilket resulterar i högre energiförbrukning eftersom den genererar en intern tankekedja innan den ger det slutgiltiga svaret. Den har ett kontextfönster på 131 000 tokens, vilket gör den lämplig för att analysera stora dokument.</p>"
            },
            "GPT-4.1 Mini": {
                "desc": "<p>En lättviktsversion av GPT 4.1, men fortfarande i kategorin stora modeller, utformad för att hålla nere kostnaderna samtidigt som den förblir konkurrenskraftig på de flesta uppgifter. Modellen stöder mycket långa frågor, vilket gör den lämplig för användning vid dokumentkorpusanalys.</p>",
                "fyi": "<p>Detta är en destillerad version av en större modell, med partiell kunskapsöverföring. Den kan bearbeta text, bilder och ljud. Dess kontextfönster kan nå upp till 1 miljon tokens, vilket gör den särskilt lämplig för att analysera mycket långa korpusar eller koddatabaser.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns tecken på att det är en medelstor modell som kräver ett kraftfullt grafikkort för att fungera. Den förmodade arkitekturen Mixture of Experts (MoE) aktiverar dock endast en delmängd av parametrar vid varje token, vilket begränsar dess energiavtryck. Tillgängliga uppskattningar förlitar sig på indirekta index som inferenskostnader och svarslatens.</p>"
            },
            "Gemini 2.5 Flash": {
                "desc": "<p>Stor multimodal och flerspråkig modell med två svarsmetoder: Användaren kan välja mellan ett resonemangsläge för mer djupgående svar, eller ett snabbläge för att generera det slutliga svaret direkt.</p>",
                "fyi": "<p>Denna modell är baserad på en \"Mixture of Experts\"-arkitektur (MoE) och har destillerats genom att endast behålla en approximation av förutsägelserna från lärarmodellen – Gemini 2.5 Pro. Den har tränats på en TPUv5p-arkitektur som innehåller möjligheten att fortsätta träningen automatiskt även vid träningsfel, datakorruption eller minnesproblem.</p>\n<p>Gemini 2.5 Flash stöder kontexter med upp till 1 miljon tokens och tre timmar videoinnehåll. Optimerad bildbehandling möjliggör bearbetning av ungefär tre gånger längre video i samma kontextfönster; endast 66 visuella tokens behövs för att generera en bildruta, jämfört med 258 tidigare. Denna modell har också inbyggt stöd för att generera ljud för dialog och talsyntes.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns bevis som tyder på att det är en stor modell som kräver flera kraftfulla grafikkort för att köras. Arkitekturen Mixture of Experts (MoE) aktiverar dock bara en del av parametrarna vid varje token, vilket begränsar dess energiförbrukning. Tillgängliga uppskattningar förlitar sig på indirekta index som inferenskostnader och svarslatens. Dess kontextfönster sträcker sig upp till 1 miljon tokens, vilket gör att den kan bearbeta mycket stora dokumentkorpusar.</p>"
            },
            "Gemma 3 12B": {
                "desc": "<p>Liten multimodal modell lämplig för vanliga uppgifter som frågor och svar, sammanfattningar och bildtolkning.</p>",
                "fyi": "<p>Den bearbetar text och bilder och kan köras lokalt på kraftfulla bärbara datorer eller servrar med bara ett grafikkort. Den har tränats för att kunna interagera med externa verktyg (till exempel webbsökning) via funktionsanrop, vilket gör den användbar i agentsammanhang.</p>",
                "size_desc": "<p>Med 12 miljarder parametrar är det en av de minsta modellerna. Den kan användas lokalt på en arbetsstation för att bevara datasekretessen, eller på en billig server för att begränsa kostnaderna jämfört med en större modell.</p>\n<p>Dess kontextfönster kan innehålla upp till 128 000 tokens, vilket gör det enkelt att bearbeta långa dokument.</p>"
            },
            "Gemma 3 27B": {
                "desc": "<p>Medelstor, multimodal modell lämplig för vanliga uppgifter som frågor och svar, sammanfattningar och bildtolkning.</p>",
                "fyi": "<p>Den kan bearbeta text och bilder på en server utrustad med ett enda kraftfullt grafikkort. Den har tränats för att kunna interagera med externa verktyg (internetsökning etc.) via funktionsanrop, vilket gör den användbar i agentsammanhang.</p>",
                "size_desc": "<p>Med 27 miljarder parametrar tillhör den kategorin medelstora modeller. Den kan driftsättas på en server med ett enda grafikkort.</p>\n<p>Den accepterar kontexter på upp till 128 000 tokens, vilket gör den lämplig för att analysera långa dokument.</p>"
            },
            "Gemma 3 4B": {
                "desc": "<p>Mycket liten, kompakt, multimodal modell lämplig för vanliga uppgifter som frågor och svar, sammanfattningar och bildtolkning.</p>",
                "fyi": "<p>Den kan bearbeta text och bilder medan den körs på enheter med låg strömförbrukning, inklusive telefoner och läsplattor. Den har tränats att interagera med externa verktyg (till exempel webbsökning) via funktionsanrop, vilket gör den användbar i agentsammanhang.</p>",
                "size_desc": "<p>Med 4 miljarder parametrar är det en av de allra minsta modellerna. Den kan användas lokalt för att bevara datasekretessen, eller på en server för att begränsa kostnaderna jämfört med en större modell.</p>\n<p>Dess kontextfönster kan nå 128 000 tokens, vilket gör att den kan analysera långa dokument.</p>"
            },
            "Gemma 3n 4B": {
                "desc": "<p>Mycket liten, kompakt modell med flera lägen, utformad för att köras lokalt på en dator eller smartphone, utan behov av en server. Den kan anpassa sin strömförsörjning efter kapacitet och behov.</p>",
                "fyi": "<p>Denna modell kan bearbeta text, bilder och ljud. Den är baserad på MatFormer-arkitekturen och ett PLE-cachesystem (per-layer embeddings), vilket endast aktiverar de användbara parametrarna beroende på uppgiften och anpassar sig till kapaciteten hos de maskiner som modellen körs på.</p>",
                "size_desc": "<p>Med 4 miljarder parametrar är det en av de minsta modellerna som finns tillgängliga. Den kan användas lokalt på en dator eller smartphone för att upprätthålla datasekretessen, eller på en server för att begränsa kostnaderna jämfört med en större modell.</p>\n<p>Dess kontextfönster går upp till 32 000 tokens.</p>"
            },
            "Grok 3 Mini": {
                "desc": "<p>En lättare version av Grok 3-modellen, vilket minskar kostnaderna samtidigt som den bibehåller god prestanda för många uppgifter. Den kan simulera en resonemangsfas innan den ger ett slutgiltigt svar.</p>",
                "fyi": "<p>Grok 3 Mini är en destillerad version av Grok 3. Den är närliggande vad gäller funktioner, samtidigt som den är snabbare och billigare.\nModellen erbjuder två lägen: ett tankeläge med stegvis resonemang för komplexa problem, och ett snabbläge för omedelbara svar.\nDess kontextfönster når 131 000 tokens, vilket gör den lämplig för att analysera långa dokument.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Trots namnet är Grok 3 Mini sannolikt en mycket stor modell som kräver flera kraftfulla grafikkort för att köras. Dessutom innehåller den en valfri resonemangsfas som involverar längre genereringstider och därmed högre strömförbrukning. Den förmodade Mixture of Experts (MoE)-arkitekturen aktiverar dock bara en del av parametrarna vid varje token, vilket begränsar dess energiavtryck. Tillgängliga uppskattningar förlitar sig på indirekta index som inferenskostnader och svarslatens.</p>"
            },
            "Hermes 3 405B": {
                "desc": "<p>Mycket stor modell omtränad från Llama 3.1 405B, justerad för att bättre möta användarnas krav och underlätta användningen av externa verktyg.</p>",
                "fyi": "<p> Denna modell är resultatet av omträning av parameteruppsättningen för Llama 3.1 405B för att göra dess beteende mindre begränsat och bättre ta hänsyn till nyanserna i användar- och systemprompter, vilket ger användaren större kontroll över modellens \"personlighet\" och beteende. Specifika resonemangsfunktioner som <strong><code>&lt;SCRATCHPAD&gt;</code></strong>, <strong><code>&lt;REASONING&gt;</code></strong> och <strong><code>&lt;THINKING&gt;</code></strong> har lagts till för att simulera resonemang kring komplexa uppgifter. Träningen använde verktyget AdamW (inlärningshastighet på 3.5×10⁻⁶), vilket hjälper modellen att lära sig effektivt genom att gradvis justera dess parametrar. Sedan finjusterades den med en metod som kallas DPO (direkt preferensoptimering), vilket förbättrar dess svar baserat på specifika preferenser. För att göra denna träning enklare och snabbare användes LoRA-adaptrar; dessa är mindre moduler som bara modifierar en del av modellen, vilket undviker behovet av att omarbeta alla parametrar samtidigt.</p>",
                "size_desc": "<p>Med 405 miljarder parametrar hamnar den här modellen i kategorin mycket stora modeller. Den kräver en server utrustad med flera kraftfulla grafikkort, vilket resulterar i betydande driftskostnader.</p>"
            },
            "Kimi K2": {
                "desc": "<p>Kimi K2, som utvecklades av Moonshot AI (亦称「月之暗面」/ Yue Zhi An Mian), ett Pekingbaserat företag, är en mycket stor kodorienterad och agentbaserad modell. Den är känd för utvecklingsuppgifter i agentiska sammanhang (t.ex. i Cursor eller Windsurf). Den har inte ett explicit \"resonemangsläge\", men för stora uppgifter delar den upp sitt svar i steg och växlar mellan åtgärder (verktygsanrop) och textskrivning.</p>",
                "fyi": "<p>För att stabilisera träning i mycket stor skala introducerade Moonshot AI MuonClip, en \"hastighetsbegränsare\" för träningsfasen som möjliggör träning av en modell av denna storlek och på en samling av 15,5 biljoner tokens utan att träningen spårar ur.</p>\n<p>På datasidan har K2 ofta arbetat med en \"simulator\" med riktiga verktyg (webbläsare, terminal, kodexekverare, API:er etc.). På samma sätt som en pilot i en simulator lär sig modellen genom att planera, försöka, misslyckas, och försöka igen, och att kedja ihop flera åtgärder för att uppnå ett mål. Som ett resultat är den särskilt bra på att kombinera verktyg och utföra uppgifter i flera steg.</p>",
                "size_desc": "<p>Med 1 biljon parametrar är den här modellen en av de största. Tack vare en Mixture of Experts-arkitektur (MoE) är den effektivare än vissa andra modeller av liknande storlek, men den kräver fortfarande en server med flera mycket kraftfulla grafikkort. Dess kontextfönster går upp till 128 000 tokens, vilket gör att den kan bearbeta ganska långa dokument.</p>"
            },
            "Llama 3.1 405B": {
                "desc": "<p>Mycket stor modell utformad för komplexa eller specialiserade uppgifter. Används ofta som en \"lärarmodell\" för att träna mer specialiserade modeller.</p>",
                "fyi": "<p>Modellen tränades på en samling av 15 biljoner tokens med 16 000 H100-grafikkort (ett av de kraftfullaste grafikkorten på marknaden år 2025). Träningen kombinerade generering av syntetisk data och direkt preferensoptimering (DPO). Denna modell används ofta för att generera syntetisk data för att träna mindre modeller. Modellen använder 8-bitarskomprimering som standard för att minska minneskraven och möjliggöra exekvering på en enda kraftfull server.</p>",
                "size_desc": "<p>Med 405 miljarder parametrar hamnar den här modellen i kategorin mycket stora modeller. Den kräver en server utrustad med flera kraftfulla grafikkort, vilket resulterar i betydande driftskostnader. Modellen har ett kontextfönster på upp till 128 000 tokens, vilket gör den lämplig för analys av långa dokument.</p>"
            },
            "Llama 3.1 8B": {
                "desc": "<p>Liten modell designad för lokal användning på en bärbar dator, som erbjuder bra funktioner för textsyntes och enkla svar.</p>",
                "fyi": "<p>Denna modell är en destillerad version av de större Llama 3-modellerna. Den tränades genom att man överförde en del av kunskapen från de större modellerna.</p>",
                "size_desc": "<p>Med 8 miljarder parametrar är denna modell en av de minsta modellerna. Den kan köras lokalt på en kraftfull dator, vilket säkerställer datakonfidentialitet, eller lagras på en server utrustad med ett enda grafikkort, vilket begränsar infrastrukturkostnaderna. Dess kontextfönster på 128 000 tokens gör att den kan bearbeta långa dokument.</p>"
            },
            "Llama 3.3 70B": {
                "desc": "<p>Stor modell designad för en mängd olika uppgifter och som kan konkurrera med större modeller.</p>",
                "fyi": "<p>Denna modell är en destillerad version av 405B-modellen, som den har att tacka för en del av sin överförda kunskap. Den har också dragit nytta av nya tekniker för anpassning och förstärkningsinlärning med onlinemiljöer (online reinforcement learning). Modellen tränas genom att låta den försöka utföra onlineuppgifter autonomt. Dess träning är baserad på 15 miljarder tokens.</p>",
                "size_desc": "<p>Med 70 miljarder parametrar tillhör den här modellen kategorin stora modeller. Den kräver flera kraftfulla grafikkort för att köras, vilket resulterar i betydande driftskostnader. Dess kontextfönster på 128 000 tokens gör att den kan bearbeta långa dokument.</p>"
            },
            "Llama 4 Scout": {
                "desc": "<p>Stor modell med ett mycket stort kontextfönster, lämplig för att till exempel sammanfatta en uppsättning dokument.</p>",
                "fyi": "<p>Denna modell samdestillerades med Behemoth, vilket innebär att den lärde sig tillsammans med den större modellen, inte efteråt som i en traditionell destillation. Den tränades på 30 biljoner tokens, och kombinerade text på 200 språk och bilder för sina inbyggda multimodala funktioner. Arkitekturen är baserad på ett Mixture of Experts (MoE)-system, med 17 miljarder aktiva parametrar, 16 experter och 109 miljarder parametrar totalt. För att balansera multimodal prestanda, resonemang och konversationskvalitet utvecklade Meta-teamet en progressiv strategi efter träning, som kombinerade adaptiv datafiltrering (för att bara behålla den mest komplexa och intressanta informationen), riktad finjustering och online-förstärkningsinlärning – modellen lärde sig genom att försöka utföra onlineuppgifter autonomt. Tack vare iRoPE-arkitekturen kan den hantera mycket långa kontextfönster, upp till 10 miljoner tokens, och kan bearbeta upp till 8 bilder samtidigt.</p>\n<p>Modellen mottogs väl vid lanseringen, särskilt för sitt imponerande kontextfönster, en nyhet inom området, samt dess kostnadseffektivitet för uppgifter som sammanfattning, verktygsanrop och förstärkt generering (RAG). Detta gör den till ett lämpligt val för automatiserade pipelines.</p>",
                "size_desc": "<p>Med 109 miljarder parametrar faller den här modellen inom kategorin stora modeller. Tack vare en Mixture of Experts (MoE)-arkitektur kan den dock hanteras på en server med ett enda högpresterande grafikkort . Dess kontextfönster kan innehålla upp till 10 miljoner tokens, vilket gör den lämplig för bearbetning av extremt långa dokumentkorpusar.</p>"
            },
            "Llama Maverick": {
                "desc": "<p>Mycket stor modell med ett mycket stort kontextfönster, lämplig till exempel för att sammanfatta flera dokument samtidigt.</p>",
                "fyi": "<p>Denna modell samdestillerades med Behemoth, vilket innebär att den lärde sig tillsammans med den större modellen, snarare än efteråt, som vid traditionell destillation. Detta möjliggör snabbare och mindre beräkningskrävande överföring av färdigheter. Den tränades på 30 biljoner tokens, och kombinerade text på 200 språk och bilder för sina inbyggda multimodala funktioner – den kan bearbeta upp till 8 bilder samtidigt. Arkitekturen är baserad på ett Mixture of Experts (MoE)-system, med 17 miljarder aktiva parametrar, 16 experter och 109 miljarder parametrar totalt. Meta-teamet utvecklade en progressiv strategi efter träning, som kombinerar adaptiv datafiltrering (att endast behålla de mest komplexa och intressanta uppgifterna), riktad finjustering och online-förstärkningsinlärning för att balansera multimodal prestanda, resonemang och konversationskvalitet. Tack vare iRoPE-arkitekturen kan den hantera mycket långa kontextfönster, upp till 10 miljoner tokens.</p>\n<p>Llama 4 Maverick-modellen presenterades som Metas direkta svar på DeepSeek-modellerna, men när den släpptes ansåg många användare att den inte levde upp till förväntningarna, särskilt inte för programmeringsuppgifter och kreativt arbete.</p>",
                "size_desc": "<p>Med 400 miljarder parametrar faller den här modellen inom kategorin stora modeller. Tack vare arkitektur baserad på \"mixture of experts\" (MoE) kräver den dock färre resurser att köra än \"täta\" modeller av samma storlek. Dess kontextfönster sträcker sig upp till 1 miljon tokens, vilket gör att den kan bearbeta mycket stora korpusar.</p>"
            },
            "Magistral Medium": {
                "desc": "<p>Medelstor, multimodal och flerspråkig resonemangsmodell. Lämplig för programmeringsuppgifter eller andra uppgifter som kräver djupgående analys, förståelse för komplexa logiska system eller planering – till exempel för agentanvändningsfall eller för att skriva långa, komplexa texter.</p>",
                "fyi": "<p>Denna modell är en del av den första generationen av Mistral AI-resonemangsmodeller (sommaren 2025). Till skillnad från de flesta andra resonemangsmodeller kan denna modell resonera på flera språk, inklusive engelska, franska, spanska, tyska, italienska, arabiska, ryska och förenklad kinesiska. Den tränades med förstärkningsinlärning på Mistral Medium 3 och destillerades inte från befintliga resonemangsmodeller. Denna modell ärver de multimodala funktionerna hos Mistral Medium 3, även om förstärkningsinlärning endast utfördes på text.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns tecken på att det är en stor modell som kräver flera kraftfulla grafikkort för att fungera. Resonemangsmodeller kräver mer datorkraft för att producera ett svar, vilket ökar deras strömförbrukning. Uppskattningar bygger på indirekta index som inferenskostnader och svarslatens.</p>\n<p>Modellen har ett kontextfönster på upp till 40 000 tokens, användbart för att analysera korta dokument men inte tillräckligt för att analysera stora korpusar.</p>"
            },
            "Magistral Small": {
                "desc": "<p>Medelstor, multimodal och flerspråkig resonemangsmodell. Lämplig för uppgifter som kräver djupgående analys, förståelse för logiska system eller planering – till exempel för agentanvändningsfall eller för att skriva långa, komplexa texter.</p>",
                "fyi": "<p>Denna modell är en del av Mistral AI:s första generation av resonemangsmodeller (sommaren 2025). Till skillnad från </p> flesta andra resonemangsmodeller kan den här modellen resonera på flera språk, inklusive engelska, franska, spanska, tyska, italienska, arabiska, ryska och förenklad kinesiska.\n<p> Träningen genomfördes i två faser. Den första, kallad <em>kallstart</em> genom destillation (från Mistral Medium 3 och OpenThoughts/OpenR1), gör det möjligt för modellen att bygga upp grundläggande resonemangsförmågor från allmän instruktionsdata (10%). Den andra är en fas med förstärkningsinlärning (RL, <em>reinforcement learning</em>) med hög entropi, där modellen uppmuntras att utforska olika och varierade lösningar snarare än att konvergera kring ett enda svar, och att generera långa kompletteringar (upp till 32 000 tokens), vilket möjliggör utveckling av resonemangsförmågor som överstiger undervisningsmodellens. </p>",
                "size_desc": "<p>Med 24 miljarder parametrar betraktas denna modell som en medelstor modell. Den kräver ett enda kraftfullt grafikkort för att köras. Resonemangsmodeller tar också längre tid att producera ett svar, vilket ökar deras strömförbrukning.</p>\n<p> Den har ett kontextfönster på upp till 40 000 tokens, användbart för att analysera korta dokument men otillräckligt för att analysera stora korpusar.</p>"
            },
            "Ministral": {
                "desc": "<p>Liten flerspråkig modell utformad för att köras på en bärbar dator utan serveranslutning, samtidigt som den erbjuder bra funktioner för textsyntes, enkla frågor och verktygsanvändning.</p>",
                "fyi": "<p>Denna modell använder en GQA-metod (Grouped query attention) för att begränsa den analyserade texten vid varje generationssteg och förbättra hastighet och minnesanvändning. Beräkningstiderna minskas utan att kvaliteten påverkas. Uppmärksamhetsmekanismen förbättras genom att man använder fönster i olika storlekar, vilket möjliggör hantering av stora kontexter (upp till 128 000 tokens) samtidigt som modellen förblir lättviktig. Den stora tokeniseraren (V3-Tekken) komprimerar språk och kod bättre, vilket förbättrar dess prestanda för flerspråkiga uppgifter.</p>",
                "size_desc": "<p>Med sina 8 miljarder parametrar tillhör den här modellen kategorin små modeller (mellan 7 och 20 miljarder parametrar). Den kan köras lokalt på en ganska kraftfull dator, vilket säkerställer datakonfidentialitet, eller på en server med ett enda grafikkort för att begränsa infrastrukturkostnaderna.</p>"
            },
            "Mistral Large 2": {
                "desc": "<p>Stor modell utformad för att hantera komplexa frågor och uppgifter, till exempel kodgenerering, verktygsanvändning, analys av långa dokument eller domänspecifik språkförståelse.</p>",
                "fyi": "<p>Denna modell tränades med en hög andel koddata (över 80 programmeringsspråk) och matematik, vilket förbättrar dess förmåga att lösa komplexa problem och använda externa verktyg.</p>",
                "size_desc": "<p>Med 123 miljarder parametrar tillhör den här modellen kategorin stora modeller. Den kräver en server utrustad med minst ett kraftfullt grafikkort, vilket innebär en betydande driftskostnad. Den har ett kontextfönster på upp till 128 000 tokens, användbart för att analysera långa dokument.</p>"
            },
            "Mistral Medium 3.1": {
                "desc": "<p>En medelstor, flerspråkig, multimodal modell som är billig jämfört med andra modeller som erbjuder liknande prestanda. Den är särskilt intressant för programmeringsuppgifter eller resonemangsuppgifter, såsom matematik.</p>",
                "fyi": "<p>Denna modell utformades för att leverera stabil prestanda till en lägre kostnad än proprietära eller halvöppna modeller. Särskild uppmärksamhet ägnades åt professionell användningsdata under träningen. Den presterar särskilt bra jämfört med andra modeller av liknande storlek för kodgenerering och matematiska uppgifter.</p>\n<p>Denna modell användes som grund för träning av Magistral Medium, en resonemangsmodell.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns tecken på att det är en stor modell som kräver flera kraftfulla grafikkort för att fungera. Tillgängliga uppskattningar bygger på indirekta index som inferenskostnader och svarslatens.</p>\n<p>Den har ett kontextfönster på upp till 128 000 tokens, vilket är användbart för att analysera långa dokument.</p>"
            },
            "Mistral Saba": {
                "desc": "<p>Medelstor modell utformad för en detaljerad språklig och kulturell förståelse av språk i Mellanöstern och Sydasien, inklusive arabiska, tamil och malayalam.</p>",
                "fyi": "<p>Träningen fokuserade främst på texter på arabiska, tamil och malayalam. Regionala korpusar valdes ut för att återspegla autentisk användning, inklusive syntax, register och dialektala varianter. För tokenisering (uppdelning av texten i grundläggande enheter som modellen kan bearbeta) användes en specialiserad strategi anpassad för språk med komplex morfologi som arabiska. Optimeringar syftar till att undvika överdriven ordfragmentering och maximera ordförrådstäckningen.</p>",
                "size_desc": "<p>Modellens exakta storlek är okänd. Det finns tecken på att det är en medelstor modell som kräver åtminstone ett kraftfullt grafikkort. Tillgängliga uppskattningar bygger på indirekta index som inferenskostnader och svarslatens.</p>\n<p>Modellen har ett kontextfönster på upp till 128 000 tokens, lämpligt för att analysera långa dokument.</p>"
            },
            "Mistral Small 3.2": {
                "desc": "<p>Trots namnet är detta en medelstor modell. Den är multimodal (kan bearbeta text och bilder) och utmärker sig genom sin noggranna frågebehandling och sin förmåga att använda avancerade verktyg.</p>",
                "fyi": "<p>Version 3.2 av denna modell är optimerad för att generera strukturerad utdata, särskilt i JSON, samtidigt som den begränsar upprepning och oönskat beteende under långa genereringstider. Den bearbetar både text- och bilddata, vilket gör det möjligt att analysera dem tillsammans.</p>",
                "size_desc": "<p>Med 32 miljarder parametrar anses denna modell medelstor. Den kan köras på en server med ett enda kraftfullt grafikkort, vilket begränsar infrastrukturkostnaderna. Den har ett kontextfönster på upp till 128 000 tokens, användbart för att analysera långa dokument.</p>"
            },
            "Nemotron Llama 3.1 70B": {
                "desc": "<p>Stor modell baserad på Llama 3.1 70B. Denna omtränade (fine-tune) version är ofta mer detaljerad och ger mer strukturerade svar.</p>",
                "fyi": "<p>Denna modell är baserad på omträning av Llama 3.1 70B, därav dess källmodell i namnet. Den introducerar förbättringar tack vare förstärkningsinlärning med mänsklig feedback (RLHF) och REINFORCE-algoritmen: Modellen utforskar olika svar, får feedback, och justerar sedan gradvis sina val för att bättre möta användarnas förväntningar. Denna anpassningsprocess används ofta när modellen är avsedd att anpassa sig till mänskliga preferenser eller för att optimera sina svar enligt specifika kriterier.</p>",
                "size_desc": "<p>Med 70 miljarder parametrar tillhör den här modellen kategorin stora modeller. Den kräver flera kraftfulla grafikkort för att fungera, vilket resulterar i betydande driftskostnader.</p>"
            },
            "Phi-4": {
                "desc": "<p>Liten, flerspråkig modell som kan använda verktyg och presterar bra på komplexa uppgifter som logik, matematik och kod, samtidigt som den förblir kompakt.</p>",
                "fyi": "<p>Denna modell använder TikTok för tokenisering, vilket förbättrar dess kapacitet i ett flerspråkigt sammanhang. Den tränades på totalt 9,8 <strong>biljoner</strong> tokens, varav 400 miljarder specifikt hämtades från högkvalitativ syntetisk data och resten från filtrerad organisk data. Träningen ägde rum på 1 920 H100-grafikkort i 21 dagar. Innovativa tekniker som självbedömning – under vilken modellen kritiserar och skriver om sina svar – och instruktionsomvändning användes för att stärka dess förståelse av instruktioner och resonemangsförmåga.</p>",
                "size_desc": "<p>Med 14 miljarder parametrar tillhör den här modellen kategorin små modeller. Den kan distribueras lokalt på en tillräckligt kraftfull dator, eller lagras på en server med ett enda grafikkort, vilket minskar infrastrukturkostnader. Kontextfönstret på 16 000 tokens kan vara begränsande för att analysera mycket långa dokument.</p>"
            },
            "Qwen 2.5 Coder 32B": {
                "desc": "<p>Medelstor modell specialiserad på programmering och användning av externa verktyg (webbsökningar, interaktioner med programvara etc.).</p>",
                "fyi": "<p>Denna modell har tränats på 5,5 biljoner tokens och över 92 programmeringsspråk, inklusive specialiserade kodningsspråk som Haskell och Racket.</p>\n<p>Tack vare sin kodprestanda kan den hantera anrop till externa verktyg väl, vilket är användbart för agentanvändning.</p>",
                "size_desc": "<p>Med 32 miljarder parametrar betraktas den här modellen som medelstor. Den kan köras på en server utrustad med ett enda kraftfullt grafikkort, vilket begränsar infrastrukturkostnaderna.</p>\n<p>Dess kontextfönster på 128 000 tokens gör att den kan bearbeta långa dokument.</p>"
            },
            "Qwen 3 30B A3B": {
                "desc": "<p>Flerspråkig modell i medelstorlek.</p>",
                "fyi": "<p>Denna MoE-modell (Mixture of Experts) har en konfiguration med totalt 128 experter, med endast 8 experter aktiverade per token, vilket möjliggör snabbare och effektivare inferens. Den använder ett system som kallas <em>global-batch</em> för att optimera arbetsfördelningen mellan experterna, så att de alla används på ett balanserat sätt.</p>\n<p>Till skillnad från andra modeller som Qwen 2.5-MoE, som återanvänder samma experter över flera lager i nätverket, tilldelar Qwen 3 30B A2B unika experter till varje lager. I praktiken innebär detta att experter från det första lagret aldrig återanvänds i efterföljande lager – varje nivå i modellen har sin egen uppsättning specialiserade experter. Denna arkitektur gör det möjligt för varje expert att fokusera uteslutande på uppgifter som är specifika för deras position i det neurala nätverket, vilket resulterar i mer detaljerad specialisering och optimerad prestanda för varje steg i informationsbearbetningen.</p>",
                "size_desc": "<p>Med 30 miljarder parametrar faller den här modellen kategorin medelstora modeller. Den kan köras på en server med ett enda kraftfullt grafikkort, vilket begränsar infrastrukturkostnaderna. Dessutom aktiverar Mixture of Experts (MoE)-arkitekturen endast en del av parametrarna vid varje token, vilket begränsar dess energiavtryck.</p>"
            },
            "Qwen 3 32B": {
                "desc": "<p>Medelstor flerspråkig modell med två svarsmetoder. Användaren kan välja mellan ett resonemangsläge för mer djupgående svar, eller ett snabbläge för att direkt generera det slutliga svaret.</p>",
                "fyi": "<p>Denna modell tränades på en mycket stor datamängd: 36 biljoner tokens på 119 språk. Träningen genomfördes i tre steg. Modellen lärde sig först från 30 miljarder tokens med en kontext på 4 000 tokens. Sedan lades 5 miljarder tokens till för att stärka dess faktakunskaper. Slutligen exponerades den för en specifik korpus för att bättre hantera mycket långa texter. Som ett resultat har den ett kontextfönster på 128 000 tokens i slutet av träningen, vilket är användbart för att läsa och analysera långa dokument.</p>",
                "size_desc": "<p>Med 32 miljarder parametrar tillhör den här modellen kategorin medelstora modeller. Den kan köras på en server utrustad med ett enda kraftfullt grafikkort, vilket begränsar infrastrukturkostnaderna.</p>\n<p>Dess kontextfönster med 128 000 tokens gör att den kan bearbeta långa dokument.</p>"
            },
            "o4 mini": {
                "desc": "<p>Mycket stor resonemangsmodell, lämplig för komplexa vetenskapliga och tekniska uppgifter och frågor.</p>",
                "fyi": "<p>Denna modell är mycket kraftfull för att analysera bilder och grafer. Den har också tränats att interagera med andra system via funktionsanrop, vilket gör den lämplig för agentiska användningsfall. Som en mycket kraftfull resonemangsmodell kan den användas för att distribuera uppgifter mellan flera mindre och/eller mer specialiserade modeller. Den har ett kontextfönster på upp till 200 000 tokens, vilket gör det enkelt att analysera långa dokument.</p>",
                "size_desc": "<p>Trots namnet och det faktum att den exakta storleken är okänd är o4 mini troligtvis en stor modell, som kräver servrar utrustade med flera grafikkort. Resonemangsmodeller som o4 mini kräver mer tid att svara eftersom en resonemangsfas föregår genereringen av det slutliga resultatet, vilket ökar deras energiförbrukning. Den antagna Mixture of Experts (MoE)-arkitekturen aktiverar dock bara en delmängd av parametrarna för att generera varje token, vilket begränsar dess energiavtryck. Storleksuppskattningar förlitar sig på indirekta index som inferenskostnader och svarslatens.</p>"
            },
            "qwq 32B": {
                "desc": "<p>Medelstor resonemangsmodell specialiserad på och mycket effektiv inom matematik, kodgenerering och logisk problemlösning.</p>",
                "fyi": "<p>Denna modell tränades med en förstärkningsinlärningsmetod (RL) för att optimera hanteringen av matematiska problem och programmeringsuppgifter. Den använder flera nya tekniker för att förbättra kvaliteten på svaren. Till exempel gör RoPE-metoden (Rotary Position Embedding) det möjligt att bättre förstå ordföljden i en text. Aktiveringsfunktionen SwiGLU är ett mer effektivt sätt att hantera beräkningar inom det neurala nätverket, vilket hjälper modellen att producera mer tillförlitliga svar. Justeringsmetoden QKV (Query Key Value-bias) förbättrar hur modellen identifierar och väljer viktig information. Slutligen, tack vare YaRN (Yet another RoPE extensioN method), kan den bearbeta mycket långa texter på upp till 130 000 tokens, vilket gör att den kan arbeta med komplexa eller mycket detaljerade dokument.</p>",
                "size_desc": "<p>Med 32 miljarder parametrar faller denna modell i kategorin medelstora modeller. Den kan köras på en server med ett enda kraftfullt grafikkort, vilket begränsar infrastrukturkostnaderna. Resonemangsmodeller av denna typ tar dock längre tid att producera ett svar eftersom en resonemangsfas föregår genereringen av det slutliga resultatet, vilket ökar energiförbrukningen.</p>"
            }
        }
    },
    "header": {
        "banner": "Claude 4, GLM 4.5, GPT OSS och andra nya modeller ansluter sig till arenan!",
        "chatbot": {
            "newDiscussion": "Ny diskussion",
            "step": "Steg",
            "stepOne": {
                "description": "Var uppmärksam på både innehåll och form, och utvärdera sedan varje svar",
                "title": "Vad tycker du om svaren?"
            },
            "stepTwo": {
                "description": "Upptäck vilken miljöpåverkan dina diskussioner har för varje modell",
                "title": "Modellerna avslöjas!"
            }
        },
        "help": {
            "link": {
                "content": "Hjälp oss förbättra jämföraren",
                "title": "Ge din åsikt om jämförelseprogrammet (öppnar ett nytt fönster)"
            }
        },
        "homeTitle": "Hem – compar:IA",
        "logoAlt": "Republiken Frankrike",
        "menu": "Meny",
        "startDiscussion": "Börja chatta",
        "subtitle": "Jämförelseverktyg för konversations-AI",
        "title": {
            "compar": "jämföra",
            "ia": "AI"
        },
        "votes": {
            "count": "{count} röster",
            "legend": "Teckenförklaring",
            "objective": "Mål: {count}",
            "tooltip": "Diskutera, rösta och hjälp oss att uppnå detta mål!<br /><strong>Dina röster är viktiga</strong> – de bidrar till compar:IA-datasetet som görs fritt tillgängligt för att förfina framtida modeller många olika språk.<br />Denna digitala allmänning bidrar till bästa <strong>respekt för den språkliga och kulturella mångfalden i framtida språkmodeller.</strong>"
        }
    },
    "home": {
        "europe": {
            "desc": "Litauen, Sverige och Danmark ansluter sig till Frankrike genom att använda jämförelseprogrammet för att förfina framtida AI-modeller på sina nationella språk.",
            "languages": {
                "da": "på danska",
                "fr": "på franska",
                "lt": "på litauiska",
                "sv": "på svenska"
            },
            "question": "Skulle du också vilja ha jämförelseverktyget på ditt språk?",
            "title": "Jämföraren <span {props}>blir europeisk!</span>"
        },
        "faq": {
            "discover": "Upptäck andra frågor",
            "title": "Era vanligaste frågor"
        },
        "intro": {
            "desc": "Chatta med två okända AI:er och utvärdera deras svar",
            "steps": {
                "a11yDesc": "1. Chatta med två anonyma AI:er: Chatta så länge du vill. 2. Ge din åsikt: Du bidrar till att förbättra AI-modeller. 3. Modellerna avslöjas: Lär dig mer om AI-modeller och deras egenskaper.",
                "title": "Hur det fungerar"
            },
            "title": "Lita inte på svaren från <span {props}>en enda AI</span>",
            "tos": {
                "accept": "Jag accepterar <a {linkProps}>användarvillkoren</a>",
                "error": "Du måste acceptera användarvillkoren för att fortsätta",
                "help": "Data delas för forskningsändamål"
            }
        },
        "origin": {
            "project": {
                "desc": "Jämföraren designades och utvecklades som en del av en statlig startup med stöd av franska kulturministeriet, och integrerades i <a {linkProps}>Beta.gouv.fr</a>-programmet inom det interministeriella digitala direktoratet (DINUM), vilket hjälper franska offentliga förvaltningar att bygga användbara, enkla och lättanvända digitala tjänster.",
                "title": "Vilka står bakom projektet?"
            },
            "team": {
                "desc": "Jämföraren stöds inom kulturministeriet av ett tvärvetenskapligt team som sammanför experter inom artificiell intelligens, utvecklare, driftsättningschefer och designers, med uppdraget att göra konversationsbaserad AI mer transparent och tillgänglig för alla.",
                "title": "Vilka är vi?"
            }
        },
        "usage": {
            "desc": "Verktyget riktar sig även till AI-experter och utbildare för mer specifika användningsområden",
            "educate": {
                "desc": "Använd jämföraren som ett utbildningsverktyg för att öka medvetenheten om AI i din målgrupp",
                "title": "Utbilda och öka medvetenheten"
            },
            "explore": {
                "desc": "Se alla modellernas egenskaper och användarsvillkor på ett ställe",
                "title": "Utforska modellerna"
            },
            "title": "Specifika användningsområden för compar:IA",
            "use": {
                "desc": "Utvecklare, forskare, modellskapare – få tillgång till compar:IAs datamängder för att förbättra modeller",
                "title": "Utnyttjande av data"
            }
        },
        "use": {
            "compare": {
                "alt": "Jämför",
                "desc": "Diskutera och utveckla ditt kritiska tänkande genom att ange dina preferenser",
                "title": "Jämför svaren från olika AI-modeller"
            },
            "desc": "compar:IA är ett gratis verktyg som hjälper till att öka medvetenheten bland medborgare om generativ AI och dess utmaningar.",
            "measure": {
                "alt": "Mät",
                "desc": "Upptäck vilken miljöpåverkan dina diskussioner har för varje modell",
                "title": "Mätning av det ekologiska fotavtrycket från frågor som ställs till AI"
            },
            "test": {
                "alt": "Testa",
                "desc": "Testa olika modeller, proprietära eller ej, små eller stora...",
                "title": "Testa de senaste AI-modellerna på ett enda ställe"
            },
            "title": "Vad används compar:IA till?"
        },
        "vote": {
            "datasetAccess": "Åtkomst till datamängderna",
            "desc": "Verktyget riktar sig även till AI-experter och utbildare för mer specifika användningsområden",
            "steps": {
                "datasets": {
                    "desc": "Alla frågor som ställs och alla röster sammanställs och publiceras fritt efter anonymisering.",
                    "title": "Datamängder efter språk"
                },
                "finetune": {
                    "desc": "I slutändan kan företag och akademiker använda datamängderna för att utbilda nya modeller som är bättre anpassade för språklig och kulturell mångfald.",
                    "title": "Språkspecifika förfinade modeller"
                },
                "prefs": {
                    "desc": "Efter att ha diskuterat med AI-modellerna anger du vilken du föredrar enligt givna kriterier, såsom svarens relevans eller användbarhet.",
                    "title": "Dina preferenser"
                }
            },
            "title": "Varför är din röst viktig?"
        }
    },
    "models": {
        "arch": {
            "title": "Visste du?",
            "types": {
                "dense": {
                    "desc": "Tät arkitektur hänvisar till en typ av neuralt nätverk där varje neuron i ett lager är kopplad till alla neuroner i nästa lager. Detta gör att alla parametrar i lagret kan bidra till beräkningen av utdata.",
                    "title": "Tät arkitektur"
                },
                "matformer": {
                    "desc": "Föreställ dig ryska dockor (matrjosjkor → matrjosjkatransformator → Matformer): Varje block innehåller flera inkapslade delmodeller av ökande storlek, som delar samma parametrar. Detta gör det möjligt att för varje begäran välja en modell med anpassad kapacitet, beroende på tillgängligt minne eller latens, utan att behöva omträna olika modeller.",
                    "title": "Matformer-arkitektur"
                },
                "moe": {
                    "desc": "Arkitekturen Mixture of Experts (MoE) använder en routingmekanism för att, beroende på indata, endast aktivera vissa specialiserade delmängder (\"experter\") av det neurala nätverket. Detta möjliggör konstruktion av mycket stora modeller samtidigt som beräkningskostnaderna hålls nere, eftersom endast en del av nätverket används i varje steg.",
                    "title": "MOE-arkitektur"
                },
                "na": {
                    "desc": "Utgivaren har inte offentliggjort information om modellens arkitektur.",
                    "title": "Okänd arkitektur"
                }
            }
        },
        "conditions": {
            "commercialUse": {
                "question": "Är kommersiell användning av modellen tillåten?",
                "title": "Kommersiell användning"
            },
            "reuse": {
                "question": "Kan jag använda modellens utdata för att träna andra modeller?",
                "subTitle": "Du kan inte använda utdata för att träna andra modeller",
                "title": "Användning av genererade resultat"
            },
            "title": "Användarvillkor",
            "types": {
                "allowed": "Tillåtet",
                "conditions": "Villkorlig",
                "forbidden": "Förbjuden"
            }
        },
        "extra": {
            "experts": {
                "api-only": "För att lära dig mer, titta på <a {linkProps}>modellens officiella webbplats</a>",
                "open-weights": "För att lära dig mer, gå till <a {linkProps}>modellens sida på Hugging Face</a>"
            },
            "impacts": "Miljöpåverkansberäkningarna baseras på projekten <a {linkProps1}>EcoLogits</a> och <a {linkProps2}>Impact CO<sub>2</sub></a>.",
            "title": "För att lära dig mer"
        },
        "licenses": {
            "commercial": "Kommersiell licens",
            "descriptions": {
                "Apache 2.0": "Denna licens låter dig använda, ändra eller sprida en modell, även för kommersiella ändamål. Utöver det garanteras rättsligt skydd genom en klausul om icke-intrång och transparens. Alla ändringar måste därför vara dokumenterade och spårbara.",
                "CC-BY-NC-4.0": "Denna licens tillåter att du sprider och ändrar innehållet, så länge upphovsmannen nämns, med förbjuder kommersiell användning. Det ger flexibilitet för icke-kommersiell användning, medan skaparens rättigheter bevaras.",
                "Gemma": "Denna licens är skapad för att uppmuntra att mjukvaran används, ändras och sprids, men innehåller en klausul som säger att alla ändrade versioner måste delas under samma licens, för att främja samarbete och transparens inom mjukvaruutveckling.",
                "Jamba Open Model": "Denna licens tillåter att koden fritt används, reproduceras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för organisationer vars omsättning överskrider 50 miljoner USD.",
                "Llama 3 Community": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad, och förbjuder att kod eller genererat material återanvänds för att träna eller utveckla konkurrerande modeller, för att därigenom skydda Metas investeringar och varumärke.",
                "Llama 3.1": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "Llama 3.3": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "Llama 4": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "MIT": "MIT-licensen är en licens för fri programvara. Den tillåter att vem som helst återanvänder, ändrar eller sprider en modell, även för kommersiella ändamål, förutsatt att den ursprungliga licensen och upphovsrättstexten skickas med.",
                "Mistral AI Non-Production": "Denna licens tillåter att du sprider och ändrar innehållet, så länge upphovsmannen nämns, med förbjuder kommersiell användning. Det ger flexibilitet för icke-kommersiell användning, medan skaparens rättigheter bevaras.",
                "propriétaire Anthropic": "Modellen är tillgänglig under en betald licens och åtkomlig via API på Anthropics plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens, eller enligt företagets villkor.",
                "propriétaire Gemini": "Modellen är tillgänglig under en betald licens och åtkomlig via Gemini API som finns tillgängligt på Google AI Studio och Vertex AI, vilket kräver en avgift per användning baserat på antalet bearbetade tokens eller enligt företagets villkor.",
                "propriétaire Liquid": "Modellen är tillgänglig under en betald licens och åtkomlig via API på Liquid AIs plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens.",
                "propriétaire Mistral": "Modellen är tillgänglig under en betald licens och åtkomlig via Mistral och andra partner-API:er. Den kräver en avgift per användning baserad på antalet bearbetade tokens.",
                "propriétaire OpenAI": "Modellen är tillgänglig under en betald licens och åtkomlig via API på OpenAIs plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens, eller enligt företagets villkor.",
                "propriétaire xAI": "Modellen är tillgänglig via xAI API. Det kräver betalning per användning baserat på antalet bearbetade tokens, eller enligt företagets villkor."
            },
            "name": "Licens {licence}",
            "noDesc": "Licensinformation saknas för denna modell.",
            "type": {
                "openSource": "Öppen källkod",
                "proprietary": "Proprietär",
                "semiOpen": "Semi-öppen"
            }
        },
        "list": {
            "filters": {
                "display": "Visa filter",
                "editor": {
                    "legend": "Utgivare"
                },
                "license": {
                    "legend": "Användarlicens"
                },
                "reset": "Rensa alla filter",
                "size": {
                    "labels": {
                        "L": "70 till 150 miljarder",
                        "M": "20 till 70 miljarder",
                        "S": "7 till 20 miljarder",
                        "XL": "> 150 miljarder",
                        "XS": "< 7 miljarder"
                    },
                    "legend": "Storlek (parametrar)"
                }
            },
            "intro": "Utforska de olika konversationsbaserade AI-modellerna, deras specifikationer och licenser.",
            "model": "modell",
            "models": "modeller",
            "noresults": "Inga modeller matchar dina sökkriterier.",
            "title": "Utforska modellerna",
            "triage": {
                "label": "Sortera efter",
                "options": {
                    "date-desc": "Utgivningsdatum (från nyaste till äldsta)",
                    "name-asc": "Modellnamn (bokstavsordning)",
                    "org-asc": "Utgivare (bokstavsordning)",
                    "params-asc": "Storlek (från minsta till största)"
                }
            }
        },
        "names": {
            "a": "Modell A",
            "b": "Modell B"
        },
        "openWeight": {
            "conditions": {
                "copyleft": "Copyleft",
                "free": "Tillåtande",
                "restricted": "Villkorlig"
            },
            "descriptions": {
                "L": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin stora modeller (mellan 70 och 100 miljarder parametrar).",
                "M": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin medelstora modeller (mellan 20 och 70 miljarder parametrar).",
                "S": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin små modeller (mellan 7 och 20 miljarder parametrar).",
                "XL": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin mycket stora modeller.",
                "XS": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin mycket små modeller (färre än 7 miljarder parametrar)."
            },
            "tooltips": {
                "copyleft": "När modellen har modifierats får den bara spridas med samma licens som källmodellen.",
                "free": "När modellen har modifierats kan den spridas under en annan licens än källmodellen.",
                "openSource": "Träningsdata, kod och vikter för denna modell (dvs. parametrarna som lärs in under träningen) kan laddas ner och modifieras fritt. Att en modell är \"öppen källkod\" ställer hårdare krav än \"öppna vikter\", särskilt på grund av behovet av transparens i träningsmaterialet, och få modeller anses vara \"öppen källkod\".",
                "openWeight": "En så kallad \"open weights\"-modell, vilket innebär att dess vikter – de parametrar som lärs in under träning – kan laddas ner fritt, vilket gör att det går att köra modellen på sin egen dator. Att en modell är \"öppen källkod\" ställer hårdare krav än \"öppna vikter\", särskilt på grund av behovet av transparens i träningsmaterialet, och få modeller anses vara \"öppen källkod\".",
                "params": "Parametrar eller vikter – ofta flera miljarder – är de variabler som lärs in av en modell under träning och som bestämmer dess svar. Ju fler parametrar, desto större inlärningskapacitet har modellen.",
                "ram": "RAM-minnet (Random Access Memory) lagrar data som bearbetas av en LLM i realtid. Ju större modellen är, desto mer RAM behöver den för att köras."
            },
            "use": {
                "attribution": "Attribution krävs",
                "commercial": "Kommersiell användning",
                "licenseType": "Licenstyp",
                "modification": "Ändring godkänd",
                "requiredRam": "Krav på RAM-minne"
            }
        },
        "parameters": "{number} parametrar",
        "ram": "{min} till {max} GB",
        "release": "Utgiven {date}",
        "size": {
            "descriptions": {
                "L": "Stora modeller kräver betydande resurser, men erbjuder bäst prestanda för avancerade uppgifter som kreativt skrivande, dialogmodellering och tillämpningar som kräver en noggrann förståelse av kontext.",
                "M": "Medelstora modeller erbjuder en bra balans mellan enkelhet och prestanda. De är mycket mindre resurskrävande än stora modeller, men kan fortfarande hantera komplexa uppgifter som sentimentanalys och resonemang.",
                "S": "En liten modell är mindre komplex och resurskrävande jämfört med större modeller, och ger fortfarande tillräcklig prestanda för många uppgifter (sammanfattning, översättning, textklassificering etc.)",
                "XL": "Dessa modeller, med hundratals miljarder parametrar, är de mest komplexa och avancerade vad gäller prestanda och noggrannhet. De kräver stora beräknings- och minnesresurser och är avsedda för mycket avancerade applikationer och högt specialiserade miljöer.",
                "XS": "Mycket små modeller, med färre än 7 miljarder parametrar, är de minst komplexa och mest resurseffektiva och ger tillräcklig prestanda för enkla uppgifter som textklassificering."
            },
            "estimated": "Uppskattad storlek ({size})",
            "title": "Storlek"
        }
    },
    "modes": {
        "big-vs-small": {
            "altLabel": "David mot Goliat",
            "description": "En liten modell mot en stor, båda valda slumpmässigt",
            "label": "David mot Goliat",
            "title": "David mot Goliat-läget"
        },
        "custom": {
            "altLabel": "Manuellt val",
            "description": "Kommer du att känna igen de två modellerna du valde?",
            "label": "Manuellt val",
            "title": "Manuellt val"
        },
        "random": {
            "altLabel": "Slumpmässigt val av modeller",
            "description": "Två modeller utvalda slumpmässigt från hela listan",
            "label": "Slump",
            "title": "Slumpläge"
        },
        "reasoning": {
            "altLabel": "Resonemangsmodeller",
            "description": "Två modeller, optimerade för komplexa uppgifter, slumpmässigt utvalda",
            "label": "Resonemang",
            "title": "Resonemangsläge"
        },
        "small-models": {
            "altLabel": "Sparsamma modeller",
            "description": "Två slumpmässigt valda modeller, ur kategorin små modeller",
            "label": "Sparsam",
            "title": "Sparsamt läge"
        }
    },
    "product": {
        "comparator": {
            "challenges": {
                "bias": {
                    "desc": "Belys AI-bias relaterat till underrepresentationen av data på andra språk än engelska och öka medvetenheten om dess konsekvenser.",
                    "title": "Kulturell och språklig bias"
                },
                "impacts": {
                    "desc": "Visa miljöeffekterna av generativ AI, som fortfarande till stor del är okända för allmänheten.",
                    "title": "Miljöpåverkan"
                },
                "pluralism": {
                    "desc": "Se till att befolkningen har tillgång till en mångfald av AI-modeller så att de kan fatta välgrundade val och utveckla en kritisk syn på dessa tekniker.",
                    "title": "Mångfald av modeller"
                },
                "thinking": {
                    "desc": "Uppmuntra kritiskt tänkande om generativ AI och dess plats i personliga och professionella praktiker (utbildning, arbete).",
                    "title": "Kritiskt tänkande och samhällsfrågor"
                },
                "title": "Den utvecklade applikationen hanterar flera utmaningar"
            },
            "cta": "Åtkomst till jämföraren",
            "europe": {
                "adventure": "Sedan hösten 2025 har Litauen, Sverige och Danmark anslutit sig till projektet!",
                "catch": "Vill du ha jämförelseverktyget på ditt språk?",
                "desc": "Jämförelsemodellen görs tillgänglig för medborgare i flera länder på deras nationella språk. Målet är att skapa datamängder för att förbättra framtida AI-modeller på dessa europeiska språk.",
                "title": "Jämföraren <span {props}>blir europeisk</span>!"
            },
            "tabLabel": "Jämföraren",
            "title": "Jämföraren låter dig skapa <span {props}>datamängder</span>, helst med fokus på <span {props}>verkliga användningsområden</span> uttryckta på <span {props}>europeiska språk</span>."
        },
        "faq": {
            "tabLabel": "Vanliga frågor"
        },
        "history": {
            "tabLabel": "Projekthistorik"
        },
        "partners": {
            "academy": {
                "catch": "Arbetar du på ett forskningsprojekt och har frågor eller förslag angående våra metoder och datamängder?",
                "desc": "Vi strävar efter att de genererade datamängderna ska användas i tvärvetenskaplig forskning som kombinerar humaniora, samhällsvetenskap och datavetenskap.",
                "title": "Akademiska partners"
            },
            "diffusion": {
                "catch": "Vill du använda jämföraren för att möta ett affärsbehov?",
                "cta": "Hör av dig",
                "desc": "Vi skapar ett nätverk av partners som integrerar jämförelseverktyget i sina tjänste- och utbildningserbjudanden.",
                "title": "Kommunikationspartners"
            },
            "institution": {
                "title": "Institutionella partners"
            },
            "services": {
                "desc": "Miljöpåverkansberäkningarna är baserade på ovanstående verktyg.",
                "title": "Tjänster som används"
            },
            "tabLabel": "Partners"
        },
        "problem": {
            "alignment": {
                "alignment": {
                    "a": "Anpassning sker efter en språkmodells förträningsfas, som ett \"efterbehandlings-\" eller \"poleringsfas\". Under förträningen lär sig modellen att förutsäga nästa ord och att generera sammanhängande text.",
                    "b": "Anpassningssteget innebär att lära modellen att bättre möta mänskliga behov, dvs. att göra den mer <strong>relevant</strong> (modellen svarar \"bättre\"), <strong>ärlig</strong> (förmåga att anta \"att den inte vet hur den ska svara\" när det inte finns tillräckligt med data), och <strong>säker</strong> (undvika att generera farligt eller olämpligt innehåll).",
                    "c": "<strong>Utan anpassning kan en språkmodell vara tekniskt skicklig men svår att använda i praktiken eftersom den inte förstår vad som förväntas av den i en konversation. </strong>",
                    "title": "Anpassning, ett avgörande steg i modellinstruktion"
                },
                "datasets": {
                    "a": "Anpassning använder mycket specifik data, speciellt skapad för att lära modellen hur den ska bete sig \"bra\".",
                    "b": "<strong>Preferensdata</strong> utgör en speciell typ av anpassningsdata, tillsammans med <strong>demonstrationsdata</strong> (exempel på konversationer mellan människor och AI-assistenter, skrivna av experter på annotering enligt exakta riktlinjer för ton och stil), <strong>säkerhetsdata</strong> (specifika exempel som lär modellen att undvika farligt innehåll genom att visa hur man avvisar problematiska förfrågningar), eller <strong>specialiserad data</strong> som täcker specifika områden (medicin, juridik, utbildning, etc.).",
                    "c": "Preferensdata presenterar flera möjliga svar på en enda fråga, rangordnade efter kvalitet av mänskliga utvärderare. Användare anger vilket svar som är bäst enligt givna kriterier, såsom relevans, användbarhet och skadlighet. När dessa datamängder har skapats används de för att träna modeller genom att justera dem enligt de preferenser som användarna uttrycker.",
                    "title": "Specifika datamängder"
                },
                "desc": "Alignment: En teknik för att minska bias som bygger på att samla in användarpreferenser",
                "diversity": {
                    "a": "För att återspegla mångfalden av kulturer och språk i de resultat som genereras av modeller, bör <strong>alignment dataset</strong> innehålla en mängd olika språk, sammanhang och exempel från vanliga användare. Att diversifiera alignment data förbättrar i slutändan modellens prestanda på två sätt:",
                    "b": "För det första <strong>minskas kulturell bias</strong> genom att man förhindrar att ett enda perspektiv – ofta från engelskspråkiga länder – dominerar AI:ns svar. Modellen lär sig därmed att inse att det finns flera giltiga sätt att närma sig samma fråga beroende på det kulturella sammanhanget.",
                    "c": "För det andra uppmuntrar denna exponering för mångfalden av språk och kulturer anpassning av svar till specifika sammanhang: En fransk användare får råd anpassade till det franska systemet, medan en dansk användare får information som motsvarar deras nationella sammanhang.",
                    "d": "Resultatet är en mer inkluderande konversationsbaserad AI-modell, som kan ta hänsyn till olika kulturer.",
                    "title": "Diversifiera data för att minska partiskhet"
                },
                "english": {
                    "a": "Preferensdata är dyra att producera eftersom det kräver <strong>kvalificerad mänsklig arbetskraft</strong> för varje exempel. Plattformar som https://chat.lmsys.org/ tillåter skapandet av dessa preferensdataset, men få användare använder dem på sitt modersmål.",
                    "b": "Preferensdataset är sällsynta, om inte obefintliga, på europeiska språk. Andelen frågor som ställs på franska i LMSYS-datasetet är till exempel mindre än 1%.",
                    "c": "comparIA kan samla in samtal på flera språk, och kulturella referenser specifika för varje region eller land – vanliga uppgifter, lokala kulinariska traditioner, utbildningssystem, historiska eller litterära referenser, etc.",
                    "title": "Bristande preferensdata för europeiska språk"
                },
                "title": "Hur kan vi minska kulturella och språkliga bias i dessa modeller?"
            },
            "diversity": {
                "diversity": {
                    "desc": "Kulturell bias kan också resultera i ofullständiga eller till och med felaktiga svar som försummar mångfalden av språk och kulturer.",
                    "title": "Försummad kulturell och språklig mångfald"
                },
                "english": {
                    "desc": "Konversationsbaserade AI:er förlitar sig på stora språkmodeller (LLM) som ofta huvudsakligen tränas på engelsk data, vilket skapar språklig och kulturell bias i de resultat de producerar.",
                    "title": "Träningsdata mestadels på engelska"
                },
                "stereotypes": {
                    "desc": "Konversationsbaserade AI-system verkar tala många språk, men resultaten de genererar är ibland stereotypa eller diskriminerande.",
                    "title": "Stereotypa svar"
                }
            },
            "tabLabel": "Det inledande problemet",
            "title": "Respekterar konversationsbaserade AI-modeller <span {props}>mångfalden</span> av europeiska språk?"
        },
        "title": "Allt du behöver veta om jämföraren"
    },
    "ranking": {
        "table": {
            "data": {
                "cols": {
                    "consumption_wh": "Energi<br>(1000 tokens)",
                    "elo": "Nöjdhetspoäng",
                    "license": "Licens",
                    "name": "Modell",
                    "organisation": "Organisation",
                    "rank": "Rang",
                    "release": "Utgivningsdatum",
                    "size": "Storlek<br>(aktiva parametrar)",
                    "total_votes": "Totalt antal röster",
                    "trust_range": "Konfidens (±)"
                }
            },
            "lastUpdate": "Uppdaterat {date}",
            "search": "Sök modell",
            "totalModels": "Totalt antal modeller:",
            "totalVotes": "Totalt antal röster:"
        },
        "title": "Modellrankning"
    },
    "reveal": {
        "equivalent": {
            "co2": {
                "label": "Mängd koldioxid",
                "tooltip": "Med mängden koldioxid menar vi de koldioxidutsläpp som globalt motsvaras av den energi som används för att köra modellen. Den återspeglar miljöpåverkan kopplad till energiförbrukningen. De verkliga utsläppen skiljer sig beroende på energiproduktionen i varje land, och de servrar som används för beräkningarna finns i olika länder. Ekvivalensberäkningen baseras på den globala genomsnittliga CO<sub>2</sub>-utsläppsmängden per förbrukad energi."
            },
            "lightbulb": {
                "label": "LED-lampa",
                "tooltip": "Baserat på förbrukningen av en vanlig 5W LED-lampa (E14)"
            },
            "streaming": {
                "label": "onlinevideor",
                "tooltip": "Data beräknad baserat på koldioxidutsläpp från en timmes onlinevideo i hög upplösning, på en TV, med Wi-Fi-anslutning (källa <a {linkProps}>ADEME</a>)"
            },
            "title": "Vilket motsvarar:"
        },
        "feedback": {
            "description": "Dela compar:AI med andra genom att dela AI-modellerna du har interagerat med! Endast namnen och energipåverkan från diskussionen kommer att synas via den här länken – din text visas inte.",
            "example": "Exempel på delade resultat",
            "moreOnVotes": "Läs mer om att rösta",
            "shareResult": "Dela ditt resultat"
        },
        "impacts": {
            "energy": {
                "label": "energiförbrukning",
                "tooltip": "Mätt i wattimmar representerar strömförbrukningen den elektricitet som modellen använder för att bearbeta en fråga och generera motsvarande svar. Ju större en modell är (i antal parametrar), desto mer energi krävs för att producera varje token."
            },
            "size": {
                "count": "miljarder parametrar",
                "estimated": "(ca)",
                "label": "modellstorlek",
                "quantized": "(kvantiserad)"
            },
            "title": "Diskussionens energiförbrukning",
            "tokens": {
                "label": "textstorlek",
                "tokens": "tokens",
                "tooltip": "AI analyserar och genererar meningar från ord eller orddelar på ungefär fyra bokstäver. Denna textenhet kallas token. Ju längre text, desto fler tokens."
            }
        }
    },
    "seo": {
        "desc": "compar:IA är ett verktyg för att jämföra olika konversationsbaserade AI-modeller, för att öka medvetenheten om utmaningarna med generativ AI (bias, miljöpåverkan) och för att skapa datamängder för språk med begränsade resurser.",
        "title": "compar:IA, jämförelseverktyget för konversations-AI",
        "titles": {
            "accessibilite": "Tillgänglighetspolicy",
            "arene": "Chat",
            "comparator": "Jämföraren",
            "datasets": "Datamängder",
            "donnees-personnelles": "Integritetspolicy",
            "duel": "AI-duell-workshop",
            "faq": "Vanliga frågor",
            "history": "Projekthistorik",
            "home": "Startsida",
            "mentions-legales": "Rättsliga meddelanden",
            "modalites": "Användarvillkor",
            "modeles": "Lista över modeller",
            "news": "Nyheter",
            "partners": "Partners",
            "problem": "Det inledande problemet",
            "product": "Produkt och partners",
            "ranking": "Rankning",
            "share": "Mina resultat"
        }
    },
    "vote": {
        "bothEqual": "Båda är lika bra",
        "choices": {
            "altText": "{choice} för modellen {model}",
            "negative": {
                "incorrect": "Fel",
                "instructions-not-followed": "Instruktioner följdes inte",
                "question": "Varför var du inte nöjd med svaret?",
                "superficial": "Ytligt"
            },
            "positive": {
                "clear-formatting": "Rensa formatering",
                "complete": "Komplett",
                "creative": "Kreativt",
                "question": "Vad tyckte du om med svaret?",
                "useful": "Användbart"
            }
        },
        "comment": {
            "add": "Lägg till kommentarer",
            "placeholder": "Du kan lägga till detaljer i svaret för modellen {model}"
        },
        "dislike": {
            "label": "Jag uppskattar inte",
            "selectedLabel": "Jag gillar inte (vald)"
        },
        "introA": "Innan vi tar avslöjar vilka modellerna var behöver vi dina preferenser.",
        "introB": "Det gör det möjligt för oss att berika compar:IA-datauppsättningarna, vars mål är att förfina framtida AI-modeller på språk med begränsade resurser",
        "like": {
            "label": "Jag uppskattar",
            "selectedLabel": "Jag uppskattar (vald)"
        },
        "qualify": {
            "addDetails": "Lägg till detaljer",
            "placeholder": "Svaren från modellen {model} är...",
            "question": "Hur betygsätter du svaren?"
        },
        "title": "Vilken AI-modell föredrar du?",
        "yours": "Din röst"
    },
    "welcome": {
        "errors": "AI kan göra misstag! Vi uppmuntrar dig att verifiera information du får från AI-modellerna.",
        "go": "Nu kör vi",
        "privacy": "Dela inte personlig information som förnamn, efternamn eller adress.",
        "title": "Välkommen till compar:IA!",
        "tos": {
            "desc": "Dina konversationer på compar:IA och de preferenser du uttrycker används anonymt för att skapa datamängder som är representativa för europeiska språk och språkbruk, i syfte att minska kulturell bias och skapa mer inkluderande framtida AI-modeller.",
            "moreInfos": "Läs mer om projektet."
        },
        "use": "Använd inte verktyget för olagliga eller skadliga ändamål."
    },
    "words": {
        "NA": "--",
        "back": "Tillbaka",
        "close": "Stäng",
        "loading": "Laddar",
        "random": "Slump",
        "regenerate": "Regenerera",
        "reset": "Återställ",
        "restart": "Börja om",
        "retry": "Börja om",
        "search": "Sök",
        "send": "Skicka",
        "tooltip": "Tips",
        "validate": "Validera"
    }
}
