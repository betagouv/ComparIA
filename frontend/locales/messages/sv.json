{
    "a11y": {
        "externalLink": "{text}"
    },
    "actions": {
        "contact": "Kontakta oss",
        "contactUs": "Kontakta oss",
        "copyLink": {
            "do": "Kopiera länk",
            "done": "Länk kopierad"
        },
        "copyMessage": {
            "do": "Kopiera meddelande",
            "done": "Meddelandet har kopierats"
        },
        "home": "Startsida",
        "returnHome": "Tillbaka till startsida",
        "seeMore": "Visa mer",
        "selectLanguage": "Välj språk",
        "vote": "Skicka åsikt"
    },
    "arenaHome": {
        "compareModels": {
            "count": "{count}/2 modeller",
            "help": "Om bara en är vald, kommer den andra att väljas slumpmässigt",
            "question": "Vilka modeller vill du jämföra?"
        },
        "modelSelection": "Val av modeller",
        "prompt": {
            "label": "Skriv ditt första meddelande",
            "placeholder": "Skriv ditt första meddelande här"
        },
        "selectModels": {
            "help": "Välj jämförelseläge",
            "question": "Vilka modeller vill du jämföra?"
        },
        "suggestions": {
            "choices": {
                "administrative": {
                    "iconAlt": "Administrativ",
                    "title": "Skriv ett administrativt dokument"
                },
                "coach": {
                    "iconAlt": "Tips",
                    "title": "Få råd om kost och träning"
                },
                "explanations": {
                    "iconAlt": "Förklaringar",
                    "title": "Förklara ett koncept enkelt"
                },
                "iasummit": {
                    "iconAlt": "AI Action Summit",
                    "title": "Prompter from medborgarkonsultation om AI ",
                    "tooltip": "Dessa frågor är resultatet av en medborgarkonsultation om IA som hölls 16/9 till 8/11 2024. Syftet var att låta medborgare och samhälle delta i AI Action Summit, och samla deras idéer om hur AI kan skapa möjligheter för alla, medan missbruk minimeras."
                },
                "ideas": {
                    "iconAlt": "Idéer",
                    "title": "Generera nya idéer"
                },
                "languages": {
                    "iconAlt": "Översättning",
                    "title": "Skriv på annat språk"
                },
                "recipes": {
                    "iconAlt": "Recept",
                    "title": "Upptäck ett nytt recept"
                },
                "recommendations": {
                    "iconAlt": "Rekommendationer",
                    "title": "Föreslå filmer, böcker och musik"
                },
                "stories": {
                    "iconAlt": "Berättelser",
                    "title": "Berätta en historia"
                }
            },
            "generateAnother": "Skapa ett nytt meddelande",
            "title": "Förslag på prompter"
        },
        "title": "Hur kan jag hjälpa dig idag?"
    },
    "chatbot": {
        "continuePrompt": "Fortsätt chatta med AI-modellerna",
        "conversation": "Konversation",
        "errors": {
            "other": {
                "message": "Ett tillfälligt fel har uppstått.",
                "retry": "Du kan försöka begära modellerna igen.",
                "title": "Ett tillfälligt fel har uppstått",
                "vote": "Eller avsluta din upplevelse genom att ge din åsikt om modellerna."
            },
            "tooLong": {
                "message": "Varje modell har en gräns för hur stora samtal den kan hantera.",
                "retry": "Du kan återuppta en konversation med två nya modeller.",
                "title": "Konversationen är för lång för en av modellerna.",
                "vote": "Du kan fortfarande ge din åsikt om dessa modeller eller starta en konversation med två nya."
            }
        },
        "loading": "Läser in svar",
        "reasoning": {
            "finished": "Resonemang avslutat",
            "inProgress": "Resonemang pågår…"
        },
        "revealButton": "Visa vilka modeller som användes"
    },
    "closeModal": "Stäng modalfönstret",
    "components": {
        "pagination": {
            "first": "Första sidan",
            "last": "Sista sidan",
            "next": "Nästa sida",
            "nth": "Sida {count}",
            "previous": "Föregående sida"
        },
        "table": {
            "linePerPage": "Antal rader per sida",
            "pageCount": "{count} rader per sida",
            "triage": "Sortera"
        },
        "theme": {
            "legend": "Välj ett tema för att anpassa webbplatsens utseende.",
            "options": {
                "dark": "Mörkt tema",
                "light": "Ljust tema",
                "system": "System"
            }
        }
    },
    "datasets": {
        "access": {
            "catch": "Modellskapare, forskare, företag, det är er tur!",
            "desc": "Frågorna som ställs på plattformen är mestadels på franska och återspeglar verklig användning utan restriktioner. Dessa datamängder finns tillgängliga på <a {linkProps}>data.gouv</a> och Hugging Face.",
            "repos": {
                "conversations": {
                    "desc": "Frågor och svar"
                },
                "reactions": {
                    "desc": "Alla reaktioner"
                }
            },
            "title": "Åtkomst till datauppsättningar från compar:IA"
        },
        "reuse": {
            "bunka": {
                "analyze": {
                    "title": "Visa analys för indikator"
                },
                "conversations": {
                    "desc": "Interaktiv visualisering av samtal där varje punkt representerar ett kluster av diskussioner som nämns av användare (som tex. utbildning, hälsa, miljö eller filosofi).",
                    "title": "Utforska visualisering av data"
                },
                "desc": "Bunka.ai-teamet genomförde en djupgående studie av interaktioner mellan användare av Compar:AI-plattformen och AI-modeller, där de undersökte vilka ämnen man föredrog, vilka frågor som ställdes, och ifall modellerna främst användes som automatiseringsverktyg eller för att assistera användaren. Denna analys baseras på ett stort urval av 25 000 konversationer.",
                "method": "Läs mer om metoderna"
            },
            "title": "Hur används dessa uppgifter?"
        }
    },
    "errors": {
        "404": {
            "desc": "Om du skrev in webbadressen i din webbläsare, kontrollera att den är korrekt. Sidan kanske inte längre är tillgänglig. <br /> I så fall kan du gå tillbaka till startsidan. <br />Kontakta oss så hjälper vi dig hitta rätt information.",
            "error": "Fel 404",
            "sorry": "Sidan kunde inte visas.",
            "title": "Sidan hittades inte"
        },
        "unexpected": {
            "desc": "Prova att ladda om sidan, eller försök igen senare.",
            "error": "Felkod {code}",
            "sorry": "Ett problem har uppstått med tjänsten. Vi arbetar på att lösa det så snabbt som möjligt.",
            "title": "Oväntat fel"
        },
        "unknown": "Ett fel har uppstått"
    },
    "faq": {
        "datasets": {
            "questions": {
                "3": {
                    "title": "Vad skiljer compar:IA från andra liknande initiativ?"
                }
            },
            "title": "Datamängd"
        },
        "ecology": {
            "questions": {
                "1": {
                    "title": "Hur beräknas miljömässiga indikatorer?"
                },
                "2": {
                    "title": "Tar miljöindikatorer hänsyn till olika typer av energikällor i olika länder?"
                },
                "3": {
                    "title": "Tar indikatorer för miljöpåverkan hänsyn till de resurser som används för att träna modellerna?"
                }
            },
            "title": "Miljömässiga indikatorer"
        },
        "i18n": {
            "questions": {
                "1": {
                    "desc": "<p> Ja, internationaliseringen av compar:AI är igång. Vi börjar med en expansion till tre pilotländer: Litauen, Sverige och Danmark. Denna första fas kommer att göra det möjligt för oss att testa tillvägagångssättet och anpassa gränssnittet till olika europeiska språkliga och kulturella sammanhang. Så småningom kan vi expandera till fler europeiska språk baserat på feedback från dessa pilotländer. Målet är att gradvis bygga en gemensam europeisk digital plattform för mänsklig utvärdering av konversationsbaserad AI, med samarbetsinriktad styrning som återstår att definiera mellan de olika deltagande länderna. </p>",
                    "title": "compar:IA fokuserade från början på franska – finns det planer för andra europeiska språk?"
                }
            },
            "title": "Internationalisering"
        },
        "models": {
            "questions": {
                "1": {
                    "desc": "<p>Vi väljer modeller baserat på deras popularitet, mångfald och relevans för användarna. Vi lägger särskild vikt vid att ta med modeller i olika storlekar, och modeller med så kallade <em>open weights</em>.</p>",
                    "title": "Hur väljs modellerna som finns tillgängliga i jämförelseverktyget?"
                },
                "2": {
                    "desc": "<p>Funktionen att fråga modellerna möjliggörs tack vare donationer från molnleverantörerna som stöder projektet: Google Cloud Platform, Hugging Face, Microsoft Azure, OVH, Scaleway.</p>",
                    "title": "Hur finansieras den här tjänsten?"
                },
                "3": {
                    "desc": "<p>Kvantiserade modeller är optimerade för att förbruka färre resurser genom att förenkla vissa beräkningar. Tekniken innebär att man minskar precisionen hos parametrarna i en AI-modell. På det sättet kan man göra <strong> modellen mindre </strong>och<strong> beräkningarna snabbare</strong>, vilket är särskilt användbart för maskiner med begränsade resurser.</p>",
                    "title": "Vad är en \"kvantiserad modell\"?"
                },
                "4": {
                    "desc": "<p> <strong>Vilka språk en modell kan hantera väl beror på språken som används i dess träningsdata.</strong> <strong> En LLM använder enorma korpusar på många språk</strong> men språkfördelningen i träningsdatan skiljer mellan modeller. En överrepresentation av engelska kan leda till begrränsningar på andra språk. Dessa begränsningar återspeglas till exempel i <strong> anglicismer eller en oförmåga att generera innehåll på vissa språk som klassificeras som \"hotade\" av UNESCO.</strong></p><p><strong>En modells noggrannhet och ordförråd beror på de data som använts för träning</strong>.</p>",
                    "title": "Finns det ett samband mellan nationaliteten hos skaparen av modellen och dess förmåga att tala flera språk?"
                },
                "5": {
                    "desc": "<p>Bara ett fåtal modeller är \"transparenta\" i den bemärkelsen. I de flesta fall är träningsdatan inte tillgänglig för allmänheten, av juridiska eller kommersiella skäl.</p>",
                    "title": "Går det att få tag på modellernas träningsdata?"
                }
            },
            "title": "Modeller"
        },
        "title": "Vanliga frågor",
        "usage": {
            "questions": {
                "1": {
                    "desc": "<p> Nuvarande konversationsmodeller kan <strong>inte citera källorna</strong> de använde för att generera ett svar. De fungerar genom att förutsäga det mest sannolika nästa ordet baserat på den statistiska fördelningen av träningsdata. Informationen är en kombination av många olika källor, och metoden kan inte hålla reda på vilka källor som har påverkat ett visst svar.</p><p>Det finns dock tekniker som <strong>Retrieval Augmented Generation (RAG)</strong> som syftar till att övervinna denna begränsning. Med RAG kan modeller få tillgång till externa kunskapsbaser och <strong>tillhandahålla kontextualiserad information genom att citera källorna</strong> Denna metod är avgörande för att förbättra transparensen och tillförlitligheten hos modellgenererade svar.</p>",
                    "title": "Kan modellerna ange specifika källor för ett svar?"
                },
                "2": {
                    "desc": "<p><strong> Nej, \"råa\" konversationsbaserade AI-modeller kan inte svara på frågor om aktuella händelser.</strong>De är tränade på statiska datamängder och kan inte interagera med webben eller öppna länkar. De har inte möjlighet att uppdatera sig själva med händelser i världen. Informationen som modellen har tillgång till är begränsad till datumet för dess senaste träning.</p><p>Om du ställer en fråga om en nyhetshändelse kommer modellen därför att förlita sig på potentiellt föråldrad information och riskera att generera felaktiga svar.</p><p>När det gäller Perplexity, Copilot och ChatGPT kombineras de \"råa\" konversationsmodellerna med andra tekniska byggstenar som gör att de kan ansluta till internet för att få tillgång till information i realtid. Dessa kallas \"konversationsagenter\".</p>",
                    "title": "Om jag ställer en fråga om aktuella händelser, kan modellen svara?"
                },
                "3": {
                    "desc": "<p>Språkmodeller bearbetar frågetexten men har inte möjlighet att interagera med webben eller öppna länkar. De tränas på en fast textdatauppsättning, och deras svar baseras på denna träningsdata. När en fråga ställs använder modellerna denna träning för att generera ett svar men kan inte komma åt ny information online.</p><p>Som jämförelse, föreställ dig en student som gör ett prov utan internetåtkomst. De kan använda sina förvärvade kunskaper för att svara på frågor, men kan inte besöka webbplatser för ytterligare information.</p>",
                    "title": "Om min fråga innehåller en länk, kan modellen komma åt den?"
                },
                "4": {
                    "desc": "<p>Varje modell har ett begränsat <strong>kontextfönster</strong>, som representerar hur mycket tidigare information modellen kan komma ihåg. I långa och komplexa konversationer kan kontextfönstret snabbt ta slut, så att modellen glömmer viktiga delar av konversationen, vilket leder till inkonsekventa svar. Ju mindre fönstret är, desto snabbare uppstår problemet.</p>",
                    "title": "Varför tappar vissa modeller snabbt tråden i samtalet?"
                },
                "5": {
                    "desc": "<p>För att få bästa resultat från en språkmodell är det viktigt att behärska konsten att skriva bra <em>prompter</em>, det vill säga förfrågningar eller instruktioner. <strong>Tydlighet är nyckeln</strong>:</p><ul><li>Använd enkelt och direkt språk och undvik frågor som är för långa eller komplexa. Dela upp förfrågningar i flera enklare frågor för mer exakta svar.</li><li><strong>Ange specifika formatbegränsningar om det behövs</strong>: Om du behöver ett svar i ett visst format (lista, tabell, sammanfattning etc.), ange det i prompten. Du kan också ange vilka steg som ska följas och andra kriterier.</li><li><strong>Specificera modellens roll</strong> Börja till exempel med \"Agera som en expert på...\" eller \"Tänk dig att du är lärare...\" för att bestämma tonen och perspektivet i svaret.</li><li><strong>Kontextualisera dina frågor</strong>. Ge om nödvändigt relevanta exempel för att vägleda modellen.</li><li><strong>Uppmuntra resonemang</strong>. Använd en tankekedja (<em>Chain-of-Thought Prompting</em>) för att be modellen förklara sitt resonemang, vilket gör svaren mer robusta.</li></ul><p>Konversationsmodeller är känsliga för variationer i formuleringar – enkelt språk, korta frågor och omformuleringar vid behov kan hjälpa till att vägleda modellen mot relevanta svar. Testa och förfina dina frågor för att hitta den mest effektiva formuleringen!</p>",
                    "title": "Hur skriver man en effektiv prompt (fråga/uppmaning) till modellen?"
                },
                "6": {
                    "desc": "<p>Konversations-AI svarar direkt genom att formulera meningar från en stor datamängd som modellen har tränats på, medan en sökmotor erbjuder länkar och resurser som användaren kan utforska på egen hand.</p>",
                    "title": "Vad är skillnaden mellan att ställa en fråga till en konversationsbaserad AI-modell och att söka på Google?"
                }
            }
        }
    },
    "footer": {
        "backHome": "Tillbaka till startsidan",
        "helpUs": "Hjälp oss att förbättra den här tjänsten!",
        "license": {
            "linkTitle": "Etalab-licens – nytt fönster",
            "mention": "Innehållet på denna webbplats är tillgängligt under <a {linkProps}>etalab-2.0-licensen</a>, förutom där rättigheter som innehas av tredje part uttryckligen anges"
        },
        "links": {
            "legal": "Rättsliga meddelanden",
            "privacy": "Integritetspolicy",
            "sources": "Källkod",
            "tos": "Användarvillkor"
        },
        "writeUs": "Om du stöter på problem eller har kommentarer om jämförelseverktyget är du välkommen att skriva till oss med hjälp av <a {linkProps}>det här formuläret</a>. Vi läser alla meddelanden.<br />Tack!"
    },
    "general": {
        "a11y": {
            "desc": "Denna tillgänglighetspolicy gäller webbplatsen <strong>comparia.beta.gouv.fr</strong>.",
            "disclaimer": "<strong> compar:IA </strong> åtar sig att göra sina digitala tjänster tillgängliga, i enlighet med artikel 47 i lag nr 2005-102 av den 11 februari 2005.",
            "improveAdress": "Adress: DINUM, 20 avenue de Ségur, 75007 Paris",
            "improveDelay": "Vi försöker svara inom 2 arbetsdagar.",
            "improveDesc": "Om du har problem att komma åt innehåll eller tjänst kan du kontakta administratören för beta.gouv.fr, för att bli hänvisad till ett tillgängligt alternativ eller få innehållet i ett annat format.",
            "improveMail": "E-post: <a {linkProps}>contact@beta.gouv.fr</a>",
            "improveTitle": "Feedback och kontakt",
            "remedyAdvocate": "Skriv ett meddelande till <a {linkProps}>rättighetsförsvararen</a>",
            "remedyAdvocateAdress": "Skicka ett brev med posten (gratis, inget frimärke behövs): Défenseur des droits - Libre réponse 71120 75342 Paris CEDEX 07",
            "remedyDelegateAdvocate": "Kontakta <a {linkProps}>representanten för rättighetsförsvararen i din region</a>",
            "remedyDesc": "Denna procedur ska användas i följande fall: Du har rapporterat ett tillgänglighetsfel till webbplatsansvarig som hindrar dig från att komma åt innehåll eller en av portalens tjänster och du inte har fått ett tillfredsställande svar.",
            "remedyList": "Du kan:",
            "remedyTitle": "Överklaga",
            "stateDesc": "Webbplatsen comparia.beta.gouv.fr uppfyller inte RGAA 4.1. Webbplatsen har ännu inte granskats. <strong>Den har dock utformats för att vara tillgänglig för så många som möjligt</strong>. Du bör därför kunna:",
            "stateNavigate": "navigera på alla sidor på webbplatsen med hjälp av ett tangentbord",
            "statePrefs": "anpassa webbplatsen efter dina preferenser (teckenstorlek, skärmzoom, ändring av typsnitt etc.) utan att innehåll förloras",
            "stateScreenReader": "visa webbplatsen med en skärmläsare.",
            "stateTitle": "Efterlevnad",
            "title": "Tillgänglighetspolicy"
        },
        "legal": {
            "a11yDesc": "Efterlevnad av digitala tillgänglighetsstandarder är ett framtida mål, men vi arbetar för att göra webbplatsen tillgänglig för alla.",
            "a11yTitle": "Tillgänglighet",
            "directorDesc": "Romain Delassus, chef för kulturministeriets digitala avdelning",
            "directorTitle": "Publikationsansvarig",
            "editorDesc": "Denna webbplats publiceras av franska kulturministeriet. Postadress: Ministère de la culture, 182 Rue Saint-Honoré, 75001 Paris",
            "editorTitle": "Redaktör"
        }
    },
    "header": {
        "subtitle": "Jämförelseverktyg för konversations-AI",
        "title": {
            "compar": "jämföra",
            "ia": "AI"
        }
    },
    "models": {
        "conditions": "Användarvillkor",
        "extra": {
            "experts": {
                "api-only": "För att lära dig mer, titta på <a {linkProps}>modellens officiella webbplats</a>",
                "open-weights": "För att lära dig mer, gå till <a {linkProps}>modellens sida på Hugging Face</a>"
            },
            "impacts": "Miljöpåverkansberäkningarna baseras på projekten <a {linkProps1}>EcoLogits</a> och <a {linkProps2}>Impact CO<sub>2</sub></a>.",
            "title": "För att lära dig mer"
        },
        "licenses": {
            "commercial": "Kommersiell licens",
            "descriptions": {
                "Apache 2.0": "Denna licens låter dig använda, ändra eller sprida en modell, även för kommersiella ändamål. Utöver det garanteras rättsligt skydd genom en klausul om icke-intrång och transparens. Alla ändringar måste därför vara dokumenterade och spårbara.",
                "CC-BY-NC-4.0": "Denna licens tillåter att du sprider och ändrar innehållet, så länge upphovsmannen nämns, med förbjuder kommersiell användning. Det ger flexibilitet för icke-kommersiell användning, medan skaparens rättigheter bevaras.",
                "Gemma": "Denna licens är skapad för att uppmuntra att mjukvaran används, ändras och sprids, men innehåller en klausul som säger att alla ändrade versioner måste delas under samma licens, för att främja samarbete och transparens inom mjukvaruutveckling.",
                "Jamba Open Model": "Denna licens tillåter att koden fritt används, reproduceras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för organisationer vars omsättning överskrider 50 miljoner USD.",
                "Llama 3 Community": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad, och förbjuder att kod eller genererat material återanvänds för att träna eller utveckla konkurrerande modeller, för att därigenom skydda Metas investeringar och varumärke.",
                "Llama 3.1": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "Llama 3.3": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "Llama 4": "Denna licens tillåter att koden fritt används, ändras och sprids, så länge upphovsmannen nämns, men sätter begränsningar för processer med över 700 miljoner användare per månad. Kod och genererat material får återanvändas för att träna eller utveckla modeller som bygger vidare på denna, så länge texten \"built with llama\" visas och ordet \"Llama\" ingår i titeln.",
                "MIT": "MIT-licensen är en licens för fri programvara. Den tillåter att vem som helst återanvänder, ändrar eller sprider en modell, även för kommersiella ändamål, förutsatt att den ursprungliga licensen och upphovsrättstexten skickas med.",
                "Mistral AI Non-Production": "Denna licens tillåter att du sprider och ändrar innehållet, så länge upphovsmannen nämns, med förbjuder kommersiell användning. Det ger flexibilitet för icke-kommersiell användning, medan skaparens rättigheter bevaras.",
                "propriétaire Anthropic": "Modellen är tillgänglig under en betald licens och åtkomlig via API på Anthropics plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens, eller enligt företagets villkor.",
                "propriétaire Gemini": "Modellen är tillgänglig under en betald licens och åtkomlig via Gemini API som finns tillgängligt på Google AI Studio och Vertex AI, vilket kräver en avgift per användning baserat på antalet bearbetade tokens eller enligt företagets villkor.",
                "propriétaire Liquid": "Modellen är tillgänglig under en betald licens och åtkomlig via API på Liquid AIs plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens.",
                "propriétaire Mistral": "Modellen är tillgänglig under en betald licens och åtkomlig via Mistral och andra partner-API:er. Den kräver en avgift per användning baserad på antalet bearbetade tokens.",
                "propriétaire OpenAI": "Modellen är tillgänglig under en betald licens och åtkomlig via API på OpenAIs plattformar. Den kräver en avgift per användning baserad på antalet bearbetade tokens, eller enligt företagets villkor.",
                "propriétaire xAI": "Modellen är tillgänglig via xAI API. Det kräver betalning per användning baserat på antalet bearbetade tokens, eller enligt företagets villkor."
            },
            "name": "Licens {licence}",
            "noDesc": "Licensinformation saknas för denna modell.",
            "type": {
                "openSource": "Öppen källkod",
                "proprietary": "Proprietär",
                "semiOpen": "Semi-öppen"
            }
        },
        "list": {
            "filters": {
                "display": "Visa filter",
                "editor": {
                    "legend": "Utgivare"
                },
                "license": {
                    "legend": "Användarlicens"
                },
                "size": {
                    "labels": {
                        "L": "70 till 150 miljarder",
                        "M": "20 till 70 miljarder",
                        "S": "7 till 20 miljarder",
                        "XL": "> 150 miljarder",
                        "XS": "< 7 miljarder"
                    }
                }
            },
            "intro": "Utforska de olika konversationsbaserade AI-modellerna, deras specifikationer och licenser.",
            "model": "modell",
            "models": "modeller",
            "noresults": "Inga modeller matchar dina sökkriterier.",
            "title": "Utforska modellerna",
            "triage": {
                "label": "Sortera efter",
                "options": {
                    "date-desc": "Utgivningsdatum (från nyaste till äldsta)",
                    "name-asc": "Modellnamn (bokstavsordning)",
                    "org-asc": "Utgivare (bokstavsordning)",
                    "params-asc": "Storlek (från minsta till största)"
                }
            }
        },
        "names": {
            "a": "Modell A",
            "b": "Modell B"
        },
        "openWeight": {
            "conditions": {
                "copyleft": "Copyleft",
                "free": "Tillåtande",
                "restricted": "Villkorlig"
            },
            "descriptions": {
                "L": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin stora modeller (mellan 70 och 100 miljarder parametrar).",
                "M": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin medelstora modeller (mellan 20 och 70 miljarder parametrar).",
                "S": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin små modeller (mellan 7 och 20 miljarder parametrar).",
                "XL": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin mycket stora modeller.",
                "XS": "Med {paramsCount} miljarder parametrar tillhör den här modellen kategorin mycket små modeller (färre än 7 miljarder parametrar)."
            },
            "tooltips": {
                "copyleft": "När modellen har modifierats får den bara spridas med samma licens som källmodellen.",
                "free": "När modellen har modifierats kan den spridas under en annan licens än källmodellen.",
                "openSource": "Träningsdata, kod och vikter för denna modell (dvs. parametrarna som lärs in under träningen) kan laddas ner och modifieras fritt. Att en modell är \"öppen källkod\" ställer hårdare krav än \"öppna vikter\", särskilt på grund av behovet av transparens i träningsmaterialet, och få modeller anses vara \"öppen källkod\".",
                "openWeight": "En så kallad \"open weights\"-modell, vilket innebär att dess vikter – de parametrar som lärs in under träning – kan laddas ner fritt, vilket gör att det går att köra modellen på sin egen dator. Att en modell är \"öppen källkod\" ställer hårdare krav än \"öppna vikter\", särskilt på grund av behovet av transparens i träningsmaterialet, och få modeller anses vara \"öppen källkod\".",
                "params": "Parametrar eller vikter – ofta flera miljarder – är de variabler som lärs in av en modell under träning och som bestämmer dess svar. Ju fler parametrar, desto större inlärningskapacitet har modellen.",
                "ram": "RAM-minnet (Random Access Memory) lagrar data som bearbetas av en LLM i realtid. Ju större modellen är, desto mer RAM behöver den för att köras."
            },
            "use": {
                "attribution": "Attribution krävs",
                "commercial": "Kommersiell användning",
                "licenseType": "Licenstyp",
                "modification": "Ändring godkänd",
                "requiredRam": "Krav på RAM-minne"
            }
        },
        "parameters": "{number} parametrar",
        "ram": "{min} till {max} GB",
        "release": "Utgiven {date}",
        "size": {
            "descriptions": {
                "L": "Stora modeller kräver betydande resurser, men erbjuder bäst prestanda för avancerade uppgifter som kreativt skrivande, dialogmodellering och tillämpningar som kräver en noggrann förståelse av kontext.",
                "M": "Medelstora modeller erbjuder en bra balans mellan enkelhet och prestanda. De är mycket mindre resurskrävande än stora modeller, men kan fortfarande hantera komplexa uppgifter som sentimentanalys och resonemang.",
                "S": "En liten modell är mindre komplex och resurskrävande jämfört med större modeller, och ger fortfarande tillräcklig prestanda för många uppgifter (sammanfattning, översättning, textklassificering etc.)",
                "XL": "Dessa modeller, med hundratals miljarder parametrar, är de mest komplexa och avancerade vad gäller prestanda och noggrannhet. De kräver stora beräknings- och minnesresurser och är avsedda för mycket avancerade applikationer och högt specialiserade miljöer.",
                "XS": "Mycket små modeller, med färre än 7 miljarder parametrar, är de minst komplexa och mest resurseffektiva och ger tillräcklig prestanda för enkla uppgifter som textklassificering."
            },
            "estimated": "Uppskattad storlek ({size})",
            "title": "Storlek"
        }
    },
    "modes": {
        "random": {
            "label": "Slump",
            "title": "Slumpläge"
        }
    }
}
