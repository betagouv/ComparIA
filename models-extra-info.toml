[Aya23-35B]
simple_name = "Aya23-35B"
organisation = "Cohere"
icon_path = "cohere.png"
params = 35
license = "CC-BY-NC-4.0"
description = "Aya 23 35B de Cohere est un modèle de taille moyenne de la famille Command R qui a spécialement été entraîné sur un corpus multilingue." 

[aya-23-8b]
simple_name = "Aya 23 8B"
organisation = "Cohere"
icon_path = "cohere.png"
params = 8
license = "CC-BY-NC-4.0"
description = "Aya 23 8B de Cohere est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue." 

[aya-expanse-8b]
simple_name = "Aya Expanse 8B"
organisation = "Cohere"
icon_path = "cohere.png"
params = 8
license = "CC-BY-NC-4.0"
description = "Aya Expanse 8B de Cohere, entreprise canadienne, est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue." 

[claude-3-5-sonnet-v2]
simple_name = "Claude 3.5 Sonnet v2"
organisation = "Anthropic"
icon_path = "anthropic.png"
friendly_size = "XL"
active_params = 100
params = 300
description = "Meilleur modèle de la famille Claude 3.5, ce modèle est spécialisé dans la génération de textes littéraires et un ton plus naturel. La version v2 est sortie en octobre 2024."
license = "propriétaire Anthropic"
url = "https://www.anthropic.com/news/3-5-models-and-computer-use"
release_date = "10/2024"

["claude-3-7-sonnet"]
simple_name = "Claude 3.7 Sonnet"
organisation = "Anthropic"
icon_path = "anthropic.png"
friendly_size = "XL"
active_params = 100
params = 300
description = "Meilleur modèle de la famille Claude, ce modèle est spécialisé dans la génération de textes littéraires et un ton plus naturel."
license = "propriétaire Anthropic"
url = "https://www.anthropic.com/news/claude-3-7-sonnet"
release_date = "02/2025"

[command-r-plus]
simple_name = "Command R+"
organisation = "Cohere"
icon_path = "cohere.png"
params = 104
license = "CC-BY-NC-4.0"
description = "Grand frère de la famille Command R de Cohere, ce modèle de langage est orienté pour l'usage professionnel et conçu spécifiquement pour les tâches de recherche et d'extraction d'informations."

[c4ai-command-r-08-2024]
simple_name = "Command R"
params = 35
organisation = "Cohere"
icon_path = "cohere.png"
license = "CC-BY-NC-4.0"
excerpt = "Actualisé en août 2024, ce modèle multilingue de l'entreprise canadienne Cohere, a été entrainé sur 23 langues et est conçu pour les tâches de recherche et d'extraction d'informations."
description = "Actualisé en août 2024, ce modèle multilingue de l'entreprise canadienne Cohere, plus petit que Command R+, a été entrainé sur 23 langues et est conçu spécifiquement pour les tâches de recherche et d'extraction d'informations."
release_date = "08/2024"

["chocolatine-14b-instruct-dpo-v1.2"]
url = "https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-DPO-v1.2"
simple_name = "Chocolatine 14B"
organisation = "jpacifico"
params = 14
icon_path = "huggingface.svg"
license = "MIT"
excerpt = "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française."
description = "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."

["chocolatine-14b-instruct-dpo-v1.2-q4"]
url = "https://huggingface.co/jpacifico/Chocolatine-14B-Instruct-DPO-v1.2-Q4_K_M-GGUF"
simple_name = "Chocolatine 14B"
organisation = "jpacifico"
params = 14
quantization = "q4"
icon_path = "huggingface.svg"
license = "MIT"
excerpt = "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française."
description = "Reposant sur le modèle Phi-3 Medium de Microsoft, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."

["chocolatine-2-14b-instruct-v2.0.3"]
url = "https://huggingface.co/jpacifico/jpacifico/Chocolatine-2-14B-Instruct-v2.0.3"
simple_name = "Chocolatine 2 14B"
organisation = "jpacifico"
params = 14
icon_path = "huggingface.svg"
license = "MIT"
excerpt = "Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française."
description = "Reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français."

["chocolatine-2-14b-instruct-v2.0.3-q8"]
url = "https://huggingface.co/jpacifico/jpacifico/Chocolatine-2-14B-Instruct-v2.0.3"
simple_name = "Chocolatine 2 14B"
organisation = "jpacifico"
params = 14
quantization = "q8"
icon_path = "huggingface.svg"
license = "MIT"
excerpt = "Sorti en février 2025, et reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française."
description = "Sorti en février 2025, et reposant sur le modèle Qwen2.5 de la société Alibaba, ce modèle a été spécialisé sur la langue française. Le nom du modèle 'Chocolatine' est un clin d'oeil au projet CroissantLLM qui était une des première initiatives de création de modèle open-source de petite taille optimisé pour le français. Il est proposé ici en version quantisée q8 (compressée de moitié)."
release_date = "02/2025"

["deepseek-r1"]
url = "https://huggingface.co/deepseek-ai/DeepSeek-R1"
simple_name = "DeepSeek R1"
organisation = "DeepSeek"
total_params = 671
active_params = 37
icon_path = "deepseek.png"
license = "MIT"
excerpt = "Sorti en janvier 2025, ce modèle est basé sur Deepseek V3 qui élabore un raisonnement avant de formuler une réponse."
description = "Sorti en janvier 2025, ce modèle phare de la société chinoise DeepSeek, est un modèle qui élabore un raisonnement avant de formuler une réponse. DeepSeek R1 est basé sur Deepseek V3, qui possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
reasoning = true
release_date = "01/2025"

["deepseek-r1-distill-llama-70b"]
url = "https://huggingface.co/deepseek-ai/deepseek-r1-distill-llama-70b"
simple_name = "DeepSeek R1 Llama 70B"
organisation = "DeepSeek"
params = 70
icon_path = "deepseek.png"
excerpt = "Ce modèle est un modèle Llama 3.3 70B transformé par l'entreprise DeepSeek pour être doté d'un système de raisonnement à travers un corpus synthétique généré par DeepSeek R1."
description = "Ce modèle est un modèle Llama 3.3 70B transformé par l'entreprise DeepSeek pour être doté d'un système de raisonnement à travers un corpus synthétique généré par DeepSeek R1. Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.3"
reasoning = true

["deepseek-v3-chat"]
url = "https://huggingface.co/deepseek-ai/DeepSeek-V3"
simple_name = "DeepSeek v3"
organisation = "DeepSeek"
total_params = 671
active_params = 37
icon_path = "deepseek.png"
license = "MIT"
excerpt = "Sorti en décembre 2024, le modèle DeepSeek V3 possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
description = "Sorti en décembre 2024, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
release_date = "12/2024"

["deepseek-v3-0324"]
url = "https://huggingface.co/deepseek-ai/DeepSeek-V3-0324"
simple_name = "DeepSeek V3 (0324)"
organisation = "DeepSeek"
total_params = 685
active_params = 37
icon_path = "deepseek.png"
license = "MIT"
excerpt = "Mis à jour en mars 2025, cette version du modèle DeepSeek V3 est la plus aboutie à ce jour."
description = "Sorti en décembre 2024 et mis à jour en mars 2025, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
release_date = "03/2025"

# ["eurollm-1.7b"]
# url = "https://huggingface.co/utter-project/eurollm-1.7b"
# simple_name = "EuroLLM 1.7B"
# organisation = "Utter"
# params = 1.7
# # icon_path = "deepseek.png"
# license = "Apache 2.0"
# # excerpt = "Sorti en décembre 2024, le modèle DeepSeek V3 possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
# # description = "Sorti en décembre 2024, ce modèle phare de la société chinoise DeepSeek possède une architecture Mixture-of-Experts qui lui permet d'être d'une très grande taille en diminuant les coûts d'inférence."
# # release_date = "12/2024"

["gemini-1.5-pro-001"]
url = "https://ai.google.dev/gemini-api"
active_params = 220
total_params = 440
simple_name = "Gemini 1.5 Pro"
organisation = "Google"
icon_path = "google.png"
friendly_size = "XL"
license = "propriétaire Gemini"
description = "Sorti en février 2024, ce modèle multilingue et multimodal est capable de traiter un très grand volume de données d’entrées qu’il s’agisse de données textuelles, d’image, de son (jusqu’à 11h d’audio) ou de vidéo (jusqu’à une heure). C’est le modèle LLM qui alimente le chatbot Gemini de Google."
excerpt = "Sorti en février 2024, ce modèle multimodal s'applique à la génération de textes et d'images, l'analyse de vidéos et la transcription d'audio."
release_date = "02/2024"

["gemini-1.5-pro-002"]
url = "https://ai.google.dev/gemini-api"
active_params = 220
total_params = 440
simple_name = "Gemini 1.5 Pro"
organisation = "Google"
icon_path = "google.png"
friendly_size = "XL"
license = "propriétaire Gemini"
description = "Sorti en février 2024 et amélioré en septembre 2024, ce modèle multilingue et multimodal est capable de traiter un très grand volume de données d’entrées qu’il s’agisse de données textuelles, d’image, de son (jusqu’à 11h d’audio) ou de vidéo (jusqu’à une heure). C’est le modèle LLM qui alimente le chatbot Gemini de Google."
excerpt = "Sorti en septembre 2024, ce modèle multimodal s'applique à la génération de textes et d'images, l'analyse de vidéos et la transcription d'audio."
release_date = "09/2024"

["gemini-2.0-flash-001"]
url = "https://ai.google.dev/gemini-api"
params = 40
simple_name = "Gemini 2.0 Flash"
organisation = "Google"
icon_path = "google.png"
friendly_size = "M"
license = "propriétaire Gemini"
description = "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés que les modèles Gemini Pro."
excerpt = "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés."
release_date = "12/2024"

["gemini-2.0-flash-exp"]
url = "https://ai.google.dev/gemini-api"
params = 40
simple_name = "Gemini 2.0 Flash"
organisation = "Google"
icon_path = "google.png"
friendly_size = "M"
license = "propriétaire Gemini"
description = "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés que les modèles Gemini Pro."
excerpt = "Sorti en décembre 2024, ce plus petit modèle multilingue et multimodal est de la famille Flash de Gemini, permettant une réponse très rapide pour des raisonnements moins avancés."
release_date = "12/2024"

["gemma-2-27b-it-q8"]
url = "https://huggingface.co/google/gemma-2-27b-it"
simple_name = "Gemma 2 27B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 27
excerpt = "Modèle performant avec une taille correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision."
description = "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions. Le modèle sollicité ici est la version quantisée (q8)."

[Gemma-2-9B-it]
url = "https://huggingface.co/google/gemma-2-9b-it"
simple_name = "Gemma 2 9B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 9
excerpt = "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
description = "Petit frère de la famille Gemma 2, ce modèle sorti en juin 2024 est entrainé pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."
release_date = "06/2024"

[gemma-2-2b-it]
url = "https://huggingface.co/google/gemma-2-2b-it"
simple_name = "Gemma 2 2B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 2
description = "Petit frère de la famille Gemma 2, ce très petit modèle sorti en juillet 2024 arrive à rivaliser avec des modèles bien plus gros."
release_date = "07/2024"

["gemma-3-12b"]
url = "https://huggingface.co/google/gemma-3-12b-it"
simple_name = "Gemma 3 12B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 12
excerpt = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des cartes graphiques à usage non professionnel."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des cartes graphiques à usage non professionnel."
release_date = "03/2025"

["gemma-3-4b"]
url = "https://huggingface.co/google/gemma-3-4b-it"
simple_name = "Gemma 3 4B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 4
excerpt = "Troisième génération de la famille Gemma de Google, sa toute petite taille lui permet de fonctionner sur des petites cartes graphiques voire sur téléphone portable."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et sa taille lui permet de fonctionner sur des petites cartes graphiques voire sur téléphone portable."
release_date = "03/2025"

["mistral-small-3.1-24b"]
url = "https://huggingface.co/mistralai/mistral-small-3.1-24b-instruct-2503"
simple_name = "Mistral Small 3.1 24B"
organisation = "Mistral AI"
icon_path = "mistral.png"
license = "Mistral"
params = 24
excerpt = "Mistral Small 3.1 24B Instruct est une variante améliorée du Mistral Small 3 (janvier 2025), dotée de 24 milliards de paramètres et de capacités multimodales avancées."
description = "Mistral Small 3.1 24B Instruct est un modèle multimodal qui offre des performances de pointe dans les tâches de raisonnement basées sur le texte et la vision, y compris l'analyse d'images, la programmation, le raisonnement mathématique et le soutien multilingue pour des dizaines de langues."
release_date = "03/2025"

["mistral-saba"]
url = "https://mistral.ai/news/mistral-saba"
simple_name = "Mistral Saba"
organisation = "Mistral AI"
icon_path = "mistral.png"
license = "propriétaire Mistral"
params = 24
excerpt = "Mistral Saba est un modèle de 24 milliards de paramètres conçu pour le Moyen-Orient et l'Asie du Sud. Il comprend l'arabe et plusieurs langues d'origine indienne (tamoul, malayalam)."
description = "Mistral Saba est un modèle propriétaire de 24 milliards de paramètres conçu pour le Moyen-Orient et l'Asie du Sud. Il comprend l'arabe et plusieurs langues d'origine indienne (tamoul, malayalam)."
release_date = "02/2025"

["command-a"]
url = "https://huggingface.co/CohereForAI/c4ai-command-a-03-2025"
simple_name = "Command A"
organisation = "Cohere"
icon_path = "cohere.png"
license = "CC-BY-NC-4.0"
params = 111
excerpt = "Command A de l'entreprise canadienne Cohere est un modèle de 111 milliards de paramètres axé sur le multilinguisme la livraison de grandes performances dans les cas d'utilisation agentique, multilingues et de codage."
description = "Command A est un modèle de 111 milliards de paramètres avec une fenêtre de contexte de 256 000, axé sur la livraison de grandes performances dans les cas d'utilisation agentique, multilingues et de codage."
release_date = "03/2025"

["aya-expanse-32b"]
url = "https://huggingface.co/CohereForAI/aya-expanse-32b"
simple_name = "Aya Expanse 32B"
organisation = "Cohere"
icon_path = "cohere.png"
license = "CC-BY-NC-4.0"
params = 32
excerpt = "Aya Expanse 32B est un modèle de l'entreprise canadienne Cohere, qui est spécialisé dans le multilinguisme. Plus de 23 langues et de nombreuses langues non-européennes sont supportées."
description = "Aya Expanse 32B est un modèle de l'entreprise canadienne Cohere, qui est spécialisé dans le multilinguisme. Plus de 23 langues sont supportées : arabe, chinois, tchèque, néerlandais, anglais, français, allemand, grec, hébreu, hindi, indonésien, italien, japonais, coreéen, persan, polonais, portugais, roumain, russe, espagnol, turc, ukrainien et vietnamien."
release_date = "12/2024"

["olmo-2-0325-32b"]
url = "https://huggingface.co/allenai/OLMo-2-0325-32B-Instruct"
simple_name = "OLMo-2 32B"
organisation = "Ai2"
icon_path = "ai2.svg"
license = "Apache 2.0"
params = 32
excerpt = "OLMo 2 32B est un modèle entièrement open source (corpus et code d'entraînement inclus) créé par l'Allen AI Institute (Ai2), publié en mars 2025. "
description = "OLMo 2 32B est un modèle entièrement open source : le corpus et le code d'entraînement sont entièrement accessibles. Cette famille de modèle OLMo a été conçue par l'Allen Institute for AI (Ai2)."
release_date = "03/2025"
fully_open_source = true

["gemma-3-27b"]
url = "https://huggingface.co/google/gemma-3-27b-it"
simple_name = "Gemma 3 27B"
organisation = "Google"
icon_path = "google.png"
license = "Gemma"
params = 27
excerpt = "Modèle de troisième génération de la famille Gemma de Google. Malgré sa moyenne taille, il pouvait rivaliser à sa sortie avec de nombreux modèles propriétaires supposés bien plus gros."
description = "Troisième génération de la famille Gemma de Google, ce modèle est multimodal et malgré sa moyenne taille, il pouvait rivaliser à sa sortie avec de nombreux modèles propriétaires supposés bien plus gros."
release_date = "03/2025"

["gpt-3.5-turbo-0301"]
url = "https://platform.openai.com/docs/models/gp#gpt-3-5-turbo"
simple_name = "GPT-3.5"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "L"
license = "propriétaire OpenAI"
description = "Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel."
excerpt = "Modèle lancé en mars 2023, GPT-3.5 est un modèle plus petit d'OpenAI suffisant pour diverses tâches de traitement du langage naturel."
release_date = "03/2023"

["gpt-4.1-mini"]
url = "https://openai.com/index/gpt-4-1/"
simple_name = "GPT-4.1 Mini"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "L"
license = "propriétaire OpenAI"
description = "GPT-4.1 Mini est le modèle de taille intermédiaire de la série GPT-4.1, remplaçant la série GPT-4o, et disponible via l'application ChatGPT de l'entreprise américaine OpenAI."
excerpt = "GPT-4.1 Mini est le modèle de taille intermédiaire de la série GPT-4.1 lancée en avril 2025, remplaçant la série GPT-4o, et disponible via l'application ChatGPT de l'entreprise américaine OpenAI."
release_date = "04/2025"

["gpt-4.1-nano"]
url = "https://openai.com/index/gpt-4-1/"
simple_name = "GPT-4.1 Nano"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "M"
license = "propriétaire OpenAI"
description = "GPT-4.1 Nano est le modèle de plus petite taille de la série GPT-4.1, remplaçant la série GPT-4o, et disponible via l'application ChatGPT de l'entreprise américaine OpenAI."
excerpt = "GPT-4.1 Nano est le modèle de plus petite taille de la série GPT-4.1 lancée en avril 2025, remplaçant la série GPT-4o, et disponible via l'application ChatGPT de l'entreprise américaine OpenAI."
release_date = "04/2025"

["gpt-4o-mini-2024-07-18"]
url = "https://platform.openai.com/docs/models/gp#gpt-4o-mini"
simple_name = "GPT-4o mini"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "M"
license = "propriétaire OpenAI"
description = "Modèle lancé en juillet 2024 et remplaçant GPT-3.5, GPT-4o mini est une version plus petite de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA."
excerpt = "Le plus petit des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en juillet 2024."
release_date = "07/2024"

[gpt-4o-2024-08-06]
url = "https://platform.openai.com/docs/models/gpt#gpt-4o"
simple_name = "GPT-4o"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "XL"
license = "propriétaire OpenAI"
description = "Modèle lancé en août 2024 et successeur de GPT-4, GPT-4o est une version améliorée de GPT-4, conçue pour diverses tâches de traitement du langage naturel via, par exemple, l'application ChatGPT de l'entreprise américaine OpenIA."
excerpt = "Le plus grand des deux modèles sur lesquels repose ChatGPT d'OpenAI, lancé en août 2024."
release_date = "08/2024"

["grok-3-mini-beta"]
url = "https://x.ai/news/grok-3"
simple_name = "Grok 3 Mini"
organisation = "xAI"
icon_path = "xai.svg"
friendly_size = "XL"
reasoning = true
license = "propriétaire xAI"
description = "Grok 3 Mini est doté de capacités de raisonnement. Il est moins coûteux que le modèle Grok 3 de xAI, aussi doté de raisonnement. C'est le modèle de base qui est ici interrogé, donc non connecté à Internet, à l'inverse du bot Grok présent sur le réseau X."
excerpt = "Grok 3 Mini est doté de capacités de raisonnement. Il est moins coûteux que le modèle Grok 3 de xAI, aussi doté de raisonnement."
release_date = "04/2025"

["hermes-3-llama-3.1-405b"]
url = "https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-405B"
simple_name = "Hermes 3 405B"
organisation = "Nous"
icon_path = "nous.webp"
params = 405
excerpt = "Sorti en juillet 2024, ce modèle est le plus grand modèle de la série Hermes. C'est une adaptation du modèle Llama 3.1 405B de Meta."
description = "Doté de 405 milliards de paramètres, ce modèle est la version la plus large de la troisième itération des modèles Hermes. Ce modèle Hermes 3 est un fine-tune conçu par l'entreprise américaine 'Nous Research' à partir du modèle Llama 3.1 405B de Meta. Sa taille et son coût élevé en inférence le réservent à des applications exigeantes de raisonnement et de programmation."
license = "Llama 3.1"
release_date = "07/2024"

["jamba-1.5-large"]
url = "https://huggingface.co/ai21labs/AI21-Jamba-1.5-Large"
simple_name = "Jamba 1.5 Large"
organisation = "AI21"
icon_path = "ai21.webp"
friendly_size = "XL"
excerpt = "Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
description = "Sorti en août 2024, ce modèle de l'entreprise AI21 est un modèle de type particulier hybride dit 'SSM' (State Space Models) et Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
total_params = 400
active_params = 94
license = "Jamba Open Model"
release_date = "08/2024"

["lfm-40b"]
url = "https://www.liquid.ai/liquid-foundation-models"
simple_name = "LFM 40B"
organisation = "Liquid"
icon_path = "liquid.svg"
friendly_size = "M"
excerpt = "Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts, dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
description = "Sorti en septembre 2024, ce modèle de l'entreprise américaine Liquid est un modèle de type Mixture of Experts (MoE), dont l'architecture cherche à tirer le meilleur parti du nombre de paramètres."
params = 40
license = "propriétaire Liquid"
release_date = "09/2024"

["llama-3.1-405b"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-405B-Instruct"
simple_name = "Llama 3.1 405B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Sorti en juillet 2024, ce modèle est le plus grand modèle de la série Llama 3.1 de Meta, optimisé pour des tâches complexes de programmation, de mathématique et de raisonnement."
description = "Doté de 405 milliards de paramètres, ce modèle est le plus gros modèle de la série Llama 3.1 de Meta. Sa taille et son coût élevé en inférence le réservent à des applications exigeantes de raisonnement et de programmation. Le modèle a été entraîné sur un corpus de 15 milliards de tokens dont la limite de connaissances est fixée au mois de décembre 2023."
params = 405
license = "Llama 3.1"
release_date = "07/2024"

["llama-4-scout"]
url = "https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E-Instruct"
simple_name = "Llama 4 Scout"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Sorti en avril 2025, ce modèle de la série Llama 4 représente la gamme la plus petite. Il possède une architecture de 128 experts et un contexte de 10 milliards de tokens."
description = "Sorti en avril 2025, ce modèle de la série Llama 4 est de la gamme la plus petite (par rapport à Maverick et Behemoth). Il possède une architecture Mixture-of-Experts avec 128 experts et un contexte étendu à 10 milliards de tokens"
total_params = 109
active_params = 17
license = "Llama 4"
release_date = "04/2025"

["llama-3.1-8b"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct"
simple_name = "Llama 3.1 8B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Version la plus légère de Llama 3.1, ce modèle est rapide et adapté aux applications courantes. Sorti en avril 2024, les données sur lesquelles il a été entraîné remontent au mois de décembre 2023."
description = "Lancé en avril 2024, ce modèle cadet de la famille Llama 3.1 a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il est adapté aux tâches multilingues et au traitement de longs textes pouvant aller jusqu’à 128 000 tokens."
params = 8
license = "Llama 3.1"
release_date = "04/2024"

["llama-3.1-70b"]
url = "https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct"
params = 70
simple_name = "Llama 3.1 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Doté de 70 milliards de paramètres et sorti en avril 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
description = "Comme les autres modèles de la famille Llama 3.1, ce modèle sorti en avril 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.1"
release_date = "04/2024"

["llama-3.3-70b"]
url = "https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct"
params = 70
simple_name = "Llama 3.3 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Doté de 70 milliards de paramètres et sorti en décembre 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
description = "Remplaçant la famille Llama 3.2, ce modèle sorti en décembre 2024 a été entraîné sur des données qui remontent au mois de décembre 2023. Inutile de l'interroger sur les temps forts des Jeux olympiques de Paris 2024 ! Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues. "
license = "Llama 3.3"
release_date = "12/2024"

["llama-3.1-nemotron-70b-instruct"]
url = "https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct"
params = 70
simple_name = "Llama 3.1 Nemotron 70B"
organisation = "NVIDIA"
icon_path = "nvidia.svg"
excerpt = "Doté de 70 milliards de paramètres, ce modèle est un fine-tune conçu par le concepteur de cartes graphiques NVIDIA à partir du modèle Llama 3.1 70B de Meta."
description = "Doté de 70 milliards de paramètres, ce modèle est un fine-tune conçu par le concepteur de cartes graphiques NVIDIA à partir du modèle Llama 3.1 70B de Meta. Sorti en octobre 2024, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
license = "Llama 3.1"
release_date = "10/2024"

[Meta-Llama-3-70B-Instruct]
params = 70
simple_name = "Llama 3 70B"
organisation = "Meta"
icon_path = "meta.svg"
excerpt = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens mais supporte un contexte relativement restreint de 8000 tokens."
description = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens."
license = "Llama 3 Community"
release_date = "04/2024"

[Meta-Llama-3-8B-Instruct]
params = 8
simple_name = "Llama 3 8B"
organisation = "Meta"
icon_path = "meta.svg"
description = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité."
excerpt = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité."
license = "Llama 3 Community"

["mistral-small-24B-Instruct-2501"]
url = "https://huggingface.co/mistralai/Mistral-Small-24B-Instruct-2501"
simple_name = "Mistral Small 3"
organisation = "Mistral AI"
params = 24
icon_path = "mistral.png"
description = "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme, possède un mode d'appel de fonction et un contexte de 32 000 tokens."
excerpt = "Sorti en janvier 2025, ce modèle est spécialisé dans le multilinguisme et possède des capacités de raisonnement avancées."
license = "Apache 2.0"
release_date = "01/2025"

[mistral-nemo-2407]
url = "https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407"
simple_name = "Mistral Nemo"
params = 12
organisation = "Mistral AI"
icon_path = "mistral.png"
description = "Sorti en juillet 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes allant jusqu’à 128 000 tokens en plus de 100 langues."
excerpt = "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter un contexte de 128k tokens en plus de 100 langues. Sorti en juillet 2024."
license = "Apache 2.0"
release_date = "07/2024"

[ministral-8b-instruct-2410]
url = "https://huggingface.co/mistralai/Ministral-8B-Instruct-2410"
simple_name = "Ministral"
params = 8
organisation = "Mistral AI"
icon_path = "mistral.png"
description = "Sorti en octobre 2024, ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes. Il parle plus de 100 langues."
excerpt = "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter plus de 100 langues. Sorti en octobre 2024."
license = "Mistral AI Non-Production"
release_date = "10/2024"

["Mixtral-8x22B-Instruct-v0.1"]
url = "https://huggingface.co/mistralai/Mixtral-8x22B-Instruct-v0.1"
simple_name = "Mixtral 8x22B"
active_params = 44
total_params = 176
organisation = "Mistral AI"
icon_path = "mistral.png"
excerpt = "Ce modèle multilingue sorti en avril 2024 a particulièrement été entraîné en anglais, français, allemand, italien et espagnol ainsi que sur des tâches de mathématiques, programmation et raisonnement."
description = "L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents. Sorti en avril 2024."
license = "Apache 2.0"
release_date = "04/2024"

["Mixtral-8x7B-Instruct-v0.1"]
url = "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
simple_name = "Mixtral-8x7B"
organisation = "Mistral AI"
active_params = 14
total_params = 56
icon_path = "mistral.png"
description = "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence."
excerpt = "Ce modèle entraîné sur un corpus multilingue est efficace pour des tâches variées et peu complexes."
license = "Apache 2.0"

["mistral-large-2411"]
url = "https://huggingface.co/mistralai/Mistral-Large-Instruct-2411"
simple_name = "Mistral Large"
organisation = "Mistral AI"
params = 123
icon_path = "mistral.png"
description = "Sorti en novembre 2024, ce modèle de la société française Mistral gère l'anglais, le français, l'italien, l'allemand et l'espagnol."
excerpt = "Sorti en novembre 2024, ce modèle de la société française Mistral gère l'anglais, le français, l'italien, l'allemand et l'espagnol."
license = "Mistral AI Research"
release_date = "11/2024"

["o3-mini"]
url = "https://openai.com/index/openai-o3-mini/"
simple_name = "o3-mini"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "XL"
reasoning = true
license = "propriétaire OpenAI"
description = "Modèle optimisé pour les tâches de raisonnement STEM (science, technologie, ingénierie, mathématiques) et l'écriture de code. Il se démarque dans les domaines scientifiques, mathématiques et de la programmation."
excerpt = "o3-mini est fait pour le raisonnement et le code. Il offre un bon équilibre entre performance, coût et latence, tout en étant plus petit que d'autres modèles de chez OpenAI."
release_date = "11/2024"

["o4-mini"]
url = "https://openai.com/index/introducing-o3-and-o4-mini/"
simple_name = "o4-mini"
organisation = "OpenAI"
icon_path = "openai.svg"
friendly_size = "XL"
reasoning = true
license = "propriétaire OpenAI"
description = "o4-mini est doté de capacités de raisonnement, et est optimisé pour les tâches de STEM (science, technologie, ingénierie, mathématiques). Il est moins coûteux que le modèle o3 d'OpenAI, aussi doté de raisonnement."
excerpt = "Modèle avec capacités de raisonnement, optimisé pour les tâches de STEM (science, technologie, ingénierie, mathématiques) et l'écriture de code."
release_date = "04/2025"

[phi-3-mini-4k]
url = "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
simple_name = "Phi-3-Mini"
organisation = "Microsoft"
params = 3.8
icon_path = "microsoft.png"
license = "MIT"
description = "Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Performant pour des tâches de génération de code, et de résumé, ce modèle compact supporte un contexte restreint de 4000 tokens."

["phi-3.5-mini-instruct"]
url = "https://huggingface.co/microsoft/Phi-3.5-mini-instruct"
simple_name = "Phi-3.5-mini"
organisation = "Microsoft"
params = 3.8
icon_path = "microsoft.png"
license = "MIT"
description = "Petit modèle de la famille Phi, remplaçant Phi-3-mini, ce modèle supporte un grand contexte de 128 000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Performant pour des tâches de génération de code et de résumé, ce modèle gère un grand contexte de 128k tokens."

["phi-4"]
url = "https://huggingface.co/microsoft/Phi-4"
simple_name = "Phi-4"
organisation = "Microsoft"
params = 14.7
icon_path = "microsoft.png"
license = "MIT"
description = "Modèle de décembre 2024 de la famille Phi, remplaçant Phi-3.5, ce petit modèle a été entrainé sur une dizaine de milliards de tokens, soit un gros corpus synthétique et filtré issu du web."
excerpt = "Modèle de décembre 2024 de la famille Phi, remplaçant Phi-3.5, ce petit modèle a été entrainé sur une dizaine de milliards de tokens, soit un gros corpus synthétique et filtré issu du web."
release_date = "12/2024"

[Phi-3-small-8k-Instruct]
license = "MIT"
simple_name = "Phi-3-small-8k-Instruct"
description = "Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Optimisé pour le raisonnement logique, ce petit modèle supporte un contexte de 8000 tokens, adapté pour la génération de code et tâches complexes."
organisation = "Microsoft"
icon_path = "microsoft.png"

["Qwen1.5-32B-Chat"]
simple_name = "Qwen1.5-32B"
organisation = "Alibaba"
icon_path = "qwen.png"
license = "Apache 2.0"

["qwen2.5-coder-32b-instruct"]
simple_name = "Qwen2.5-Coder-32B"
organisation = "Alibaba"
icon_path = "qwen.png"
params = 32
license = "Apache 2.0"
description = "Modèle de la famille CodeQwen et produit par l'entreprise chinoise Alibaba, ce modèle est spécialisé dans la génération de code et l'usage de son modèle en tant qu'agent."
url = "https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct"
release_date = "11/2024"

[Qwen2-57B-A14B-Instruct]
simple_name = "Qwen2-57B-A14B-Instruct"
organisation = "Alibaba"
icon_path = "qwen.png"
license = "Apache 2.0"

[Qwen2-72b-instruct]
url = "https://huggingface.co/Qwen/Qwen2-72B-Instruct"
simple_name = "Qwen2-72b-instruct"
organisation = "Alibaba"
icon_path = "qwen.png"
license = "Apache 2.0"

[Qwen2-7B-instruct]
url = "https://huggingface.co/Qwen/Qwen2-7B-Instruct"
simple_name = "Qwen2-7B"
organisation = "Alibaba"
icon_path = "qwen.png"
description = "Petit frère de la famille Qwen2 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
excerpt = "Supportant un contexte de 130k tokens, ce petit modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement."
params = 7
license = "Apache 2.0"

["Qwen2.5-7B-instruct"]
url = "https://huggingface.co/Qwen/Qwen2.5-7B-Instruct"
simple_name = "Qwen2.5-7B"
organisation = "Alibaba"
icon_path = "qwen.png"
dataset = "private"
description = "Petit modèle de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
excerpt = "Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement."
params = 7
license = "Apache 2.0"

["Qwen2.5-32B-instruct"]
url = "https://huggingface.co/Qwen/Qwen2.5-32B-Instruct"
simple_name = "Qwen2.5-32B"
organisation = "Alibaba"
icon_path = "qwen.png"
description = "Modèle intermédiaire de la famille Qwen2.5 et produit par l'entreprise chinoise Alibaba, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes."
excerpt = "Supportant un contexte de 130k tokens, ce modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement."
params = 32
license = "Apache 2.0"

["qwq-32b"]
url = "https://huggingface.co/Qwen/QwQ-32B"
simple_name = "QwQ-32B"
organisation = "Alibaba"
icon_path = "qwen.png"
description = "QwQ-32B est un modèle de raisonnement de taille moyenne de la série Qwen 2.5. Il est capable d'atteindre des performances compétitives par rapport aux modèles de raisonnement de pointe comme DeepSeek-R1 et o1-mini."
excerpt = "Modèle de raisonnement de taille moyenne, QwQ-32B atteint des performances compétitives par rapport aux modèles de raisonnement de pointe comme DeepSeek-R1 et o1-mini."
params = 32
license = "Apache 2.0"
reasoning = true

["qwen3-32b"]
url = "https://huggingface.co/Qwen/Qwen3-32B"
simple_name = "Qwen3-32B"
organisation = "Alibaba"
icon_path = "qwen.png"
description = "Qwen 3 32B est un modèle de la série Qwen 3 de l'entreprise Alibaba. Il peut fonctionner avec mode de raisonnement ou classiquement."
excerpt = "Qwen 3 32B est un modèle de la série Qwen 3 de l'entreprise Alibaba. Il intègre un mode de raisonnement activable."
params = 32
license = "Apache 2.0"
reasoning = true

["Yi-1.5-9B-Chat"]
url = "https://huggingface.co/01-ai/Yi-1.5-9B-Chat"
simple_name = "Yi-1.5 9B"
organisation = "01-ai"
icon_path = "yi.svg"
description = "Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage."
excerpt = "Yi 1.5 est un modèle de l'entreprise chinoise 01-ai, spécialisé en code, maths, raisonnement, suivi d'instructions, avec une solide compréhension du langage."
params = 9
license = "Apache 2.0"

[claude-4-sonnet]
arch = "maybe-dense"
distribution = "api-only"
friendly_size = "XL"
icon_path = "anthropic.png"
id = "claude-4-sonnet"
license = "propriétaire Anthropic"
organisation = "Anthropic"
params = 200
reasoning = false
release_date = "05/2025"
required_ram = 200
reuse = false
simple_name = "Claude 4 Sonnet"
excerpt = "Très grand modèle multimodal et multilingue, très puissant en code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale."
description = "Très grand modèle multimodal et multilingue, très puissant en code, avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement la réponse finale. Claude 4 Sonnet est une version plus compacte de Claude 4 Opus optimisée pour la vitesse, l’efficacité et l’accessibilité. Il est un peu moins à l’aise sur les tâches qui demandent un raisonnement complexe en plusieurs étapes. En contrepartie, il est nettement moins coûteux, plus rapide, peut générer de plus longs textes et consomme moins d’énergie que Opus.\n\nLe modèle offre deux modes d’utilisation : un mode réflexion avec un raisonnement étape par étape pour les problèmes complexes, et un mode rapide pour les réponses directes. À la différence d’autres modèles, le mode raisonnement n’a pas été surtout entraîné sur des données mathématiques, mais adapté à des cas d’usage réels."

["gemini-2.5-flash"]
arch = "moe"
distribution = "api-only"
friendly_size = "L"
icon_path = "google.png"
id = "gemini-2.5-flash"
license = "propriétaire Gemini"
organisation = "Google"
params = 70
reasoning = false
release_date = "06/2025"
required_ram = 70
reuse = false
simple_name = "Gemini 2.5 Flash"
excerpt = "Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale."
description = "Grand modèle multimodal et multilingue avec deux modalités de réponses: l’utilisateur peut choisir entre un mode de raisonnement, pour des réponses plus approfondies, ou un mode rapide, pour générer directement à la réponse finale. Ce modèle repose sur une architecture de mélange d’experts (MoE, Mixture of Experts) et a été distillé en ne conservant qu'une approximation des prédictions du modèle enseignant - Gemini 2.5 Pro. Il a été entraîné sur une architecture TPUv5p intégrant des avancées comme la possibilité de poursuivre l'entraînement automatiquement même en cas d’erreurs d’entraînement, de corruption de données ou de problèmes de mémoire.\n\nGemini 2.5 Flash gère des contextes allant jusqu'à 1 million de jetons, et trois heures de contenu vidéo. L'optimisation du traitement de la vision permet de traiter des vidéos environ trois fois plus longues dans la même fenêtre de contexte: seuls 66 jetons visuels sont nécessaires pour générer une image contre 258 auparavant. Ce modèle permet  également la génération audio native pour les dialogues et la synthèse vocale."
      

[gemma-3n-e4b-it]
arch = "matformer"
commercial_use = true
distribution = "open-weights"
friendly_size = "XS"
icon_path = "google.png"
id = "gemma-3n-e4b-it"
license = "Gemma"
organisation = "Google"
params = 4
reasoning = false
release_date = "05/2025"
required_ram = 4
reuse = true
simple_name = "Gemma 3n 4B"
excerpt = "Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin."
description = "Très petit modèle multimodal et compact conçu pour fonctionner localement sur un ordinateur ou un smartphone, sans recours à un serveur - il est capable d’adapter sa puissance selon la capacité de la capacité et le besoin. Ce modèle peut traiter du texte, des images et de l’audio. Il repose sur l’architecture MatFormer et un système de cache PLE (per-layer embeddings), qui active uniquement les paramètres utiles selon la tâche, s'adaptant à la capacité des machines sur lesquelles fonctionne le modèle."

# [gpt-5]
# arch = "maybe-moe"
# distribution = "api-only"
# friendly_size = "M"
# icon_path = "openai.svg"
# id = "gpt-5"
# license = "propriétaire OpenAI"
# organisation = "OpenAI"
# params = 35
# reasoning = false
# release_date = "08/2025"
# required_ram = 35
# reuse = false
# simple_name = "GPT 5"

[gpt-5-mini]
arch = "maybe-moe"
distribution = "api-only"
friendly_size = "M"
icon_path = "openai.svg"
id = "gpt-5-mini"
license = "propriétaire OpenAI"
organisation = "OpenAI"
params = 35
reasoning = false
release_date = "08/2025"
required_ram = 35
reuse = false
simple_name = "GPT 5 Mini"
description = "Le GPT-5 Mini est une version allégée du modèle GPT-5 principal. Il est conçu pour être utilisé dans des environnements où il est nécessaire de limiter les coûts, par exemple à grande échelle. Son modèle de raisonnement est presque aussi performant que celui du modèle principal (`gpt-5-thinking`) malgré sa taille plus petite. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois."

[gpt-5-nano]
arch = "maybe-moe"
distribution = "api-only"
friendly_size = "M"
icon_path = "openai.svg"
id = "gpt-5-nano"
license = "propriétaire OpenAI"
organisation = "OpenAI"
params = 35
reasoning = false
release_date = "04/2025"
required_ram = 35
reuse = false
simple_name = "GPT 5 Nano"
description = "Le GPT-5 Nano est la plus petite et la plus rapide version du modèle de raisonnement GPT-5. Il est conçu pour des contextes où une latence ou un coût ultra-faible est nécessaire. Grâce à sa fenêtre de contexte de 400 000 jetons, il peut accepter de longues requêtes, ce rend possible l'analyse de plusieurs documents à la fois."

[gpt-oss-120b]
arch = "maybe-moe"
commercial_use = true
distribution = "open-weights"
friendly_size = "M"
icon_path = "openai.svg"
id = "gpt-oss-120b"
license = "Apache 2.0"
organisation = "OpenAI"
params = 35
reasoning = false
release_date = "08/2025"
required_ram = 35
reuse = true
simple_name = "GPT OSS-120B"
description = "Le plus grand des deux premiers modèles semi-ouverts d'OpenAI depuis GPT-2. Conçu en réponse à la montée en puissance des acteurs open source comme Meta (LLaMA) et Mistral, il s'agit d'un modèle de raisonnement performant, notamment sur des tâches complexes et dans des environnements « agentiques »."

[gpt-oss-20b]
arch = "maybe-moe"
commercial_use = true
distribution = "open-weights"
friendly_size = "M"
icon_path = "openai.svg"
id = "gpt-oss-20b"
license = "Apache 2.0"
organisation = "OpenAI"
params = 35
reasoning = false
release_date = "08/2025"
required_ram = 35
reuse = true
simple_name = "GPT OSS-20B"
description = "Le plus petit des deux modèles semi-ouverts d'OpenAI. Il a été conçu en réponse à la concurrence de l'open source et est destiné aux cas d'utilisation nécessitant une faible latence ainsi qu'aux déploiements locaux ou spécialisés."

[llama-maverick]
arch = "moe"
commercial_use = true
distribution = "open-weights"
friendly_size = "XL"
icon_path = "meta.svg"
id = "llama-maverick"
license = "Llama 4"
organisation = "Meta"
params = 400
reasoning = false
release_date = "04/2025"
required_ram = 400
reuse = true
simple_name = "Llama Maverick"
description = "Très grand modèle doté d’une très large fenêtre de contexte, adapté par exemple au résumé de plusieurs documents en même temps."

[magistral-medium]
arch = "maybe-dense"
distribution = "api-only"
friendly_size = "M"
icon_path = "mistral.png"
id = "magistral-medium"
description = "Modèle de raisonnement de taille moyenne multimodal et multilingue. Adapté à des tâches de programmation ou autres tâches nécessitant analyse approfondie compréhension de systèmes logiques complexes ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes."
license = "propriétaire Mistral"
organisation = "Mistral AI"
params = 35
reasoning = false
release_date = "06/2025"
required_ram = 35
reuse = false
simple_name = "Magistral Medium"

[magistral-small-2506]
description = "Modèle de raisonnement de taille moyenne, multimodal et multilingue. Adapté à des tâches nécessitant une analyse approfondie, compréhension de systèmes logiques ou planification - par exemple pour des cas d’usages agentiques ou de la rédaction de longs contenus complexes."
arch = "dense"
commercial_use = true
distribution = "open-weights"
friendly_size = "M"
icon_path = "mistral.png"
id = "magistral-small-2506"
license = "Apache 2.0"
organisation = "Mistral AI"
params = 24
reasoning = false
release_date = "06/2025"
required_ram = 24
reuse = true
simple_name = "Magistral Small"

[mistral-medium-2506]
arch = "maybe-dense"
distribution = "api-only"
friendly_size = "M"
icon_path = "mistral.png"
id = "mistral-medium-2506"
description = "Modèle de taille moyenne multilingue, multimodal et peu couteux par rapport à d’autres modèles qui offrent des performances similaires. Il est particulièrement intéressant pour des tâches de programmation ou des tâches de raisonnement, par exemple les mathématiques."
license = "propriétaire Mistral"
organisation = "Mistral AI"
params = 35
reasoning = false
release_date = "06/2025"
required_ram = 35
reuse = false
simple_name = "Mistral Medium 2506"

[mistral-small-3-2]
arch = "dense"
commercial_use = true
distribution = "open-weights"
friendly_size = "M"
description = "Malgré son nom, c’est un modèle de taille moyenne. Il est multimodal (capable de traiter texte et images) et il se démarque par un respect précis des requêtes et sa capacité à utiliser des outils avancées."
icon_path = "mistral.png"
id = "mistral-small-3-2"
license = "Apache 2.0"
organisation = "Mistral AI"
params = 24
reasoning = false
release_date = "06/2025"
required_ram = 24
reuse = true
simple_name = "Mistral Small 3.2"

[qwen-2-5-max-0125]
arch = "moe"
distribution = "api-only"
friendly_size = "XL"
icon_path = "qwen.png"
id = "qwen-2-5-max-0125"
license = "propriétaire Alibaba"
organisation = "Alibaba"
params = 200
description = "Très grand modèle de raisonnement spécialisé et très performant en mathématiques, code et résolution de problèmes logiques."
reasoning = false
release_date = "04/2025"
required_ram = 200
reuse = false
simple_name = "Qwen 2.5 max 0125"

[qwen3-30b-a3b]
arch = "moe"
commercial_use = true
distribution = "open-weights"
friendly_size = "M"
icon_path = "qwen.png"
id = "qwen3-30b-a3b"
license = "Apache 2.0"
organisation = "Alibaba"
params = 30
reasoning = false
release_date = "05/2025"
required_ram = 30
excerpt = "Modèle de taille moyenne multilingue."
description = "Ce modèle MoE (Mixture of Experts) se distingue par une configuration de 128 experts au total, avec seulement 8 experts activés par jeton, ce qui permet une inférence plus rapide et plus efficace. Il utilise un système appelé *global-batch* pour optimiser la répartition du travail entre les experts, afin qu'ils soient tous utilisés de manière équilibrée.\n\nContrairement à d'autres modèles comme Qwen 2.5-MoE qui recyclent les mêmes experts à travers plusieurs couches du réseau, Qwen 3 30B A2B attribue des experts uniques à chaque couche. Concrètement, cela signifie que les experts de la première couche ne sont jamais réutilisés dans les couches suivantes - chaque niveau du modèle dispose de son propre ensemble d'experts spécialisés. Cette architecture permet à chaque expert de se concentrer exclusivement sur les tâches spécifiques à sa position dans le réseau neuronal, résultant en une spécialisation plus fine et des performances optimisées pour chaque étape du traitement de l'information."
reuse = true
simple_name = "Qwen 3 30B A3B"

# FIXME:
# [qwen3-235b-a22b-thinking-2507]
# [kimi-k2]
# [glm-4.5]