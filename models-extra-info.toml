["Albert MFS"]
simple_name = "Albert MFS"
organisation = "Albert"
country = "France"
icon_path = "marianne.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "communiqué"
conditions = "free"

[Alpaca-13B]
simple_name = "Alpaca-13B"
organisation = "Stanford"
country = "États-Unis"
icon_path = "stanford.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

[Aya23-35B]
simple_name = "Aya23-35B"
organisation = "Cohere"
country = "Canada"
icon_path = "cohere.png"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"
params = 35
license = "CC-BY-NC-4.0"
description = "Aya 23 35B de Cohere est un modèle de taille moyenne de la famille Command R qui a spécialement été entraîné sur un corpus multilingue." 

[aya-23-8b]
simple_name = "Aya 23 8B"
organisation = "Cohere"
country = "Canada"
icon_path = "cohere.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"
params = 8
license = "CC-BY-NC-4.0"
description = "Aya 23 8B de Cohere est un petit modèle de la famille Command R qui a spécialement été entraîné sur un corpus multilingue." 

["Claude 3 Opus"]
simple_name = "Claude 3 Opus"
organisation = "Anthropic"
country = "États-Unis"
icon_path = "anthropic.png"
friendly_size = "L"
dataset = "private"
distribution = "api-only"
conditions = "restricted"

["Claude 3.5 Sonnet"]
simple_name = "Claude 3.5 Sonnet"
organisation = "Anthropic"
country = "États-Unis"
icon_path = "anthropic.png"
friendly_size = "L"
dataset = "private"
distribution = "api-only"
conditions = "restricted"

[command-r-plus]
simple_name = "Command R+"
organisation = "Cohere"
country = "Canada"
icon_path = "cohere.png"
friendly_size = "L"
distribution = "open-weights"
dataset = "private"
params = 104
license = "CC-BY-NC-4.0"
conditions = "restricted"
description = "Grand frère de la famille Command R de Cohere, ce modèle de langage est orienté pour l'usage professionnel et conçu spécifiquement pour les tâches de recherche et d'extraction d'informations."

["Command R"]
simple_name = "Command R"
organisation = "Cohere"
country = "Canada"
icon_path = "cohere.png"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

["CroissantLLMChat-v0.1"]
simple_name = "CroissantLLMChat-v0.1"
organisation = "ILLUIN"
country = "France"
icon_path = "illuin.png"
friendly_size = "XS"
distribution = "open-weights"
dataset = "public"
conditions = "free"

["gemini-1.5-pro-001"]
active_params = 220
total_params = 440
simple_name = "Gemini 1.5 Pro"
organisation = "Google"
country = "États-Unis"
icon_path = "google.png"
friendly_size = "XL"
dataset = "private"
distribution = "api-only"
license = "propriétaire Gemini"
conditions = "restricted"
description = "Sorti en février 2024 et constamment amélioré, ce modèle multilingue et multimodal est capable de traiter un grand volume de données d’entrées qu’il s’agisse de données textuelles, d’image, de son (jusqu’à 11h d’audio) ou de vidéo (jusqu’à une heure). C’est le modèle LLM qui alimente le chatbot Gemini de Google."
excerpt = "Ce modèle est idéal pour des applications variées comme la génération de textes et d'images, l'analyse de vidéos et la transcription d'audio."

["Gemma 2-27b-it"]
simple_name = "Gemma 2-27b-it"
organisation = "Google"
country = "États-Unis"
icon_path = "google.png"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"
license = "Gemma"
params = 27
excerpt = "Modèle performant avec une latence correcte, son coût relativement élevé le destine à des usages spécifiques nécessitant une grande précision."
description = "Avec trois fois plus de paramètre que son petit frère de la famille Gemma 2, ce modèle est plus précis pour répondre aux instructions."

[Gemma-2-9B-it]
simple_name = "Gemma 2 9B"
organisation = "Google"
country = "États-Unis"
icon_path = "google.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
license = "Gemma"
params = 9
conditions = "restricted"
excerpt = "Moins performant que le 27B, ce modèle est plus abordable mais présente une latence légèrement supérieure, convenant à des tâches moins complexes."
description = "Modèle intermédiaire de la famille Gemma 2, ce modèle est particulièrement doué pour répondre à des instructions spécifiques, traiter des requêtes complexes et offrir des solutions créatives."

[gemma-2-2b-it]
simple_name = "Gemma 2 2B"
organisation = "Google"
country = "États-Unis"
icon_path = "google.png"
friendly_size = "XS"
distribution = "open-weights"
dataset = "private"
license = "Gemma"
params = 2
conditions = "restricted"
description = "Petit frère de la famille Gemma 2, ce très petit modèle sorti en juillet 2024 arrive à rivaliser avec des modèles bien plus gros."

["GPT-3.5-Turbo-0613"]
simple_name = "GPT-3.5-Turbo-0613"
organisation = "OpenAI"
country = "États-Unis"
icon_path = "openai.png"
friendly_size = "L"
dataset = "private"
distribution = "api-only"
conditions = "restricted"

[GPT-4-Turbo-2024-04-09]
simple_name = "GPT-4-Turbo-2024-04-09"
organisation = "OpenAI"
country = "États-Unis"
icon_path = "openai.png"
friendly_size = "L"
dataset = "private"
distribution = "api-only"
conditions = "restricted"

["llama-3.1-405b"]
simple_name = "Llama 3.1 405B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "XL"
dataset = "private"
distribution = "open-weights"
conditions = "free"
excerpt = "Ce modèle très puissant présente un coût élevé qui le réserve à des applications exigeantes de raisonnement et de programmation."
description = "Modèle le plus puissant de la famille Llama 3.1 avec 405 milliards de paramètres, il est doté de capacités accrues pour des tâches de programmation, de mathématique et de raisonnement."
params = 405
license = "Llama 3.1 Community"

["llama-3.1-8b"]
simple_name = "Llama 3.1 8B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "S"
dataset = "private"
distribution = "open-weights"
conditions = "free"
excerpt = "Version plus petite du 405B, ce modèle est rapide et présente un coût réduit, adapté à des applications plus courantes."
description = "Cadet de la famille Llama 3.1, ce petit modèle est adapté aux tâches multilingues et au traitement de longs textes pouvant aller jusqu’à 128 000 tokens."
params = 8
license = "Llama 3.1 Community"

["llama-3.1-70b"]
params = 70
simple_name = "Llama 3.1 70B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "L"
dataset = "private"
distribution = "open-weights"
conditions = "free"
excerpt = "Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
description = "Avec 70 milliards de paramètres, ce modèle est performant pour générer et comprendre des textes complexes dans diverses langues."
license = "Llama 3.1 Community"

[Meta-Llama-3-70B-Instruct]
params = 70
simple_name = "Llama 3 70B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "L"
dataset = "private"
distribution = "open-weights"
conditions = "restricted"
excerpt = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens mais supporte un contexte relativement restreint de 8000 tokens."
description = "Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens, puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens."
license = "Llama 3 Community"

[Meta-Llama-3-8B-Instruct]
params = 8
simple_name = "Llama 3 8B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"
description = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité."
excerpt = "Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité."
license = "Llama 3 Community"

[mistral-nemo-2407]
simple_name = "Mistral Nemo"
params = 12
organisation = "Mistral"
country = "France"
icon_path = "mistral.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "free"
description = "Ce modèle de petit gabarit est entraîné pour des tâches de raisonnement, de connaissances générales et de programmation. Il utilise le tokenizer Tekken, efficace pour compresser des textes allant jusqu’à 128 000 tokens en plus de 100 langues."
excerpt = "Optimisé pour un temps de réaction rapide, ce modèle est idéal pour des applications nécessitant des réponses immédiates et peut supporter un contexte de 128k jetons en plus de 100 langues."
license = "Apache 2.0"

["Mistral-7B-Instruct-v0.3"]
simple_name = "Mistral-7B"
params = 7
organisation = "Mistral"
country = "France"
icon_path = "mistral.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "free"
license = "Apache 2.0"

[Mistral-Large-2402]
simple_name = "Mistral-Large-2402"
organisation = "Mistral"
country = "France"
icon_path = "mistral.png"
friendly_size = "L"
dataset = "private"
distribution = "api-only"
conditions = "restricted"

["Mixtral-8x22B-Instruct-v0.1"]
simple_name = "Mixtral 8x22B"
active_params = 44
total_params = 176
organisation = "Mistral"
country = "France"
icon_path = "mistral.png"
friendly_size = "L"
distribution = "open-weights"
dataset = "private"
conditions = "free"
excerpt = "Plus puissant que son petit frère 8x7B, ce modèle offre de meilleures performances à un prix raisonnable, adapté pour des besoins plus intensifs."
description = "L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents."
license = "Apache 2.0"

["Mixtral-8x7B-Instruct-v0.1"]
simple_name = "Mixtral-8x7B"
organisation = "Mistral"
active_params = 14
total_params = 56
country = "France"
icon_path = "mistral.png"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "free"
description = "Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence."
excerpt = "Ce modèle entraîné sur un corpus multilingue offre de bonnes performances à un coût moyen, efficace pour des tâches variées."
license = "Apache 2.0"

[phi-3-mini-4k]
simple_name = "Phi-3-Mini"
organisation = "Microsoft"
country = "États-Unis"
params = 3.8
icon_path = "microsoft.png"
friendly_size = "XS"
distribution = "open-weights"
dataset = "private"
license = "MIT"
conditions = "free"
description = "Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Performant pour des tâches de génération de code, et de résumé, ce modèle compact supporte un contexte restreint de 4k jetons."

["phi-3.5-mini-instruct"]
simple_name = "Phi-3.5-mini"
organisation = "Microsoft"
country = "États-Unis"
params = 3.8
icon_path = "microsoft.png"
friendly_size = "XS"
distribution = "open-weights"
dataset = "private"
license = "MIT"
conditions = "free"
description = "Petit modèle de la famille Phi, remplaçant Phi-3-mini, ce modèle supporte un grand contexte de 128 000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Performant pour des tâches de génération de code et de résumé, ce modèle gère un grand contexte de 128k jetons."

[Phi-3-small-8k-Instruct]
license = "MIT"
simple_name = "Phi-3-small-8k-Instruct"
description = "Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré."
excerpt = "Optimisé pour le raisonnement logique, ce petit modèle supporte un contexte de 8k jetons, adapté pour la génération de code et tâches complexes."
organisation = "Microsoft"
country = "États-Unis"
icon_path = "microsoft.png"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

["Qwen1.5-32B-Chat"]
simple_name = "Qwen1.5-32B"
organisation = "Alibaba"
country = "Chine"
icon_path = "qwen.jpg"
friendly_size = "M"
dataset = "private"
distribution = "open-weights"
conditions = "restricted"

[Qwen2-57B-A14B-Instruct]
simple_name = "Qwen2-57B-A14B-Instruct"
organisation = "Alibaba"
country = "Chine"
icon_path = "qwen.jpg"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

[Qwen2-72b-instruct]
simple_name = "Qwen2-72b-instruct"
organisation = "Alibaba"
country = "Chine"
icon_path = "qwen.jpg"
friendly_size = "L"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

[Qwen2-7B-instruct]
simple_name = "Qwen2-7B"
organisation = "Alibaba"
country = "Chine"
icon_path = "qwen.jpg"
friendly_size = "S"
distribution = "open-weights"
dataset = "private"
conditions = "free"
description = "Petit frère de la famille Qwen2, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes"
excerpt = "Supportant un contexte de 130k jetons, ce petit modèle multilingue et polyvalent est performant pour des tâches de traduction, de résumé, d’analyse et de raisonnement."
params = 7
license = "Apache 2.0"

[Vicuna-13B]
simple_name = "Vicuna-13B"
organisation = "Meta"
country = "États-Unis"
icon_path = "meta.svg"
friendly_size = "S"
dataset = "private"
distribution = "open-weights"
conditions = "restricted"

[Vicuna-33B]
simple_name = "Vicuna-33B"
organisation = "LMSYS"
country = "États-Unis"
icon_path = "lmsys.png"
friendly_size = "M"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"

["Zephyr-ORPO-141b-A35b-v0.1"]
simple_name = "Zephyr-ORPO-141b-A35b-v0.1"
organisation = "Hugging Face"
country = "France"
friendly_size = "L"
active_params = 35
total_params = 141
icon_path = "huggingface.svg"
distribution = "open-weights"
dataset = "private"
conditions = "restricted"
