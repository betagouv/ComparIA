arène,Nom,Nom standard Hugging Face ou lien,DESCRIPTION,Organisation,Famille,Taille,Paramètres,Code source,Corpus d'entrainement,Distribution,contexte,RAM systeme pour installation locale,Conditions d'usage,Licence,régime,Pool(s) envisagée(s),Date,open source ou propriétaire,Nombre de tokens (corpus),knowledge cutoff,Fourni via,Remarques,priorité intégration arène
No,Gemma-2-9B-it,google/gemma-2-9b-it,,Google,Gemma,S,9B,poids communiqués,non communiqué,téléchargeable,,16Go,,Gemma license,,"Modèles de base, Petits modèles",01/02/2024,open source,,,"Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21), HF Serverless Inference API (https://www.notion.so/HF-Serverless-Inference-API-2c1035a3ee0e4aeaa04bd1aa36a27a42?pvs=21)",,2 - oui fortement souhaité
No,Gemini 1.5 Pro API-0409-Preview,,,Google,Gemini,L,Non public,propriétaire,non communiqué,API uniquement,,non pertinent,,Proprietary,,Gros modèles (>30B),01/11/2023,propriétaire,,,,,3 - oui serait un plus
No,GPT-4-Turbo-2024-04-09,,,OpenAI,OpenAI,L,Non public,propriétaire,non communiqué,API uniquement,,non pertinent,,Proprietary,,Modèles de base,01/12/2023,propriétaire,,,,,4 - non pas prioritaire
No,Llama-2-13b-chat,,,Meta,Llama2,S,13B,poids communiqués,non communiqué,téléchargeable,,,,Llama 2 Community,,,01/07/2023,open source,,,,Remplacé,4 - non pas prioritaire
No,Zephyr-ORPO-141b-A35b-v0.1,https://huggingface.co/HuggingFaceH4/zephyr-orpo-141b-A35b-v0.1,,Hugging Face,Mistral,,141B,poids communiqués,non communiqué,téléchargeable,,,,Apache-2.0,,"Fine-tunes à visée francophone, Gros modèles (>30B)",01/04/2024,open source,,,,,4 - non pas prioritaire
No,Mistral-Large-2402,,,Mistral,Mistral,L,Non public,propriétaire,non communiqué,API uniquement,32k,non pertinent,,Proprietary,propriétaire,Gros modèles (>30B),,propriétaire,,,Mistral? (https://www.notion.so/Mistral-83986a1d09b74efa92cc018ae0bab341?pvs=21),,1 - oui absolument
No,GPT-3.5-Turbo-0613,,,OpenAI,OpenAI,L,Non public,propriétaire,non communiqué,API uniquement,,non pertinent,,Proprietary,,Gros modèles (>30B),01/09/2021,propriétaire,,,,,4 - non pas prioritaire
Yes,CroissantLLMChat-v0.1,croissantllm/CroissantLLMChat-v0.1,"Entrainé sur 3000 milliards de tokens, ce modèle se distingue par son bilinguisme : 50% des données d’entraînement sont en français, le reste en anglais.",ILLUIN,Llama2,XS,1.3B,poids communiqués,communiqué,téléchargeable,2k,5Go,Libre,MIT,,"Fine-tunes à visée francophone, Petits modèles",,open source,3T,,HF Serverless Inference API (https://www.notion.so/HF-Serverless-Inference-API-2c1035a3ee0e4aeaa04bd1aa36a27a42?pvs=21),Trop vieux,3 - oui serait un plus
No,Phi-3-Mini-4k-Instruct,microsoft/Phi-3-mini-4k-instruct,"Petit frère de la famille Phi3, ce modèle supporte un contexte de 4000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.",Microsoft,Phi,XS,3.8B,poids communiqués,non communiqué,téléchargeable,,,Libre,MIT,,"Modèles de base, Petits modèles",01/10/2023,open source,,,HF Serverless Inference API (https://www.notion.so/HF-Serverless-Inference-API-2c1035a3ee0e4aeaa04bd1aa36a27a42?pvs=21),,2 - oui fortement souhaité
No,Vicuna-13B,,,Meta,Llama2,S,13B,poids communiqués,,téléchargeable,,,,Llama 2 Community,,,01/07/2023,open source,,,,Trop vieux,4 - non pas prioritaire
Yes,Mixtral-8x7b-Instruct-v0.1,,"Petit frère de la famille Mixtral, ce modèle est capable de traiter des contextes de 32 000 tokens et supporte l'anglais, le français, l'italien, l'allemand et l'espagnol. (…) Grâce à l’architecture SMoE (sparse mixture of experts), seule une fraction des paramètres est activée pour chaque inférence, réduisant ainsi les coûts et la latence. ",Mistral,Mistral,M,8x7B,poids communiqués,non communiqué,téléchargeable,32k,100Go,Libre,Apache-2.0,,Modèles de base,01/12/2023,open source,,,Scaleway Managed Inference (https://www.notion.so/Scaleway-Managed-Inference-a48411e0f9fe41899a2ebf3b00aef6b2?pvs=21),,1 - oui absolument
Yes,Llama-3-70b-Instruct,,"Lancé en avril 2024, ce modèle a été entrainé sur plus de 15 000 milliards de tokens (…) puis spécialisé pour le dialogue à partir de données d’instructions et d’annotations faites par des humains. Il supporte un contexte de 8000 tokens.",Meta,Llama3,L,70B,poids communiqués,non communiqué,téléchargeable,8k,32Go,Sous condition,Llama 3 Community,,Gros modèles (>30B),01/12/2023,open source,15T+,12/2023,OVH AI Endpoints (https://www.notion.so/OVH-AI-Endpoints-577749cf595a46289e532f1ec30063eb?pvs=21),,2 - oui fortement souhaité
No,Qwen1.5-32B-Chat,,,Alibaba,Qwen,M,32B,,non communiqué,téléchargeable,,,,Qianwen LICENSE,,,01/02/2024,open source,,,,Remplacé,4 - non pas prioritaire
Yes,Llama-3-8b-Instruct,meta-llama/Meta-Llama-3-8B-Instruct,"Petit frère de la famille Llama 3, ce modèle est optimisé pour les dialogues, avec une attention particulière portée sur l’efficacité et la sécurité.",Meta,Llama3,S,8B,poids communiqués,non communiqué,téléchargeable,8k,16Go,Sous condition,Llama 3 Community,,"Modèles de base, Petits modèles",18/04/2023,open source,15T+,03/2023,"HF Serverless Inference API (https://www.notion.so/HF-Serverless-Inference-API-2c1035a3ee0e4aeaa04bd1aa36a27a42?pvs=21), Scaleway Managed Inference (https://www.notion.so/Scaleway-Managed-Inference-a48411e0f9fe41899a2ebf3b00aef6b2?pvs=21)",,1 - oui absolument
No,Vicuna-33B,,,LMSYS,Llama2,M,33B,poids communiqués,,téléchargeable,,,,Non-commercial,,,01/08/2023,open source,,,,Trop vieux,4 - non pas prioritaire
No,Alpaca-13B,,,Stanford,Llama2,S,13B,poids communiqués,,téléchargeable,,,,Non-commercial,,,01/03/2023,open source,,,,Trop vieux,4 - non pas prioritaire
No,Mistral-7B-Instruct-v0.3,,,Mistral,Mistral,S,7B,poids communiqués,non communiqué,téléchargeable,32k,16Go,,Apache-2.0,,"Modèles de base, Petits modèles",01/12/2023,open source,,,OVH AI Endpoints (https://www.notion.so/OVH-AI-Endpoints-577749cf595a46289e532f1ec30063eb?pvs=21),,1 - oui absolument
No,Qwen2-7B-Instruct-GPTQ-Int8,Qwen/Qwen2-7B-Instruct,"Petit frère de la famille Qwen2, ce modèle peut prendre en charge jusqu’à 130 000 tokens pour traiter de longs textes",Alibaba,Qwen,S,7B,poids communiqués,non communiqué,téléchargeable,32k,16Go,Libre,Qianwen LICENSE,,"Modèles de base, Petits modèles",01/06/2024,open source,,,,,1 - oui absolument
No,Command R+,,,Cohere,Command R,L,104B,poids communiqués,non communiqué,téléchargeable,,,,CC-BY-NC-4.0,,Modèles de base,01/03/2024,open source,,,,Trop gros,4 - non pas prioritaire
No,Claude 3 Opus,,,Anthropic,Claude,L,Non public,propriétaire,non communiqué,API uniquement,,,,Proprietary,,Gros modèles (>30B),01/08/2023,propriétaire,,,Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21),,4 - non pas prioritaire
No,Aya23-8B,CohereForAI/aya-23-8b,,Cohere,Command R,S,8B,poids communiqués,non communiqué,téléchargeable,,16Go,,,,"Fine-tunes à visée francophone, Petits modèles",01/05/2024,open source,,,HF Serverless Inference API (https://www.notion.so/HF-Serverless-Inference-API-2c1035a3ee0e4aeaa04bd1aa36a27a42?pvs=21),Multilingue,1 - oui absolument
No,Aya23-35B,CohereForAI/aya-23-35B,,Cohere,Command R,M,35B,poids communiqués,non communiqué,téléchargeable,,,,,,"Gros modèles (>30B), Modèles de base",01/05/2024,open source,,,"Scaleway Compute Instances (https://www.notion.so/Scaleway-Compute-Instances-ad379814c4ce43cbb0cd1a679e7380d7?pvs=21), HF Inference Endpoints (https://www.notion.so/HF-Inference-Endpoints-4a22b3c55e0444c18383ed0f3ec196e1?pvs=21), Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21)",Multilingue,2 - oui fortement souhaité
No,Command R,,,Cohere,Command R,M,35B,poids communiqués,non communiqué,téléchargeable,,,,CC-BY-NC-4.0,,"Gros modèles (>30B), Modèles de base",01/03/2024,open source,,,"Scaleway Compute Instances (https://www.notion.so/Scaleway-Compute-Instances-ad379814c4ce43cbb0cd1a679e7380d7?pvs=21), HF Inference Endpoints (https://www.notion.so/HF-Inference-Endpoints-4a22b3c55e0444c18383ed0f3ec196e1?pvs=21), Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21)",Remplacé ?,3 - oui serait un plus
Yes,Albert MFS,AgentPublic/albertlight-7b,,Albert,Albert,S,8B,poids communiqués,communiqué,téléchargeable,4k,16Go,Libre,,permissif,Fine-tunes à visée francophone,,open source,,,Albert (https://www.notion.so/Albert-c651904c5bfb41c38e8622730d07bca9?pvs=21),,1 - oui absolument
No,Gemma 2-27b-it,google/gemma-2-27b-it,,Google,Gemma,M,27B,poids communiqués,non communiqué,téléchargeable,,,,Gemma license,,Moyen modèle,,open source,,,Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21),,3 - oui serait un plus
No,Qwen2-72b-instruct,Qwen/Qwen2-72B-Instruct,,Alibaba,Qwen,L,72B,poids communiqués,non communiqué,téléchargeable,130k,,,tongyi-qianwen license,,Gros modèles (>30B),01/06/2024,open source,,,,,3 - oui serait un plus
No,Qwen2-57B-A14B-Instruct,Qwen/Qwen2-57B-A14B-Instruct,,Alibaba,Qwen,M,57B,poids communiqués,non communiqué,téléchargeable,,,,Apache-2.0,,Gros modèles (>30B),01/06/2024,open source,,,,,4 - non pas prioritaire
No,Phi-3-small-8k-Instruct,microsoft/Phi-3-small-8k-instruct,"Grand frère de la famille Phi3, ce modèle supporte un contexte de 8000 tokens et a été entrainé sur des jeux de données synthétiques et issus du web filtré.",Microsoft,Phi,S,8B,poids communiqués,non communiqué,téléchargeable,,,,MIT,,"Modèles de base, Petits modèles",01/10/2023,open source,,,,,2 - oui fortement souhaité
No,Claude 3.5 Sonnet,,,Anthropic,Claude,L,Non public,propriétaire,non communiqué,API uniquement,,,,Proprietary,,Gros modèles (>30B),20/06/2024,propriétaire,,,Google AI Vertex (https://www.notion.so/Google-AI-Vertex-491e280ab47e4bd287c2fb30a6b2029e?pvs=21),,3 - oui serait un plus
No,Yi,,,,,,,,non communiqué,,,,,,,,,,,,,,
No,InternLM,,,,,,,,,,,,,,,,,,,,,,
Yes,Mixtral-8x22b-Instruct-v0.1,,L’architecture SMoE (sparse mixture of experts) de ce modèle le rend plus rapide et optimise le rapport entre sa taille et son coût. (…) Seuls 39Mds de paramètres sont actifs sur 141Mds. La fenêtre contextuelle de 64000 tokens permet de rappeler des informations précises à partir de grands documents.,Mistral,Mistral,L,8x7B,poids communiqués,non communiqué,téléchargeable,64k,220Go,Libre,Apache-2.0,permissif,"Fine-tunes à visée francophone, Modèles de base",01/12/2023,open source,,,Scaleway Managed Inference (https://www.notion.so/Scaleway-Managed-Inference-a48411e0f9fe41899a2ebf3b00aef6b2?pvs=21),,1 - oui absolument