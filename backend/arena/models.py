"""
Data validation models using Pydantic.

Defines all data structures for:
- Conversation data (messages, participant info, metadata)
"""

from datetime import datetime
from functools import cached_property
from typing import Annotated, Literal, TypedDict, Union, get_args
from uuid import uuid4

from pydantic import BaseModel, ConfigDict, Field, PlainSerializer, computed_field

from backend.config import (
    BLIND_MODE_INPUT_CHAR_LEN_LIMIT,
    DEFAULT_SELECTION_MODE,
    CountryPortal,
    CustomModelsSelection,
    SelectionMode,
)
from backend.language_models.models import Endpoint, LanguageModel
from backend.language_models.utils import Consumption
from backend.utils.countries import CountryPortalAnno

MessageRole = Literal["user", "assistant", "system"]
BotPos = Literal["a", "b"]
BOT_POS: tuple[BotPos, ...] = get_args(BotPos)
BotChoice = BotPos | Literal["both_equal"]


class ErrorDetails(BaseModel):
    message: str
    pos: BotPos | None = None


class BaseMessage(BaseModel):
    """
    Base message class with strict typing.

    All messages have a role and content.
    """

    role: MessageRole
    content: str

    # use assignment validation since messages are updated gradually
    model_config = ConfigDict(validate_assignment=True)


class SystemMessage(BaseMessage):
    """System prompt message."""

    role: Literal["system"] = "system"


class UserMessage(BaseMessage):
    """User input message."""

    role: Literal["user"] = "user"


class AssistantMessage(BaseMessage):
    """
    Assistant response message with required metadata.

    Metadata must include generation_id, bot, and output_tokens.
    """

    class AssistantMessageMetadata(BaseModel):
        generation_id: str
        bot: BotPos
        output_tokens: int | None = None  # FIXME required?
        duration: float | None = None

    role: Literal["assistant"] = "assistant"
    reasoning: str | None = None
    metadata: AssistantMessageMetadata
    reaction: Union["ReactionData", None] = None


# Union type for any message
AnyMessage = SystemMessage | UserMessage | AssistantMessage


# New Conversation model (runtime, single model)
class Conversation(BaseModel):
    """
    Represents a conversation with a single AI model.

    Stores messages, model information, and metadata about the conversation.
    Each conversation has a unique ID and tracks the model's endpoint for API calls.
    """

    conv_id: str = Field(default_factory=lambda: str(uuid4()).replace("-", ""))
    model_name: str
    messages: list[AnyMessage]

    @property
    def has_system_msg(self) -> bool:
        return len(self.messages) > 0 and isinstance(self.messages[0], SystemMessage)

    @computed_field  # type: ignore[prop-decorator]
    @property
    def system_msg(self) -> str | None:
        return self.messages[0].content if self.has_system_msg else None

    @computed_field  # type: ignore[prop-decorator]
    @property
    def tokens(self) -> int:
        """
        Sum the total output tokens across all bot messages in a conversation.

        Returns:
            int: Total output tokens generated by the model
        """
        # Add up output_tokens from metadata of all assistant messages
        return sum(
            msg.metadata.output_tokens
            for msg in self.messages
            if isinstance(msg, AssistantMessage) and msg.metadata.output_tokens
        )

    @property
    def reactions(self) -> list["ReactionData"]:
        return [
            msg.reaction
            for msg in self.messages
            if isinstance(msg, AssistantMessage) and msg.reaction
        ]

    @cached_property
    def llm(self) -> LanguageModel:
        from backend.language_models.data import get_models

        return get_models().enabled[self.model_name]

    @cached_property
    def endpoint(self) -> Endpoint:
        return self.llm.endpoint


def create_conversation(llm_id: str, user_msg: UserMessage) -> Conversation:
    """Create a single conversation with system prompt if configured."""
    conv = Conversation(model_name=llm_id, messages=[])
    if conv.llm.system_prompt:
        conv.messages.append(SystemMessage(content=conv.llm.system_prompt))
    conv.messages.append(user_msg)

    return conv


# New Conversations model (paired conversations for arena)
class Conversations(BaseModel):
    """
    Paired conversations for arena comparison.

    Wraps two Conversation objects for type-safe handling.

    Args:
        session_hash: Unique session identifier
        ip: User'ip (WARNING: PII)
        visitor_id: Matomo/Piwik visitor ID from cookies.
        country_portal: Related country specific dataset ('da' for example)
        cohorts: Comma separated list str of cohorts, to be excluded from datasets if not empty str
        mode: Model selection mode (e.g., "random", "big-vs-small")
        custom_models_selection: Possible tuple of user selected llm ids
        category: Prompt category (e.g., "writing", "coding")
        conversation_a: First conversation state (dict with messages, model_name, etc.)
        conversation_b: Second conversation state (dict with messages, model_name, etc.)
    """

    created_at: Annotated[datetime, PlainSerializer(lambda v: v.isoformat())] = Field(
        default_factory=datetime.now
    )
    # Session
    session_hash: str
    ip: str  # WARNING: PII
    visitor_id: str | None
    country_portal: CountryPortal
    cohorts: str
    # Prompt data
    mode: SelectionMode
    custom_models_selection: CustomModelsSelection
    category: str | None = None
    # Data
    conversation_a: Conversation
    conversation_b: Conversation
    vote: Union["VoteBody", None] = None
    error: ErrorDetails | None = None
    # Status
    is_streaming: bool = False

    @computed_field  # type: ignore[prop-decorator]
    @property
    def conv_turns(self) -> int:
        """
        Count the number of conversation turns (user messages or exchanges).

        A turn is one user message and one llm response pair.
        """
        conv = self.conversation_a
        return (len(conv.messages) - (1 if conv.has_system_msg else 0)) // 2

    @computed_field  # type: ignore[prop-decorator]
    @property
    def conversation_pair_id(self) -> str:
        return f"{self.conversation_a.conv_id}-{self.conversation_b.conv_id}"

    @computed_field  # type: ignore[prop-decorator]
    @property
    def model_pair_name(self) -> list[str]:
        return sorted([self.conversation_a.model_name, self.conversation_b.model_name])

    @computed_field  # type: ignore[prop-decorator]
    @property
    def opening_msg(self) -> str:
        conv = self.conversation_a
        return conv.messages[1 if conv.has_system_msg else 0].content

    @computed_field  # type: ignore[prop-decorator]
    @property
    def is_unedited_prompt(self) -> bool:
        """
        Check if the user's opening message is from a suggested prompt.

        Used to identify whether the user wrote custom text or used a predefined prompt.

        Returns:
            bool: True if message matches a suggested prompt, False otherwise
        """
        # FIXME legacy?
        if not self.category:
            return False

        from backend.config import prompts_table

        # Check if the exact message exists in the category's prompt list
        return self.opening_msg in prompts_table[self.category]

    def store_to_session(self) -> None:
        """
        Store conversation pair to Redis.
        """
        from backend.arena.session import store_session_conversations

        data = self.model_dump(exclude_computed_fields=True)

        store_session_conversations(self.session_hash, data)

    @staticmethod
    def from_session(session_hash: str) -> "Conversations":
        """
        Build a Conversations from data stored in Redis for an active session.
        """
        from backend.arena.session import retrieve_session_conversations

        data = retrieve_session_conversations(session_hash)

        return Conversations(**data)


def create_conversations(
    llm_id_a: str,
    llm_id_b: str,
    args: "AddFirstTextBody",
    category: str | None,
    session_hash: str,
    ip: str,
    visitor_id: str | None,
) -> Conversations:
    """Create paired conversations for arena comparison."""
    user_msg = UserMessage(content=args.prompt_value)
    conv_a = create_conversation(llm_id_a, user_msg)
    conv_b = create_conversation(llm_id_b, user_msg)

    return Conversations(
        session_hash=session_hash,
        ip=ip,
        visitor_id=visitor_id,
        country_portal=args.country_portal,
        cohorts=args.cohorts,
        mode=args.mode,
        custom_models_selection=args.custom_models_selection,
        category=category,
        conversation_a=conv_a,
        conversation_b=conv_b,
    )


# Request/Response models for FastAPI endpoints
PromptField = Field(min_length=1, max_length=BLIND_MODE_INPUT_CHAR_LEN_LIMIT)


class AddFirstTextBody(BaseModel):
    """Request body for add_first_text endpoint."""

    prompt_value: str = PromptField
    mode: SelectionMode = DEFAULT_SELECTION_MODE
    custom_models_selection: CustomModelsSelection = None
    country_portal: CountryPortalAnno
    # We force cohorts not to be None to make sure cohorts detection has been called on frontend
    cohorts: str


class AddTextBody(BaseModel):
    """Request body for add_text endpoint."""

    message: str = PromptField


PositiveReaction = Literal["useful", "complete", "creative", "clear_formatting"]
POSITIVE_REACTIONS: tuple[PositiveReaction, ...] = get_args(PositiveReaction)
NegativeReaction = Literal["incorrect", "superficial", "instructions_not_followed"]
NEGATIVE_REACTIONS: tuple[NegativeReaction, ...] = get_args(NegativeReaction)
REACTIONS = POSITIVE_REACTIONS + NEGATIVE_REACTIONS


class ReactionBody(BaseModel):
    """Request body for updating message reactions."""

    bot: BotPos
    index: int
    value: str
    liked: bool | None
    prefs: list[PositiveReaction] | list[NegativeReaction]
    comment: str | None


class ReactionData(ReactionBody):
    liked: bool


class VoteBody(BaseModel):
    """Request body for submitting a vote after conversation."""

    chosen_llm: BotChoice
    prefs_a: list[PositiveReaction | NegativeReaction] = []
    prefs_b: list[PositiveReaction | NegativeReaction] = []
    comment_a: str
    comment_b: str


class RevealModelData(TypedDict):
    llm: LanguageModel
    conso: Consumption


class RevealData(TypedDict):
    b64: str
    chosen_llm: BotChoice
    a: RevealModelData
    b: RevealModelData
