"""
Environmental impact calculations and reveal screen data generation.

This module computes the ecological impact of LLM inference using the ecologits library,
converting technical metrics (energy, CO2) into user-friendly comparisons (LED lightbulbs, video streaming).

Functions:
- convert_range_to_value: Normalize impact ranges to single values
- calculate_lightbulb_consumption: Energy equivalent in LED light hours
- calculate_streaming_hours: CO2 equivalent in video streaming hours
- get_reveal_data: Main function generating reveal screen data
"""

import logging

from backend.arena.models import BotChoice, Conversations, RevealData
from backend.arena.utils import sum_tokens
from backend.language_models.utils import (
    calculate_lightbulb_consumption,
    calculate_streaming_hours,
    convert_range_to_value,
    get_llm_consumption,
    get_llm_impact,
)

logger = logging.getLogger("languia")


def get_chosen_llm(conversations: Conversations) -> BotChoice | None:
    if conversations.vote:
        return conversations.vote.chosen_llm

    reactions: dict[BotPos, dict[int, bool]] = {
        "a": {r.index: r.liked for r in conversations.conversation_a.reactions},
        "b": {r.index: r.liked for r in conversations.conversation_b.reactions},
    }

    indexes: set[int] = set([*reactions["a"].keys(), *reactions["b"].keys()])

    if not indexes:
        # No reactions
        return None

    scores = {"a": 0, "b": 0}
    for index in indexes:
        for pos in ("a", "b"):
            liked = reactions[pos].get(index, None)
            if liked is not None:
                scores[pos] += 1 if liked else -1

    if scores["a"] > scores["b"]:
        return "a"
    elif scores["b"] > scores["a"]:
        return "b"
    else:
        return "both_equal"


def get_reveal_data(
    conversations: Conversations, chosen_model: BotChoice
) -> RevealData:
    """
    Build reveal screen data with model comparison and environmental impact metrics.

    Calculates environmental impact (energy, CO2 emissions) and creates data for the
    reveal screen shown after voting. Includes model metadata, token counts, and
    user-friendly comparisons (LED lightbulb hours, video streaming equivalents).

    Args:
        conversations: Conversations model
        chosen_model: User's choice ("a", "b", or "both_equal")

    Returns:
        dict: Reveal data containing:
            - b64: Base64-encoded JSON summary (compact storage/transmission)
            - model_a/model_b: Full model metadata dicts
            - chosen_model: User's model preference
            - model_a/b_kwh: Energy consumption in kilowatt-hours
            - model_a/b_co2: CO2 emissions in kilograms
            - model_a/b_tokens: Total output tokens generated by each model
            - streaming_a/b + streaming_a/b_unit: Equivalent video streaming hours
            - lightbulb_a/b + lightbulb_a/b_unit: Equivalent 5W LED light hours

    Process:
        1. Load model definitions from config
        2. Calculate total output tokens for each conversation
        3. Compute environmental impact using ecologits library
        4. Convert impact values to user-friendly comparisons (lightbulb, streaming)
        5. Encode summary to base64 for efficient storage
        6. Return comprehensive metrics for reveal screen display
    """
    import base64
    import json

    from backend.language_models.data import get_models

    models = get_models().all
    conv_a = conversations.conversation_a
    conv_b = conversations.conversation_b

    # Load complete model metadata from config
    model_a = models[conv_a.model_name]
    model_b = models[conv_b.model_name]

    # Calculate total tokens generated by each model
    model_a_tokens = sum_tokens(conv_a.messages)
    logger.debug(f"output_tokens (model a): {model_a_tokens}")

    model_b_tokens = sum_tokens(conv_b.messages)
    logger.debug(f"output_tokens (model b): {model_b_tokens}")

    # Default values in case impact calculation fails
    model_a_kwh = 0.0
    model_b_kwh = 0.0
    model_a_co2 = 0.0
    model_b_co2 = 0.0
    lightbulb_a, lightbulb_a_unit = 0, "h"
    lightbulb_b, lightbulb_b_unit = 0, "h"
    streaming_a, streaming_a_unit = 0, "h"
    streaming_b, streaming_b_unit = 0, "h"

    # Calculate environmental impact using ecologits library
    # Uses model parameters, active parameters (for MoE), and token count
    try:
        model_a_impact = get_llm_impact(model_a, model_a_tokens, None)
        if model_a_impact:
            # Extract and normalize energy and CO2 values (handles value ranges)
            model_a_kwh = convert_range_to_value(model_a_impact.energy.value)
            model_a_co2 = convert_range_to_value(model_a_impact.gwp.value)
            # Convert energy to LED lightbulb comparison (5W LED light)
            lightbulb_a, lightbulb_a_unit = calculate_lightbulb_consumption(model_a_kwh)
            # Convert CO2 to video streaming comparison
            streaming_a, streaming_a_unit = calculate_streaming_hours(model_a_co2)
    except Exception as e:
        logger.warning(
            f"Could not calculate impact for model A ({conv_a.model_name}): {e}"
        )

    try:
        model_b_impact = get_llm_impact(model_b, model_b_tokens, None)
        if model_b_impact:
            model_b_kwh = convert_range_to_value(model_b_impact.energy.value)
            model_b_co2 = convert_range_to_value(model_b_impact.gwp.value)
            lightbulb_b, lightbulb_b_unit = calculate_lightbulb_consumption(model_b_kwh)
            streaming_b, streaming_b_unit = calculate_streaming_hours(model_b_co2)
    except Exception as e:
        logger.warning(
            f"Could not calculate impact for model B ({conv_b.model_name}): {e}"
        )

    # Create compact summary for encoding
    data = {
        "a": conv_a.model_name,  # Model A identifier
        "b": conv_b.model_name,  # Model B identifier
        "ta": model_a_tokens,  # Model A token count
        "tb": model_b_tokens,  # Model B token count
    }

    # Add user's choice to summary (for verification/tracking)
    if chosen_model == "model-a":
        data["c"] = "a"
    elif chosen_model == "model-b":
        data["c"] = "b"

    # Encode summary as base64 for safe storage/transmission
    jsonstring = json.dumps(data).encode("ascii")
    b64 = base64.b64encode(jsonstring).decode("ascii")

    # Get model dump (handle both LanguageModel objects and dicts)
    model_a_dict = (
        model_a.model_dump()
        if hasattr(model_a, "model_dump")
        else (model_a if model_a else {})
    )
    model_b_dict = (
        model_b.model_dump()
        if hasattr(model_b, "model_dump")
        else (model_b if model_b else {})
    )

    # Return comprehensive reveal data for frontend display
    return dict(
        b64=b64,  # Encoded summary
        model_a=model_a_dict,  # Full model A metadata
        model_b=model_b_dict,  # Full model B metadata
        chosen_model=chosen_model,  # User's preference
        # Energy metrics
        model_a_kwh=model_a_kwh,
        model_b_kwh=model_b_kwh,
        # Environmental metrics (CO2)
        model_a_co2=model_a_co2,
        model_b_co2=model_b_co2,
        # Token usage
        model_a_tokens=model_a_tokens,
        model_b_tokens=model_b_tokens,
        # Video streaming equivalent (user-friendly CO2 comparison)
        streaming_a=streaming_a,
        streaming_a_unit=streaming_a_unit,
        streaming_b=streaming_b,
        streaming_b_unit=streaming_b_unit,
        # LED lightbulb equivalent (user-friendly energy comparison)
        lightbulb_a=lightbulb_a,
        lightbulb_a_unit=lightbulb_a_unit,
        lightbulb_b=lightbulb_b,
        lightbulb_b_unit=lightbulb_b_unit,
    )
